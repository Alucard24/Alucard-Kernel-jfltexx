commit 9b2dac37484a2f2f9c3e885520a8708119c85e03
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 29 23:50:49 2014 +0100

    Importing from my cm-branch.... 1a

commit 60d7577a9035d33769cd2169d4fc2fc219502ea2
Author: Hareesh Gundu <hareeshg@codeaurora.org>
Date:   Fri Jan 3 11:56:44 2014 +0530

    msm: kgsl: Fix mem leak when page allocation fails
    
    Page allocation may fail during OOM condition. Update
    sglen and size to correct value (allocated) so that
    they get freed in kgsl_sharedmem_free().
    
    Change-Id: I1aa0ca77bb767523dfcf0f6cc3a21eac2b6b80e4
    Signed-off-by: Hareesh Gundu <hareeshg@codeaurora.org>

commit 3edc03ef31fea29696bf0268d7a963b3cb7a60a2
Author: Paul Reioux <reioux@gmail.com>
Date:   Wed Mar 26 07:10:08 2014 -0700

    net/bluetooth/hci_conn.c: GCC 4.8.x fix up again
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 0601f9b6af314baf6898d53d184ea9c96e974d7f
Author: Jason Hrycay <jason.hrycay@motorola.com>
Date:   Tue Mar 26 20:04:25 2013 -0500

    ARM: module: add text.unlikely section to unwind
    
    Add handling for the .text.unlikely unwind sections for cases where
    real used code get incorrectly placed by compiler. This solves cases
    in which a frequently used function triggers the collectoin of the
    current stack trace when using CONFIG_ARM_UNWIND. An example would
    be when using CONFIG_SLUB_DEBUG. The constant error printing can
    flood the kernel log buffers and cause sluggishness.
    
    Change-Id: I8ca99de24695a46e7fe461b5687c824b900908c8
    Signed-off-by: Jason Hrycay <jason.hrycay@motorola.com>
    Reviewed-on: http://gerrit.pcs.mot.com/527901
    SLT-Approved: Gerrit Code Review <gerrit-blurdev@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Christopher Fries <qcf001@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>
    Reviewed-by: Check Patch <CHEKPACH@motorola.com>
    Reviewed-by: Klocwork kwcheck <klocwork-kwcheck@sourceforge.mot.com>

commit ac888621c2abe8889e5e28176c9710d2a5dfb99f
Author: Sana Venkat Raju <c_vsana@codeaurora.org>
Date:   Sun Dec 15 11:51:17 2013 +0530

    msm: buspm: Correct size type in buspm_xfer_req
    
    Variable size inside buspm_xfer_req
    can be negative and thus would be a
    large positive number resulting in arbitary
    Kernel read. This patch corrects this value.
    
    CRs-Fixed: 563529
    Change-Id: I877db6960530d6c7d1486da9cfdc15a43e57e152
    Signed-off-by: Sana Venkat Raju <c_vsana@codeaurora.org>

commit f1f131bb33cbca29383ab88d311dd095a7a8f3ed
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Mar 23 01:28:53 2014 -0700

    pm-8x60.c: complaining of power collapse within 1 ms is pointless
    
    since latency for cores is > 10 ms
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mach-msm/pm-8x60.c

commit 16139bad5e17f35f793d36227fc9ea588bcd4bea
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Mar 20 23:13:07 2014 -0700

    drivers/usb/gadget/f_qdss.c: gcc 4.8 fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 0f4a63d7b0f085c2ad23610d6524b78fb4d3bef5
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Mar 20 23:12:40 2014 -0700

    net/bluetooth/hci_conn.c: gcc 4.8.x fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1d882a0e573aecceaf5d1aaf874a0204922e2902
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 29 01:33:02 2014 +0100

    fixed dcvs freq table size

commit 56aacdada2717e31b71cebd12a08e859d5d5845b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 28 22:15:58 2014 +0100

    delayed call for alucard_hotplug!
    
    Conflicts:
    	arch/arm/mach-msm/Makefile

commit a78ff60baafd2a9b453928f6e5f18593eb638d6b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 27 23:25:06 2014 +0100

    Alucard_hotplug: Adjustes some startup values.

commit bf43624c2e9839e94cc139ce2ffba3a55eda34b6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 27 22:37:33 2014 +0100

    Merged intelli_plug with the one existing in Yuri branch. Thanks to him:)

commit 9180b55dd2a9b8d4e42c4c5841cea7f0c0aa8fab
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 27 22:30:18 2014 +0100

    Alucard_hotplug: security check during hotplugging!

commit 8dc84340bb49aa92a4d9ad3f4815f17bf6d0c44d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 27 20:13:46 2014 +0100

    Trying to fix RR on GE.

commit c6a13175a00d8a92780c777e6f8a54e9e80bfd7a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 27 00:56:59 2014 +0100

    Revert "Inserted L15 step frequency into apuclock-8064.c."
    
    This reverts commit b5806ce150a67f2a0c9bfa78ff05619317d1bfae.

commit b67944c096b73c8021e17c8bd508215859fd0f59
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 27 00:08:53 2014 +0100

    Optimized alucard_hotplug and intelli_plug.

commit a2bd85a2908ee3e9aceb2b53e1a3d92f54d403ce
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 22:34:35 2014 +0100

    Intelli_plug: small adjustments!

commit 0cd9fa7149566f1fb8f196f1ecd0767a74ef2d18
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 22:32:33 2014 +0100

    alucard_hotplug: little adjustments

commit d35551dc6917cd5317984bec8fa702c37746c81c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 19:24:09 2014 +0100

    alucard_hotplug: some adjustments

commit ba6ccd043302db36f612274f0168f731284f7271
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 18:05:07 2014 +0100

    Fixed a mess inside alucard_htoplug

commit 23354decfcd3def98f594ccd58d201a5b469ccd5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 17:20:02 2014 +0100

    Fixed alucard_hotplug.

commit 47a45fbd9a368e35bbb7320b3d5263fcc6258d31
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 17:19:30 2014 +0100

    use system_wq for intelli_plug.

commit 633a32e408278f4afab5e135df3ce9a29a10c361
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 16:44:19 2014 +0100

    Fixed intelliplug cancelling workqueue!

commit fa4413bce38e3a9ef03ff774e85be4b3f808c4db
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 16:39:14 2014 +0100

    alucard_hotplug: Code optimizations!

commit 8ca20118887c702df098878a43732bd3cd099ae6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 16:20:19 2014 +0100

    Finally fixed intelli_plug conflicts! Moved into sys kernel folder.

commit b54cf0b533863c7c4acba70252af5ac49948ad94
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 14:12:50 2014 +0100

    Revert "Merged intelliplug governor from Yuri branch. Thanks to him :)"
    
    This reverts commit 4be4acca60cf25d6ec429314482317b2f7af75ee.

commit 9fd00f176158d652c854de6ba153becdcf9bf310
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 14:11:50 2014 +0100

    Revert "Intelliplug: switch to normal priority workqueue."
    
    This reverts commit d6343cbbc8546f5463b4ca228e270e0ed2dc8c69.

commit d6343cbbc8546f5463b4ca228e270e0ed2dc8c69
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 13:11:05 2014 +0100

    Intelliplug: switch to normal priority workqueue.

commit 70663401d0cdd9928ec34bbc8a6018c47bdd0a81
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 08:53:02 2014 +0100

    Alucard hotplug: Fixed cpus hotpluggig after waking up phase

commit 4be4acca60cf25d6ec429314482317b2f7af75ee
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 02:51:22 2014 +0100

    Merged intelliplug governor from Yuri branch. Thanks to him :)

commit b8a505def900d532cac02ca0fffe168f555e6c95
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 02:32:01 2014 +0100

    Removed many white spaces!

commit 1827ba03afc17f07c2412cc89e4f9b5cb1e9f62f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 02:20:19 2014 +0100

    Optimized alucard hotplug.

commit 9e3732f43f859b113cf64c66cbce81ccff11b9c4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 26 00:14:50 2014 +0100

    use system_wq for my governors and hotplug!

commit b5806ce150a67f2a0c9bfa78ff05619317d1bfae
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 21:54:56 2014 +0100

    Inserted L15 step frequency into apuclock-8064.c.

commit 6dccbb692712f11a5f34e014e8f9c3154a3e2e56
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 20:58:21 2014 +0100

    TW KK Test Kernel config.

commit b13856f60fb6a70268988c56c96c3ee3dafefe01
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 19:11:42 2014 +0100

    some little changes in alucard_hotplug!

commit a848342c121f2789998ede521a9560d93c22f901
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 18:29:17 2014 +0100

    Implemente need_load_eval for my hotplug and fixed its code into my governors!

commit 8d1ddd72c5dcb6831bf688d57f6fe29cd7b42b26
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 17:35:49 2014 +0100

    Implemented need_load_eval function for my governors. Fixed global cpufreq previous commit!

commit 7abeffa73b74a8633aa18c94a07b5c2f18e037ad
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 16:34:17 2014 +0100

    Imported and adjusted cpufreq get_cpu_idle_time from 3.14

commit abb67bdfbea6f53b4c91e3402f95f5c9baebef7e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 16:16:12 2014 +0100

    Adjusted cpu idle time for all my governors and my hotplug!

commit 8547848470c7275c75bb01d093a15bac14f72029
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 14:34:34 2014 +0100

    Linux 3.4.84

commit 66ae08a5beb754e64fa6c1aa1ec58cc54632085e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 14:16:39 2014 +0100

    Include cypress_touch_key for dvfs disabling!

commit 0ba2ce04d444e20b0a6bbef5fb1b2e9e62a556b6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 24 21:47:58 2014 +0100

    power efficient wq for alucard_hotplug!
    
    Conflicts:
    	arch/arm/mach-msm/alucard_hotplug.c

commit a5f37c711a51113c57f58a181827bd2d5cd1597b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 12:58:40 2014 +0100

    Code style for alucard_hotplug!

commit 57df66cecd41f2f8e136caefc24034b9f29dba57
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 00:36:26 2014 +0100

    reverted system power efficient wq for DVFS!

commit fdd86e8068712c9ce23c6a144266aafff5024974
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 00:31:01 2014 +0100

    Enabled intelliplug in ge kernel config

commit a9d7f718386410c5c619ea8f306927b3f9a1fd0c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 25 00:25:16 2014 +0100

    Fixed intelliplug on my kernel!

commit f112c636c28950116be7b7102be5a880eae7227d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 24 23:29:12 2014 +0100

    Enabled intelliplug in kernel config!

commit 5e30f019fdad5295f0063d6e361f04fc7cfe91a4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 24 23:23:22 2014 +0100

    Fixed intelli_plug. Thanks to Yuri, Faux.

commit c4fefbf74de0cd53102367a79cddb87822ced342
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 23 19:21:24 2014 +0100

    Use system_power_efficient_wq workqueue for Alucard, Nightmare and Darkness governors!

commit 2703e576f3ef3dc3846908035f0f3d51efaa1171
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 23 13:46:03 2014 +0100

    New kernel released for GE KK Roms!

commit 2a378d20f1d4e0dd7207a2639948c5f6ed781faf
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 23 13:40:20 2014 +0100

    New kernel released for tw kk rom!

commit e9a4da7ddcfec4730b78c6fadbff9e851f94e0fc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 23 00:27:55 2014 +0100

    Replace workqueue with the one existing in my cm branch!

commit 75eb34c5b294e5e0a5fbc3ce57f9eff38636847f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 23 00:25:36 2014 +0100

    Modified alucard hotplug. Dedicated a separate workstruct with hi priority to it

commit c316dc01b75c7508611b488eb6ebb852bfe52a46
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 22 23:15:33 2014 +0100

    Merged some parts from official Samsung kernel!

commit ef7a5c243c6d5a0aa12de44f9ce0935f483f21b0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 22 19:16:07 2014 +0100

    Disabled intelli_plug in kernel config!

commit 90708180e151ad12b4b9c5cddc171ee636ca39e0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 22 03:01:20 2014 +0100

    Disabled intelli_plug hotplug for coexistence problems!

commit 3883b9c0ddc688166106d0e14cfb108d264cc887
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 22 01:41:13 2014 +0100

    Modified alucard_hotplug. No delayed work pending when it is disabled!

commit 80cec2e157bad98ac4779523268c2112d9d6e6a5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 21 22:08:35 2014 +0100

    Removed useless header.

commit 1d368b942e74234cd8383821fb6f7505c47c1bdb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 21 17:53:39 2014 +0100

    Disabled KGSL power ctrl for TW-4.4.

commit 85615f90b8e23bfce18049e43b414e5b4c4ad68a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 21 17:50:42 2014 +0100

    Disabled KGSL power ctrl!

commit fd6616f2a93a39ef0f31463879678f6b5bb33233
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 21 17:17:52 2014 +0100

    Removed dvfs status and implemented dvfs boost mode!

commit 0343430ac4b76fa272913929ff83c00004979657
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 21 15:35:15 2014 +0100

    RR, while phone(old bootloader) is plugged to AC Charger, seems to be fixed

commit db47180fcebca66a7a82cdc8a2128abe3030c731
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 21 01:17:26 2014 +0100

    Code style!

commit 8ac949b41db7f4ccb3c56ec6eae0a06f7db7a004
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 21 00:31:31 2014 +0100

    New kernel for GE 4.4 Released.

commit 0a698799acc38617c203e3f74866d31f0be8618b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 20 22:48:31 2014 +0100

    New kernel TW 4.4 Released.

commit 4ac47a40d207635c9f1128868c78065f153b45d6
Author: Ravi Aravamudhan <aravamud@codeaurora.org>
Date:   Tue Jan 21 14:31:08 2014 -0800

    diag: Add new polling command
    
    Diag driver should now support a new polling command for the tools
    to recognize the target.
    
    Change-Id: Ia7966cc74c5c74693cebfd64b178831e5ca6f2dd
    Signed-off-by: Ravi Aravamudhan <aravamud@codeaurora.org>

commit e0cb88352dfe15fba1169663f8a8f560ae174d62
Author: Katish Paran <kparan@codeaurora.org>
Date:   Tue Jan 21 08:19:25 2014 +0000

    diag: dci: Preventing unitialized stack memcpy operation
    
    At certain point in diag driver there can be memcpy operation
    for the unintialized stack. Due to uninitilization of memory
    user can obatin valuable information on accessing this stack
    in userspace. This patch fixes that issue.
    
    Signed-off-by: Katish Paran <kparan@codeaurora.org>

commit 42860aba122975f03dc7c26bcf9b94e773dd3272
Author: Katish Paran <kparan@codeaurora.org>
Date:   Mon Jan 13 23:15:59 2014 +0530

    diag: Reduce error message frequency
    
    Some diag error messages are printed with high frequency. This
    cause flooding of kernel logs. Rate limiting these messages will
    help reading kernel logs.
    
    Signed-off-by: Katish Paran <kparan@codeaurora.org>

commit 7eba9f1b742729f9c90e43a27c39ec2473a648aa
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri May 13 15:03:24 2011 +0100

    mm: vmscan: If kswapd has been running too long, allow it to sleep
    
    Under constant allocation pressure, kswapd can be in the situation where
    sleeping_prematurely() will always return true even if kswapd has been
    running a long time. Check if kswapd needs to be scheduled.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>

commit 541c9d506251f434d811190d59f75c52b2e9a818
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 20 13:23:13 2014 +0100

    Merging usb drivers and mmc!

commit 23cdf47f3be075426b6c999fec2c0fbd05ee68ef
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Mar 19 12:40:19 2014 +0200

    random: entropy tweaks are all the rage nowadays

commit 1c2c3e8763f70721288a0b33ef0dc706f1e60466
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 20 01:52:00 2014 +0100

    updated msm arm hotplug!

commit d1a47b7e01e278b535fcf1f7125fec450a841891
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 20 00:33:11 2014 +0100

    Modified config for tw-4.4

commit 1bae1a71bc1a701ef3dccb4cc9d665f6b8719545
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 19 23:28:01 2014 +0100

    Implemented CPU OVERCLOCK!

commit b1f860773161c54b58ff86ab064715e0530d0ba8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 19 17:50:06 2014 +0100

    Improved cpu perfermoance a bit!

commit 3052e138320922462564865e3bbf7bcb3a139072
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 19 16:47:37 2014 +0100

    apuclock8064.c, code style!

commit 8e2576c0f92d21c33c300cc8c373b45a1b959c83
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 18 23:43:22 2014 +0100

    Modified kernel config! Fixed installing apps into extsdcard!

commit 47db607e1794e4680aa61c25a3398a56ac85a6d4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 18 01:52:37 2014 +0100

    New test kernel for GE roms!

commit 3c78135290a5c76eea26617f1834a6762ace67ed
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 18 00:30:28 2014 +0100

    Fixed permissions!

commit 297c54489de206e4d19407d63f93838b689d1af7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 17 23:05:50 2014 +0100

    Updated ge configuration!

commit a34824c7dce63a5162eaa36c516467a6669701c9
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Tue Aug 13 19:27:00 2013 -0700

    cpufreq: Save user policy min/max instead of policy min/max during hotplug
    
    When a CPU is hotplugged off, various fields of the policy are saved so
    that they can be restored when the CPU is hotplugged back in.
    
    The existing code saves the policy min/max field during hotplug remove and
    restores it into user policy min/max during hotplug add. This results in
    the loss of the user policy min/max values across a hotplug remove/add.
    
    Fix it by saving the user policy min/max instead of policy min/max during
    hotplug remove. During a hotplug add, after user policy min/max is
    restored, the policy min/max is already recomputed. So, there's no risk of
    going beyound any limits imposed by the CPUFREQ_ADJUST/INCOMPATIBLE
    notifiers.
    
    Change-Id: Ib8e09fa324c1f80f095c5754b5dcf2685e8e4a66
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 2d38698f94d8015ee94628a03e6f6c72f814378b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 15 18:57:20 2014 +0100

    schedule delayed work to queue delayed work for Alucard_hotplug!
    
    Conflicts:
    	arch/arm/mach-msm/alucard_hotplug.c

commit a5b322b7eda15c53fdf64bb8a09eed618c8fe397
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 16 19:15:16 2014 +0100

    Restore MSM AVS HW. restored schedule work on for alucad_hotplug and fixed some step freq.!

commit 03baf05931f0da1fb719a48f197a0b7699ca8762
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 16 17:57:30 2014 +0100

    Revert "cache_erp: Use seq_printf and modernize procfs code"
    
    This reverts commit ba4fe77fb39cbe882f5116b3031290ed6f2434a8.

commit f1ae19288caadedf9550ce4d5f7aeb4b7f372b2c
Author: dorimanx <yuri@bynet.co.il>
Date:   Sat Mar 15 21:06:01 2014 +0200

    Full sync for arch/arm/kernel/sched_clock.c with KK kernel Source.

commit c8cdf16d51ee89e6e228a963dada3e44dc55ce51
Author: Felipe Balbi 2 <balbi@ti.com>
Date:   Tue Oct 23 19:00:03 2012 +0100

    ARM: 7565/1: sched: stop sched_clock() during suspend
    
    The scheduler imposes a requirement to sched_clock()
    which is to stop the clock during suspend, if we don't
    do that any RT thread will be rescheduled in the future
    which might cause any sort of problems.
    
    This became an issue on OMAP when we converted omap-i2c.c
    to use threaded IRQs, it turned out that depending on how
    much time we spent on suspend, the I2C IRQ thread would
    end up being rescheduled so far in the future that I2C
    transfers would timeout and, because omap_hsmmc depends
    on an I2C-connected device to detect if an MMC card is
    inserted in the slot, our rootfs would just vanish.
    
    arch/arm/kernel/sched_clock.c already had an optional
    implementation (sched_clock_needs_suspend()) which would
    handle scheduler's requirement properly, what this patch
    does is simply to make that implementation non-optional.
    
    Note that this has the side-effect that printk timings
    won't reflect the actual time spent on suspend so other
    methods to measure that will have to be used.
    
    This has been tested with beagleboard XM (OMAP3630) and
    pandaboard rev A3 (OMAP4430). Suspend to RAM is now working
    after this patch.
    
    Thanks to Kevin Hilman for helping out with debugging.
    
    Change-Id: Ie2f9e3b22eb3d1f3806cf8c598f22e2fa1b8651f
    Acked-by: Kevin Hilman <khilman@ti.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 6a4dae5e138a32b45ca5218cc2b81802f9d378c3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit e0ebba340a739039ce6f82868b97abaebf79ef9d
Author: Colin Cross <ccross@android.com>
Date:   Tue Aug 7 19:05:10 2012 +0100

    ARM: 7486/1: sched_clock: update epoch_cyc on resume
    
    Many clocks that are used to provide sched_clock will reset during
    suspend.  If read_sched_clock returns 0 after suspend, sched_clock will
    appear to jump forward.  This patch resets cd.epoch_cyc to the current
    value of read_sched_clock during resume, which causes sched_clock() just
    after suspend to return the same value as sched_clock() just before
    suspend.
    
    In addition, during the window where epoch_ns has been updated before
    suspend, but epoch_cyc has not been updated after suspend, it is unknown
    whether the clock has reset or not, and sched_clock() could return a
    bogus value.  Add a suspended flag, and return the pre-suspend epoch_ns
    value during this period.
    
    The new behavior is triggered by calling setup_sched_clock_needs_suspend
    instead of setup_sched_clock.
    
    Change-Id: I7441ef74dc6802c00eea61f3b8c0a25ac00a724d
    Signed-off-by: Colin Cross <ccross@android.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 237ec6f2e51d2fc2ff37c7c5f1ccc9264d09c85b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit ba4fe77fb39cbe882f5116b3031290ed6f2434a8
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Jun 25 20:31:14 2013 -0700

    cache_erp: Use seq_printf and modernize procfs code
    
    In future kernels create_proc_entry() is going to be removed.
    Let's modernize this code and use regular file operations and
    seq_printf() to avoid experiencing compilation problems in future
    kernels.
    
    Change-Id: Ifc11287a8aefa8b2af3fb7d68fd96c1db8f2f098
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/cache_erp.c

commit 8766a96bf08b1360d873964f8318466ee6d827ce
Author: Neil Leeder <nleeder@codeaurora.org>
Date:   Tue Jun 4 10:45:17 2013 -0400

    Perf: Add cortex A7 perf support
    
    Support perf events for cortex-A7 PMU
    
    Change-Id: Ie44b077d56f9b538e99ae675efaf1829c1b752b7
    Signed-off-by: Neil Leeder <nleeder@codeaurora.org>

commit 0afbeb9c26542e222e54c93081246c2bc6add23a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 16 14:41:31 2014 +0100

    Disabled MSM AVS HW and included Exfat into kernel!

commit 7f7134f639b2ad275e8f0d57ed7c5be44a4995f5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 15 18:57:20 2014 +0100

    schedule delayed work to queue delayed work for Alucard_hotplug!

commit a08cc14132ea419592306797b59b9a3b24287971
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 15 18:56:33 2014 +0100

    Corrections part 3

commit aca6382792db2c9af18da3b1022eaee5b9112136
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 23:51:06 2014 +0100

    Corrections part 2!

commit 6e567f6e293fa400b4a199dbb668a8e143dd1d81
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 23:23:48 2014 +0100

    Corrections!

commit 83b8e3075bdc510f084a879ce641a9182fa0f1b3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 14:46:30 2014 +0100

    New parameter for DVFS touch interface. Enable/Disable frequency changing for gpu!

commit 869a0a9d5b19242e71683c858043c923d7070f30
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 13:29:02 2014 +0100

    Fixed dvfs touch if attributes creation!

commit 0e42edfc273a847e61e259afc803c9ed3d8a3e3e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 13:28:33 2014 +0100

    Modified kernel configuration!

commit c8a9257e7356ed968e5afc467277e89af96f7c3e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 12:45:09 2014 +0100

    CodeStyle, removed info and some adjustments!

commit 8647a9944d76a020534dc1f9e1bf19a505c97df3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 12:22:39 2014 +0100

    Modified dvfs touch if parameters name!

commit 7eddba5d0949c9410b7d80dbf559646ed1aa7fa2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 01:07:26 2014 +0100

    Revert "Removed selinux!"
    
    This reverts commit 4a4edb748f9beab2b20b72d81fd828a3c9e10a55.

commit 9ee99247d2fd07e261ace4f3c3235705d8bfc44a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 00:49:11 2014 +0100

    Revert "Patched msm cpufreq as my cm 11.0 branch!"
    
    This reverts commit bb21d5aaac1ef13c2afe4f3171e32aaeaf392890.

commit 5b33180e452c687f595fc775d23d53a57d46dd42
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 14 00:47:18 2014 +0100

    Optimized dvfs touch booster interface!

commit 271c5d6646c34136de3d3db46ff5f51956c0681a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 13 23:29:23 2014 +0100

    fixed dvfs touch booster status!

commit 9a1125d7f30df6e08d685c3a02c13746ac350d22
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 13 23:25:15 2014 +0100

    DVFS touch booster status!

commit 4a4edb748f9beab2b20b72d81fd828a3c9e10a55
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Mar 13 17:06:45 2014 +0100

    Removed selinux!

commit bb21d5aaac1ef13c2afe4f3171e32aaeaf392890
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 22:08:04 2014 +0100

    Patched msm cpufreq as my cm 11.0 branch!

commit 6f8d5fb53857d160b803018ce2070308f9fc3973
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Wed Feb 19 12:45:15 2014 -0700

    Remove a butt load of debug
    
    Conflicts:
    	drivers/slimbus/slimbus.c

commit 077becf33df71bcaeea3731d00608f7ee4ac0fdc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 12 21:47:47 2014 +0100

    Code Style. Removed debug and spaces!

commit 306c6ed305229495f4130fa4adef13a396e9d3f7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 12 12:29:46 2014 +0100

    Linux 3.4.83
    
    Conflicts:
    	arch/arm/mm/proc-v7.S
    	mm/memory_hotplug.c

commit 969684b227ca9add13bfd70d460eb795af74b973
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 12 01:54:14 2014 +0100

    Revert "ARM: 7587/1: implement optimized percpu variable access"
    
    This reverts commit 6f47626b99b55270c0ec88c8c43599abdec079a7.

commit c002d71d1bfa26e02edfa513c2b80803aac90cc6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 23:46:41 2014 +0100

    Remove many pr_info debug and a little change!

commit b75aa0ab8f80d102f6af55bf9c8a5d73911f398e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 23:13:40 2014 +0100

    Updated kernel config. ARCH_RANDOM and some USB debug for TW KK ROM!

commit 6439ed27b12c2130e777c0abf664abe41b511244
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 23:11:35 2014 +0100

    Updated building scripts for tw, ge kk

commit d30c177bda5d1a33edd20d9b72919ba242a19eba
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 22:43:37 2014 +0100

    Using deferrable work instead of delayed work on alucard_hotplug!

commit 4cceaaaca488834b4c5edefeb70e7fbab9fa84ff
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 22:42:57 2014 +0100

    Revert "Patched msm cpufreq as my cm 11.0 branch!"
    
    This reverts commit b883800c346c92a1dfec481eaa3eed2e0d799427.

commit 096b190d1952ad5c394de41ced8ecc044c9c5fc6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 22:42:31 2014 +0100

    ENABLED ARCH_TIMER!

commit b883800c346c92a1dfec481eaa3eed2e0d799427
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 22:08:04 2014 +0100

    Patched msm cpufreq as my cm 11.0 branch!

commit 6f47626b99b55270c0ec88c8c43599abdec079a7
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Thu Nov 29 20:39:54 2012 +0100

    ARM: 7587/1: implement optimized percpu variable access
    
    Use the previously unused TPIDRPRW register to store percpu offsets.
    TPIDRPRW is only accessible in PL1, so it can only be used in the kernel.
    
    This replaces 2 loads with a mrc instruction for each percpu variable
    access. With hackbench, the performance improvement is 1.4% on Cortex-A9
    (highbank). Taking an average of 30 runs of "hackbench -l 1000" yields:
    
    Before: 6.2191
    After: 6.1348
    
    Will Deacon reported similar delta on v6 with 11MPCore.
    
    The asm "memory clobber" are needed here to ensure the percpu offset
    gets reloaded. Testing by Will found that this would not happen in
    __schedule() which is a bit of a special case as preemption is disabled
    but the execution can move cores.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 3f0093e1415b6937f559bbcd0ab26115970ad87e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 13:22:19 2014 +0100

    Replace all ramaining INIT_DELAYED_WORK_DEFERRABLE with new define -->

commit fcceb43d12c2b14d5453abf831dcc383982d892a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 13:11:59 2014 +0100

    Removed intellidemand governor. We don't use it!

commit 325d73180f5559ed69efca1138dd7952a0488a67
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Mar 10 19:10:01 2014 +0200

    Replace all ramaining INIT_DELAYED_WORK_DEFERRABLE with new define -->
    INIT_DEFERRABLE_WORK
    
    Conflicts:
    	drivers/cpufreq/cpufreq_intellidemand.c
    	drivers/devfreq/devfreq.c
    	drivers/power/battery_current_limit.c

commit 87a0f9a6631dbfc993ba24e73aaf7570f5a690be
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:23 2012 -0700

    workqueue: make deferrable delayed_work initializer names consistent
    
    Initalizers for deferrable delayed_work are confused.
    
    * __DEFERRED_WORK_INITIALIZER()
    * DECLARE_DEFERRED_WORK()
    * INIT_DELAYED_WORK_DEFERRABLE()
    
    Rename them to
    
    * __DEFERRABLE_WORK_INITIALIZER()
    * DECLARE_DEFERRABLE_WORK()
    * INIT_DEFERRABLE_WORK()
    
    This patch doesn't cause any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    
    Conflicts:
    	drivers/cpufreq/cpufreq_conservative.c
    	drivers/cpufreq/cpufreq_ondemand.c

commit 70a0e9dee0725cfdfa8ada4c40df0747f64b6af6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:23 2012 -0700

    workqueue: cosmetic whitespace updates for macro definitions
    
    Consistently use the last tab position for '\' line continuation in
    complex macro definitions.  This is to help the following patches.
    
    This patch is cosmetic.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 78090b2639c6204ff862a2409df8f6e533577e6a
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:45 2012 -0700

    workqueue: set delayed_work->timer function on initialization
    
    delayed_work->timer.function is currently initialized during
    queue_delayed_work_on().  Export delayed_work_timer_fn() and set
    delayed_work timer function during delayed_work initialization
    together with other fields.
    
    This ensures the timer function is always valid on an initialized
    delayed_work.  This is to help mod_delayed_work() implementation.
    
    To detect delayed_work users which diddle with the internal timer,
    trigger WARN if timer function doesn't match on queue.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 58638c8294d6cfbb8ae646895958e00c02327a30
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 11 01:35:46 2014 +0100

    Added compiling scripts and kernel config for ge 4.4!

commit b63ac0c8f152b9e1896afbb3f5797e8c7ca9e3e0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 10 23:03:58 2014 +0100

    Fixed ondemand governor!

commit 82e4d480305d0250b80045c76d72eed58a9f25f6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 10 22:56:46 2014 +0100

    Automatically changing frequency from 384000 to 378000 kHz. Stupid but it works!

commit 8400eeb8643d81c3a0ad5fa4b666388f7b71cda5
Author: savoca <adeddo27@gmail.com>
Date:   Sun Mar 2 15:30:52 2014 -0500

    intelli_plug: Remove eco cores option and enable Strict Mode

commit ad64ea7eb174c3744d97d9a331e40e3d9fcf8b8b
Author: Arve Hjønnevåg <arve@android.com>
Date:   Mon Nov 26 19:30:22 2012 -0800

    mmc: block: Remove call to mmc_blk_set_blksize
    
    It no longer exists.
    
    CRs-Fixed: 561382
    Change-Id: I1ca5e8f93241264ed9ffdce53067cb16476c1c66
    Signed-off-by: Arve Hjønnevåg <arve@android.com>
    Signed-off-by: Asutosh Das <asutoshd@codeaurora.org>

commit c2e799bf63e7e03a815f62fe6ca58063e2227295
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 23:36:56 2014 +0100

    dvfs touch booster: Use the power efficient workqueue.

commit 998709fddfbc56d2b01dc0e8722e688d2947f12a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 23:32:16 2014 +0100

    Updated configuration and disable faux gamma color because of failed booting!

commit 1515aa86ca4c015a33609601e3eef627d7678b15
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 15:26:28 2014 +0100

    Updated configuration!

commit c33553785b751228a61e97a17cb26ddd5fcf0271
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 15:23:16 2014 +0100

    Removed useless file!

commit 3e95611b91dbd84e4d0e1e5525e04ffce82a429d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 15 20:32:41 2014 +0100

    Support for google arm-eabi-4.8 gcc!
    
    Conflicts:
    	Makefile

commit 7a3521fde00423dbefa3e016a2f8db973179c815
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 14:45:27 2014 +0100

    Fixed previous commits!

commit 10219edb86ed13ad394148e21002fab681ea561c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 14:18:36 2014 +0100

    Restored and updated workqueue!

commit 472764058bdcbc10787689efc44274accd3352e6
Author: Luis Cruz <ljc2491@gmail.com>
Date:   Wed Dec 18 08:49:10 2013 -0600

    regulator: core: Use the power efficient workqueue for delayed powerdown
    
    Adapted for 3.4 from lsk-v3.10
    
    There is no need to use a normal per-CPU workqueue for delayed power downs
    as they're not timing or performance critical and waking up a core for them
    would defeat some of the point.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Liam Girdwood <liam.r.girdwood@intel.com>
    (cherry picked from commit 070260f07c7daec311f2466eb9d1df475d5a46f8)
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 52c24d1bc66092fe3065b33b9b4610fb165ace44
Author: Luis Cruz <ljc2491@gmail.com>
Date:   Wed Dec 18 08:51:56 2013 -0600

    ASoC: jack: Use power efficient workqueue
    
    Adapted for 3.4 from lsk-v3.10
    
    The accessory detect debounce work is not performance sensitive so let
    the scheduler run it wherever is most efficient rather than in a per CPU
    workqueue by using the system power efficient workqueue.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    (cherry picked from commit e6058aaadcd473e5827720dc143af56aabbeecc7)
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 69daf0b9465894776f6bd58572f8f62c1ed48c8c
Author: Mark Brown <broonie@linaro.org>
Date:   Thu Jul 18 11:52:17 2013 +0100

    ASoC: pcm: Use the power efficient workqueue for delayed powerdown
    
    Adapted for 3.4 from lsk-v3.10
    
    There is no need to use a normal per-CPU workqueue for delayed power downs
    as they're not timing or performance critical and waking up a core for them
    would defeat some of the point.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    (cherry picked from commit d4e1a73acd4e894f8332f2093bceaef585cfab67)
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 4dda70968d67bf2bccd432eecc6e30e268c1c359
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:55 2013 +0530

    PHYLIB: queue work on system_power_efficient_wq
    
    Adapted for 3.4 from lsk-v3.10
    
    Phylib uses workqueues for multiple purposes. There is no real dependency of
    scheduling these on the cpu which scheduled them.
    
    On a idle system, it is observed that and idle cpu wakes up many times just to
    service this work. It would be better if we can schedule it on a cpu which the
    scheduler believes to be the most appropriate one.
    
    This patch replaces system_wq with system_power_efficient_wq for PHYLIB.
    
    Cc: David S. Miller <davem@davemloft.net>
    Cc: netdev@vger.kernel.org
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit bbb47bdeae756f04b896b55b51f230f3eb21f207)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 28f218fbeba1bc951806f9b1350274cdd39423e6
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:56 2013 +0530

    block: queue work on power efficient wq
    
    Adapted for 3.4 from lsk-v3.10
    
    Block layer uses workqueues for multiple purposes. There is no real dependency
    of scheduling these on the cpu which scheduled them.
    
    On a idle system, it is observed that and idle cpu wakes up many times just to
    service this work. It would be better if we can schedule it on a cpu which the
    scheduler believes to be the most appropriate one.
    
    This patch replaces normal workqueues with power efficient versions.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit 695588f9454bdbc7c1a2fbb8a6bfdcfba6183348)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit c2f6bf63d071e044f37da6fdef8eaa3aa2bf1e46
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:54 2013 +0530

    workqueue: Add system wide power_efficient workqueues
    
    Adapted for 3.4 from lsk-v3.10
    
    This patch adds system wide workqueues aligned towards power saving. This is
    done by allocating them with WQ_UNBOUND flag if 'wq_power_efficient' is set to
    'true'.
    
    tj: updated comments a bit.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit 0668106ca3865ba945e155097fb042bf66d364d3)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>

commit 02249c3e06c229a3259714b0d50ca52df741f700
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Apr 8 16:45:40 2013 +0530

    workqueues: Introduce new flag WQ_POWER_EFFICIENT for power oriented workqueues
    
    Adapted for 3.4 from lsk-v3.10
    
    Workqueues can be performance or power-oriented. Currently, most workqueues are
    bound to the CPU they were created on. This gives good performance (due to cache
    effects) at the cost of potentially waking up otherwise idle cores (Idle from
    scheduler's perspective. Which may or may not be physically idle) just to
    process some work. To save power, we can allow the work to be rescheduled on a
    core that is already awake.
    
    Workqueues created with the WQ_UNBOUND flag will allow some power savings.
    However, we don't change the default behaviour of the system.  To enable
    power-saving behaviour, a new config option CONFIG_WQ_POWER_EFFICIENT needs to
    be turned on. This option can also be overridden by the
    workqueue.power_efficient boot parameter.
    
    tj: Updated config description and comments.  Renamed
        CONFIG_WQ_POWER_EFFICIENT to CONFIG_WQ_POWER_EFFICIENT_DEFAULT.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Amit Kucheria <amit.kucheria@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit cee22a15052faa817e3ec8985a28154d3fabc7aa)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 008852d97cba88bee34f24b0ecdb684c986646e7
Author: Alexander Martinz <eviscerationls@gmail.com>
Date:   Mon Dec 23 21:11:05 2013 +0100

    mm: set swappiness to 0
    
    we have about 2gb ram, we dont need swapiness at all
    
    Change-Id: Icc8725e6fa0e269017ef652555993358db8fafec

commit 90eb7ee27df9eb79532e52d937dd37da3a9cfda7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 13:39:23 2014 +0100

    Fixed zcache!

commit 83dbf4a26b40eca84c4b3a98eb8a13b76750ef79
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:55:00 2014 +0100

    Revert "mm: push lru index into shrink_[in]active_list()"
    
    This reverts commit 5455b2a2d8e814a7ebd126e7a99c40c37200e86d.

commit 44a4d6c681eae76cf6ccb7f15cd6050ab0de5ee3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:59 2014 +0100

    Revert "mm: vmscan: remove lumpy reclaim"
    
    This reverts commit afba3004fbb81a9ca936626e85b2cd1ebb6a2948.

commit 6475bbb055c8df717a8f2399ba30ceeab6855989
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:59 2014 +0100

    Revert "mm: vmscan: do not stall on writeback during memory compaction"
    
    This reverts commit f8de05cf7ee47ad0cb0511a6a4fdde724e09ce77.

commit d6885908ddd9a8fb441f7ee941e328ac07f4659b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:58 2014 +0100

    Revert "mm: vmscan: remove reclaim_mode_t"
    
    This reverts commit f13e176176f75156d66dee653a8c1c8e11ab60c2.

commit 714d36a84d5a212e90537c7a4ec438216991ebd4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:58 2014 +0100

    Revert "mm: remove lru type checks from __isolate_lru_page()"
    
    This reverts commit 5731e2737515477f09288bfdbc79b172ca18d12f.

commit 386d13c5019ea2bf6fb6dfd8709f4b361163a83c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:57 2014 +0100

    Revert "mm/memcg: kill mem_cgroup_lru_del()"
    
    This reverts commit 819ad728aaee16a2dc8c43e21d791f7d94c268b4.

commit df066f1b35aa77e8e79f50abf7b140a5293b3584
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:57 2014 +0100

    Revert "mm/memcg: scanning_global_lru means mem_cgroup_disabled"
    
    This reverts commit f7a7aeac429c196e3a163443788bb96d4167630a.

commit ecc6c899ddf977b9a0fdd8eb54f1c71957b36438
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:56 2014 +0100

    Revert "mm/memcg: use vm_swappiness from target memory cgroup"
    
    This reverts commit 7c4127affa24e8c3e407fec436ad2e76c0922f51.

commit e7550563631024fde9166e6b4f7b1871475b6e7f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:55 2014 +0100

    Revert "mm: remove swap token code"
    
    This reverts commit 4bb594f1f75b594fc3d94063c50ce59b69b43f5f.

commit 329a4dc3247c23bce417562a36b12fe7297abd0e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:55 2014 +0100

    Revert "mm/vmscan: store "priority" in struct scan_control"
    
    This reverts commit e79db9a933e39431f3e96c52223c728053f0525a.

commit 621132206b45f34651f1084c31701435b41a7963
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:54 2014 +0100

    Revert "mm: memcg: count pte references from every member of the reclaimed hierarchy"
    
    This reverts commit f62e77490b11f3643ef600491072a9dc3c383d00.

commit 42f4e62424ec17d6b965d8980db507b9cc5ff9dc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:54 2014 +0100

    Revert "mm/vmscan: push zone pointer into shrink_page_list()"
    
    This reverts commit 6f3629e6e7f5d144e1c54d8111b24ba4422a638f.

commit 52d7f7c74f5d958dfebc76314b4e23867f6691c6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:53 2014 +0100

    Revert "mm/vmscan: push lruvec pointer into isolate_lru_pages()"
    
    This reverts commit 6d318ffdca643db5fe765b2f4daefe44a8b0183c.

commit de093b7afbd0d5546eced799932b380cb29da763
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:52 2014 +0100

    Revert "mm/vmscan: remove update_isolated_counts()"
    
    This reverts commit 0976a46719066dc17b378ed37a84b5b0fb918bad.

commit dfabef1c397106fe4efafb5776601f94ea0ae14b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:52 2014 +0100

    Revert "mm: cma: discard clean pages during contiguous allocation instead of migration"
    
    This reverts commit 94bd7f609a5699b569056ade654f46fe35c52623.

commit 72d8bc9fcc08a3079318206673da0df7788f9903
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:51 2014 +0100

    Revert "cma: decrease cc.nr_migratepages after reclaiming pagelist"
    
    This reverts commit 2598476ebf1b8bacc80d00fd097b6db90c0c7be7.

commit 1025f9f66b5393f3602fc83595ce3d1280fa7c76
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:51 2014 +0100

    Revert "mm/compaction.c: fix deferring compaction mistake"
    
    This reverts commit 928445c3a575c337d99a85e74e6d0a87f7167b25.

commit 5e6a67e1a42458438409fbf99b205ed8dd84a50b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:50 2014 +0100

    Revert "mm: compaction: Abort async compaction if locks are contended or taking too long"
    
    This reverts commit 15a146d44b10debad804ab9eba896615a2d9e298.

commit 5b900cad8f7dbdadeecc947ef098dd5a9368a012
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:49 2014 +0100

    Revert "mm: compaction: update comment in try_to_compact_pages"
    
    This reverts commit e766354ebf828f2f7204537160ce844163066522.

commit 80b56e6842e332ddf538e5561b4bf5626d30babe
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:49 2014 +0100

    Revert "mm: compaction: capture a suitable high-order page immediately when it is made available"
    
    This reverts commit 705fbe63dfb8b7e448a2b24d9b78d0e2d21aedc5.

commit 79d3092bed871a21184a761d73b046f5d746bf30
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:48 2014 +0100

    Revert "mm: compaction: abort compaction loop if lock is contended or run too long"
    
    This reverts commit 820f80002b6c006c156b21065435f543b49747d1.

commit 5bf2d757520ce8dd4437d851fcfd819159fd00be
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:48 2014 +0100

    Revert "mm: compaction: move fatal signal check out of compact_checklock_irqsave"
    
    This reverts commit e4a5e6a4188627341678d3bbd8129c464356c6c1.

commit 87f23d69d41de1a9039ad08a91a21cb265be7b11
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:47 2014 +0100

    Revert "mm: compaction: Update try_to_compact_pages()kerneldoc comment"
    
    This reverts commit b6b99c8a91d23ef70eb26acd60c92dc1bf434fde.

commit 142013048135060d7f92f61fa423bc26eafc7522
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:47 2014 +0100

    Revert "mm: compaction: acquire the zone->lru_lock as late as possible"
    
    This reverts commit 670cc90b200eec69f65fe358c11fac1f31d4bedc.

commit 6caa0b46e97d9f1165281e268571894d05ed0ae5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:46 2014 +0100

    Revert "mm: compaction: acquire the zone->lock as late as possible"
    
    This reverts commit 725f942a29d707d3e0d7efb1e55b94e0dd867872.

commit 891de75ebcf60237494216743357eeabfbb9760b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:46 2014 +0100

    Revert "cma: redirect page allocation to CMA"
    
    This reverts commit 3c9b0f80868155b54f22f9d3fed2ab2f3794e1f7.

commit be00654536b1d30d86d6c960badd5e6443eb9aef
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:45 2014 +0100

    Revert "mm: compaction: cache if a pageblock was scanned and no pages were isolated"
    
    This reverts commit 2f25e6e6b7b9dd41bbd3708e7ef06212685215cb.

commit f071b8e98e28d1970c806b6d762c1ec5d31b1068
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:44 2014 +0100

    Revert "mm: cma: WARN if freed memory is still in use"
    
    This reverts commit adf00c52b51e40a5b37cc5db00ff3808d89fca7e.

commit d2be0e8b73a64245ab9cfafd4b92087ec4e44406
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:44 2014 +0100

    Revert "mm: do not use page_count() without a page pin"
    
    This reverts commit 0e95f33de6afa0cb5b1ec519772935b9eaebe75a.

commit 833b42b12e937ceee95066a9d3205992c6d63869
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:43 2014 +0100

    Revert "mm: clean up __count_immobile_pages()"
    
    This reverts commit a3ad6947967acb643cf32339e793695db1491efb.

commit 83e20836b8f6bc19aa228f786d849a073c766f8b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:43 2014 +0100

    Revert "mm: compaction: Restart compaction from near where it left off"
    
    This reverts commit 16bdc5a3430a4050c9f039b42b7d367b859d90d3.

commit 91a4e3eb1c1214f565afe21bcde1e268a16486af
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:42 2014 +0100

    Revert "mm: compaction: clear PG_migrate_skip based on compaction and reclaim activity"
    
    This reverts commit d28129965a48da4b06eadf0e461b677cf1aa11ec.

commit 58653252b7917b17f221f0a654756c469a33ae92
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:42 2014 +0100

    Revert "CMA: migrate mlocked pages"
    
    This reverts commit 93c81ed20fca75afa57186a32c8df5aa75c3d54a.

commit a8de42368d29493d104e06552122aaac76c233da
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:41 2014 +0100

    Revert "mm: compaction: correct the nr_strict va isolated check for CMA"
    
    This reverts commit 507c9e24aa914f2a3a92d91b1741d5b379ba86fb.

commit 02e063e4e8a7e395467e6747b8278b2b4beb060a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:41 2014 +0100

    Revert "mm: compaction: validate pfn range passed to isolate_freepages_block"
    
    This reverts commit ed0552aa5197e3877cf254856b9d5d3d24753756.

commit c2013b4375a1a3f7f0d23ec92df062322c42176b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:40 2014 +0100

    Revert "mm: compaction: Move migration fail/success stats to migrate.c"
    
    This reverts commit 701786423e838575d6a0a6c81a2b5324996370c1.

commit c388437a575f9dcd7ea7f6869625112f2725127d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:39 2014 +0100

    Revert "mm: compaction: Add scanned and isolated counters for compaction"
    
    This reverts commit 1f13b7cb0ed2cd0565d2714c431b3862e355909b.

commit c0cb60c1db1c15da28263d65435b4cdddb3481bc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:39 2014 +0100

    Revert "mm: cma: skip watermarks check for already isolated blocks in split_free_page()"
    
    This reverts commit 1291c6f837e296e647828abe66d66e792700b09d.

commit c3e54d304a86ba54afba73049bdaa3ba4c59b20c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:38 2014 +0100

    Revert "mm: compaction: partially revert capture of suitable high-order page"
    
    This reverts commit 79820064ef0158d078b198ed59c05e20afb3042b.

commit dbf7e511fb82e91723ec686382abab3d79c4c410
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:37 2014 +0100

    Revert "compaction: fix build error in CMA && !COMPACTION"
    
    This reverts commit 4fc87e43343feed5d19822bb9ef79e4748ec906a.

commit 281045191bc6aff519badd67add7be17b009602d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:37 2014 +0100

    Revert "mm: vmscan: fix endless loop in kswapd balancing"
    
    This reverts commit c6bb10239df7342ade4f18c3253eaff85c7f9327.

commit 914843ef4b75a724bff46a0770623650b554cc0f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:54:36 2014 +0100

    Revert "mm: vmscan: do not keep kswapd looping forever due to individual uncompactable zones"
    
    This reverts commit 422f5c4fa24e90351be47e8a2e125599ecfc9cd5.

commit d4b588ac9b200d66e3a4f26d3c5e15163cfe8c5e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:51:38 2014 +0100

    Revert "msm: cpufreq: Initialize cpufreq driver early at boot"
    
    This reverts commit 6fc2557cd3b1bfd9b392694454f550b5d51a6d8c.

commit de723469808ba8e91903a750117e2a3bb60c4273
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:51:26 2014 +0100

    Revert "msm: cpufreq: Ensure cpufreq change happens on corresponding CPU"
    
    This reverts commit 656512c5908617bc10c20884b6e997046becf5d0.

commit 7c2654c807a904b4ed1ed29b265a6bb68d9c2c02
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:51:14 2014 +0100

    Revert "msm: cpufreq: Relax constraints on "msm-cpufreq" workqueue"
    
    This reverts commit 0a3b4a1f7081a85ff9e3b01322b4d0c3a59b24a6.

commit bbd2558304955abe89e4bea9e1841b56e2a234e8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:50:15 2014 +0100

    Revert "mm: vmalloc: use const void * for caller argument"
    
    This reverts commit 5d4b6861542fa5b1aeb29c22b92bec3c6ae3d1b6.

commit a7c992f82f7128b8d7a6a95dfaa90724abd7d888
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:49:53 2014 +0100

    Revert "Fix compilation issues"
    
    This reverts commit e360fbafd12efb34479278ff020bf9957df29380.

commit 044567bfdd6f4b17395673070a1b1c85ec4e4901
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:49:42 2014 +0100

    Revert "underp"
    
    This reverts commit f4a8fec782a759578964cbe7bbad2def9f7131aa.

commit f91e4f96cbdf0956920caa46e90db1aecd1caaee
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:49:31 2014 +0100

    Revert "coresight: add jtag fuse driver"
    
    This reverts commit 8523247cdb7d4951392edc0970af256ed7550a30.

commit 4f265e17d97d1b7b34e32c2f923c9a5a56e5f217
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:49:19 2014 +0100

    Revert "msm: cpufreq: Add support for CPU clocks and msm-cpufreq device"
    
    This reverts commit 8bf62a7151c2fc59e7c1d5bc525d07277541780a.

commit 258165e829d05ad47c03b05bf5f3e5c33ffa3410
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:49:08 2014 +0100

    Revert "msm: cpufreq: Update frequency index"
    
    This reverts commit fa4d83b2ea8d946e9646201e66e0b84c5f264e17.

commit 8b63d687173d7ebeeead4c8cd399696cd95aabb8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:48:09 2014 +0100

    Revert "mm: add a field to store names for private anonymous memory"
    
    This reverts commit f3bb217b443417854ef2dc41ace6301a93e0638d.

commit 2f767c2e9c1adb26810b09e35c8eb78d684807ca
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 22:47:18 2014 +0100

    Revert "Fixing previous commits!"
    
    This reverts commit 9976b493514b4c99269a06361344e6b537ab0853.

commit 9976b493514b4c99269a06361344e6b537ab0853
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 9 12:05:09 2014 +0100

    Fixing previous commits!

commit d4f858d3ddebb84e30c5331b7c9de82b78922b5f
Author: Ming Lei <ming.lei@canonical.com>
Date:   Wed Feb 27 17:05:19 2013 -0800

    block/partitions: optimize memory allocation in check_partition()
    
    Currently, sizeof(struct parsed_partitions) may be 64KB in 32bit arch, so
    it is easy to trigger page allocation failure by check_partition,
    especially in hotplug block device situation(such as, USB mass storage,
    MMC card, ...), and Felipe Balbi has observed the failure.
    
    This patch does below optimizations on the allocation of struct
    parsed_partitions to try to address the issue:
    
    - make parsed_partitions.parts as pointer so that the pointed memory can
      fit in 32KB buffer, then approximate 32KB memory can be saved
    
    - vmalloc the buffer pointed by parsed_partitions.parts because 32KB is
      still a bit big for kmalloc
    
    - given that many devices have the partition count limit, so only
      allocate disk_max_parts() partitions instead of 256 partitions always
    
    Change-Id: Iee8afc643ea8e7dd2d7c038c542a0377f71bd34e
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Reported-by: Felipe Balbi <balbi@ti.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d4fa2ad119e3dca116e0f5da931db98c624e2a20
Author: mrg666 <drgungor@hotmail.com>
Date:   Sun Dec 22 20:52:32 2013 -0500

    jbd2: optimize jbd2_journal_force_commit
    
    Current implementation of jbd2_journal_force_commit() is suboptimal because
    result in empty and useless commits. But callers just want to force and wait
    any unfinished commits. We already have jbd2_journal_force_commit_nested()
    which does exactly what we want, except we are guaranteed that we do not hold
    journal transaction open.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    
    Change-Id: I5c041a1898838e880714a913b5a915f105a8dfb9

commit 85f0575e55dab1c120e8013caa60f1e5601d5eaa
Author: Andrey Sidorov <qrxd43@motorola.com>
Date:   Wed Sep 19 18:14:53 2012 +0000

    ext4: speed up truncate/unlink by not using bforget() unless needed
    
    Do not iterate over data blocks scanning for bh's to forget as they're
    never exist. This improves time taken by unlink / truncate syscall.
    Tested by continuously truncating file that is being written by dd.
    Another test is rm -rf of linux tree while tar unpacks it. With
    ordered data mode condition unlikely(!tbh) was always met in
    ext4_free_blocks. With journal data mode tbh was found only few times,
    so optimisation is also possible.
    
    Unlinking fallocated 60G file after doing sync && echo 3 >
    /proc/sys/vm/drop_caches && time rm --help
    
    X86 before (linux 3.6-rc4):
    real    0m2.710s
    user    0m0.000s
    sys     0m1.530s
    
    X86 after:
    real    0m0.644s
    user    0m0.003s
    sys     0m0.060s
    
    MIPS before (linux 2.6.37):
    real    0m 4.93s
    user    0m 0.00s
    sys     0m 4.61s
    
    MIPS after:
    real    0m 0.16s
    user    0m 0.00s
    sys     0m 0.06s
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrey Sidorov <qrxd43@motorola.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    
    Change-Id: Ie78a945cb82b3892eaf88701f2dc3b7726104fb5

commit c87b627377ffe713a493537cad350dff82f0609f
Author: Alexander Martinz <eviscerationls@gmail.com>
Date:   Wed Dec 25 15:01:44 2013 +0100

    Revert "mm: panic on the first bad page table entry access"
    
    This reverts commit 215a2c3137b906afd66c93eb9fb3479a1f06e1c9.
    
    Change-Id: Ica69d573fe0e2fab19ddfc3979513bdedb69c717

commit ce51b7b59689818a6ecafc10b83604aedaeef8d2
Author: Subbaraman Narayanamurthy <subbaram@codeaurora.org>
Date:   Fri Sep 20 15:23:56 2013 -0700

    debug-pagealloc: Panic on pagealloc corruption
    
    Currently, we just print the pagealloc corruption warnings and
    proceed. Sometimes, we are getting multiple errors printed down
    the line. It will be good to get the device state as early as
    possible when we get the first pagealloc error.
    
    Change-Id: I79155ac8a039b30a3a98d5dd1384d3923082712f
    Signed-off-by: Subbaraman Narayanamurthy <subbaram@codeaurora.org>

commit f38fc2dc416c268fa0ad467e494cf0f759a8a0b3
Author: Pushkar Joshi <pushkarj@codeaurora.org>
Date:   Wed Sep 11 11:23:26 2013 -0700

    mm: panic on the first bad page table entry access
    
    Sometimes having a number of bad page table entries precipitates in
    a crash much later. Because of this, we do not have any context for
    the point at which the first bad pte entry was encountered. Hence,
    panic on first such instance to help gather context for debug.
    
    Change-Id: Idddf2b977214eb1463d08e16630e98264b9af487
    Signed-off-by: Pushkar Joshi <pushkarj@codeaurora.org>

commit 6588811e9f4c3f4769e940ce68f8bf4b969add2b
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jan 23 21:45:48 2013 +0000

    slub: tid must be retrieved from the percpu area of the current processor
    
    As Steven Rostedt has pointer out: rescheduling could occur on a
    different processor after the determination of the per cpu pointer and
    before the tid is retrieved. This could result in allocation from the
    wrong node in slab_alloc().
    
    The effect is much more severe in slab_free() where we could free to the
    freelist of the wrong page.
    
    The window for something like that occurring is pretty small but it is
    possible.
    
    Change-Id: I9e658e97ed2096f658d6e67739764a86151f13e3
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Git-commit: 7cccd80b4397699902aced1ad3d692d384aaab77
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit f06a8ca20d9be24aa5949ffd0c9b336dd5dae2c5
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed Jun 5 10:13:11 2013 +0200

    sched: Fix clear NOHZ_BALANCE_KICK
    
    I have faced a sequence where the Idle Load Balance was sometime not
    triggered for a while on my platform, in the following scenario:
    
     CPU 0 and CPU 1 are running tasks and CPU 2 is idle
    
     CPU 1 kicks the Idle Load Balance
     CPU 1 selects CPU 2 as the new Idle Load Balancer
     CPU 2 sets NOHZ_BALANCE_KICK for CPU 2
     CPU 2 sends a reschedule IPI to CPU 2
    
     While CPU 3 wakes up, CPU 0 or CPU 1 migrates a waking up task A on CPU 2
    
     CPU 2 finally wakes up, runs task A and discards the Idle Load Balance
           task A quickly goes back to sleep (before a tick occurs on CPU 2)
     CPU 2 goes back to idle with NOHZ_BALANCE_KICK set
    
    Whenever CPU 2 will be selected as the ILB, no reschedule IPI will be sent
    because NOHZ_BALANCE_KICK is already set and no Idle Load Balance will be
    performed.
    
    We must wait for the sched softirq to be raised on CPU 2 thanks to another
    part the kernel to come back to clear NOHZ_BALANCE_KICK.
    
    The proposed solution clears NOHZ_BALANCE_KICK in schedule_ipi if
    we can't raise the sched_softirq for the Idle Load Balance.
    
    Change since V1:
    
    - move the clear of NOHZ_BALANCE_KICK in got_nohz_idle_kick if the ILB
      can't run on this CPU (as suggested by Peter)
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1370419991-13870-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Git-commit: 873b4c65b519fd769940eb281f77848227d4e5c1
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [smuckle@codeaurora.org: minor merge resolution for 3.4 in scheduler_ipi()]
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>
    Change-Id: I3548612057cccc2ecc29429c129c44183083831f

commit 90dd4305c1b0ad8c19de01661a08a1352567e4d1
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Tue Nov 19 14:16:53 2013 -0800

    tracing/sched: add load balancer tracepoint
    
    When doing performance analysis it can be useful to see exactly
    what is going on with the load balancer - when it runs and why
    exactly it may not be redistributing load.
    
    This additional tracepoint will show the idle context of the
    load balance operation (idle, not idle, newly idle), various
    values from the load balancing operation, the final result,
    and the new balance interval.
    
    Change-Id: I9e5c97ae3878bea44e60d189ff3cec2275f2c75e
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit f3bb217b443417854ef2dc41ace6301a93e0638d
Author: Colin Cross <ccross@android.com>
Date:   Wed Jun 26 17:26:01 2013 -0700

    mm: add a field to store names for private anonymous memory
    
    Userspace processes often have multiple allocators that each do
    anonymous mmaps to get memory.  When examining memory usage of
    individual processes or systems as a whole, it is useful to be
    able to break down the various heaps that were allocated by
    each layer and examine their size, RSS, and physical memory
    usage.
    
    This patch adds a user pointer to the shared union in
    vm_area_struct that points to a null terminated string inside
    the user process containing a name for the vma.  vmas that
    point to the same address will be merged, but vmas that
    point to equivalent strings at different addresses will
    not be merged.
    
    Userspace can set the name for a region of memory by calling
    prctl(PR_SET_VMA, PR_SET_VMA_ANON_NAME, start, len, (unsigned long)name);
    Setting the name to NULL clears it.
    
    The names of named anonymous vmas are shown in /proc/pid/maps
    as [anon:<name>] and in /proc/pid/smaps in a new "Name" field
    that is only present for named vmas.  If the userspace pointer
    is no longer valid all or part of the name will be replaced
    with "<fault>".
    
    The idea to store a userspace pointer to reduce the complexity
    within mm (at the expense of the complexity of reading
    /proc/pid/mem) came from Dave Hansen.  This results in no
    runtime overhead in the mm subsystem other than comparing
    the anon_name pointers when considering vma merging.  The pointer
    is stored in a union with fieds that are only used on file-backed
    mappings, so it does not increase memory usage.
    
    Change-Id: Ie2ffc0967d4ffe7ee4c70781313c7b00cf7e3092
    Signed-off-by: Colin Cross <ccross@android.com>

commit e2d3d1cdce4764fe5c9fb6c861e4bc08099d07c4
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Sep 23 15:19:49 2013 -0700

    smp: Relax irqs_disable() warning in smp_call_function_single()
    
    The change that long-ago added the interrupts-disabled warning in
    smp_call_function_single() cited the following deadlock as the reason:
    	CPU A				CPU B
    	Disable interrupts
    					smp_call_function()
    					Take call_lock
    					Send IPIs
    					Wait for all cpus to acknowledge IPI
    					CPU A has not responded, spin waiting
    					for cpu A to respond, holding call_lock
    	smp_call_function()
    	Spin waiting for call_lock
    	Deadlock			Deadlock
    
    Such a deadlock is not possible however, in the event that
    smp_call_function_single() is called for the currently-executing CPU.
    While not a common usecase, relaxing the warning may simplify some
    driver implementations.
    
    The 'acpuclock-krait' driver in the MSM ARM sub-architecture provides
    one example, where smp_call_function_single() is sometimes called from
    a cpuidle path with interrupts disabled, but only ever from the target
    CPU. Other callers of that same function may cross-call, but only do so
    when interrupts are enabled.
    
    Change-Id: I647b3b46fe6aa4dfb06f7750396986b80b92d700
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit ae928fc1566fa6e3ab5b91c394f7763ec6678887
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed May 30 14:18:41 2012 -0600

    driver core: Add iommu_group tracking to struct device
    
    IOMMU groups allow IOMMU drivers to represent DMA visibility
    and isolation of devices.  Multiple devices may be grouped
    together for the purposes of DMA.  Placing a pointer on
    struct device enable easy access for things like streaming
    DMA programming and drivers like VFIO.
    
    Change-Id: I0a92a94aea0ac699676914a033477243a70dd0b0
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit c356ef5cc363cb051bec32e3209e1bfb03252a09
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Apr 9 10:57:54 2013 -0700

    msm: scm: Add scm_call_atomic3
    
    Some drivers need to pass three arguments atomically. Add support
    for it.
    
    Change-Id: I0077d26eeab0e6a3828f994fd4aabc4d73d48b31
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 6cda2acbe33e6e9ac174111a376d09712f1c15ae
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri Mar 8 09:03:42 2013 -0800

    ARM: Add support for 64 bit register reads/writes
    
    Add macros to read and write to 64 bit registers.
    
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    
    Conflicts:
    	arch/arm/include/asm/io.h
    
    Change-Id: Ied2b38e6a3803377cd271fe7b13f6cf0fc586e70

commit 310a3cb1ce3bfa7ad238f86c7831270a999c0786
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri Apr 26 13:47:11 2013 -0700

    msm: Fix mem leak when using per-process pages
    
    When GPU is using per process page tables the code creates
    a new page table (domain) for every process. There is also
    some associated structures allocated to keep track of each
    domain. However, when cleaning up the memory used for a page
    table only the domain is freed and not the associated structures.
    
    Add a function to allow a client to properly unregister a domain
    which will free all the memory associated with a domain.
    Call the unregister function from kgsl driver code.
    
    CRs-fixed: 480801
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/iommu_domains.c
    
    Change-Id: I78365cac8afb900450b512527f0cb71da5e9ddb2
    
    Conflicts:
    	arch/arm/mach-msm/iommu_domains.c

commit cae09d64ab8457f7b189ae689bc61eff33da8ada
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Nov 3 02:54:34 2013 -0800

    arm: unwind: Remove logspam while in debug mode
    
    Change-Id: I87501ca4609a883a99cddd2836c5d036e2cacd06

commit eb55b3a91aa363ccc864027fdf40de14e44bed55
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Mon May 28 14:16:57 2012 -0400

    ext4: Update from upstream
    
    ext4: fix potential NULL dereference in ext4_free_inodes_counts()
    
    commit bb3d132a24cd8bf5e7773b2d9f9baa58b07a7dae upstream.
    
    The ext4_get_group_desc() function returns NULL on error, and
    ext4_free_inodes_count() function dereferences it without checking.
    There is a check on the next line, but it's too late.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: force ro mount if ext4_setup_super() fails
    
    commit 7e84b6216467b84cd332c8e567bf5aa113fd2f38 upstream.
    
    If ext4_setup_super() fails i.e. due to a too-high revision,
    the error is logged in dmesg but the fs is not mounted RO as
    indicated.
    
    Tested by:
    
    [164919.759248] EXT4-fs (sdb6): revision level too high, forcing read-only mode
    /dev/sdb6 /mnt/test2 ext4 rw,seclabel,relatime,data=ordered 0 0
    
    Reviewed-by: Andreas Dilger <adilger@whamcloud.com>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix potential integer overflow in alloc_flex_gd()
    
    commit 967ac8af4475ce45474800709b12137aa7634c77 upstream.
    
    In alloc_flex_gd(), when flexbg_size is large, kmalloc size would
    overflow and flex_gd->groups would point to a buffer smaller than
    expected, causing OOB accesses when it is used.
    
    Note that in ext4_resize_fs(), flexbg_size is calculated using
    sbi->s_log_groups_per_flex, which is read from the disk and only bounded
    to [1, 31]. The patch returns NULL for too large flexbg_size.
    
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Haogang Chen <haogangchen@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: disallow hard-linked directory in ext4_lookup
    
    commit 7e936b737211e6b54e34b71a827e56b872e958d8 upstream.
    
    A hard-linked directory to its parent can cause the VFS to deadlock,
    and is a sign of a corrupted file system.  So detect this case in
    ext4_lookup(), before the rmdir() lockup scenario can take place.
    
    Change-Id: I0e6af09e4131d07448c73fc10e7bf3ed4234ff37
    Signed-off-by: Andreas Dilger <adilger@dilger.ca>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: add missing save_error_info() to ext4_error()
    
    commit f3fc0210c0fc91900766c995f089c39170e68305 upstream.
    
    The ext4_error() function is missing a call to save_error_info().
    Since this is the function which marks the file system as containing
    an error, this oversight (which was introduced in 2.6.36) is quite
    significant, and should be backported to older stable kernels with
    high urgency.
    
    Reported-by: Ken Sumrall <ksumrall@google.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: ksumrall@google.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't trash state flags in EXT4_IOC_SETFLAGS
    
    commit 79906964a187c405db72a3abc60eb9b50d804fbc upstream.
    
    In commit 353eb83c we removed i_state_flags with 64-bit longs, But
    when handling the EXT4_IOC_SETFLAGS ioctl, we replace i_flags
    directly, which trashes the state flags which are stored in the high
    32-bits of i_flags on 64-bit platforms.  So use the the
    ext4_{set,clear}_inode_flags() functions which use atomic bit
    manipulation functions instead.
    
    Reported-by: Tao Ma <boyu.mt@taobao.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: add ext4_mb_unload_buddy in the error path
    
    commit 02b7831019ea4e7994968c84b5826fa8b248ffc8 upstream.
    
    ext4_free_blocks fails to pair an ext4_mb_load_buddy with a matching
    ext4_mb_unload_buddy when it fails a memory allocation.
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: remove mb_groups before tearing down the buddy_cache
    
    commit 95599968d19db175829fb580baa6b68939b320fb upstream.
    
    We can't have references held on pages in the s_buddy_cache while we are
    trying to truncate its pages and put the inode.  All the pages must be
    gone before we reach clear_inode.  This can only be gauranteed if we
    can prevent new users from grabbing references to s_buddy_cache's pages.
    
    The original bug can be reproduced and the bug fix can be verified by:
    
    while true; do mount -t ext4 /dev/ram0 /export/hda3/ram0; \
    	umount /export/hda3/ram0; done &
    
    while true; do cat /proc/fs/ext4/ram0/mb_groups; done
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't set i_flags in EXT4_IOC_SETFLAGS
    
    commit b22b1f178f6799278d3178d894f37facb2085765 upstream.
    
    Commit 7990696 uses the ext4_{set,clear}_inode_flags() functions to
    change the i_flags automatically but fails to remove the error setting
    of i_flags.  So we still have the problem of trashing state flags.
    Fix this by removing the assignment.
    
    Signed-off-by: Tao Ma <boyu.mt@taobao.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix the free blocks calculation for ext3 file systems w/ uninit_bg
    
    commit b0dd6b70f0fda17ae9762fbb72d98e40a4f66556 upstream.
    
    Ext3 filesystems that are converted to use as many ext4 file system
    features as possible will enable uninit_bg to speed up e2fsck times.
    These file systems will have a native ext3 layout of inode tables and
    block allocation bitmaps (as opposed to ext4's flex_bg layout).
    Unfortunately, in these cases, when first allocating a block in an
    uninitialized block group, ext4 would incorrectly calculate the number
    of free blocks in that block group, and then errorneously report that
    the file system was corrupt:
    
    EXT4-fs error (device vdd): ext4_mb_generate_buddy:741: group 30, 32254 clusters in bitmap, 32258 in gd
    
    This problem can be reproduced via:
    
        mke2fs -q -t ext4 -O ^flex_bg /dev/vdd 5g
        mount -t ext4 /dev/vdd /mnt
        fallocate -l 4600m /mnt/test
    
    The problem was caused by a bone headed mistake in the check to see if a
    particular metadata block was part of the block group.
    
    Many thanks to Kees Cook for finding and bisecting the buggy commit
    which introduced this bug (commit fd034a84e1, present since v3.2).
    
    Reported-by: Sander Eikelenboom <linux@eikelenboom.it>
    Reported-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Tested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix duplicated mnt_drop_write call in EXT4_IOC_MOVE_EXT
    
    commit 331ae4962b975246944ea039697a8f1cadce42bb upstream.
    
    Caused, AFAICS, by mismerge in commit ff9cb1c4eead ("Merge branch
    'for_linus' into for_linus_merged")
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: pass a char * to ext4_count_free() instead of a buffer_head ptr
    
    commit f6fb99cadcd44660c68e13f6eab28333653621e6 upstream.
    
    Make it possible for ext4_count_free to operate on buffers and not
    just data in buffer_heads.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix overhead calculation used by ext4_statfs()
    
    commit 952fc18ef9ec707ebdc16c0786ec360295e5ff15 upstream.
    
    Commit f975d6bcc7a introduced bug which caused ext4_statfs() to
    miscalculate the number of file system overhead blocks.  This causes
    the f_blocks field in the statfs structure to be larger than it should
    be.  This would in turn cause the "df" output to show the number of
    data blocks in the file system and the number of data blocks used to
    be larger than they should be.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix hole punch failure when depth is greater than 0
    
    commit 968dee77220768a5f52cf8b21d0bdb73486febef upstream.
    
    Whether to continue removing extents or not is decided by the return
    value of function ext4_ext_more_to_rm() which checks 2 conditions:
    a) if there are no more indexes to process.
    b) if the number of entries are decreased in the header of "depth -1".
    
    In case of hole punch, if the last block to be removed is not part of
    the last extent index than this index will not be deleted, hence the
    number of valid entries in the extent header of "depth - 1" will
    remain as it is and ext4_ext_more_to_rm will return 0 although the
    required blocks are not yet removed.
    
    This patch fixes the above mentioned problem as instead of removing
    the extents from the end of file, it starts removing the blocks from
    the particular extent from which removing blocks is actually required
    and continue backward until done.
    
    Signed-off-by: Ashish Sangwan <ashish.sangwan2@gmail.com>
    Signed-off-by: Namjae Jeon <linkinjeon@gmail.com>
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't let i_reserved_meta_blocks go negative
    
    commit 97795d2a5b8d3c8dc4365d4bd3404191840453ba upstream.
    
    If we hit a condition where we have allocated metadata blocks that
    were not appropriately reserved, we risk underflow of
    ei->i_reserved_meta_blocks.  In turn, this can throw
    sbi->s_dirtyclusters_counter significantly out of whack and undermine
    the nondelalloc fallback logic in ext4_nonda_switch().  Warn if this
    occurs and set i_allocated_meta_blocks to avoid this problem.
    
    This condition is reproduced by xfstests 270 against ext2 with
    delalloc enabled:
    
    Mar 28 08:58:02 localhost kernel: [  171.526344] EXT4-fs (loop1): delayed block allocation failed for inode 14 at logical offset 64486 with max blocks 64 with error -28
    Mar 28 08:58:02 localhost kernel: [  171.526346] EXT4-fs (loop1): This should not happen!! Data will be lost
    
    270 ultimately fails with an inconsistent filesystem and requires an
    fsck to repair.  The cause of the error is an underflow in
    ext4_da_update_reserve_space() due to an unreserved meta block
    allocation.
    
    Signed-off-by: Brian Foster <bfoster@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: undo ext4_calc_metadata_amount if we fail to claim space
    
    commit 03179fe92318e7934c180d96f12eff2cb36ef7b6 upstream.
    
    The function ext4_calc_metadata_amount() has side effects, although
    it's not obvious from its function name.  So if we fail to claim
    space, regardless of whether we retry to claim the space again, or
    return an error, we need to undo these side effects.
    
    Otherwise we can end up incorrectly calculating the number of metadata
    blocks needed for the operation, which was responsible for an xfstests
    failure for test #271 when using an ext2 file system with delalloc
    enabled.
    
    Reported-by: Brian Foster <bfoster@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: make sure the journal sb is written in ext4_clear_journal_err()
    
    commit d796c52ef0b71a988364f6109aeb63d79c5b116b upstream.
    
    After we transfer set the EXT4_ERROR_FS bit in the file system
    superblock, it's not enough to call jbd2_journal_clear_err() to clear
    the error indication from journal superblock --- we need to call
    jbd2_journal_update_sb_errno() as well.  Otherwise, when the root file
    system is mounted read-only, the journal is replayed, and the error
    indicator is transferred to the superblock --- but the s_errno field
    in the jbd2 superblock is left set (since although we cleared it in
    memory, we never flushed it out to disk).
    
    This can end up confusing e2fsck.  We should make e2fsck more robust
    in this case, but the kernel shouldn't be leaving things in this
    confused state, either.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: avoid kmemcheck complaint from reading uninitialized memory
    
    commit 7e731bc9a12339f344cddf82166b82633d99dd86 upstream.
    
    Commit 03179fe923 introduced a kmemcheck complaint in
    ext4_da_get_block_prep() because we save and restore
    ei->i_da_metadata_calc_last_lblock even though it is left
    uninitialized in the case where i_da_metadata_calc_len is zero.
    
    This doesn't hurt anything, but silencing the kmemcheck complaint
    makes it easier for people to find real bugs.
    
    Addresses https://bugzilla.kernel.org/show_bug.cgi?id=45631
    (which is marked as a regression).
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix long mount times on very big file systems
    
    commit 0548bbb85337e532ca2ed697c3e9b227ff2ed4b4 upstream.
    
    Commit 8aeb00ff85a: "ext4: fix overhead calculation used by
    ext4_statfs()" introduced a O(n**2) calculation which makes very large
    file systems take forever to mount.  Fix this with an optimization for
    non-bigalloc file systems.  (For bigalloc file systems the overhead
    needs to be set in the the superblock.)
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix kernel BUG on large-scale rm -rf commands
    
    commit 89a4e48f8479f8145eca9698f39fe188c982212f upstream.
    
    Commit 968dee7722: "ext4: fix hole punch failure when depth is greater
    than 0" introduced a regression in v3.5.1/v3.6-rc1 which caused kernel
    crashes when users ran run "rm -rf" on large directory hierarchy on
    ext4 filesystems on RAID devices:
    
        BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
    
        Process rm (pid: 18229, threadinfo ffff8801276bc000, task ffff880123631710)
        Call Trace:
         [<ffffffff81236483>] ? __ext4_handle_dirty_metadata+0x83/0x110
         [<ffffffff812353d3>] ext4_ext_truncate+0x193/0x1d0
         [<ffffffff8120a8cf>] ? ext4_mark_inode_dirty+0x7f/0x1f0
         [<ffffffff81207e05>] ext4_truncate+0xf5/0x100
         [<ffffffff8120cd51>] ext4_evict_inode+0x461/0x490
         [<ffffffff811a1312>] evict+0xa2/0x1a0
         [<ffffffff811a1513>] iput+0x103/0x1f0
         [<ffffffff81196d84>] do_unlinkat+0x154/0x1c0
         [<ffffffff8118cc3a>] ? sys_newfstatat+0x2a/0x40
         [<ffffffff81197b0b>] sys_unlinkat+0x1b/0x50
         [<ffffffff816135e9>] system_call_fastpath+0x16/0x1b
        Code: 8b 4d 20 0f b7 41 02 48 8d 04 40 48 8d 04 81 49 89 45 18 0f b7 49 02 48 83 c1 01 49 89 4d 00 e9 ae f8 ff ff 0f 1f 00 49 8b 45 28 <48> 8b 40 28 49 89 45 20 e9 85 f8 ff ff 0f 1f 80 00 00 00
    
        RIP  [<ffffffff81233164>] ext4_ext_remove_space+0xa34/0xdf0
    
    This could be reproduced as follows:
    
    The problem in commit 968dee7722 was that caused the variable 'i' to
    be left uninitialized if the truncate required more space than was
    available in the journal.  This resulted in the function
    ext4_ext_truncate_extend_restart() returning -EAGAIN, which caused
    ext4_ext_remove_space() to restart the truncate operation after
    starting a new jbd2 handle.
    
    Reported-by: Maciej Żenczykowski <maze@google.com>
    Reported-by: Marti Raudsepp <marti@juffo.org>
    Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: ignore last group w/o enough space when resizing instead of BUG'ing
    
    commit 03c1c29053f678234dbd51bf3d65f3b7529021de upstream.
    
    If the last group does not have enough space for group tables, ignore
    it instead of calling BUG_ON().
    
    Reported-by: Daniel Drake <dsd@laptop.org>
    Signed-off-by: Yongqiang Yang <xiaoqiangnk@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't copy non-existent gdt blocks when resizing
    
    commit 6df935ad2fced9033ab52078825fcaf6365f34b7 upstream.
    
    The resize code was copying blocks at the beginning of each block
    group in order to copy the superblock and block group descriptor table
    (gdt) blocks.  This was, unfortunately, being done even for block
    groups that did not have super blocks or gdt blocks.  This is a
    complete waste of perfectly good I/O bandwidth, to skip writing those
    blocks for sparse bg's.
    
    Signed-off-by: Yongqiang Yang <xiaoqiangnk@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: avoid duplicate writes of the backup bg descriptor blocks
    
    commit 2ebd1704ded88a8ae29b5f3998b13959c715c4be upstream.
    
    The resize code was needlessly writing the backup block group
    descriptor blocks multiple times (once per block group) during an
    online resize.
    
    Signed-off-by: Yongqiang Yang <xiaoqiangnk@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix potential deadlock in ext4_nonda_switch()
    
    commit 00d4e7362ed01987183e9528295de3213031309c upstream.
    
    In ext4_nonda_switch(), if the file system is getting full we used to
    call writeback_inodes_sb_if_idle().  The problem is that we can be
    holding i_mutex already, and this causes a potential deadlock when
    writeback_inodes_sb_if_idle() when it tries to take s_umount.  (See
    lockdep output below).
    
    As it turns out we don't need need to hold s_umount; the fact that we
    are in the middle of the write(2) system call will keep the superblock
    pinned.  Unfortunately writeback_inodes_sb() checks to make sure
    s_umount is taken, and the VFS uses a different mechanism for making
    sure the file system doesn't get unmounted out from under us.  The
    simplest way of dealing with this is to just simply grab s_umount
    using a trylock, and skip kicking the writeback flusher thread in the
    very unlikely case that we can't take a read lock on s_umount without
    blocking.
    
    Also, we now check the cirteria for kicking the writeback thread
    before we decide to whether to fall back to non-delayed writeback, so
    if there are any outstanding delayed allocation writes, we try to get
    them resolved as soon as possible.
    
       [ INFO: possible circular locking dependency detected ]
       3.6.0-rc1-00042-gce894ca #367 Not tainted
       -------------------------------------------------------
       dd/8298 is trying to acquire lock:
        (&type->s_umount_key#18){++++..}, at: [<c02277d4>] writeback_inodes_sb_if_idle+0x28/0x46
    
       but task is already holding lock:
        (&sb->s_type->i_mutex_key#8){+.+...}, at: [<c01ddcce>] generic_file_aio_write+0x5f/0xd3
    
       which lock already depends on the new lock.
    
       2 locks held by dd/8298:
        #0:  (sb_writers#2){.+.+.+}, at: [<c01ddcc5>] generic_file_aio_write+0x56/0xd3
        #1:  (&sb->s_type->i_mutex_key#8){+.+...}, at: [<c01ddcce>] generic_file_aio_write+0x5f/0xd3
    
       stack backtrace:
       Pid: 8298, comm: dd Not tainted 3.6.0-rc1-00042-gce894ca #367
       Call Trace:
        [<c015b79c>] ? console_unlock+0x345/0x372
        [<c06d62a1>] print_circular_bug+0x190/0x19d
        [<c019906c>] __lock_acquire+0x86d/0xb6c
        [<c01999db>] ? mark_held_locks+0x5c/0x7b
        [<c0199724>] lock_acquire+0x66/0xb9
        [<c02277d4>] ? writeback_inodes_sb_if_idle+0x28/0x46
        [<c06db935>] down_read+0x28/0x58
        [<c02277d4>] ? writeback_inodes_sb_if_idle+0x28/0x46
        [<c02277d4>] writeback_inodes_sb_if_idle+0x28/0x46
        [<c026f3b2>] ext4_nonda_switch+0xe1/0xf4
        [<c0271ece>] ext4_da_write_begin+0x27/0x193
        [<c01dcdb0>] generic_file_buffered_write+0xc8/0x1bb
        [<c01ddc47>] __generic_file_aio_write+0x1dd/0x205
        [<c01ddce7>] generic_file_aio_write+0x78/0xd3
        [<c026d336>] ext4_file_write+0x480/0x4a6
        [<c0198c1d>] ? __lock_acquire+0x41e/0xb6c
        [<c0180944>] ? sched_clock_cpu+0x11a/0x13e
        [<c01967e9>] ? trace_hardirqs_off+0xb/0xd
        [<c018099f>] ? local_clock+0x37/0x4e
        [<c0209f2c>] do_sync_write+0x67/0x9d
        [<c0209ec5>] ? wait_on_retry_sync_kiocb+0x44/0x44
        [<c020a7b9>] vfs_write+0x7b/0xe6
        [<c020a9a6>] sys_write+0x3b/0x64
        [<c06dd4bd>] syscall_call+0x7/0xb
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix crash when accessing /proc/mounts concurrently
    
    commit 50df9fd55e4271e89a7adf3b1172083dd0ca199d upstream.
    
    The crash was caused by a variable being erronously declared static in
    token2str().
    
    In addition to /proc/mounts, the problem can also be easily replicated
    by accessing /proc/fs/ext4/<partition>/options in parallel:
    
    $ cat /proc/fs/ext4/<partition>/options > options.txt
    
    ... and then running the following command in two different terminals:
    
    $ while diff /proc/fs/ext4/<partition>/options options.txt; do true; done
    
    This is also the cause of the following a crash while running xfstests
    
    	https://bugs.launchpad.net/bugs/1053019
    	https://bugzilla.kernel.org/show_bug.cgi?id=47731
    
    Signed-off-by: Herton Ronaldo Krzesinski <herton.krzesinski@canonical.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Brad Figg <brad.figg@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: move_extent code cleanup
    
    commit 03bd8b9b896c8e940f282f540e6b4de90d666b7c upstream.
    
    - Remove usless checks, because it is too late to check that inode != NULL
      at the moment it was referenced several times.
    - Double lock routines looks very ugly and locking ordering relays on
      order of i_ino, but other kernel code rely on order of pointers.
      Let's make them simple and clean.
    - check that inodes belongs to the same SB as soon as possible.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: online defrag is not supported for journaled files
    
    commit f066055a3449f0e5b0ae4f3ceab4445bead47638 upstream.
    
    Proper block swap for inodes with full journaling enabled is
    truly non obvious task. In order to be on a safe side let's
    explicitly disable it for now.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: always set i_op in ext4_mknod()
    
    commit 6a08f447facb4f9e29fcc30fb68060bb5a0d21c2 upstream.
    
    ext4_special_inode_operations have their own ifdef CONFIG_EXT4_FS_XATTR
    to mask those methods. And ext4_iget also always sets it, so there is
    an inconsistency.
    
    Signed-off-by: Bernd Schubert <bernd.schubert@itwm.fraunhofer.de>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: race-condition protection for ext4_convert_unwritten_extents_endio
    
    commit dee1f973ca341c266229faa5a1a5bb268bed3531 upstream.
    
    We assumed that at the time we call ext4_convert_unwritten_extents_endio()
    extent in question is fully inside [map.m_lblk, map->m_len] because
    it was already split during submission.  But this may not be true due to
    a race between writeback vs fallocate.
    
    If extent in question is larger than requested we will split it again.
    Special precautions should being done if zeroout required because
    [map.m_lblk, map->m_len] already contains valid data.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: Avoid underflow in ext4_trim_fs()
    
    commit 5de35e8d5c02d271c20e18337e01bc20e6ef472e upstream.
    
    Currently if len argument in ext4_trim_fs() is smaller than one block,
    the 'end' variable underflow. Avoid that by returning EINVAL if len is
    smaller than file system block.
    
    Also remove useless unlikely().
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix unjournaled inode bitmap modification
    
    commit ffb5387e85d528fb6d0d924abfa3fbf0fc484071 upstream.
    
    commit 119c0d4460b001e44b41dcf73dc6ee794b98bd31 changed
    ext4_new_inode() such that the inode bitmap was being modified
    outside a transaction, which could lead to corruption, and was
    discovered when journal_checksum found a bad checksum in the
    journal during log replay.
    
    Nix ran into this when using the journal_async_commit mount
    option, which enables journal checksumming.  The ensuing
    journal replay failures due to the bad checksums led to
    filesystem corruption reported as the now infamous
    "Apparent serious progressive ext4 data corruption bug"
    
    [ Changed by tytso to only call ext4_journal_get_write_access() only
      when we're fairly certain that we're going to allocate the inode. ]
    
    I've tested this by mounting with journal_checksum and
    running fsstress then dropping power; I've also tested by
    hacking DM to create snapshots w/o first quiescing, which
    allows me to test journal replay repeatedly w/o actually
    power-cycling the box.  Without the patch I hit a journal
    checksum error every time.  With this fix it survives
    many iterations.
    
    Reported-by: Nix <nix@esperi.org.uk>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix memory leak in ext4_xattr_set_acl()'s error path
    
    commit 24ec19b0ae83a385ad9c55520716da671274b96c upstream.
    
    In ext4_xattr_set_acl(), if ext4_journal_start() returns an error,
    posix_acl_release() will not be called for 'acl' which may result in a
    memory leak.
    
    This patch fixes that.
    
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Eugene Shatokhin <eugene.shatokhin@rosalab.ru>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix possible use after free with metadata csum
    
    commit aeb1e5d69a5be592e86a926be73efb38c55af404 upstream.
    
    Commit fa77dcfafeaa introduces block bitmap checksum calculation into
    ext4_new_inode() in the case that block group was uninitialized.
    However we brelse() the bitmap buffer before we attempt to checksum it
    so we have no guarantee that the buffer is still there.
    
    Fix this by releasing the buffer after the possible checksum
    computation.
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Acked-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix extent tree corruption caused by hole punch
    
    commit c36575e663e302dbaa4d16b9c72d2c9a913a9aef upstream.
    
    When depth of extent tree is greater than 1, logical start value of
    interior node is not correctly updated in ext4_ext_rm_idx.
    
    Signed-off-by: Forrest Liu <forrestl@synology.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Ashish Sangwan <ashishsangwan2@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: check dioread_nolock on remount
    
    commit 261cb20cb2f0737a247aaf08dff7eb065e3e5b66 upstream.
    
    Currently we allow enabling dioread_nolock mount option on remount for
    filesystems where blocksize < PAGE_CACHE_SIZE.  This isn't really
    supported so fix the bug by moving the check for blocksize !=
    PAGE_CACHE_SIZE into parse_options(). Change the original PAGE_SIZE to
    PAGE_CACHE_SIZE along the way because that's what we are really
    interested in.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: do not try to write superblock on ro remount w/o journal
    
    commit d096ad0f79a782935d2e06ae8fb235e8c5397775 upstream.
    
    When a journal-less ext4 filesystem is mounted on a read-only block
    device (blockdev --setro will do), each remount (for other, unrelated,
    flags, like suid=>nosuid etc) results in a series of scary messages
    from kernel telling about I/O errors on the device.
    
    This is becauese of the following code ext4_remount():
    
           if (sbi->s_journal == NULL)
                    ext4_commit_super(sb, 1);
    
    at the end of remount procedure, which forces writing (flushing) of
    a superblock regardless whenever it is dirty or not, if the filesystem
    is readonly or not, and whenever the device itself is readonly or not.
    
    We only need call ext4_commit_super when the file system had been
    previously mounted read/write.
    
    Thanks to Eric Sandeen for help in diagnosing this issue.
    
    Signed-off-By: Michael Tokarev <mjt@tls.msk.ru>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: lock i_mutex when truncating orphan inodes
    
    commit 721e3eba21e43532e438652dd8f1fcdfce3187e7 upstream.
    
    Commit c278531d39 added a warning when ext4_flush_unwritten_io() is
    called without i_mutex being taken.  It had previously not been taken
    during orphan cleanup since races weren't possible at that point in
    the mount process, but as a result of this c278531d39, we will now see
    a kernel WARN_ON in this case.  Take the i_mutex in
    ext4_orphan_cleanup() to suppress this warning.
    
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Zheng Liu <wenqing.lz@taobao.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: init pagevec in ext4_da_block_invalidatepages
    
    commit 66bea92c69477a75a5d37b9bfed5773c92a3c4b4 upstream.
    
    ext4_da_block_invalidatepages is missing a pagevec_init(),
    which means that pvec->cold contains random garbage.
    
    This affects whether the page goes to the front or
    back of the LRU when ->cold makes it to
    free_hot_cold_page()
    
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Reviewed-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: CAI Qian <caiqian@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: check bh in ext4_read_block_bitmap()
    
    commit 15b49132fc972c63894592f218ea5a9a61b1a18f upstream.
    
    Validate the bh pointer before using it, since
    ext4_read_block_bitmap_nowait() might return NULL.
    
    I've seen this in fsfuzz testing.
    
     EXT4-fs error (device loop0): ext4_read_block_bitmap_nowait:385: comm touch: Cannot get buffer for block bitmap - block_group = 0, block_bitmap = 3925999616
     BUG: unable to handle kernel NULL pointer dereference at           (null)
     IP: [<ffffffff8121de25>] ext4_wait_block_bitmap+0x25/0xe0
     ...
     Call Trace:
      [<ffffffff8121e1e5>] ext4_read_block_bitmap+0x35/0x60
      [<ffffffff8125e9c6>] ext4_free_blocks+0x236/0xb80
      [<ffffffff811d0d36>] ? __getblk+0x36/0x70
      [<ffffffff811d0a5f>] ? __find_get_block+0x8f/0x210
      [<ffffffff81191ef3>] ? kmem_cache_free+0x33/0x140
      [<ffffffff812678e5>] ext4_xattr_release_block+0x1b5/0x1d0
      [<ffffffff812679be>] ext4_xattr_delete_inode+0xbe/0x100
      [<ffffffff81222a7c>] ext4_free_inode+0x7c/0x4d0
      [<ffffffff812277b8>] ? ext4_mark_inode_dirty+0x88/0x230
      [<ffffffff8122993c>] ext4_evict_inode+0x32c/0x490
      [<ffffffff811b8cd7>] evict+0xa7/0x1c0
      [<ffffffff811b8ed3>] iput_final+0xe3/0x170
      [<ffffffff811b8f9e>] iput+0x3e/0x50
      [<ffffffff812316fd>] ext4_add_nondir+0x4d/0x90
      [<ffffffff81231d0b>] ext4_create+0xeb/0x170
      [<ffffffff811aae9c>] vfs_create+0xac/0xd0
      [<ffffffff811ac845>] lookup_open+0x185/0x1c0
      [<ffffffff8129e3b9>] ? selinux_inode_permission+0xa9/0x170
      [<ffffffff811acb54>] do_last+0x2d4/0x7a0
      [<ffffffff811af743>] path_openat+0xb3/0x480
      [<ffffffff8116a8a1>] ? handle_mm_fault+0x251/0x3b0
      [<ffffffff811afc49>] do_filp_open+0x49/0xa0
      [<ffffffff811bbaad>] ? __alloc_fd+0xdd/0x150
      [<ffffffff8119da28>] do_sys_open+0x108/0x1f0
      [<ffffffff8119db51>] sys_open+0x21/0x30
      [<ffffffff81618959>] system_call_fastpath+0x16/0x1b
    
    Also fix comment for ext4_read_block_bitmap_nowait()
    
    Signed-off-by: Eryu Guan <guaneryu@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix race in ext4_mb_add_n_trim()
    
    commit f1167009711032b0d747ec89a632a626c901a1ad upstream.
    
    In ext4_mb_add_n_trim(), lg_prealloc_lock should be taken when
    changing the lg_prealloc_list.
    
    Signed-off-by: Niu Yawei <yawei.niu@intel.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix xattr block allocation/release with bigalloc
    
    commit 1231b3a1eb5740192aeebf5344dd6d6da000febf upstream.
    
    Currently when new xattr block is created or released we we would call
    dquot_free_block() or dquot_alloc_block() respectively, among the else
    decrementing or incrementing the number of blocks assigned to the
    inode by one block.
    
    This however does not work for bigalloc file system because we always
    allocate/free the whole cluster so we have to count with that in
    dquot_free_block() and dquot_alloc_block() as well.
    
    Use the clusters-to-blocks conversion EXT4_C2B() when passing number of
    blocks to the dquot_alloc/free functions to fix the problem.
    
    The problem has been revealed by xfstests #117 (and possibly others).
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix free clusters calculation in bigalloc filesystem
    
    commit 304e220f0879198b1f5309ad6f0be862b4009491 upstream.
    
    ext4_has_free_clusters() should tell us whether there is enough free
    clusters to allocate, however number of free clusters in the file system
    is converted to blocks using EXT4_C2B() which is not only wrong use of
    the macro (we should have used EXT4_NUM_B2C) but it's also completely
    wrong concept since everything else is in cluster units.
    
    Moreover when calculating number of root clusters we should be using
    macro EXT4_NUM_B2C() instead of EXT4_B2C() otherwise the result might be
    off by one. However r_blocks_count should always be a multiple of the
    cluster ratio so doing a plain bit shift should be enough here. We
    avoid using EXT4_B2C() because it's confusing.
    
    As a result of the first problem number of free clusters is much bigger
    than it should have been and ext4_has_free_clusters() would return 1 even
    if there is really not enough free clusters available.
    
    Fix this by removing the EXT4_C2B() conversion of free clusters and
    using bit shift when calculating number of root clusters. This bug
    affects number of xfstests tests covering file system ENOSPC situation
    handling. With this patch most of the ENOSPC problems with bigalloc file
    system disappear, especially the errors caused by delayed allocation not
    having enough space when the actual allocation is finally requested.
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix the wrong number of the allocated blocks in ext4_split_extent()
    
    commit 3a2256702e47f68f921dfad41b1764d05c572329 upstream.
    
    This commit fixes a wrong return value of the number of the allocated
    blocks in ext4_split_extent.  When the length of blocks we want to
    allocate is greater than the length of the current extent, we return a
    wrong number.  Let's see what happens in the following case when we
    call ext4_split_extent().
    
      map: [48, 72]
      ex:  [32, 64, u]
    
    'ex' will be split into two parts:
      ex1: [32, 47, u]
      ex2: [48, 64, w]
    
    'map->m_len' is returned from this function, and the value is 24.  But
    the real length is 16.  So it should be fixed.
    
    Meanwhile in this commit we use right length of the allocated blocks
    when get_reserved_cluster_alloc in ext4_ext_handle_uninitialized_extents
    is called.
    
    Signed-off-by: Zheng Liu <wenqing.lz@taobao.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix data=journal fast mount/umount hang
    
    commit 2b405bfa84063bfa35621d2d6879f52693c614b0 upstream.
    
    In data=journal mode, if we unmount the file system before a
    transaction has a chance to complete, when the journal inode is being
    evicted, we can end up calling into jbd2_log_wait_commit() for the
    last transaction, after the journalling machinery has been shut down.
    
    Arguably we should adjust ext4_should_journal_data() to return FALSE
    for the journal inode, but the only place it matters is
    ext4_evict_inode(), and so to save a bit of CPU time, and to make the
    patch much more obviously correct by inspection(tm), we'll fix it by
    explicitly not trying to waiting for a journal commit when we are
    evicting the journal inode, since it's guaranteed to never succeed in
    this case.
    
    This can be easily replicated via:
    
         mount -t ext4 -o data=journal /dev/vdb /vdb ; umount /vdb
    
    ------------[ cut here ]------------
    WARNING: at /usr/projects/linux/ext4/fs/jbd2/journal.c:542 __jbd2_log_start_commit+0xba/0xcd()
    Hardware name: Bochs
    JBD2: bad log_start_commit: 3005630206 3005630206 0 0
    Modules linked in:
    Pid: 2909, comm: umount Not tainted 3.8.0-rc3 #1020
    Call Trace:
     [<c015c0ef>] warn_slowpath_common+0x68/0x7d
     [<c02b7e7d>] ? __jbd2_log_start_commit+0xba/0xcd
     [<c015c177>] warn_slowpath_fmt+0x2b/0x2f
     [<c02b7e7d>] __jbd2_log_start_commit+0xba/0xcd
     [<c02b8075>] jbd2_log_start_commit+0x24/0x34
     [<c0279ed5>] ext4_evict_inode+0x71/0x2e3
     [<c021f0ec>] evict+0x94/0x135
     [<c021f9aa>] iput+0x10a/0x110
     [<c02b7836>] jbd2_journal_destroy+0x190/0x1ce
     [<c0175284>] ? bit_waitqueue+0x50/0x50
     [<c028d23f>] ext4_put_super+0x52/0x294
     [<c020efe3>] generic_shutdown_super+0x48/0xb4
     [<c020f071>] kill_block_super+0x22/0x60
     [<c020f3e0>] deactivate_locked_super+0x22/0x49
     [<c020f5d6>] deactivate_super+0x30/0x33
     [<c0222795>] mntput_no_expire+0x107/0x10c
     [<c02233a7>] sys_umount+0x2cf/0x2e0
     [<c02233ca>] sys_oldumount+0x12/0x14
     [<c08096b8>] syscall_call+0x7/0xb
    ---[ end trace 6a954cc790501c1f ]---
    jbd2_log_wait_commit: error: j_commit_request=-1289337090, tid=0
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: convert number of blocks to clusters properly
    
    commit 810da240f221d64bf90020f25941b05b378186fe upstream.
    
    We're using macro EXT4_B2C() to convert number of blocks to number of
    clusters for bigalloc file systems.  However, we should be using
    EXT4_NUM_B2C().
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: CAI Qian <caiqian@redhat.com>
    Signed-off-by: Lingzhu Xiang <lxiang@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: use atomic64_t for the per-flexbg free_clusters count
    
    commit 90ba983f6889e65a3b506b30dc606aa9d1d46cd2 upstream.
    
    A user who was using a 8TB+ file system and with a very large flexbg
    size (> 65536) could cause the atomic_t used in the struct flex_groups
    to overflow.  This was detected by PaX security patchset:
    
    http://forums.grsecurity.net/viewtopic.php?f=3&t=3289&p=12551#p12551
    
    This bug was introduced in commit 9f24e4208f7e, so it's been around
    since 2.6.30.  :-(
    
    Fix this by using an atomic64_t for struct orlav_stats's
    free_clusters.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Lingzhu Xiang <lxiang@redhat.com>
    Reviewed-by: CAI Qian <caiqian@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix journal callback list traversal
    
    commit 5d3ee20855e28169d711b394857ee608a5023094 upstream.
    
    It is incorrect to use list_for_each_entry_safe() for journal callback
    traversial because ->next may be removed by other task:
    ->ext4_mb_free_metadata()
      ->ext4_mb_free_metadata()
        ->ext4_journal_callback_del()
    
    This results in the following issue:
    
    WARNING: at lib/list_debug.c:62 __list_del_entry+0x1c0/0x250()
    Hardware name:
    list_del corruption. prev->next should be ffff88019a4ec198, but was 6b6b6b6b6b6b6b6b
    Modules linked in: cpufreq_ondemand acpi_cpufreq freq_table mperf coretemp kvm_intel kvm crc32c_intel ghash_clmulni_intel microcode sg xhci_hcd button sd_mod crc_t10dif aesni_intel ablk_helper cryptd lrw aes_x86_64 xts gf128mul ahci libahci pata_acpi ata_generic dm_mirror dm_region_hash dm_log dm_mod
    Pid: 16400, comm: jbd2/dm-1-8 Tainted: G        W    3.8.0-rc3+ #107
    Call Trace:
     [<ffffffff8106fb0d>] warn_slowpath_common+0xad/0xf0
     [<ffffffff8106fc06>] warn_slowpath_fmt+0x46/0x50
     [<ffffffff813637e9>] ? ext4_journal_commit_callback+0x99/0xc0
     [<ffffffff8148cae0>] __list_del_entry+0x1c0/0x250
     [<ffffffff813637bf>] ext4_journal_commit_callback+0x6f/0xc0
     [<ffffffff813ca336>] jbd2_journal_commit_transaction+0x23a6/0x2570
     [<ffffffff8108aa42>] ? try_to_del_timer_sync+0x82/0xa0
     [<ffffffff8108b491>] ? del_timer_sync+0x91/0x1e0
     [<ffffffff813d3ecf>] kjournald2+0x19f/0x6a0
     [<ffffffff810ad630>] ? wake_up_bit+0x40/0x40
     [<ffffffff813d3d30>] ? bit_spin_lock+0x80/0x80
     [<ffffffff810ac6be>] kthread+0x10e/0x120
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
     [<ffffffff818ff6ac>] ret_from_fork+0x7c/0xb0
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
    
    This patch fix the issue as follows:
    - ext4_journal_commit_callback() make list truly traversial safe
      simply by always starting from list_head
    - fix race between two ext4_journal_callback_del() and
      ext4_journal_callback_try_del()
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix online resizing for ext3-compat file systems
    
    commit c5c72d814cf0f650010337c73638b25e6d14d2d4 upstream.
    
    Commit fb0a387dcdc restricts block allocations for indirect-mapped
    files to block groups less than s_blockfile_groups.  However, the
    online resizing code wasn't setting s_blockfile_groups, so the newly
    added block groups were not available for non-extent mapped files.
    
    Reported-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix Kconfig documentation for CONFIG_EXT4_DEBUG
    
    commit 7f3e3c7cfcec148ccca9c0dd2dbfd7b00b7ac10f upstream.
    
    Fox the Kconfig documentation for CONFIG_EXT4_DEBUG to match the
    change made by commit a0b30c1229: ext4: use module parameters instead
    of debugfs for mballoc_debug
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: add check for inodes_count overflow in new resize ioctl
    
    commit 3f8a6411fbada1fa482276591e037f3b1adcf55b upstream.
    
    Addresses-Red-Hat-Bugzilla: #913245
    
    Reported-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Lingzhu Xiang <lxiang@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: limit group search loop for non-extent files
    
    commit e6155736ad76b2070652745f9e54cdea3f0d8567 upstream.
    
    In the case where we are allocating for a non-extent file,
    we must limit the groups we allocate from to those below
    2^32 blocks, and ext4_mb_regular_allocator() attempts to
    do this initially by putting a cap on ngroups for the
    subsequent search loop.
    
    However, the initial target group comes in from the
    allocation context (ac), and it may already be beyond
    the artificially limited ngroups.  In this case,
    the limit
    
    	if (group == ngroups)
    		group = 0;
    
    at the top of the loop is never true, and the loop will
    run away.
    
    Catch this case inside the loop and reset the search to
    start at group 0.
    
    [sandeen@redhat.com: add commit msg & comments]
    
    Signed-off-by: Lachlan McIlroy <lmcilroy@redhat.com>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext3,ext4: don't mess with dir_file->f_pos in htree_dirblock_to_tree()
    
    commit 64cb927371cd2ec43758d8a094a003d27bc3d0dc upstream.
    
    Both ext3 and ext4 htree_dirblock_to_tree() is just filling the
    in-core rbtree for use by call_filldir().  All updates of ->f_pos are
    done by the latter; bumping it here (on error) is obviously wrong - we
    might very well have it nowhere near the block we'd found an error in.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix data offset overflow in ext4_xattr_fiemap() on 32-bit archs
    
    commit a60697f411eb365fb09e639e6f183fe33d1eb796 upstream.
    
    On 32-bit architectures with 32-bit sector_t computation of data offset
    in ext4_xattr_fiemap() can overflow resulting in reporting bogus data
    location. Fix the problem by typing block number to proper type before
    shifting.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix overflow when counting used blocks on 32-bit architectures
    
    commit 8af8eecc1331dbf5e8c662022272cf667e213da5 upstream.
    
    The arithmetics adding delalloc blocks to the number of used blocks in
    ext4_getattr() can easily overflow on 32-bit archs as we first multiply
    number of blocks by blocksize and then divide back by 512. Make the
    arithmetics more clever and also use proper type (unsigned long long
    instead of unsigned long).
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't allow ext4_free_blocks() to fail due to ENOMEM
    
    commit e7676a704ee0a1ef71a6b23760b5a8f6896cb1a1 upstream.
    
    The filesystem should not be marked inconsistent if ext4_free_blocks()
    is not able to allocate memory.  Unfortunately some callers (most
    notably ext4_truncate) don't have a way to reflect an error back up to
    the VFS.  And even if we did, most userspace applications won't deal
    with most system calls returning ENOMEM anyway.
    
    Reported-by: Nagachandra P <nagachandra@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: make sure group number is bumped after a inode allocation race
    
    commit a34eb503742fd25155fd6cff6163daacead9fbc3 upstream.
    
    When we try to allocate an inode, and there is a race between two
    CPU's trying to grab the same inode, _and_ this inode is the last free
    inode in the block group, make sure the group number is bumped before
    we continue searching the rest of the block groups.  Otherwise, we end
    up searching the current block group twice, and we end up skipping
    searching the last block group.  So in the unlikely situation where
    almost all of the inodes are allocated, it's possible that we will
    return ENOSPC even though there might be free inodes in that last
    block group.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix mount/remount error messages for incompatible mount options
    
    commit 6ae6514b33f941d3386da0dfbe2942766eab1577 upstream.
    
    Commit 5688978 ("ext4: improve handling of conflicting mount options")
    introduced incorrect messages shown while choosing wrong mount options.
    
    First of all, both cases of incorrect mount options,
    "data=journal,delalloc" and "data=journal,dioread_nolock" result in
    the same error message.
    
    Secondly, the problem above isn't solved for remount option: the
    mismatched parameter is simply ignored.  Moreover, ext4_msg states
    that remount with options "data=journal,delalloc" succeeded, which is
    not true.
    
    To fix it up, I added a simple check after parse_options() call to
    ensure that data=journal and delalloc/dioread_nolock parameters are
    not present at the same time.
    
    Signed-off-by: Piotr Sarna <p.sarna@partner.samsung.com>
    Acked-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    jbd2: Fix use after free after error in jbd2_journal_dirty_metadata()
    
    commit 91aa11fae1cf8c2fd67be0609692ea9741cdcc43 upstream.
    
    When jbd2_journal_dirty_metadata() returns error,
    __ext4_handle_dirty_metadata() stops the handle. However callers of this
    function do not count with that fact and still happily used now freed
    handle. This use after free can result in various issues but very likely
    we oops soon.
    
    The motivation of adding __ext4_journal_stop() into
    __ext4_handle_dirty_metadata() in commit 9ea7a0df seems to be only to
    improve error reporting. So replace __ext4_journal_stop() with
    ext4_journal_abort_handle() which was there before that commit and add
    WARN_ON_ONCE() to dump stack to provide useful information.
    
    Reported-by: Sage Weil <sage@inktank.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: Remove unused variable
    
    Change-Id: I7ffd1345e7a6acdd9f443efa8296938b8cc7f1e7
    
    jbd2: don't write superblock when if its empty
    
    commit eeecef0af5ea4efd763c9554cf2bd80fc4a0efd3 upstream.
    
    This sequence:
    
    results in an IO error when unmounting the RO filesystem:
    
    [  318.020828] Buffer I/O error on device loop1, logical block 196608
    [  318.027024] lost page write due to I/O error on loop1
    [  318.032088] JBD2: Error -5 detected when updating journal superblock for loop1-8.
    
    This was a regression introduced by commit 24bcc89c7e7c: "jbd2: split
    updating of journal superblock and marking journal empty".
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    jbd2: fix use after free in jbd2_journal_dirty_metadata()
    
    commit ad56edad089b56300fd13bb9eeb7d0424d978239 upstream.
    
    jbd2_journal_dirty_metadata() didn't get a reference to journal_head it
    was working with. This is OK in most of the cases since the journal head
    should be attached to a transaction but in rare occasions when we are
    journalling data, __ext4_journalled_writepage() can race with
    jbd2_journal_invalidatepage() stripping buffers from a page and thus
    journal head can be freed under hands of jbd2_journal_dirty_metadata().
    
    Fix the problem by getting own journal head reference in
    jbd2_journal_dirty_metadata() (and also in jbd2_journal_set_triggers()
    which can possibly have the same issue).
    
    Reported-by: Zheng Liu <gnehzuil.liu@gmail.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    jbd2: fix race between jbd2_journal_remove_checkpoint and ->j_commit_callback
    
    commit 794446c6946513c684d448205fbd76fa35f38b72 upstream.
    
    The following race is possible:
    
    [kjournald2]                              other_task
    jbd2_journal_commit_transaction()
      j_state = T_FINISHED;
      spin_unlock(&journal->j_list_lock);
                                             ->jbd2_journal_remove_checkpoint()
    					   ->jbd2_journal_free_transaction();
    					     ->kmem_cache_free(transaction)
      ->j_commit_callback(journal, transaction);
        -> USE_AFTER_FREE
    
    WARNING: at lib/list_debug.c:62 __list_del_entry+0x1c0/0x250()
    Hardware name:
    list_del corruption. prev->next should be ffff88019a4ec198, but was 6b6b6b6b6b6b6b6b
    Modules linked in: cpufreq_ondemand acpi_cpufreq freq_table mperf coretemp kvm_intel kvm crc32c_intel ghash_clmulni_intel microcode sg xhci_hcd button sd_mod crc_t10dif aesni_intel ablk_helper cryptd lrw aes_x86_64 xts gf128mul ahci libahci pata_acpi ata_generic dm_mirror dm_region_hash dm_log dm_mod
    Pid: 16400, comm: jbd2/dm-1-8 Tainted: G        W    3.8.0-rc3+ #107
    Call Trace:
     [<ffffffff8106fb0d>] warn_slowpath_common+0xad/0xf0
     [<ffffffff8106fc06>] warn_slowpath_fmt+0x46/0x50
     [<ffffffff813637e9>] ? ext4_journal_commit_callback+0x99/0xc0
     [<ffffffff8148cae0>] __list_del_entry+0x1c0/0x250
     [<ffffffff813637bf>] ext4_journal_commit_callback+0x6f/0xc0
     [<ffffffff813ca336>] jbd2_journal_commit_transaction+0x23a6/0x2570
     [<ffffffff8108aa42>] ? try_to_del_timer_sync+0x82/0xa0
     [<ffffffff8108b491>] ? del_timer_sync+0x91/0x1e0
     [<ffffffff813d3ecf>] kjournald2+0x19f/0x6a0
     [<ffffffff810ad630>] ? wake_up_bit+0x40/0x40
     [<ffffffff813d3d30>] ? bit_spin_lock+0x80/0x80
     [<ffffffff810ac6be>] kthread+0x10e/0x120
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
     [<ffffffff818ff6ac>] ret_from_fork+0x7c/0xb0
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
    
    In order to demonstrace this issue one should mount ext4 with mount -o
    discard option on SSD disk.  This makes callback longer and race
    window becomes wider.
    
    In order to fix this we should mark transaction as finished only after
    callbacks have completed
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    	fs/jbd2/commit.c
    
    jbd2: fix theoretical race in jbd2__journal_restart
    
    commit 39c04153fda8c32e85b51c96eb5511a326ad7609 upstream.
    
    Once we decrement transaction->t_updates, if this is the last handle
    holding the transaction from closing, and once we release the
    t_handle_lock spinlock, it's possible for the transaction to commit
    and be released.  In practice with normal kernels, this probably won't
    happen, since the commit happens in a separate kernel thread and it's
    unlikely this could all happen within the space of a few CPU cycles.
    
    On the other hand, with a real-time kernel, this could potentially
    happen, so save the tid found in transaction->t_tid before we release
    t_handle_lock.  It would require an insane configuration, such as one
    where the jbd2 thread was set to a very high real-time priority,
    perhaps because a high priority real-time thread is trying to read or
    write to a file system.  But some people who use real-time kernels
    have been known to do insane things, including controlling
    laser-wielding industrial robots.  :-)
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Change-Id: I1464ba68cf74daf1f7dc1be391c3b2469460bc97
    
    Conflicts:
    	fs/ext4/namei.c

commit 2a2af2d54b2a3aabe97b32aaa515d5c5b03dc2f2
Author: Joel King <joelking@codeaurora.org>
Date:   Wed Apr 17 08:40:50 2013 -0700

    msm: mdm2: Add delay between subsequent PS_HOLD for 8064 fusion3
    
    This feature was previously added for the 8960 sglte target,
    in commit 21274cf2219899ef9b46cf8b9fd8bf4ce52d057b, but it also
    applies to the 8064 sglte2 target. The original commit message
    follows.
    
    During SSR first a reset of external modem is issued
    and mdm_do_soft_power_on() toggles ap2mdm_soft_reset which
    in turn toogles the PS_HOLD. Then a part of SSR external
    modem is powered up and mdm_do_soft_power_on() again toggles
    the gpio. For PMIC register stabilization we need a 1sec delay
    between subsequent mdm_do_soft_power_on().
    By default the delay is 500msec, so adding another 500msec for
    stablization.
    
    Signed-off-by: Joel King <joelking@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/board-8064.c
    
    Change-Id: I8ac22d94205cb309dd5ba4b1f3a20933632298db

commit a7b6bd2bc7f8482fd9cfb84998b6f7973ae9f10e
Author: Arne Coucheron <arco68@gmail.com>
Date:   Fri Sep 27 16:53:10 2013 +0200

    defconfig: Fix selinux typos
    
    Change-Id: I9b3e5b9ac50449a6190b1876cde912d7174b7023

commit a09f37144a251488723575524c3fc51ba86e8f68
Author: Peter Boonstoppel <pboonstoppel@nvidia.com>
Date:   Thu Aug 9 15:34:47 2012 -0700

    sched: Unthrottle rt runqueues in __disable_runtime()
    
    migrate_tasks() uses _pick_next_task_rt() to get tasks from the
    real-time runqueues to be migrated. When rt_rq is throttled
    _pick_next_task_rt() won't return anything, in which case
    migrate_tasks() can't move all threads over and gets stuck in an
    infinite loop.
    
    Instead unthrottle rt runqueues before migrating tasks.
    
    Additionally: move unthrottle_offline_cfs_rqs() to rq_offline_fair()
    
    Change-Id: If8a4a399f1a14b7f4789c1b205dcfadbde555214
    Signed-off-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/5FBF8E85CA34454794F0F7ECBA79798F379D3648B7@HQMAIL04.nvidia.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Git-commit: a4c96ae319b8047f62dedbe1eac79e321c185749
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
    
    Conflicts:
    	kernel/sched/core.c
    	kernel/sched/sched.h

commit b281c44c0444ece9426db8ef99a0943e2cad4d27
Author: Rohit Vaswani <rvaswani@codeaurora.org>
Date:   Fri Jan 18 19:11:43 2013 -0800

    ARM: Drop VCM framework
    
    This framework wasn't accepted upstream and is not used. Drop it.
    
    Change-Id: Ieb381a679873cbfb4baf245a5bcb8df1c730d964
    Signed-off-by: Rohit Vaswani <rvaswani@codeaurora.org>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 9e3d7ad5ae92c0872cfa6addba52d4feb3de9c1d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jul 5 13:11:31 2012 +0100

    ARM: fix warning caused by wrongly typed arm_dma_limit
    
    arch/arm/mm/init.c: In function 'arm_memblock_init':
    arch/arm/mm/init.c:380: warning: comparison of distinct pointer types lacks a cast
    
    by fixing the typecast in its definition when DMA_ZONE is disabled.
    This was missed in 4986e5c7c (ARM: mm: fix type of the arm_dma_limit
    global variable).
    
    Change-Id: Id076f2bebe307609265afdd4229181d2004c5f9c
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 35de0a40c916d771b43228ebdb0047292db02b86
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Apr 19 17:35:51 2013 -0700

    Revert "Revert "ARM: 7169/1: topdown mmap support""
    
    This reverts commit f27e4f0e730b99ca4dabed0b408d96dbf73a8fac.
    
    An alternate fix has been proposed where userspace programs
    set ADDR_COMPAT_LAYOUT. Bring back the topdown mmap support.
    
    Change-Id: Ibd6e74d406db3a5ddb609e2d2a7a6e9dc2080eca
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	arch/arm/include/asm/pgtable.h

commit deeb7bc9e09f6e91ace4396f64d688b123daed0e
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Fri Mar 8 15:11:03 2013 -0800

    ARM: mm: Remove SW emulation for ARM domain manager
    
    8x50 is no longer supported in the msm-3.4 kernel, so remove this
    feature.
    
    Change-Id: I2156ef22cca82d3cce6a7d39e366b93cab32f811
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 49caf8772109e6ca5fda8158f4d50268f84cd2f2
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Mon Mar 25 23:37:45 2013 +0530

    msm: pm: avoid array overrun for msm_pm_sleep_modes
    
    The array is not defined for the maximum possible size.
    So it is possible that the array may overrun when it is
    dereferenced using MSM_PM_MODE macro.
    
    Add a dummy entry to make sure array is of max size.
    
    Change-Id: I5783b0aa6c8295c5c5aabcb498700c6af1a3eba5
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit fa4d83b2ea8d946e9646201e66e0b84c5f264e17
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Thu Aug 8 11:59:23 2013 -0700

    msm: cpufreq: Update frequency index
    
    A deprecated api can be used to limit the minimum and maximum cpu speeds.
    Another driver temporarily needs to switch back to this api, instead of
    using the new version. Make this api functional again by ensuring a stale
    value is updated properly.
    
    Change-Id: Ia63772e43ec534cf2bb58e7627a87315dae5d891
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 8bf62a7151c2fc59e7c1d5bc525d07277541780a
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Sat Jun 8 04:03:32 2013 -0700

    msm: cpufreq: Add support for CPU clocks and msm-cpufreq device
    
    Add support for using clock APIs to do CPU/L2 frequency scaling. When clock
    APIs are used to control CPU/L2 frequencies, the cpufreq driver must handle
    the CPU -> L2 -> DDR bandwidth voting. Use a msm-cpufreq device to provide
    the per SoC list of CPU frequencies and their L2/DDR bandwidth mapping
    information.
    
    If CPU/L2 clocks are not available, fallback to using acpuclock APIs. We
    eventually want to remove the use of non-standard acpuclock APIs.
    
    Change-Id: I2c4e2c3967d73e8cdbd9833f3cb36f3d75e27b4a
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/cpufreq.c

commit 8523247cdb7d4951392edc0970af256ed7550a30
Author: Pratik Patel <pratikp@codeaurora.org>
Date:   Thu Jun 13 23:21:40 2013 -0700

    coresight: add jtag fuse driver
    
    Add support for JTag Fuse driver which can be used by other
    JTag save-restore driver(s) to query the state of the jtag fuses to
    determine if the Hardware they manage is functionally disabled or
    not.
    
    Drivers can then take necessary actions like failing the probe if
    the Hardware they manage is functionally disabled.
    
    Change-Id: Ie8c0dc159e52cf869d3ed63b45e5332d3e380e6d
    Signed-off-by: Pratik Patel <pratikp@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/Makefile

commit f4a8fec782a759578964cbe7bbad2def9f7131aa
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 27 23:25:08 2013 -0700

    underp
    
    Change-Id: I1345b31ae45e22a9df3bb3be2b02d4f259a16b28

commit e360fbafd12efb34479278ff020bf9957df29380
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 27 23:17:20 2013 -0700

    Fix compilation issues
    
    Change-Id: I0f1bef2ff96a7989a3524af57f3f3064e4cd3c25

commit 5d4b6861542fa5b1aeb29c22b92bec3c6ae3d1b6
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Apr 13 12:32:09 2012 +0200

    mm: vmalloc: use const void * for caller argument
    
    'const void *' is a safer type for caller function type. This patch
    updates all references to caller function type.
    
    Change-Id: If950cfcfc63911756ac3709c8bf6da10c8b98f1b
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Git-commit: 5e6cafc83e30f0f70c79a2b7aef237dc57e29f02
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 7a1538dcde53304d9829f5ece28db7adb72aab21
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Jul 30 09:11:33 2012 +0200

    ARM: dma-mapping: remove custom consistent dma region
    
    This patch changes dma-mapping subsystem to use generic vmalloc areas
    for all consistent dma allocations. This increases the total size limit
    of the consistent allocations and removes platform hacks and a lot of
    duplicated code.
    
    Atomic allocations are served from special pool preallocated on boot,
    because vmalloc areas cannot be reliably created in atomic context.
    
    Change-Id: Ibb2230e80249598a81122083bf3fa2f050a0a71e
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Git-commit: e9da6e9905e639b0f842a244bc770b48ad0523e9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixups and tweaking of some prototypes]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	arch/arm/mm/dma-mapping.c

commit b4a0a5afad809bccebba7d14ad600bd83342684a
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Tue Dec 11 09:22:53 2012 -0800

    msm: Kconfig: Disable SPARSEMEM for A-Family targets
    
    SPARSEMEM is no longer needed since the features that required it
    (memory hotplug, for example) have been superseded by other
    solutions. Remove it since it introduces overhead.
    
    CRs-Fixed: 430996
    Change-Id: I25ff8591dae1e48b5b0bf8a0669196a6d7d0cd85
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit afe8c94e95c5f1cb6812030239c37ae125026853
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Wed Jun 19 11:17:41 2013 -0700

    iommu: msm: Handle unmapping of PTE properly
    
    The unmap api is currently not handling unmapping of page table
    entries (PTE) properly. The generic function that calls the msm
    unmap API expects the unmap call to unmap as much as possible
    and then return the amount that was unmapped.
    In addition the unmap function does not support an arbitrary input
    length. However, the function that calls the msm unmap function
    assumes that this is supported.
    
    Both these issues can cause mappings to not be unmapped which will
    cause subsequent mappings to fail because the mapping already exists.
    
    Change-Id: I638d5c38673abe297a701de9b7209c962564e1f1
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit fa8ca70a190644903bd471b209d694bfbb999833
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Fri Apr 19 13:45:03 2013 -0600

    iommu: msm: prevent partial mappings on error
    
    If msm_iommu_map_range() fails mid way through the va
    range with an error, clean up the PTEs that have already
    been created so they are not leaked.
    
    Change-Id: Ie929343cd6e36cade7b2cc9b4b4408c3453e6b5f
    CRs-Fixed: 478304
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit a940934252b75594b74a79fe35dd9d7282087357
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Apr 22 10:01:55 2013 -0700

    iommu: msm: Don't treat address 0 as an error case
    
    Currently, the iommu page table code treats a scattergather
    list with physical address 0 as an error. This may not be
    correct in all cases. Physical address 0 is a valid part
    of the system and may be used for valid page allocations.
    Nothing else in the system checks for physical address 0
    for error so don't treat it as an error.
    
    Change-Id: Ie9f0dae9dace4fff3b1c3449bc89c3afdd2e63a0
    CRs-Fixed: 478304
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 7f644027882d4458d45952cb6bda363dbee1dae0
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Mon Jan 21 14:09:15 2013 -0700

    iommu: msm: check range before mapping for IOMMU-v1
    
    Make sure iommu_map_range() does not leave a partial
    mapping on error if part of the range is already mapped.
    
    Change-Id: I108b45ce8935b73ecb65f375930fe5e00b8d91eb
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit e11c1deb87a0d79eab7296e78372ac1978968de9
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri Mar 8 10:50:48 2013 -0800

    iommu: msm: Use phys_addr_t for physical addresses
    
    IOMMU map and unmap function should be using phys_addr_t
    instead of unsigned int which will not work properly with
    LPAE.
    
    Change-Id: I22b31b4f13a27c0280b0d88643a8a30d019e6e90
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 3edd53b6636b767c25ac588d475e482b246836ec
Author: Kevin Matlage <kmatlage@codeaurora.org>
Date:   Fri Feb 1 12:41:04 2013 -0700

    iommu: msm: Let IOMMUv1 use all possible page sizes
    
    Allow the IOMMUv1 to use 16M, 1M, 64K or 4K iommu
    pages when physical and virtual addresses are
    appropriately aligned. This can reduce TLB misses
    when large buffers are mapped.
    
    Change-Id: Iffcaa04097fc3877962f3954d73a6ba448dca20b
    Signed-off-by: Kevin Matlage <kmatlage@codeaurora.org>

commit 18c4a8bf5ec37ffff91ae78dfe2aea1d6c52d87b
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Tue Jan 15 15:38:52 2013 -0800

    msm: iommu: re-use existing buffers for `extra' mappings
    
    There is a bug on some hardware that requires us to overmap Iommu
    mappings by 2x. Currently, we set aside a dummy buffer onto which we
    map all of these dummy mappings. In general, for large mappings it's
    nice to use the more efficient iommu_map_range instead of calling
    iommu_map repeatedly. However, with our current approach in
    msm_iommu_map_extra we can't use iommu_map_range for page_sizes larger
    than the dummy buffer. To avoid wasting memory by increasing the size
    of the dummy buffer, we can simply remap on top of the the buffer
    being mapped in the first place. Since the second mapping should never
    be used (besides by the buggy hardware) this should not be a problem.
    
    Re-use existing buffers for all `extra' mappings. Essentially, map the
    same physical address range twice. To be extra safe, make the second
    mapping read-only.
    
    Change-Id: I35462ad50de8da1f2befa3e3f0895925535cdc98
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit 13fb96c1a460c9aea04a2e38830c69b9afc4cd16
Author: Chintan Pandya <cpandya@codeaurora.org>
Date:   Tue Jan 29 19:40:01 2013 +0530

    ion: cma: Add debug heap ops for CMA heap
    
    For tracking CMA allocation by address and fragmentation
    (if any), add debug heap ops which gives buffer allocation
    info.
    
    Change-Id: Ia8bed38034b85b2d4dcf84811a348bbbe50dc16b
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit 6769af47291a5bb5d0fb680d54756d192dc9e50d
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Apr 9 10:48:22 2013 -0700

    lib: memory_alloc: Support 64-bit physical addresses
    
    The current memory allocation code does not support physical
    addresses above 32-bits, meaning that LPAE is not supported.
    Change the types of the code internally to support 64-bit
    physical addresses. Full 64-bit virtual addresses may not be
    fully supported so this code will still need an update when
    that happens.
    
    Change-Id: I0c4a7b76784981ffcd7f4776d51e05f3592bb7b8
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 5814681028a0a83597461ab77e325053d51fecb4
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Apr 8 14:43:14 2013 -0700

    lib: genalloc: Use 64 bit types for holding allocations
    
    genalloc may be used to allocate physical addresses in addition
    to virtual addresses. On LPAE based systems, physical addresses
    are 64-bit which does not fit in an unsigned long. Change the
    type used for allocation internally to be 64 bit to ensure that
    genalloc can properly allocate physical addresses > 4GB.
    
    Ideally this will just be a temporary workaround until either
    a proper fix comes elsewhere or an alternative API is used to
    manage physical memory.
    
    Change-Id: Ib10b887730e0c6916de5d1e6f77e771c6cde14bb
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit eeb24b54b7f3a57e475bee7d4750f41049f37d33
Author: Stepan Moskovchenko <stepanm@codeaurora.org>
Date:   Mon Jan 21 13:27:25 2013 -0800

    lib: vsprintf: Add %pa format specifier for phys_addr_t types
    
    Add the %pa format specifier for printing a phys_addr_t
    type and its derivative types (such as resource_size_t),
    since the physical address size on some platforms can vary
    based on build options, regardless of the native integer
    type.
    
    Change-Id: I2ba0003d689a9a2bd13f1a1e2d897b6eacc5d224
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 13d03dd1d55e0c9b4d7663f9845048ccd4f34e47
Author: Abhijeet Dharmapurikar <adharmap@codeaurora.org>
Date:   Wed Aug 7 20:17:07 2013 -0700

    irq: display the wakeup depth and disable depth of an irq
    
    The disable depth helps to track whether the interrupt is disabled
    of not. This is immensely useful in debugging interrupt related issues
    esp in cases when the driver enables/disables a device's
    interrupt at runtime.
    
    Also, the wakeup depth helps to track which interrupts are configured
    as wakeup. This helps to debug issues related to an interrupt not waking
    up the system if it were configured a wakeup interrupt.
    
    Add code to show the wakeup depth and disable depth.
    
    Change-Id: I780a8f7dd24d4acfb2da98761c873dd09a154a2e
    Signed-off-by: Abhijeet Dharmapurikar <adharmap@codeaurora.org>

commit 5d30ef515f47d1af9a5d293657033b767adc8e03
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Tue Apr 23 08:54:33 2013 +0000

    cpuidle: make a single register function for all
    
    The usual scheme to initialize a cpuidle driver on a SMP is:
    
    	cpuidle_register_driver(drv);
    	for_each_possible_cpu(cpu) {
    		device = &per_cpu(cpuidle_dev, cpu);
    		cpuidle_register_device(device);
    	}
    
    This code is duplicated in each cpuidle driver.
    
    On UP systems, it is done this way:
    
    	cpuidle_register_driver(drv);
    	device = &per_cpu(cpuidle_dev, cpu);
    	cpuidle_register_device(device);
    
    On UP, the macro 'for_each_cpu' does one iteration:
    
            for ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask)
    
    Hence, the initialization loop is the same for UP than SMP.
    
    Beside, we saw different bugs / mis-initialization / return code unchecked in
    the different drivers, the code is duplicated including bugs. After fixing all
    these ones, it appears the initialization pattern is the same for everyone.
    
    Please note, some drivers are doing dev->state_count = drv->state_count. This is
    not necessary because it is done by the cpuidle_enable_device function in the
    cpuidle framework. This is true, until you have the same states for all your
    devices. Otherwise, the 'low level' API should be used instead with the specific
    initialization for the driver.
    
    Let's add a wrapper function doing this initialization with a cpumask parameter
    for the coupled idle states and use it for all the drivers.
    
    That will save a lot of LOC, consolidate the code, and the modifications in the
    future could be done in a single place. Another benefit is the consolidation of
    the cpuidle_device variable which is now in the cpuidle framework and no longer
    spread accross the different arch specific drivers.
    
    Change-Id: I8f75188d5770871620f758c2973b74384ac570d5
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Karthik Parsha <kparsha@codeaurora.org>
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>

commit 1d9f966d05eb76938d4f95fadcda7ec85d7dd692
Author: Taniya Das <tdas@codeaurora.org>
Date:   Fri Aug 24 20:15:24 2012 +0530

    ARM: fiq: Add fiq_set_type to configure lines for FIQ
    
    The lines to be used as FIQ needs to be configured to be
    set for the type as EGDE/LEVEL.
    
    Change-Id: Iee746662ecb3e36141aabfa5c5e9813144a836ad
    Signed-off-by: Taniya Das <tdas@codeaurora.org>

commit fe60f5f381a84260d9c3266a15354e5ab787ee69
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Apr 10 13:23:27 2013 -0700

    arm: arch_timer: Allow mmio and cp15 timers to coexist
    
    We need to support both instances of the architected timers at
    runtime so that we can register an mmio timer as the broadcast
    device. We also need to mark the cp15 timers as FEAT_C3_STOP
    because they lost their state in certain low power modes.
    
    Get rid of the pointer indirection and just have multiple
    functions for the mmio and cp15 cases. We also assume that the
    first frame is always available to us to simplify the logic. This
    should be good enough for us for now.
    
    Change-Id: I96f529744f9fa47c04efaf716e475267bae7043c
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 91e6c6fd0d215e09926ab0524da3eb38af8dd5ec
Author: Abhimanyu Kapur <abhimany@codeaurora.org>
Date:   Tue Mar 26 16:26:49 2013 -0700

    ARM: architected timers: remove percpu_timer_setup
    
    A call to percpu_timer_setup is not needed as its done
    by the smp routines. This also leads to a compilation
    failure on non-SMP targets.
    
    Change-Id: I261f9c3df659f469de883263bf5f8e2a768a6b1d
    Signed-off-by: Abhimanyu Kapur <abhimany@codeaurora.org>

commit 8991838d376a34d826a7b5a3d99e1aa36e829001
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jan 20 10:47:00 2012 +0000

    ARM: architected timers: add support for UP timer
    
    If CONFIG_LOCAL_TIMERS is not defined, let the architected timer
    driver register a single clock_event_device that is used as a
    global timer.
    
    Change-Id: I28f8380ef88f16739c9f75ca21ecccb8e6241711
    Git-commit: 273d16adbccdfe3e4a9d02d286b8f1d76dc9e63f
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Abhimanyu Kapur <abhimany@codeaurora.org>

commit 8ce7956f5c9045398fffedbf6fcb677fd1bd335b
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Thu Jul 26 12:00:03 2012 -0700

    arm: Move to upstream udelay via timer implementation
    
    This is a squash of a handful of changes and reverts of the
    Qualcomm specific implementation:
    
      Revert "arm: Implement a timer based __delay() loop"
    
      This reverts commit 976eafa8b18252876e15f861944acf693b07ce7e.
    
      Revert "arm: Allow machines to override __delay()"
    
      This reverts commit bc0ef8ab167272890f1aab62928b04a9aeb87ce9.
    
      Revert "arm: Translate delay.S into (mostly) C"
    
      This reverts commit 8d5868d8205d10a0a8e423f53e9cc9bb3e9d1a34.
    
      ARM: 7451/1: arch timer: implement read_current_timer and get_cycles
    
      This patch implements read_current_timer using the architected timers
      when they are selected via CONFIG_ARM_ARCH_TIMER. If they are detected
      not to be usable at runtime, we return -ENXIO to the caller.
    
      Furthermore, if read_current_timer is exported then we can implement
      get_cycles in terms of it for use as both an entropy source and for
      implementing __udelay and friends.
    
      Tested-by: Shinya Kuribayashi <shinya.kuribayashi.px@renesas.com>
      Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
      Signed-off-by: Will Deacon <will.deacon@arm.com>
      Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
      ARM: 7452/1: delay: allow timer-based delay implementation to be
      selected
    
      This patch allows a timer-based delay implementation to be selected by
      switching the delay routines over to use get_cycles, which is
      implemented in terms of read_current_timer. This further allows us to
      skip the loop calibration and have a consistent delay function in the
      face of core frequency scaling.
    
      To avoid the pain of dealing with memory-mapped counters, this
      implementation uses the co-processor interface to the architected timers
      when they are available. The previous loop-based implementation is
      kept around for CPUs without the architected timers and we retain both
      the maximum delay (2ms) and the corresponding conversion factors for
      determining the number of loops required for a given interval. Since the
      indirection of the timer routines will only work when called from C,
      the sa1100 sleep routines are modified to branch to the loop-based delay
      functions directly.
    
      Tested-by: Shinya Kuribayashi <shinya.kuribayashi.px@renesas.com>
      Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
      Signed-off-by: Will Deacon <will.deacon@arm.com>
      Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
      ARM: delay: set loops_per_jiffy when moving to timer-based loop
    
      The delay functions may be called by some platforms between switching to
      the timer-based delay loop but before calibration. In this case, the
      initial loops_per_jiffy may not be suitable for the timer (although a
      compromise may be achievable) and delay times may be considered too
      inaccurate.
    
      This patch updates loops_per_jiffy when switching to the timer-based
      delay loop so that delays are consistent prior to calibration.
    
      Signed-off-by: Will Deacon <will.deacon@arm.com>
    
      ARM: delay: add registration mechanism for delay timer sources
    
      The current timer-based delay loop relies on the architected timer to
      initiate the switch away from the polling-based implementation. This is
      unfortunate for platforms without the architected timers but with a
      suitable delay source (that is, constant frequency, always powered-up
      and ticking as long as the CPUs are online).
    
      This patch introduces a registration mechanism for the delay timer
      (which provides an unconditional read_current_timer implementation) and
      updates the architected timer code to use the new interface.
    
      Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
      Signed-off-by: Will Deacon <will.deacon@arm.com>
    
      ARM: export default read_current_timer
    
      read_current_timer is used by get_cycles since "ARM: 7538/1: delay:
      add registration mechanism for delay timer sources", and get_cycles
      can be used by device drivers in loadable modules, so it has to
      be exported.
    
      Without this patch, building imote2_defconfig fails with
    
      ERROR: "read_current_timer" [crypto/tcrypt.ko] undefined!
    
      Signed-off-by: Arnd Bergmann <arnd@arndb.de>
      Cc: Stephen Boyd <sboyd@codeaurora.org>
      Cc: Jonathan Austin <jonathan.austin@arm.com>
      Cc: Will Deacon <will.deacon@arm.com>
      Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Change-Id: If1ad095d6852f5966ea995856103e06de6ab2f59
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    
    Conflicts:
    	arch/arm/lib/delay.c

commit d1897698ab3e02fbfaac02ff88831aec86f0e02d
Author: Neil Leeder <nleeder@codeaurora.org>
Date:   Thu May 23 16:05:41 2013 -0400

    msm: arm: make nohlt readable
    
    Make the debugfs node 'nohlt' readable.
    
    It is a common use case to want to turn nohlt
    on or off, but there is no way to see the current state.
    Writing to nohlt increments or decrements its reference
    counter, and does not absolutely set the state. By making
    it readable, its current state can be checked so that
    it can be determined if a write is needed to change the
    state.
    
    Change-Id: I08e327808299aef3e125f04bff8b6aad28d020cc
    Signed-off-by: Neil Leeder <nleeder@codeaurora.org>

commit 1d1cf7b11da077f67ee2a90c92a93fae474d2f53
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Mon Jun 17 15:40:58 2013 -0700

    ARM: sched_clock: Load cycle count after epoch stabilizes
    
    There is a small race between when the cycle count is read from
    the hardware and when the epoch stabilizes. Consider this
    scenario:
    
     CPU0                           CPU1
     ----                           ----
     cyc = read_sched_clock()
     cyc_to_sched_clock()
                                     update_sched_clock()
                                      ...
                                      cd.epoch_cyc = cyc;
      epoch_cyc = cd.epoch_cyc;
      ...
      epoch_ns + cyc_to_ns((cyc - epoch_cyc)
    
    The cyc on cpu0 was read before the epoch changed. But we
    calculate the nanoseconds based on the new epoch by subtracting
    the new epoch from the old cycle count. Since epoch is most likely
    larger than the old cycle count we calculate a large number that
    will be converted to nanoseconds and added to epoch_ns, causing
    time to jump forward too much.
    
    Fix this problem by reading the hardware after the epoch has
    stabilized.
    
    Change-Id: I995133b229b2c2fedd5091406d1dc366d8bfff7b
    Cc: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Git-commit: 336ae1180df5f69b9e0fb6561bec01c5f64361cf
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [sboyd: reworked for file movement kernel/time -> arm/kernel]
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 10a65c67fb95c152840f0ea5d447d12b11746a50
Author: Felipe Balbi 2 <balbi@ti.com>
Date:   Tue Oct 23 19:00:03 2012 +0100

    ARM: 7565/1: sched: stop sched_clock() during suspend
    
    The scheduler imposes a requirement to sched_clock()
    which is to stop the clock during suspend, if we don't
    do that any RT thread will be rescheduled in the future
    which might cause any sort of problems.
    
    This became an issue on OMAP when we converted omap-i2c.c
    to use threaded IRQs, it turned out that depending on how
    much time we spent on suspend, the I2C IRQ thread would
    end up being rescheduled so far in the future that I2C
    transfers would timeout and, because omap_hsmmc depends
    on an I2C-connected device to detect if an MMC card is
    inserted in the slot, our rootfs would just vanish.
    
    arch/arm/kernel/sched_clock.c already had an optional
    implementation (sched_clock_needs_suspend()) which would
    handle scheduler's requirement properly, what this patch
    does is simply to make that implementation non-optional.
    
    Note that this has the side-effect that printk timings
    won't reflect the actual time spent on suspend so other
    methods to measure that will have to be used.
    
    This has been tested with beagleboard XM (OMAP3630) and
    pandaboard rev A3 (OMAP4430). Suspend to RAM is now working
    after this patch.
    
    Thanks to Kevin Hilman for helping out with debugging.
    
    Change-Id: Ie2f9e3b22eb3d1f3806cf8c598f22e2fa1b8651f
    Acked-by: Kevin Hilman <khilman@ti.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 6a4dae5e138a32b45ca5218cc2b81802f9d378c3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 81b130c8eca1b35357e9306ad9ee0d3eb490ebfe
Author: Colin Cross <ccross@android.com>
Date:   Tue Aug 7 19:05:10 2012 +0100

    ARM: 7486/1: sched_clock: update epoch_cyc on resume
    
    Many clocks that are used to provide sched_clock will reset during
    suspend.  If read_sched_clock returns 0 after suspend, sched_clock will
    appear to jump forward.  This patch resets cd.epoch_cyc to the current
    value of read_sched_clock during resume, which causes sched_clock() just
    after suspend to return the same value as sched_clock() just before
    suspend.
    
    In addition, during the window where epoch_ns has been updated before
    suspend, but epoch_cyc has not been updated after suspend, it is unknown
    whether the clock has reset or not, and sched_clock() could return a
    bogus value.  Add a suspended flag, and return the pre-suspend epoch_ns
    value during this period.
    
    The new behavior is triggered by calling setup_sched_clock_needs_suspend
    instead of setup_sched_clock.
    
    Change-Id: I7441ef74dc6802c00eea61f3b8c0a25ac00a724d
    Signed-off-by: Colin Cross <ccross@android.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 237ec6f2e51d2fc2ff37c7c5f1ccc9264d09c85b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 3510b9c07533977b539e61f5d94e6a152ead3750
Author: Peter Maydell <peter.maydell@linaro.org>
Date:   Thu Jul 12 23:57:35 2012 +0100

    ARM: 7465/1: Handle >4GB memory sizes in device tree and mem=size@start option
    
    The memory regions which are passed to arm_add_memory() from
    device tree blobs via early_init_dt_add_memory_arch() can
    have sizes which are larger than will fit in a 32 bit integer,
    so switch to using a phys_addr_t to hold them, to avoid
    silently dropping the top 32 bits of the size. Similarly, use
    phys_addr_t in early_mem() so that mem=size@start command line
    options specifying more than 4GB behave sensibly.
    
    Change-Id: I8ea0fa31904903aa78750dadb2e830fa5c6deb1b
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Maydell <peter.maydell@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Git-commit: a5d5f7daa744b34477c4a12728bde0a1694a1707
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 295418c83d142d226318f3237d35fb40d9103345
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Aug 25 13:23:48 2013 -0700

    dma-contiguous: Fix warning when device tree is disabled
    
    Change-Id: I4ed7b4a92e257919d5edae16e22e92cff4317241

commit 91731aeb862c0567b6ddf5f26bf645cf2d90d5b5
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jun 12 09:38:43 2013 -0700

    arm: Add definitions for pte_mkexec/pte_mknexec
    
    Other architectures define pte_mkexec to mark a pte as executable.
    Add pte_mkexec for ARM to get the same functionality. Although no
    other architectures currently define it, also add pte_mknexec to
    explicitly allow a pte to be marked as non executable.
    
    Change-Id: I673b48a1d93bf00682edbb3609a478eb28eb67a1
    CRs-Fixed: 498398
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit fff5fbdbf57a2b621d41184d370292e7217f69e4
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Aug 25 13:19:23 2013 -0700

    arm: mm: Fix a merge issue
    
    Change-Id: Ia8eb75606ed6659b4800290372cba0709d7915cf

commit 0c1509774b906d485c5aee9212154da132428111
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Jun 6 12:05:01 2012 +0200

    ARM: mm: fix type of the arm_dma_limit global variable
    
    arm_dma_limit stores physical address of maximal address accessible by DMA,
    so the phys_addr_t type makes much more sense for it instead of u32. This
    patch fixes the following build warning:
    
    arch/arm/mm/init.c:380: warning: comparison of distinct pointer types lacks a cast
    
    Change-Id: Iaba492cb9d35692b0830239b044f9c4e32d7ac16
    Reported-by: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 367fcb5a6b4eade32594095cbb3de37a0762404c
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Thu May 9 16:11:45 2013 -0700

    msm: Update meminfo to reflect consecutive memory holes
    
    The device tree now has a node for each of the memory regions
    belonging to the subsystems. The meminfo blocks are updated to
    reflect these memory holes in such a way that every meminfo block
    is of a non-zero size. The start and end of the largest memory
    hole is calculated using the meminfo.
    
    Change-Id: I9eb9984f9d74cf2a493846cb2ae7f83db9f92719
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit fd7c4317cdace8023e0b35dcbdc383e57147801b
Author: Larry Bassel <lbassel@codeaurora.org>
Date:   Mon Feb 25 10:54:16 2013 -0800

    msm: correctly handle multiple memory holes
    
    The function adjust_meminfo() did not correctly set the
    size of any meminfo which was beyond the second memory hole.
    
    Change-Id: I5bf6c5b30f7657ce9b975c9b4afb8fb3b2ce343f
    CRs-Fixed: 455187
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit 1da7075c4c230ff249b617ca7b2487b6ddceb6f4
Author: Greg Reid <greid@codeaurora.org>
Date:   Fri Oct 12 12:20:31 2012 -0400

    kernel: gtod: Add MSM-specific user-accessible timers
    
    Enable MSM-specific timers to be readable from user-space.
    This allows implementation of a higher-performance
    gettimeofday in user-space.
    
    Change-Id: I1f322b5396ee335b10aeb81c681593621d151176
    Signed-off-by: Brent DeGraaf <bdegraaf@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/Kconfig

commit 79d243aa17446c7117af17fb8a7d60b866678699
Author: Stepan Moskovchenko <stepanm@codeaurora.org>
Date:   Thu Sep 27 13:25:23 2012 -0700

    ARM: proc: Add Krait proc info
    
    Add processor info for the Qualcomm, Inc. Krait family of
    processors, to use the generic ARMv7 initialisation
    procedure but explicitly enable the IDIV hardware
    capability flag.
    
    Change-Id: I080f60189fada2e4f87c79c6b8ef522cdc070759
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit d45243edae9c5c759acd5d0b11b6f24463a07b20
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Jun 17 10:53:05 2013 -0700

    drivers: Add option to reserve default CMA region
    
    CMA provides good utilization of memory but for some use cases, the
    allocation time is too costly. Add a Kconfig option to allow the
    default region to be permanently set aside for contiguous use cases.
    
    Change-Id: I1eef508d37cf6ae3b7b7a652fc59391b186fc122
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 65a8306aa4b05d53abc04f2d0964c77d72fb943d
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Apr 26 15:51:06 2013 -0700

    cma: Allow option to use strict memblock_reserved memory
    
    Despite all the performance optimizations, some clients are
    still unable to use CMA because of the allocation latency.
    Rather than make those clients use a separate set of APIs,
    extend the CMA code to allow clients to keep the memory out of
    the buddy allocator. Since the pages never actually go to the
    buddy allocator, allocation and freeing is only based on the bitmap
    allocator to find the appropriate region.
    
    Change-Id: Ia31bb1212fd7b19280361128453c8d25369ce592
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit dfd66db51657cbdac1b5ddb55fad5ed57c0215f9
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Mar 19 12:49:10 2013 -0700

    cma: use MEMBLOCK_ALLOC_ANYWHERE for placing CMA regions
    
    MEMBLOCK_ALLOC_ANYWHERE allocates blocks from anywhere,
    including highmem. Use this flag to allow CMA regions to
    be placed in highmem as opposed to lowmem.
    
    Change-Id: Id5fa36a96e46d60f0e898d764a1f4c8a0a37f5f8
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 0ced4781455bcbad3a7b492a7421be6546e2c5f7
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu Mar 14 19:13:49 2013 -0700

    cma: Remove restriction on region names
    
    CMA currently restricts region names to 'region@x'. Devicetree
    does not support the same value of x to be used multiple times.
    This means that the devicetree cannot have multiple regions
    be dynamically placed (x = 0). Remove the naming restriction
    for CMA regions.
    
    Change-Id: If647f8d7e6323497952431ae5b8cae05ba17af50
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 25c0ee24b5a89e9aee4f225bc8cef37800af275e
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Feb 26 10:38:34 2013 -0800

    cma: Add support for associating regions by name
    
    Currently, the devicetree lookup code assumes that all
    CMA regions are present at a fixed address and uses the
    fixed address for associating CMA regions to devices.
    This presents a problem for dynamically assigning regions.
    Device names get mangled/changed between flattened and
    populated devicetree so relying on that is unworkable.
    Add a separate name binding to allow lookup later between
    devices.
    
    Change-Id: Iaacd9888ea708d7293f1120e2b8c473c5c601f3d
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit cf0c2d66612555a8dbb309886824a25025a11312
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Feb 26 10:33:04 2013 -0800

    cma: Fix up devicetree bindings
    
    The correct binding for regions is linux,contiguous-regions.
    Fix it.
    
    Change-Id: I4bbb4cd3e880c75d917b5a5a081861b3197adfa3
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 58fd9195eb5182b22ec7ca612bbdafe59e43b6fa
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Feb 26 10:26:30 2013 -0800

    cma: Remove __init annotations from data structures
    
    Several of the CMA data structures are used after initialization
    remove the __init annotations from them.
    
    Change-Id: Iff48ed88eef7b8fffdfba4b868cc69ded3c6df42
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 65551384379752ecaec1d0d91d4aa738a1b70b9c
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Feb 14 13:45:28 2013 +0100

    drivers: dma-contiguous: add initialization from device tree
    
    Add device tree support for contiguous memory regions defined in device
    tree. Initialization is done in 2 steps. First, the contiguous memory is
    reserved, what happens very early, when only flattened device tree is
    available. Then on device initialization the corresponding cma regions are
    assigned to device structures.
    
    Change-Id: Ic242499b64875ee57a346d7cbc8a34ebd64e68d2
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2e4ddca774cb1417daa52e17dd3bf72f3f10a594
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Feb 14 13:45:27 2013 +0100

    drivers: dma-contiguous: clean source code and prepare for device tree
    
    This patch cleans the initialization of dma contiguous framework. The
    all-in-one dma_declare_contiguous() function is now separated into
    dma_contiguous_reserve_area() which only steals the the memory from
    memblock allocator and dma_contiguous_add_device() function, which
    assigns given device to the specified reserved memory area. This improves
    the flexibility in defining contiguous memory areas and assigning device
    to them, because now it is possible to assign more than one device to
    the given contiguous memory area. This split in initialization is also
    required for upcoming device tree support.
    
    Change-Id: Ibddd1c9abc6550ee62b09645e7a3355256838bfe
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 96b1eaf1fd79b34f3242424497a6676808c39fa8
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Jul 6 12:02:04 2012 +0200

    mm: cma: fix condition check when setting global cma area
    
    dev_set_cma_area incorrectly assigned cma to global area on first call
    due to incorrect check. This patch fixes this issue.
    
    Change-Id: I24f410bb08c1419e1f8408f30536d82886e104e0
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 8c336f50d11b4df714f447bd2865416f78f5ddaa
Author: Michal Nazarewicz <mina86@mina86.com>
Date:   Wed Sep 5 07:50:41 2012 +0200

    drivers: dma-contiguous: refactor dma_alloc_from_contiguous()
    
    The dma_alloc_from_contiguous() function returns either a valid pointer
    to a page structure or NULL, the error code set when pageno >= cma->count
    is not used at all and can be safely removed.
    
    This commit also changes the function to avoid goto and have only one exit
    path and one place where mutex is unlocked.
    
    Change-Id: I6de94bd6fbe2288382ca9b5e5a1f691aabe0bae5
    Signed-off-by: Michal Nazarewicz <mina86@mina86.com>
    [fixed compilation break caused by missing semicolon]
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f54d13babcb9a52e879d88d5bac0397e21bf5d82
Author: Vitaly Andrianov <vitalya@ti.com>
Date:   Wed Dec 5 09:29:25 2012 -0500

    drivers: cma: represent physical addresses as phys_addr_t
    
    This commit changes the CMA early initialization code to use phys_addr_t
    for representing physical addresses instead of unsigned long.
    
    Without this change, among other things, dma_declare_contiguous() simply
    discards any memory regions whose address is not representable as unsigned
    long.
    
    This is a problem on 32-bit PAE machines where unsigned long is 32-bit
    but physical address space is larger.
    
    Change-Id: I0651ae4bfdbd86139398433f4a3f632efd482b8a
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 4cce9ddf1345b722d90acff6e04ab67da948a0b9
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Aug 27 20:27:19 2012 +0200

    mm: cma: fix alignment requirements for contiguous regions
    
    Contiguous Memory Allocator requires each of its regions to be aligned
    in such a way that it is possible to change migration type for all
    pageblocks holding it and then isolate page of largest possible order from
    the buddy allocator (which is MAX_ORDER-1). This patch relaxes alignment
    requirements by one order, because MAX_ORDER alignment is not really
    needed.
    
    Change-Id: I57798e709636873da27520bc42f7bdb03af15beb
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    CC: Michal Nazarewicz <mina86@mina86.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 38ac1c5ee99b07bee61560e6f1cea43a3caf7faf
Author: Liam Mark <lmark@codeaurora.org>
Date:   Wed Jan 16 10:14:40 2013 -0800

    ion: tracing: add ftrace events for ion allocations
    
    Add ftrace events for ion allocations to make it easier to profile
    their performance.
    
    Change-Id: I9f32e076cd50d7d3a145353dfcef74f0f6cdf8a0
    Signed-off-by: Liam Mark <lmark@codeaurora.org>

commit d80a0aa5c63d4a1c5a5278728233c6e29cd7cda6
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Jun 28 12:52:17 2013 -0700

    mm: Remove __init annotations from free_bootmem_late
    
    free_bootmem_late is currently set up to only be used in init
    functions. Some clients need to use this function past initcalls.
    The functions themselves have no restrictions on being used later
    minus the __init annotations so remove the annotation.
    
    Change-Id: I7c7e15cf2780a8843ebb4610da5b633c9abb0b3d
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d09cc34f1fb9ff679e1e4fc35facba605d61cd75
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Fri Jun 14 17:39:33 2013 -0700

    msm: Increase the kernel virtual area to include lowmem
    
    Even though lowmem is accounted for in vmalloc space, allocation
    comes only from the region bounded by VMALLOC_START and VMALLOC_END.
    The kernel virtual area can now allocate from any unmapped region
    starting from PAGE_OFFSET.
    
    Change-Id: I291b9eb443d3f7445fd979bd7b09e9241ff22ba3
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit b3dbb7bc08b534af268f4cadb1c4c7f8bfbd6260
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Mon Jun 10 17:14:21 2013 -0700

    msm: Allow lowmem to be non contiguous and mixed.
    
    Any image that is expected to have a lifetime of
    the entire system can give the virtual address
    space back for use in vmalloc.
    
    Change-Id: I81ce848cd37e8573d706fa5d1aa52147b3c8da12
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit c7a9248d3ea8f06d68639e655e264971daa3b505
Author: Larry Bassel <lbassel@codeaurora.org>
Date:   Tue Apr 2 10:55:31 2013 -0700

    msm: simplify placing memory pools
    
    The algorithm for placing memory pools was manually inspecting
    the memblock data structures and then doing a memblock_remove()
    on the region it decided that the memory pool should be located at.
    
    If other code already did a memblock_remove() it was possible
    that the memblock data structures which this algorithm
    was inspecting would not be correct and the memory pool might
    be placed overlapping this previous allocation.
    
    Instead use arm_memblock_steal() which will not have this
    problem (and also lets the memblock subsystem do
    the allocation without any manual inspection of memblocks).
    
    Change-Id: I7db5cba0a8d5f222f9f2f131b4b42eb81171c45b
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit 3e42c66a835b0653030da871449f40795e00a8f6
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Mar 19 10:18:16 2013 -0700

    msm: Remove android_pmem.h from board and devices files
    
    CONFIG_ANDROID_PMEM is completely deprecated. Remove all references
    to the android pmem header.
    
    Change-Id: Ic68a3af6b9cd01a439a841af26ae6dba09db2911
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit b7d9ab4c71e279062cd28f43a229fe6491a9ba04
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Mar 20 15:04:10 2013 -0700

    msm: Completely remove memory from the memblock allocator
    
    memblock_remove may not be sufficient to prevent placing
    regions on top of each other. Call memblock_reserve and
    memblock_free as well to completely remove the memory
    from the allocator.
    
    Change-Id: Iec481d7ce7a51c243e43a2d9be2fbfbf7b2717ed
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f67223da46efff07e20eb142696ca99607946dac
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Wed Oct 3 17:01:40 2012 -0700

    msm: memory: use memblock instead of meminfo
    
    msm currently uses meminfo (not memblock) in memory.c for several
    purposes (mainly the placement of the memory pool(s)).  When the code
    was written originally, memblock didn't exist yet.
    
    Now that memblock exists, this is not strictly correct, but works as
    long as nobody calls memblock_remove() besides this code (if that
    occurs then the meminfo data no longer reflect reality and will likely
    cause overlap problems).
    
    To avoid problems meminfo usage should be replaced with memblock.
    
    Also, since memblock regions of the same type can't be contiguous, the
    total_size function, which calculated the aggregate size of contiguous
    meminfo banks, can be removed.
    
    Change-Id: I34054c4fe367986f472bdaae65be67cd8fbf273a
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit 3990e11a15f58918be035d8077f2e8e73b06f902
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Wed Oct 3 16:43:28 2012 -0700

    msm: memory: remove obsolete stable_size function
    
    The notion of `stable' memory is going away (i.e. all memory is now
    treated as `stable'). Remove the stable_size function. Remove the
    notion of stability from the total_stable_size function and rename it
    to total_size.
    
    Change-Id: I8fd08a36b4948d57f430aea9113a0c704b8a00d0
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit 26e100853f0497d66620932ad3d598204a9c9e4a
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Mar 12 14:01:38 2013 -0700

    msm: Remove alloc bootmem aligned
    
    This function has no more callers and should never be used again.
    Remove it.
    
    Change-Id: I3006c03ba2294c7dfb49539dff92b21b6670b4be
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f2263c0b57418e508cdb6a0f1686286d4d2e1ee2
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 27 22:40:44 2013 -0700

    jf: Remove obsolete DMM
    
    Change-Id: I74e6213ad69e2aad88dd646d0f41d5db5ee9e596
    
    Conflicts:
    	arch/arm/mach-msm/board-jf_eur.c
    	arch/arm/mach-msm/board-jf_tmo.c

commit bc77518d72cf17c89016df8e98cc654f8a31f9c7
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Apr 23 16:32:15 2013 -0700

    arm: mm: Use phys_addr_t properly for ioremap functions
    
    Several of the ioremap functions use unsigned long in places
    resulting in truncation if physical addresses greater than
    4G are passed in. Change the types appropriately.
    
    Change-Id: I1f8fb6999d6dbc4ca84c14c8163a72c1dc341143
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ccffdd1cc932c18581f3235ebc4abf3cf39854cb
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Mar 18 11:50:22 2013 -0700

    msm: Use phys_addr_t for the memory hole
    
    On systems with 64-bit physical addresses, unsigned long
    is not big enough to store the memory hole information.
    Use phys_addr_t to store this information.
    
    Change-Id: I5d5f2fff9864a0da4f8f14085d9d61a1419fc550
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 5bafdacf4ad82a18f587418f235606103f2af739
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu Mar 14 20:08:00 2013 -0700

    msm: Account for unaligned memory hole
    
    The memory hole is required to be section size aligned. The
    PMD size internally may be greater than the section size. If the
    end of the memory hole is not PMD aligned, we may end up with the
    case where the physical address is PMD aligned but not the virtual
    address or vice versa. This results in crashes in features such
    as CMA that (reasonablly) assume that the virtual address will be PMD
    aligned if and only if the physical address is aligned. Account
    for this in the memory hole code by aligning the virtual address
    up to PMD_SIZE if there is a mismatch with the physical address.
    This does result in a loss in virtual address space but the
    amount should be small compared to what is gained by by turning
    on CONFIG_DONT_MAP_HOLE_AFTER_MEMBANK0. The virtual address loss
    will go away if the memory hole is properly aligned to PMD_SIZE.
    
    Change-Id: I985568a7cc6bc22bdd65586111ff078ab61b6c34
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit b2abd2f56d5983187d8ef28c614086a4e35f0a61
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Apr 5 14:12:53 2013 -0700

    arm: highmem: Add support for flushing kmap_atomic mappings
    
    The highmem code provides kmap_flush_unused to ensure all kmap
    mappings are really removed if they are unused. This code does not
    handle kmap_atomic mappings since they are managed separately.
    This prevents an issue for any code which relies on having absolutely
    no mappings for a particular page. Rather than pay the penalty of
    having CONFIG_DEBUG_HIGHMEM on all the time, add functionality
    to remove the kmap_atomic mappings in a similar way to kmap_flush_unused.
    
    Change-Id: Ieb25da809b377b1fae1629e2cb75f8aebc1c1bca
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit b4a028f191c8de9f5a87201cb52f94f812fe1030
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Aug 13 00:22:28 2012 +0100

    ARM: Allow arm_memblock_steal() to remove memory from any RAM region
    
    Allow arm_memblock_steal() to remove memory from any RAM region,
    including highmem areas.  This allows memory to be stolen from the
    very top of declared memory, including highmem areas, rather than
    our precious lowmem.
    
    Change-Id: Ib4dbc4e7bc34d402134d96db5ed8118eaffa0245
    Acked-by: Sascha Hauer <s.hauer@pengutronix.de>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Git-commit: 7ac68a4c1de6aac5b0bb231fe6d8505ebe5686d9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit bba9a0b5cc54ec7b8d9ac3917adbc48e8eaf9c91
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Thu Jul 18 15:29:20 2013 -0700

    msm: Adjust the low memory boundary
    
    The unused virtual address space in low memory is given to
    vmalloc for use. This reduces the low memory space and
    increases vmalloc space. Adjust the vmalloc_min in order
    to increase the low memory.
    
    Change-Id: I0dbef5b6e5ec3d19f5f93f06ed03a4cf1215dc4d
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit 7f562ca47f351d88f583ff035c20df7f0ae0b118
Author: Larry Bassel <lbassel@codeaurora.org>
Date:   Mon Jul 29 13:43:17 2013 -0700

    msm: Make CONFIG_STRICT_MEMORY_RWX even stricter
    
    If CONFIG_STRICT_MEMORY_RWX was set, the first section (containing
    the kernel page table and the initial code) and the section
    containing the init code were both given RWX permission, which is
    a potential security hole.
    
    Pad the first section after the initial code (which will never
    be executed when the MMU is on) to make the rest of the kernel
    text start in the second section and make the first section RW.
    
    Move some data which had ended up in the "init text"
    section into the "init data" one, as this is RW, not RX.
    Make the "init text" RX.
    
    We will not free the section containing the "init text",
    because if we do, the kernel will allocate memory for RW data there.
    
    Change-Id: I6ca5f4e07342c374246f04a3fee18042fd47c33b
    CRs-fixed: 513919
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit ef067b0f7f0a2ca4f657865ee711517c37f012cc
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Jul 9 11:06:35 2013 -0700

    arm: mm: Add export symbol for set_memory_* functions
    
    Modules may need to use the set_memory_* functions if they
    explicitly need to have self modifying code or other features
    that rely on changing memory that normally shouldn't be changed.
    Match other architectures and add the exports.
    
    Change-Id: Ifa473d6d57ce113227fa4cd6bd25f7b18bd1cca0
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 997ee6a85bae80261ea031cf64a966939e5e2908
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jun 12 09:44:18 2013 -0700

    arm: mm: Define set_memory_* functions for ARM
    
    Other architectures define various set_memory functions to allow
    attributes to be changed (e.g. set_memory_x, set_memory_rw, etc.)
    Currently, these functions are missing on ARM. Define these in an
    appropriate manner for ARM.
    
    Change-Id: I5083aa985d31e195f138ce7ed26d2a1d4b80faae
    CRs-Fixed: 498398
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e9aff6e6d288fc8e3d629d78385d19176fcd8abf
Author: R Sricharan <r.sricharan@ti.com>
Date:   Sun Mar 17 10:35:41 2013 +0530

    ARM: LPAE: Fix mapping in alloc_init_section for unaligned addresses
    
    With LPAE enabled, alloc_init_section() does not map the entire
    address space for unaligned addresses.
    
    The issue also reproduced with CMA + LPAE. CMA tries to map 16MB
    with page granularity mappings during boot. alloc_init_pte()
    is called and out of 16MB, only 2MB gets mapped and rest remains
    unaccessible.
    
    Because of this OMAP5 boot is broken with CMA + LPAE enabled.
    Fix the issue by ensuring that the entire addresses are
    mapped.
    
    Change-Id: I035cc491c4bccd810ecd53c60cbff0494899fcfa
    Signed-off-by: R Sricharan <r.sricharan@ti.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <chris@cloudcar.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Patch-mainline: linux-arm-kernel @ 03/18/2013, 22:05
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e4c691f8779086fabb2b962479636dc33b6e9870
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Feb 27 15:05:34 2013 -0800

    Revert "ARM: allow the kernel text section to be made read-only"
    
    This reverts commit e5e483d13369ab62d95f1738edce3cd64c7ca2ff.
    
    This change breaks LPAE support. Other patches provide the same
    functionality so just revert the patch.
    
    Change-Id: Iea1e88064a1618ca1fc9d717ee5a4ffe77681745
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit c5925c9fbf0775e79b6cefba11e3e7cb3eb3336c
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Thu Nov 1 21:57:36 2012 -0700

    msm: 8974: Enable DONT_MAP_HOLE_AFTER_MEMBANK0
    
    If this config option is enabled, the kernel does not map the
    memory corresponding to the largest hole into the virtual
    memory resulting in more lowmem. If multiple holes exist, the
    largest hole in the first 256MB of the memory is not mapped.
    
    In device tree based targets the kernel is no longer dependant
    on the bootloader for the start and size of the memory holes.
    Thus the meminfo is adjusted to reflect the memory hole, if it
    is present, using the start and end of the memory hole, while
    parsing the device tree.
    
    For non-device tree based targets, the meminfo is used to find
    the start and size of the memory hole. No adjustment is needed
    to be made to the meminfo, as the hole is taken into account,
    based on the memory information passed by the atags.
    
    Change-Id: Ib2619f72ac2b4142330534c8bbe1c7d9f64ea38c
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit 9e92d923c486f7339a7683863abd16b739d13faf
Author: Yan He <yanhe@codeaurora.org>
Date:   Thu Jul 18 10:28:23 2013 -0700

    msm: sps: free the interrupt for satellite mode
    
    When BAM is registered in satellite mode on apps side, we do not
    need to disable BAM HW when apps side does not use that BAM any
    more. But we still should free the interrupt. This is addressed
    in this change.
    
    Change-Id: I955ea1a341036efa82f0ecc0f45cad5ec41651ae
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit f1790c526396f74ad899485bd8e78983f1652ced
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon May 20 13:26:20 2013 -0700

    msm: sps: turn off BAM DMA clock after initial config
    
    For power saving, SPS driver will turn off BAM DMA clock after the
    initial configuration of BAM DMA. The clients of BAM DMA will vote
    for the clock when BAM DMA is used.
    
    Change-Id: I388a21cff3c619ef49705a064d7d5ef760fa9ef8
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit fe91a62fffd732ce0adbfb33b16ae32cda8c51c1
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon May 20 13:17:29 2013 -0700

    msm: sps: enable BAM DMA client to control BAM DMA clock
    
    For power saving reason, SPS driver will turn off the clock of
    BAM DMA after the initial configuration. A new API is provided
    to BAM DMA clients to vote for or relinquish BAM DMA clock when
    they start/stop using BAM DMA.
    
    Change-Id: I03f8a6cfa19c1722ca4e92e0785ca309fdd10ba2
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 0df2a77e7177c2ffb275066963164eba59a9a937
Author: Yan He <yanhe@codeaurora.org>
Date:   Fri May 17 13:14:25 2013 -0700

    msm: sps: remove the address checking for descriptors
    
    0x0 is a valid physical address for drivers to use. Thus, remove
    the checking of physical address for descriptors.
    
    Change-Id: I19b8e215a4538ec0a9b7def505f9577e204c7730
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 606c22d8bd16d700d2b75b11f84a73ed5fe99e08
Author: Yan He <yanhe@codeaurora.org>
Date:   Tue Feb 26 17:42:19 2013 -0800

    msm: sps: improve BAM inactivity timer function
    
    1> Add the support for BAM global inactivity timer which is a new feature
    in B family.
    2> There is an update that changes the time unit of the inactivity timer
    counter from 0.1ms to 0.125ms. Thus, we adjust the parameter in SW driver
    for this update.
    3> Inactivity timer timeout interrupt will become a new type of BAM global
    interrupts, and it is a normal interrupt different from other BAM global
    error interrupts. Thus, change the log level for callback message from INFO
    to DEBUG.
    4> Change timer result argument of inactivity timer API to an optional one
    for client drivers.
    
    Change-Id: If223ef311f0393f2f2243f6b8a862fe760ff5214
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 03046f155109d7794bf4d5fbce135479034ee452
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon Apr 8 13:00:56 2013 -0700

    msm: sps: adapt to clock API change
    
    With the change in clock driver, clk_get() can return -EPROBE_DEFER even
    when the clock is actually present in HW. Thus, we need to return
    -EPROBE_DEFER in SPS driver probe function if clk_get() fails.
    
    Change-Id: Ia1b9f18425ca5432f4a56419adf142f2674ce64a
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 0a3b4a1f7081a85ff9e3b01322b4d0c3a59b24a6
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Thu May 23 16:19:47 2013 -0700

    msm: cpufreq: Relax constraints on "msm-cpufreq" workqueue
    
    This workqueue is not used in memory reclaim paths, so the
    WQ_MEM_RECLAIM flag is not needed. Additionally, there is no
    need to restrict the queue to having one in-flight work item.
    Remove these constraints.
    
    Change-Id: I9edde40917d3ec885ce061133de20680634321d0
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/cpufreq.c

commit 656512c5908617bc10c20884b6e997046becf5d0
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Fri Jun 7 17:23:09 2013 -0700

    msm: cpufreq: Ensure cpufreq change happens on corresponding CPU
    
    Checking the cpus_allowed mask of the current thread before changing the
    frequency doesn't guarantee that the rest of the execution will continue on
    the same CPU. The only way to guarantee this is to schedule a work on the
    specific CPU and also prevent hotplug  of that CPU (already done by
    existing code).
    
    Change-Id: I51a02fcc777a47d3c16f2d83c47e96f2c59f7ae6
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/cpufreq.c

commit 6fc2557cd3b1bfd9b392694454f550b5d51a6d8c
Author: Praveen Chidambaram <pchidamb@codeaurora.org>
Date:   Tue Apr 30 15:25:08 2013 -0600

    msm: cpufreq: Initialize cpufreq driver early at boot
    
    MSM cpufreq driver registering with the CPUFreq framework during
    late_initcall() may cause the device to be available a few seconds after
    the clock driver is ready for changing frequency. This could be bad for
    boot and/or thermal performance, since the cpu will continue to boot at
    whatever frequency the bootloaders set it to.
    
    Initialize cpufreq driver as part of device_initcalL() but only after
    all the clock drivers have had a chance to initialize their devices.
    
    Change-Id: Iaa4867108f59a368ee5b511d35ea2e8e219d39d3
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/Makefile

commit 820ed12304378547d40ae34b43ef75c664f146c8
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Wed Feb 13 23:01:06 2013 +0530

    msm: hotplug: remove code related to cpu_killed
    
    Earlier the completion event cpu_killed was used to synchronize
    notify to hot plug framework that the CPU is dead. But now it is
    replaced with another completion event cpu_died and it is placed
    in the hotplug framework itself. So there is no need for a
    platform to implement a completion event.
    
    But in msm platform driver the completion event related code
    was not removed with the changes in hotplug framework. This
    patch removes the unused code.
    
    Change-Id: I4418e5e397821e39a7b6a6efa997c4a866e9b46b
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit f90bacc4fd89b232ecad9d15f664bbd357497004
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Aug 14 14:27:41 2013 -0700

    trace: power: Adding trace events for cpufreq
    
    Added two events to help debugging cpufreq:
    1) cpufreq_sampling_event: To capture governor sampling instants
       when cpu load is evaluated and an appropriate frequency is
       selected.
    2) cpufreq_freq_synced: to capture frequency synchronization of
       destination cpu with the source cpu triggered by foreground
       thread migrations.
    
    Change-Id: I75bc7c3104b6e0750d9ede95602098d335f4f533
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit 23a8f7fec551b2ebc2e6e597d1736f6accc81808
Author: Chintan Pandya <cpandya@codeaurora.org>
Date:   Tue Jul 2 16:00:49 2013 -0700

    sysctl: Don't scan for the leaks on headers
    
    These header allocations have life cycle till the device goes
    shutdown. So, considering them as leak is false positive. Remove
    them by marking kmemleak_not_leak
    
    CRs-Fixed: 466552
    Change-Id: Id1571b78365e533ddfe866d45cef8f89b0b62bc7
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit 62eb25282fcc59d9a1dc5f3d7f6677f0f47e5553
Author: xiaogang <xiaogang@codeaurora.org>
Date:   Tue May 28 15:38:17 2013 +0800

    fs: vfat: reduce the worst case latencies
    
    When a block partition is mounted with FAT file system
    and MS_DIRSYNC option is used, some file system operations
    like create, rename shall sleep in caller's context until
    all the metadata have been committed to the non-volatile memory.
    Since this operation is blocking call for user context,
    the WRITE_SYNC option must be used instead of WRITE
    (async operation) which incur inherent latencies while
    flushing the meta-data corresponding to directory entries
    
    Change-Id: I41c514889873a39d564271db0a421e6c66e5ae33
    Signed-off-by: xiaogang <xiaogang@codeaurora.org>

commit 44b4b68dfdeda85b1543580868779abcf13ad528
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sun Jul 7 17:33:05 2013 +0300

    block: Add URGENT request notification support to CFQ scheduler
    
    When the scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. This is done
    in order to reduce the latency of an urgent request. For example:
    long WRITE may be stopped to handle an urgent READ.
    
    Change-Id: I3072b8a1423870fed9c04c28d93caaf9557a7b89
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit e4db2d70b98690da962b8b238c4452babdb624a1
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Mon Aug 5 16:32:13 2013 -0700

    lib: spinlock_debug: increase spin dump timeout
    
    On certain targets the worst case console buffer draining time can
    exceed the aggressive spin dump timeout value. Relax the timeout value
    to cater to these scenarios.
    
    Change-Id: I7312ffe06bafd2dc7adf0f4e8b73a53fc5839235
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit 1a84375cb4ff9526f1fa4b1470117c2c5e77bfbe
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Thu Aug 15 11:26:26 2013 -0700

    Revert "msm: move printk out of spin lock low_water_lock"
    
    This reverts commit 749c4210f338496abf2e16ef51308ff756b34f7b.
    
    This patch is a hack intended to solve a particular instance of a more
    generic problem. The cause of the spindump timeout has been fixed with
    the commit f37a3dc536e4ecd72961a2d9c77f5587c5c55d8f. Furthermore this
    change also introduces a race condition since the printk is actually
    intended to be protected with the spinlock.
    
    Change-Id: I8ca057f47760c7bb5b826cceeac92ae5e124e904
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit 602c289d6a007d1ce8b764e7039cd4882cbe211f
Author: Mike Galbraith <bitbucket@online.de>
Date:   Mon Jan 28 11:19:25 2013 +0000

    sched: Fix select_idle_sibling() bouncing cow syndrome If the previous CPU is cache affine and idle, select it. The current implementation simply traverses the sd_llc domain, taking the first idle CPU encountered, which walks buddy pairs hand in hand over the package, inflicting excruciating pain.
    
    1 tbench pair (worst case) in a 10 core + SMT package:
    
      pre   15.22 MB/sec 1 procs
      post 252.01 MB/sec 1 procs
    
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1359371965.5783.127.camel@marge.simpson.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	kernel/sched/fair.c
    
    Change-Id: I4ca60be0d759d4881fad55be78d9f075aea50121

commit 422f5c4fa24e90351be47e8a2e125599ecfc9cd5
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue Dec 4 11:11:31 2012 -0500

    mm: vmscan: do not keep kswapd looping forever due to individual uncompactable zones
    
    When a zone meets its high watermark and is compactable in case of
    higher order allocations, it contributes to the percentage of the node's
    memory that is considered balanced.
    
    This requirement, that a node be only partially balanced, came about
    when kswapd was desparately trying to balance tiny zones when all bigger
    zones in the node had plenty of free memory.  Arguably, the same should
    apply to compaction: if a significant part of the node is balanced
    enough to run compaction, do not get hung up on that tiny zone that
    might never get in shape.
    
    When the compaction logic in kswapd is reached, we know that at least
    25% of the node's memory is balanced properly for compaction (see
    zone_balanced and pgdat_balanced).  Remove the individual zone checks
    that restart the kswapd cycle.
    
    Otherwise, we may observe more endless looping in kswapd where the
    compaction code loops back to reclaim because of a single zone and
    reclaim does nothing because the node is considered balanced overall.
    
    See for example
    
      https://bugzilla.redhat.com/show_bug.cgi?id=866988
    
    Change-Id: I9c8f180466fea814c3038fa019f317799ddc0bcb
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-and-tested-by: Thorsten Leemhuis <fedora@leemhuis.info>
    Reported-by: Jiri Slaby <jslaby@suse.cz>
    Tested-by: John Ellson <john.ellson@comcast.net>
    Tested-by: Zdenek Kabelac <zkabelac@redhat.com>
    Tested-by: Bruno Wolff III <bruno@wolff.to>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c702418f8a2fa6cc92e84a39880d458faf7af9cc
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit c6bb10239df7342ade4f18c3253eaff85c7f9327
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Nov 29 13:54:23 2012 -0800

    mm: vmscan: fix endless loop in kswapd balancing
    
    Kswapd does not in all places have the same criteria for a balanced
    zone.  Zones are only being reclaimed when their high watermark is
    breached, but compaction checks loop over the zonelist again when the
    zone does not meet the low watermark plus two times the size of the
    allocation.  This gets kswapd stuck in an endless loop over a small
    zone, like the DMA zone, where the high watermark is smaller than the
    compaction requirement.
    
    Add a function, zone_balanced(), that checks the watermark, and, for
    higher order allocations, if compaction has enough free memory.  Then
    use it uniformly to check for balanced zones.
    
    This makes sure that when the compaction watermark is not met, at least
    reclaim happens and progress is made - or the zone is declared
    unreclaimable at some point and skipped entirely.
    
    Change-Id: I17a375f356189bde63406e2127153287208157a8
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: George Spelvin <linux@horizon.com>
    Reported-by: Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
    Reported-by: Tomas Racek <tracek@redhat.com>
    Tested-by: Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 60cefed485a02bd99b6299dad70666fe49245da7
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit 4fc87e43343feed5d19822bb9ef79e4748ec906a
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Dec 20 15:05:06 2012 -0800

    compaction: fix build error in CMA && !COMPACTION
    
    isolate_freepages_block() and isolate_migratepages_range() are used for
    CMA as well as compaction so it breaks build for CONFIG_CMA &&
    !CONFIG_COMPACTION.
    
    This patch fixes it.
    
    Change-Id: I5163a4f20cc2002e19f87d1652ccabd75694b644
    [akpm@linux-foundation.org: add "do { } while (0)", per Mel]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 010fc29a45a2e8dbc08bf45ef80b8622619aaae0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 79820064ef0158d078b198ed59c05e20afb3042b
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Jan 11 14:32:16 2013 -0800

    mm: compaction: partially revert capture of suitable high-order page
    
    Eric Wong reported on 3.7 and 3.8-rc2 that ppoll() got stuck when
    waiting for POLLIN on a local TCP socket.  It was easier to trigger if
    there was disk IO and dirty pages at the same time and he bisected it to
    commit 1fb3f8ca0e92 ("mm: compaction: capture a suitable high-order page
    immediately when it is made available").
    
    The intention of that patch was to improve high-order allocations under
    memory pressure after changes made to reclaim in 3.6 drastically hurt
    THP allocations but the approach was flawed.  For Eric, the problem was
    that page->pfmemalloc was not being cleared for captured pages leading
    to a poor interaction with swap-over-NFS support causing the packets to
    be dropped.  However, I identified a few more problems with the patch
    including the fact that it can increase contention on zone->lock in some
    cases which could result in async direct compaction being aborted early.
    
    In retrospect the capture patch took the wrong approach.  What it should
    have done is mark the pageblock being migrated as MIGRATE_ISOLATE if it
    was allocating for THP and avoided races that way.  While the patch was
    showing to improve allocation success rates at the time, the benefit is
    marginal given the relative complexity and it should be revisited from
    scratch in the context of the other reclaim-related changes that have
    taken place since the patch was first written and tested.  This patch
    partially reverts commit 1fb3f8ca0e92 ("mm: compaction: capture a
    suitable high-order page immediately when it is made available").
    
    Change-Id: I985725a72aac0fdecbf4310c04d176f39e0386dd
    Reported-and-tested-by: Eric Wong <normalperson@yhbt.net>
    Tested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 8fb74b9fb2b182d54beee592350d9ea1f325917a
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 1291c6f837e296e647828abe66d66e792700b09d
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Dec 11 16:02:57 2012 -0800

    mm: cma: skip watermarks check for already isolated blocks in split_free_page()
    
    Since commit 2139cbe627b8 ("cma: fix counting of isolated pages") free
    pages in isolated pageblocks are not accounted to NR_FREE_PAGES counters,
    so watermarks check is not required if one operates on a free page in
    isolated pageblock.
    
    Change-Id: Id9b38d2e4e504336e11274a30044e3aacbc37d03
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 2e30abd1730751d58463d88bc0844ab8fd7112a9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixups due to being merged
    out of order. Bring over an added 'feature' where we don't
    obey watermarks for CMA pages]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 1f13b7cb0ed2cd0565d2714c431b3862e355909b
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 12:00:10 2012 +0100

    mm: compaction: Add scanned and isolated counters for compaction
    
    Compaction already has tracepoints to count scanned and isolated pages
    but it requires that ftrace be enabled and if that information has to be
    written to disk then it can be disruptive. This patch adds vmstat counters
    for compaction called compact_migrate_scanned, compact_free_scanned and
    compact_isolated.
    
    With these counters, it is possible to define a basic cost model for
    compaction. This approximates of how much work compaction is doing and can
    be compared that with an oprofile showing TLB misses and see if the cost of
    compaction is being offset by THP for example. Minimally a compaction patch
    can be evaluated in terms of whether it increases or decreases cost. The
    basic cost model looks like this
    
    Fundamental unit u:	a word	sizeof(void *)
    
    Ca  = cost of struct page access = sizeof(struct page) / u
    
    Cmc = Cost migrate page copy = (Ca + PAGE_SIZE/u) * 2
    Cmf = Cost migrate failure   = Ca * 2
    Ci  = Cost page isolation    = (Ca + Wi)
    	where Wi is a constant that should reflect the approximate
    	cost of the locking operation.
    
    Csm = Cost migrate scanning = Ca
    Csf = Cost free    scanning = Ca
    
    Overall cost =	(Csm * compact_migrate_scanned) +
    	      	(Csf * compact_free_scanned)    +
    	      	(Ci  * compact_isolated)	+
    		(Cmc * pgmigrate_success)	+
    		(Cmf * pgmigrate_failed)
    
    Where the values are read from /proc/vmstat.
    
    This is very basic and ignores certain costs such as the allocation cost
    to do a migrate page copy but any improvement to the model would still
    use the same vmstat counters.
    
    Change-Id: I9db1a609fc86a95e3fd8d3774de994197ecb9adf
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Git-commit: 397487db696cae0b026a474a5cd66f4e372995e6
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 701786423e838575d6a0a6c81a2b5324996370c1
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 10:46:20 2012 +0100

    mm: compaction: Move migration fail/success stats to migrate.c
    
    The compact_pages_moved and compact_pagemigrate_failed events are
    convenient for determining if compaction is active and to what
    degree migration is succeeding but it's at the wrong level. Other
    users of migration may also want to know if migration is working
    properly and this will be particularly true for any automated
    NUMA migration. This patch moves the counters down to migration
    with the new events called pgmigrate_success and pgmigrate_fail.
    The compact_blocks_moved counter is removed because while it was
    useful for debugging initially, it's worthless now as no meaningful
    conclusions can be drawn from its value.
    
    Change-Id: I43d66b61a2a6be571ed025213d7f1b9defb1a18f
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Git-commit: 5647bc293ab15f66a7b1cda850c5e9d162a6c7c2
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ed0552aa5197e3877cf254856b9d5d3d24753756
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Dec 6 19:01:14 2012 +0000

    mm: compaction: validate pfn range passed to isolate_freepages_block
    
    Commit 0bf380bc70ec ("mm: compaction: check pfn_valid when entering a
    new MAX_ORDER_NR_PAGES block during isolation for migration") added a
    check for pfn_valid() when isolating pages for migration as the scanner
    does not necessarily start pageblock-aligned.
    
    Since commit c89511ab2f8f ("mm: compaction: Restart compaction from near
    where it left off"), the free scanner has the same problem.  This patch
    makes sure that the pfn range passed to isolate_freepages_block() is
    within the same block so that pfn_valid() checks are unnecessary.
    
    In answer to Henrik's wondering why others have not reported this:
    reproducing this requires a large enough hole with the right aligment to
    have compaction walk into a PFN range with no memmap.  Size and
    alignment depends in the memory model - 4M for FLATMEM and 128M for
    SPARSEMEM on x86.  It needs a "lucky" machine.
    
    Change-Id: I5de91fc8d9792cb339b87fe9ef058143cde5995a
    Reported-by: Henrik Rydberg <rydberg@euromail.se>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 60177d31d215bc2b4c5a7aa6f742800e04fa0a92
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 507c9e24aa914f2a3a92d91b1741d5b379ba86fb
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 13:56:57 2012 -0700

    mm: compaction: correct the nr_strict va isolated check for CMA
    
    Thierry reported that the "iron out" patch for isolate_freepages_block()
    had problems due to the strict check being too strict with "mm:
    compaction: Iron out isolate_freepages_block() and
    isolate_freepages_range() -fix1".  It's possible that more pages than
    necessary are isolated but the check still fails and I missed that this
    fix was not picked up before RC1.  This same problem has been identified
    in 3.7-RC1 by Tony Prisk and should be addressed by the following patch.
    
    Change-Id: I2aefac60b55ee48cf3f9f31dbd7894c024e64f28
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Tony Prisk <linux@prisktech.co.nz>
    Reported-by: Thierry Reding <thierry.reding@avionic-design.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 0db63d7e25f96e2c6da925c002badf6f144ddf30
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 93c81ed20fca75afa57186a32c8df5aa75c3d54a
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:48 2012 -0700

    CMA: migrate mlocked pages
    
    Presently CMA cannot migrate mlocked pages so it ends up failing to allocate
    contiguous memory space.
    
    This patch makes mlocked pages be migrated out.  Of course, it can affect
    realtime processes but in CMA usecase, contiguous memory allocation failing
    is far worse than access latency to an mlocked page being variable while
    CMA is running.  If someone wants to make the system realtime, he shouldn't
    enable CMA because stalls can still happen at random times.
    
    Change-Id: I560f43fdeb94f8fd2a4cc9e2ac12a1593ca38ecb
    [akpm@linux-foundation.org: tweak comment text, per Mel]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e46a28790e594c0876d1a84270926abf75460f61
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixups]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d28129965a48da4b06eadf0e461b677cf1aa11ec
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:47 2012 -0700

    mm: compaction: clear PG_migrate_skip based on compaction and reclaim activity
    
    Compaction caches if a pageblock was scanned and no pages were isolated so
    that the pageblocks can be skipped in the future to reduce scanning.  This
    information is not cleared by the page allocator based on activity due to
    the impact it would have to the page allocator fast paths.  Hence there is
    a requirement that something clear the cache or pageblocks will be skipped
    forever.  Currently the cache is cleared if there were a number of recent
    allocation failures and it has not been cleared within the last 5 seconds.
    Time-based decisions like this are terrible as they have no relationship
    to VM activity and is basically a big hammer.
    
    Unfortunately, accurate heuristics would add cost to some hot paths so
    this patch implements a rough heuristic.  There are two cases where the
    cache is cleared.
    
    1. If a !kswapd process completes a compaction cycle (migrate and free
       scanner meet), the zone is marked compact_blockskip_flush. When kswapd
       goes to sleep, it will clear the cache. This is expected to be the
       common case where the cache is cleared. It does not really matter if
       kswapd happens to be asleep or going to sleep when the flag is set as
       it will be woken on the next allocation request.
    
    2. If there have been multiple failures recently and compaction just
       finished being deferred then a process will clear the cache and start a
       full scan.  This situation happens if there are multiple high-order
       allocation requests under heavy memory pressure.
    
    The clearing of the PG_migrate_skip bits and other scans is inherently
    racy but the race is harmless.  For allocations that can fail such as THP,
    they will simply fail.  For requests that cannot fail, they will retry the
    allocation.  Tests indicated that scanning rates were roughly similar to
    when the time-based heuristic was used and the allocation success rates
    were similar.
    
    Change-Id: If690ae126badb9f9cc5632e9ffb9d376bf210fb0
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 62997027ca5b3d4618198ed8b1aba40b61b1137b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	include/linux/mmzone.h

commit 16bdc5a3430a4050c9f039b42b7d367b859d90d3
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:45 2012 -0700

    mm: compaction: Restart compaction from near where it left off
    
    This is almost entirely based on Rik's previous patches and discussions
    with him about how this might be implemented.
    
    Order > 0 compaction stops when enough free pages of the correct page
    order have been coalesced.  When doing subsequent higher order
    allocations, it is possible for compaction to be invoked many times.
    
    However, the compaction code always starts out looking for things to
    compact at the start of the zone, and for free pages to compact things to
    at the end of the zone.
    
    This can cause quadratic behaviour, with isolate_freepages starting at the
    end of the zone each time, even though previous invocations of the
    compaction code already filled up all free memory on that end of the zone.
     This can cause isolate_freepages to take enormous amounts of CPU with
    certain workloads on larger memory systems.
    
    This patch caches where the migration and free scanner should start from
    on subsequent compaction invocations using the pageblock-skip information.
     When compaction starts it begins from the cached restart points and will
    update the cached restart points until a page is isolated or a pageblock
    is skipped that would have been scanned by synchronous compaction.
    
    Change-Id: Ib788e0bfd803f5abdd8476b9e203a0b6420e194b
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c89511ab2f8fe2b47585e60da8af7fd213ec877e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixup]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	include/linux/mmzone.h

commit a3ad6947967acb643cf32339e793695db1491efb
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:43:01 2012 -0700

    mm: clean up __count_immobile_pages()
    
    The __count_immobile_pages() naming is rather awkward.  Choose a more
    clear name and add a comment.
    
    Change-Id: Ic1d8573bbc7eaa82dd5e3f9a1199ee6dd4ac9fc0
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 80934513b230bfcf70265f2ef0fdae89fb391633
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 0e95f33de6afa0cb5b1ec519772935b9eaebe75a
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:42:59 2012 -0700

    mm: do not use page_count() without a page pin
    
    d179e84ba ("mm: vmscan: do not use page_count without a page pin") fixed
    this problem in vmscan.c but same problem is in __count_immobile_pages().
    
    I copy and paste d179e84ba's contents for description.
    
    "It is unsafe to run page_count during the physical pfn scan because
    compound_head could trip on a dangling pointer when reading
    page->first_page if the compound page is being freed by another CPU."
    
    Change-Id: I2b620c07adfa2c5a0f56219a11c042dc97428085
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Wanpeng Li <liwp.linux@gmail.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 97d255c816946388bab504122937730d3447c612
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit adf00c52b51e40a5b37cc5db00ff3808d89fca7e
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Dec 20 15:05:18 2012 -0800

    mm: cma: WARN if freed memory is still in use
    
    Memory returned to free_contig_range() must have no other references.
    Let kernel to complain loudly if page reference count is not equal to 1.
    
    Change-Id: If1b84bb383d97eff441d7a1e18b874c64b7f5f85
    [rientjes@google.com: support sparsemem]
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bcc2b02f4c1b36bc67272df7119b75bac78525ab
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2f25e6e6b7b9dd41bbd3708e7ef06212685215cb
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:41 2012 -0700

    mm: compaction: cache if a pageblock was scanned and no pages were isolated
    
    When compaction was implemented it was known that scanning could
    potentially be excessive.  The ideal was that a counter be maintained for
    each pageblock but maintaining this information would incur a severe
    penalty due to a shared writable cache line.  It has reached the point
    where the scanning costs are a serious problem, particularly on
    long-lived systems where a large process starts and allocates a large
    number of THPs at the same time.
    
    Instead of using a shared counter, this patch adds another bit to the
    pageblock flags called PG_migrate_skip.  If a pageblock is scanned by
    either migrate or free scanner and 0 pages were isolated, the pageblock is
    marked to be skipped in the future.  When scanning, this bit is checked
    before any scanning takes place and the block skipped if set.
    
    The main difficulty with a patch like this is "when to ignore the cached
    information?" If it's ignored too often, the scanning rates will still be
    excessive.  If the information is too stale then allocations will fail
    that might have otherwise succeeded.  In this patch
    
    o CMA always ignores the information
    o If the migrate and free scanner meet then the cached information will
      be discarded if it's at least 5 seconds since the last time the cache
      was discarded
    o If there are a large number of allocation failures, discard the cache.
    
    The time-based heuristic is very clumsy but there are few choices for a
    better event.  Depending solely on multiple allocation failures still
    allows excessive scanning when THP allocations are failing in quick
    succession due to memory pressure.  Waiting until memory pressure is
    relieved would cause compaction to continually fail instead of using
    reclaim/compaction to try allocate the page.  The time-based mechanism is
    clumsy but a better option is not obvious.
    
    Change-Id: I17a4887aca9bb3d2d9d3756089ad7c9b89922727
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bb13ffeb9f6bfeb301443994dfbf29f91117dfb3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixup due to merging patches out of order]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	include/linux/mmzone.h

commit 3c9b0f80868155b54f22f9d3fed2ab2f3794e1f7
Author: Heesub Shin <heesub.shin@samsung.com>
Date:   Mon Jan 7 11:10:13 2013 +0900

    cma: redirect page allocation to CMA
    
    CMA pages are designed to be used as fallback for movable allocations
    and cannot be used for non-movable allocations. If CMA pages are
    utilized poorly, non-movable allocations may end up getting starved if
    all regular movable pages are allocated and the only pages left are
    CMA. Always using CMA pages first creates unacceptable performance
    problems. As a midway alternative, use CMA pages for certain
    userspace allocations. The userspace pages can be migrated or dropped
    quickly which giving decent utilization.
    
    Change-Id: I6165dda01b705309eebabc6dfa67146b7a95c174
    CRs-Fixed: 452508
    [lauraa@codeaurora.org: Missing CONFIG_CMA guards, add commit text]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	include/linux/mmzone.h

commit 725f942a29d707d3e0d7efb1e55b94e0dd867872
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:36 2012 -0700

    mm: compaction: acquire the zone->lock as late as possible
    
    Compaction's free scanner acquires the zone->lock when checking for
    PageBuddy pages and isolating them.  It does this even if there are no
    PageBuddy pages in the range.
    
    This patch defers acquiring the zone lock for as long as possible.  In the
    event there are no free pages in the pageblock then the lock will not be
    acquired at all which reduces contention on zone->lock.
    
    Change-Id: Iaf8b30f94e43b03ac93d751b7b97dbe0b76b87af
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Tested-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f40d1e42bb988d2a26e8e111ea4c4c7bac819b7e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 670cc90b200eec69f65fe358c11fac1f31d4bedc
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:33 2012 -0700

    mm: compaction: acquire the zone->lru_lock as late as possible
    
    Richard Davies and Shaohua Li have both reported lock contention problems
    in compaction on the zone and LRU locks as well as significant amounts of
    time being spent in compaction.  This series aims to reduce lock
    contention and scanning rates to reduce that CPU usage.  Richard reported
    at https://lkml.org/lkml/2012/9/21/91 that this series made a big
    different to a problem he reported in August:
    
       http://marc.info/?l=kvm&m=134511507015614&w=2
    
    Patch 1 defers acquiring the zone->lru_lock as long as possible.
    
    Patch 2 defers acquiring the zone->lock as lock as possible.
    
    Patch 3 reverts Rik's "skip-free" patches as the core concept gets
    	reimplemented later and the remaining patches are easier to
    	understand if this is reverted first.
    
    Patch 4 adds a pageblock-skip bit to the pageblock flags to cache what
    	pageblocks should be skipped by the migrate and free scanners.
    	This drastically reduces the amount of scanning compaction has
    	to do.
    
    Patch 5 reimplements something similar to Rik's idea except it uses the
    	pageblock-skip information to decide where the scanners should
    	restart from and does not need to wrap around.
    
    I tested this on 3.6-rc6 + linux-next/akpm. Kernels tested were
    
    akpm-20120920	3.6-rc6 + linux-next/akpm as of Septeber 20th, 2012
    lesslock	Patches 1-6
    revert		Patches 1-7
    cachefail	Patches 1-8
    skipuseless	Patches 1-9
    
    Stress high-order allocation tests looked ok.  Success rates are more or
    less the same with the full series applied but there is an expectation
    that there is less opportunity to race with other allocation requests if
    there is less scanning.  The time to complete the tests did not vary that
    much and are uninteresting as were the vmstat statistics so I will not
    present them here.
    
    Using ftrace I recorded how much scanning was done by compaction and got this
    
                                3.6.0-rc6     3.6.0-rc6   3.6.0-rc6  3.6.0-rc6 3.6.0-rc6
                                akpm-20120920 lockless  revert-v2r2  cachefail skipuseless
    
    Total   free    scanned         360753976  515414028  565479007   17103281   18916589
    Total   free    isolated          2852429    3597369    4048601     670493     727840
    Total   free    efficiency        0.0079%    0.0070%    0.0072%    0.0392%    0.0385%
    Total   migrate scanned         247728664  822729112 1004645830   17946827   14118903
    Total   migrate isolated          2555324    3245937    3437501     616359     658616
    Total   migrate efficiency        0.0103%    0.0039%    0.0034%    0.0343%    0.0466%
    
    The efficiency is worthless because of the nature of the test and the
    number of failures.  The really interesting point as far as this patch
    series is concerned is the number of pages scanned.  Note that reverting
    Rik's patches massively increases the number of pages scanned indicating
    that those patches really did make a difference to CPU usage.
    
    However, caching what pageblocks should be skipped has a much higher
    impact.  With patches 1-8 applied, free page and migrate page scanning are
    both reduced by 95% in comparison to the akpm kernel.  If the basic
    concept of Rik's patches are implemened on top then scanning then the free
    scanner barely changed but migrate scanning was further reduced.  That
    said, tests on 3.6-rc5 indicated that the last patch had greater impact
    than what was measured here so it is a bit variable.
    
    One way or the other, this series has a large impact on the amount of
    scanning compaction does when there is a storm of THP allocations.
    
    This patch:
    
    Compaction's migrate scanner acquires the zone->lru_lock when scanning a
    range of pages looking for LRU pages to acquire.  It does this even if
    there are no LRU pages in the range.  If multiple processes are compacting
    then this can cause severe locking contention.  To make matters worse
    commit b2eef8c0 ("mm: compaction: minimise the time IRQs are disabled
    while isolating pages for migration") releases the lru_lock every
    SWAP_CLUSTER_MAX pages that are scanned.
    
    This patch makes two changes to how the migrate scanner acquires the LRU
    lock.  First, it only releases the LRU lock every SWAP_CLUSTER_MAX pages
    if the lock is contended.  This reduces the number of times it
    unnecessarily disables and re-enables IRQs.  The second is that it defers
    acquiring the LRU lock for as long as possible.  If there are no LRU pages
    or the only LRU pages are transhuge then the LRU lock will not be acquired
    at all which reduces contention on zone->lru_lock.
    
    Change-Id: If518f3acb4db4ba579b708889de0fa9f42366899
    [minchan@kernel.org: augment comment]
    [akpm@linux-foundation.org: tweak comment text]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 2a1402aa044b55c2d30ab0ed9405693ef06fb07c
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit b6b99c8a91d23ef70eb26acd60c92dc1bf434fde
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:31 2012 -0700

    mm: compaction: Update try_to_compact_pages()kerneldoc comment
    
    Parameters were added without documentation, tut tut.
    
    Change-Id: I1355906b3a3a6e3319a0fedc8ba28c3327a0c8f2
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 661c4cb9b829110cb68c18ea05a56be39f75a4d2
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e4a5e6a4188627341678d3bbd8129c464356c6c1
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:30 2012 -0700

    mm: compaction: move fatal signal check out of compact_checklock_irqsave
    
    Commit c67fe3752abe ("mm: compaction: Abort async compaction if locks
    are contended or taking too long") addressed a lock contention problem
    in compaction by introducing compact_checklock_irqsave() that effecively
    aborting async compaction in the event of compaction.
    
    To preserve existing behaviour it also moved a fatal_signal_pending()
    check into compact_checklock_irqsave() but that is very misleading.  It
    "hides" the check within a locking function but has nothing to do with
    locking as such.  It just happens to work in a desirable fashion.
    
    This patch moves the fatal_signal_pending() check to
    isolate_migratepages_range() where it belongs.  Arguably the same check
    should also happen when isolating pages for freeing but it's overkill.
    
    Change-Id: I026ab765b4160bdb6bbb8b7359be24b6159e382c
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3cc668f4e30fbd97b3c0574d8cac7a83903c9bc7
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 820f80002b6c006c156b21065435f543b49747d1
Author: Shaohua Li <shli@kernel.org>
Date:   Mon Oct 8 16:32:27 2012 -0700

    mm: compaction: abort compaction loop if lock is contended or run too long
    
    isolate_migratepages_range() might isolate no pages if for example when
    zone->lru_lock is contended and running asynchronous compaction. In this
    case, we should abort compaction, otherwise, compact_zone will run a
    useless loop and make zone->lru_lock is even contended.
    
    An additional check is added to ensure that cc.migratepages and
    cc.freepages get properly drained whan compaction is aborted.
    
    Change-Id: Ia33b52655f9926d0bb2cde95492066bd8132149d
    [minchan@kernel.org: Putback pages isolated for migration if aborting]
    [akpm@linux-foundation.org: compact_zone_order requires non-NULL arg contended]
    [akpm@linux-foundation.org: make compact_zone_order() require non-NULL arg `contended']
    [minchan@kernel.org: Putback pages isolated for migration if aborting]
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e64c5237cf6ff474cb2f3f832f48f2b441dd9979
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 705fbe63dfb8b7e448a2b24d9b78d0e2d21aedc5
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:29:12 2012 -0700

    mm: compaction: capture a suitable high-order page immediately when it is made available
    
    While compaction is migrating pages to free up large contiguous blocks
    for allocation it races with other allocation requests that may steal
    these blocks or break them up.  This patch alters direct compaction to
    capture a suitable free page as soon as it becomes available to reduce
    this race.  It uses similar logic to split_free_page() to ensure that
    watermarks are still obeyed.
    
    Change-Id: I46fc38ca67bc50aa7a77a59255caf563f50343a9
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 1fb3f8ca0e9222535a39b884cb67a34628411b9f
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e766354ebf828f2f7204537160ce844163066522
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:29:09 2012 -0700

    mm: compaction: update comment in try_to_compact_pages
    
    Allocation success rates have been far lower since 3.4 due to commit
    fe2c2a106663 ("vmscan: reclaim at order 0 when compaction is enabled").
    This commit was introduced for good reasons and it was known in advance
    that the success rates would suffer but it was justified on the grounds
    that the high allocation success rates were achieved by aggressive
    reclaim.  Success rates are expected to suffer even more in 3.6 due to
    commit 7db8889ab05b ("mm: have order > 0 compaction start off where it
    left") which testing has shown to severely reduce allocation success
    rates under load - to 0% in one case.
    
    This series aims to improve the allocation success rates without
    regressing the benefits of commit fe2c2a106663.  The series is based on
    latest mmotm and takes into account the __GFP_NO_KSWAPD flag is going
    away.
    
    Patch 1 updates a stale comment seeing as I was in the general area.
    
    Patch 2 updates reclaim/compaction to reclaim pages scaled on the number
    	of recent failures.
    
    Patch 3 captures suitable high-order pages freed by compaction to reduce
    	races with parallel allocation requests.
    
    Patch 4 fixes the upstream commit [7db8889a: mm: have order > 0 compaction
    	start off where it left] to enable compaction again
    
    Patch 5 identifies when compacion is taking too long due to contention
    	and aborts.
    
    STRESS-HIGHALLOC
    		 3.6-rc1-akpm	  full-series
    Pass 1          36.00 ( 0.00%)    51.00 (15.00%)
    Pass 2          42.00 ( 0.00%)    63.00 (21.00%)
    while Rested    86.00 ( 0.00%)    86.00 ( 0.00%)
    
    From
    
      http://www.csn.ul.ie/~mel/postings/mmtests-20120424/global-dhp__stress-highalloc-performance-ext3/hydra/comparison.html
    
    I know that the allocation success rates in 3.3.6 was 78% in comparison
    to 36% in in the current akpm tree.  With the full series applied, the
    success rates are up to around 51% with some variability in the results.
    This is not as high a success rate but it does not reclaim excessively
    which is a key point.
    
    MMTests Statistics: vmstat
    Page Ins                                     3050912     3078892
    Page Outs                                    8033528     8039096
    Swap Ins                                           0           0
    Swap Outs                                          0           0
    
    Note that swap in/out rates remain at 0. In 3.3.6 with 78% success rates
    there were 71881 pages swapped out.
    
    Direct pages scanned                           70942      122976
    Kswapd pages scanned                         1366300     1520122
    Kswapd pages reclaimed                       1366214     1484629
    Direct pages reclaimed                         70936      105716
    Kswapd efficiency                                99%         97%
    Kswapd velocity                             1072.550    1182.615
    Direct efficiency                                99%         85%
    Direct velocity                               55.690      95.672
    
    The kswapd velocity changes very little as expected.  kswapd velocity is
    around the 1000 pages/sec mark where as in kernel 3.3.6 with the high
    allocation success rates it was 8140 pages/second.  Direct velocity is
    higher as a result of patch 2 of the series but this is expected and is
    acceptable.  The direct reclaim and kswapd velocities change very little.
    
    If these get accepted for merging then there is a difficulty in how they
    should be handled.  7db8889a ("mm: have order > 0 compaction start off
    where it left") is broken but it is already in 3.6-rc1 and needs to be
    fixed.  However, if just patch 4 from this series is applied then Jim
    Schutt's workload is known to break again as his workload also requires
    patch 5.  While it would be preferred to have all these patches in 3.6 to
    improve compaction in general, it would at least be acceptable if just
    patches 4 and 5 were merged to 3.6 to fix a known problem without breaking
    compaction completely.  On the face of it, that would force
    __GFP_NO_KSWAPD patches to be merged at the same time but I can do a
    version of this series with __GFP_NO_KSWAPD change reverted and then
    rebase it on top of this series.  That might be best overall because I
    note that the __GFP_NO_KSWAPD patch should have removed
    deferred_compaction from page_alloc.c but it didn't but fixing that causes
    collisions with this series.
    
    This patch:
    
    The comment about order applied when the check was order >
    PAGE_ALLOC_COSTLY_ORDER which has not been the case since c5a73c3d ("thp:
    use compaction for all allocation orders").  Fixing the comment while I'm
    in the general area.
    
    Change-Id: Ida65d938b78618ec098d5e511e88cf39578ba606
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 4ffb6335da87b51c17e7ff6495170785f21558dd
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 15a146d44b10debad804ab9eba896615a2d9e298
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Aug 21 16:16:17 2012 -0700

    mm: compaction: Abort async compaction if locks are contended or taking too long
    
    Jim Schutt reported a problem that pointed at compaction contending
    heavily on locks.  The workload is straight-forward and in his own words;
    
    	The systems in question have 24 SAS drives spread across 3 HBAs,
    	running 24 Ceph OSD instances, one per drive.  FWIW these servers
    	are dual-socket Intel 5675 Xeons w/48 GB memory.  I've got ~160
    	Ceph Linux clients doing dd simultaneously to a Ceph file system
    	backed by 12 of these servers.
    
    Early in the test everything looks fine
    
      procs -------------------memory------------------ ---swap-- -----io---- --system-- -----cpu-------
       r  b       swpd       free       buff      cache   si   so    bi    bo   in   cs  us sy  id wa st
      31 15          0     287216        576   38606628    0    0     2  1158    2   14   1  3  95  0  0
      27 15          0     225288        576   38583384    0    0    18 2222016 203357 134876  11 56  17 15  0
      28 17          0     219256        576   38544736    0    0    11 2305932 203141 146296  11 49  23 17  0
       6 18          0     215596        576   38552872    0    0     7 2363207 215264 166502  12 45  22 20  0
      22 18          0     226984        576   38596404    0    0     3 2445741 223114 179527  12 43  23 22  0
    
    and then it goes to pot
    
      procs -------------------memory------------------ ---swap-- -----io---- --system-- -----cpu-------
       r  b       swpd       free       buff      cache   si   so    bi    bo   in   cs  us sy  id wa st
      163  8          0     464308        576   36791368    0    0    11 22210  866  536   3 13  79  4  0
      207 14          0     917752        576   36181928    0    0   712 1345376 134598 47367   7 90   1  2  0
      123 12          0     685516        576   36296148    0    0   429 1386615 158494 60077   8 84   5  3  0
      123 12          0     598572        576   36333728    0    0  1107 1233281 147542 62351   7 84   5  4  0
      622  7          0     660768        576   36118264    0    0   557 1345548 151394 59353   7 85   4  3  0
      223 11          0     283960        576   36463868    0    0    46 1107160 121846 33006   6 93   1  1  0
    
    Note that system CPU usage is very high blocks being written out has
    dropped by 42%. He analysed this with perf and found
    
      perf record -g -a sleep 10
      perf report --sort symbol --call-graph fractal,5
        34.63%  [k] _raw_spin_lock_irqsave
                |
                |--97.30%-- isolate_freepages
                |          compaction_alloc
                |          unmap_and_move
                |          migrate_pages
                |          compact_zone
                |          compact_zone_order
                |          try_to_compact_pages
                |          __alloc_pages_direct_compact
                |          __alloc_pages_slowpath
                |          __alloc_pages_nodemask
                |          alloc_pages_vma
                |          do_huge_pmd_anonymous_page
                |          handle_mm_fault
                |          do_page_fault
                |          page_fault
                |          |
                |          |--87.39%-- skb_copy_datagram_iovec
                |          |          tcp_recvmsg
                |          |          inet_recvmsg
                |          |          sock_recvmsg
                |          |          sys_recvfrom
                |          |          system_call
                |          |          __recv
                |          |          |
                |          |           --100.00%-- (nil)
                |          |
                |           --12.61%-- memcpy
                 --2.70%-- [...]
    
    There was other data but primarily it is all showing that compaction is
    contended heavily on the zone->lock and zone->lru_lock.
    
    commit [b2eef8c0: mm: compaction: minimise the time IRQs are disabled
    while isolating pages for migration] noted that it was possible for
    migration to hold the lru_lock for an excessive amount of time. Very
    broadly speaking this patch expands the concept.
    
    This patch introduces compact_checklock_irqsave() to check if a lock
    is contended or the process needs to be scheduled. If either condition
    is true then async compaction is aborted and the caller is informed.
    The page allocator will fail a THP allocation if compaction failed due
    to contention. This patch also introduces compact_trylock_irqsave()
    which will acquire the lock only if it is not contended and the process
    does not need to schedule.
    
    Change-Id: Ia5318c923b903948072ff279dc9aed698bb6d02d
    Reported-by: Jim Schutt <jaschut@sandia.gov>
    Tested-by: Jim Schutt <jaschut@sandia.gov>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c67fe3752abe6ab47639e2f9b836900c3dc3da84
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixup in isolate_migratepages_range]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	mm/page_alloc.c

commit 928445c3a575c337d99a85e74e6d0a87f7167b25
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Aug 21 16:16:03 2012 -0700

    mm/compaction.c: fix deferring compaction mistake
    
    Commit aff622495c9a ("vmscan: only defer compaction for failed order and
    higher") fixed bad deferring policy but made mistake about checking
    compact_order_failed in __compact_pgdat().  So it can't update
    compact_order_failed with the new order.  This ends up preventing
    correct operation of policy deferral.  This patch fixes it.
    
    Change-Id: I8d65a5511d90e6c02bb58cd7bc5743e4327271f9
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c81758fbe0fdbbc0c74b37798f55bd9c91d5c068
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2598476ebf1b8bacc80d00fd097b6db90c0c7be7
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:51 2012 -0700

    cma: decrease cc.nr_migratepages after reclaiming pagelist
    
    reclaim_clean_pages_from_list() reclaims clean pages before migration so
    cc.nr_migratepages should be updated.  Currently, there is no problem but
    it can be wrong if we try to use the value in future.
    
    Change-Id: I8b3f1238645ba1b3adcc0fe3c41e10f7074b9a96
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: beb51eaa88238daba698ad837222ad277d440b6d
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Change cc to be structure instead of pointer]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 94bd7f609a5699b569056ade654f46fe35c52623
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:31:55 2012 -0700

    mm: cma: discard clean pages during contiguous allocation instead of migration
    
    Drop clean cache pages instead of migration during alloc_contig_range() to
    minimise allocation latency by reducing the amount of migration that is
    necessary.  It's useful for CMA because latency of migration is more
    important than evicting the background process's working set.  In
    addition, as pages are reclaimed then fewer free pages for migration
    targets are required so it avoids memory reclaiming to get free pages,
    which is a contributory factor to increased latency.
    
    I measured elapsed time of __alloc_contig_migrate_range() which migrates
    10M in 40M movable zone in QEMU machine.
    
    Before - 146ms, After - 7ms
    
    Change-Id: Ia527b7253bc5fa63b555ac445b676588b6def119
    [akpm@linux-foundation.org: fix nommu build]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Mel Gorman <mgorman@suse.de>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Rik van Riel <riel@redhat.com>
    Tested-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 02c6de8d757cb32c0829a45d81c3dfcbcafd998b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Fixups in mm/internal.h due to contexts]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 0976a46719066dc17b378ed37a84b5b0fb918bad
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:59 2012 -0700

    mm/vmscan: remove update_isolated_counts()
    
    update_isolated_counts() is no longer required, because lumpy-reclaim was
    removed.  Insanity is over, now there is only one kind of inactive page.
    
    Change-Id: Ib2a40af679a00d23b22800d0e513f60838285a15
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 95d918fc009072c2f88ce2e8b5db2e5abfad7c3e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 6d318ffdca643db5fe765b2f4daefe44a8b0183c
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:58 2012 -0700

    mm/vmscan: push lruvec pointer into isolate_lru_pages()
    
    Move the mem_cgroup_zone_lruvec() call from isolate_lru_pages() into
    shrink_[in]active_list().  Further patches push it to shrink_zone() step
    by step.
    
    Change-Id: I59593e52a524b1b6713c0421c3ed956f78c1e1a8
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 5dc35979e444b50d5551bdeb7a7abc5c69c875d0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 6f3629e6e7f5d144e1c54d8111b24ba4422a638f
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:59 2012 -0700

    mm/vmscan: push zone pointer into shrink_page_list()
    
    It doesn't need a pointer to the cgroup - pointer to the zone is enough.
    This patch also kills the "mz" argument of page_check_references() - it is
    unused after "mm: memcg: count pte references from every member of the
    reclaimed hierarch"
    
    Change-Id: I9b219780be851f696dc5e3b7fc21889035d00313
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 6a18adb35c27848195c938b0779ce882d63d3ed1
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f62e77490b11f3643ef600491072a9dc3c383d00
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue May 29 15:06:25 2012 -0700

    mm: memcg: count pte references from every member of the reclaimed hierarchy
    
    The rmap walker checking page table references has historically ignored
    references from VMAs that were not part of the memcg that was being
    reclaimed during memcg hard limit reclaim.
    
    When transitioning global reclaim to memcg hierarchy reclaim, I missed
    that bit and now references from outside a memcg are ignored even during
    global reclaim.
    
    Reverting back to traditional behaviour - count all references during
    global reclaim and only mind references of the memcg being reclaimed
    during limit reclaim would be one option.
    
    However, the more generic idea is to ignore references exactly then when
    they are outside the hierarchy that is currently under reclaim; because
    only then will their reclamation be of any use to help the pressure
    situation.  It makes no sense to ignore references from a sibling memcg
    and then evict a page that will be immediately refaulted by that sibling
    which contributes to the same usage of the common ancestor under
    reclaim.
    
    The solution: make the rmap walker ignore references from VMAs that are
    not part of the hierarchy that is being reclaimed.
    
    Flat limit reclaim will stay the same, hierarchical limit reclaim will
    mind the references only to pages that the hierarchy owns.  Global
    reclaim, since it reclaims from all memcgs, will be fixed to regard all
    references.
    
    Change-Id: I3a3f39693cf5644870213df28238acf00d7417dd
    [akpm@linux-foundation.org: name the args in the declaration]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Konstantin Khlebnikov<khlebnikov@openvz.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c3ac9a8ade65ccbfd145fbff895ae8d8d62d09b0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e79db9a933e39431f3e96c52223c728053f0525a
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:57 2012 -0700

    mm/vmscan: store "priority" in struct scan_control
    
    In memory reclaim some function have too many arguments - "priority" is
    one of them.  It can be stored in struct scan_control - we construct them
    on the same level.  Instead of an open coded loop we set the initial
    sc.priority, and do_try_to_free_pages() decreases it down to zero.
    
    Change-Id: I7c03b4367fe3787dfd82afe1ff21469a99bdb04f
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 9e3b2f8cd340e13353a44c9a34caef2848131ed7
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	mm/vmscan.c

commit 4bb594f1f75b594fc3d94063c50ce59b69b43f5f
Author: Rik van Riel <riel@redhat.com>
Date:   Tue May 29 15:06:18 2012 -0700

    mm: remove swap token code
    
    The swap token code no longer fits in with the current VM model.  It
    does not play well with cgroups or the better NUMA placement code in
    development, since we have only one swap token globally.
    
    It also has the potential to mess with scalability of the system, by
    increasing the number of non-reclaimable pages on the active and
    inactive anon LRU lists.
    
    Last but not least, the swap token code has been broken for a year
    without complaints, as reported by Konstantin Khlebnikov.  This suggests
    we no longer have much use for it.
    
    The days of sub-1G memory systems with heavy use of swap are over.  If
    we ever need thrashing reducing code in the future, we will have to
    implement something that does scale.
    
    Change-Id: I6d287cfc3c3206ca24da2de0c1392e5fdfcfabe8
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Bob Picco <bpicco@meloft.net>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e709ffd6169ccd259eb5874e853303e91e94e829
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 7c4127affa24e8c3e407fec436ad2e76c0922f51
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:57 2012 -0700

    mm/memcg: use vm_swappiness from target memory cgroup
    
    Use vm_swappiness from memory cgroup which is triggered this memory
    reclaim.  This is more reasonable and allows to kill one argument.
    
    Change-Id: I6aa49763a5746f021ae084885df6764bb7835a62
    [akpm@linux-foundation.org: fix build (patch skew)]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujtisu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3d58ab5c97fa2d145050242137ac39ca7d3bc2fc
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f7a7aeac429c196e3a163443788bb96d4167630a
Author: Hugh Dickins <hughd@google.com>
Date:   Tue May 29 15:06:52 2012 -0700

    mm/memcg: scanning_global_lru means mem_cgroup_disabled
    
    Although one has to admire the skill with which it has been concealed,
    scanning_global_lru(mz) is actually just an interesting way to test
    mem_cgroup_disabled().  Too many developer hours have been wasted on
    confusing it with global_reclaim(): just use mem_cgroup_disabled().
    
    Change-Id: I9f8f70b41b002cc8c4583a0a9869459da24b4fe5
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c3c787e8c38557ccf44c670d73aebe630a2b1479
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 819ad728aaee16a2dc8c43e21d791f7d94c268b4
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:54 2012 -0700

    mm/memcg: kill mem_cgroup_lru_del()
    
    This patch kills mem_cgroup_lru_del(), we can use
    mem_cgroup_lru_del_list() instead.  On 0-order isolation we already have
    right lru list id.
    
    Change-Id: I403d40074299fb5f125603435c057071714d5b92
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bbf808ed7de68fdf626fd4f9718d88cf03ce13a9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 5731e2737515477f09288bfdbc79b172ca18d12f
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:54 2012 -0700

    mm: remove lru type checks from __isolate_lru_page()
    
    After patch "mm: forbid lumpy-reclaim in shrink_active_list()" we can
    completely remove anon/file and active/inactive lru type filters from
    __isolate_lru_page(), because isolation for 0-order reclaim always
    isolates pages from right lru list.  And pages-isolation for lumpy
    shrink_inactive_list() or memory-compaction anyway allowed to isolate
    pages from all evictable lru lists.
    
    Change-Id: I2a1a0325b1d193f4ca5e3ea7d5eda9b8bf7c6698
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f3fd4a61928a5edf5b033a417e761b488b43e203
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f13e176176f75156d66dee653a8c1c8e11ab60c2
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:20 2012 -0700

    mm: vmscan: remove reclaim_mode_t
    
    There is little motiviation for reclaim_mode_t once RECLAIM_MODE_[A]SYNC
    and lumpy reclaim have been removed.  This patch gets rid of
    reclaim_mode_t as well and improves the documentation about what
    reclaim/compaction is and when it is triggered.
    
    Change-Id: If95bc163647b1cfb93d7f3b8435060fed1e2aabf
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 23b9da55c5b0feb484bd5e8615f4eb1ce4169453
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	mm/vmscan.c

commit f8de05cf7ee47ad0cb0511a6a4fdde724e09ce77
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:19 2012 -0700

    mm: vmscan: do not stall on writeback during memory compaction
    
    This patch stops reclaim/compaction entering sync reclaim as this was
    only intended for lumpy reclaim and an oversight.  Page migration has
    its own logic for stalling on writeback pages if necessary and memory
    compaction is already using it.
    
    Waiting on page writeback is bad for a number of reasons but the primary
    one is that waiting on writeback to a slow device like USB can take a
    considerable length of time.  Page reclaim instead uses
    wait_iff_congested() to throttle if too many dirty pages are being
    scanned.
    
    Change-Id: I14f312b1a51ee093d9d4adda5c87e57f1b83e03d
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 41ac1999c3e3563e1810b14878a869c79c9368bb
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit afba3004fbb81a9ca936626e85b2cd1ebb6a2948
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:19 2012 -0700

    mm: vmscan: remove lumpy reclaim
    
    This series removes lumpy reclaim and some stalling logic that was
    unintentionally being used by memory compaction.  The end result is that
    stalling on dirty pages during page reclaim now depends on
    wait_iff_congested().
    
    Four kernels were compared
    
      3.3.0     vanilla
      3.4.0-rc2 vanilla
      3.4.0-rc2 lumpyremove-v2 is patch one from this series
      3.4.0-rc2 nosync-v2r3 is the full series
    
    Removing lumpy reclaim saves almost 900 bytes of text whereas the full
    series removes 1200 bytes.
    
         text     data      bss       dec     hex  filename
      6740375  1927944  2260992  10929311  a6c49f  vmlinux-3.4.0-rc2-vanilla
      6739479  1927944  2260992  10928415  a6c11f  vmlinux-3.4.0-rc2-lumpyremove-v2
      6739159  1927944  2260992  10928095  a6bfdf  vmlinux-3.4.0-rc2-nosync-v2
    
    There are behaviour changes in the series and so tests were run with
    monitoring of ftrace events.  This disrupts results so the performance
    results are distorted but the new behaviour should be clearer.
    
    fs-mark running in a threaded configuration showed little of interest as
    it did not push reclaim aggressively
    
      FS-Mark Multi Threaded
                              3.3.0-vanilla       rc2-vanilla       lumpyremove-v2r3       nosync-v2r3
      Files/s  min           3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Files/s  mean          3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Files/s  stddev        0.00 ( 0.00%)        0.00 ( 0.00%)        0.00 ( 0.00%)        0.00 ( 0.00%)
      Files/s  max           3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Overhead min      508667.00 ( 0.00%)   521350.00 (-2.49%)   544292.00 (-7.00%)   547168.00 (-7.57%)
      Overhead mean     551185.00 ( 0.00%)   652690.73 (-18.42%)   991208.40 (-79.83%)   570130.53 (-3.44%)
      Overhead stddev    18200.69 ( 0.00%)   331958.29 (-1723.88%)  1579579.43 (-8578.68%)     9576.81 (47.38%)
      Overhead max      576775.00 ( 0.00%)  1846634.00 (-220.17%)  6901055.00 (-1096.49%)   585675.00 (-1.54%)
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             309.90    300.95    307.33    298.95
      User+Sys Time Running Test (seconds)        319.32    309.67    315.69    307.51
      Total Elapsed Time (seconds)               1187.85   1193.09   1191.98   1193.73
    
      MMTests Statistics: vmstat
      Page Ins                                       80532       82212       81420       79480
      Page Outs                                  111434984   111456240   111437376   111582628
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                           44881       27889       27453       34843
      Kswapd pages scanned                        25841428    25860774    25861233    25843212
      Kswapd pages reclaimed                      25841393    25860741    25861199    25843179
      Direct pages reclaimed                         44881       27889       27453       34843
      Kswapd efficiency                                99%         99%         99%         99%
      Kswapd velocity                            21754.791   21675.460   21696.029   21649.127
      Direct efficiency                               100%        100%        100%        100%
      Direct velocity                               37.783      23.375      23.031      29.188
      Percentage direct scans                           0%          0%          0%          0%
    
    ftrace showed that there was no stalling on writeback or pages submitted
    for IO from reclaim context.
    
    postmark was similar and while it was more interesting, it also did not
    push reclaim heavily.
    
      POSTMARK
                                           3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      Transactions per second:               16.00 ( 0.00%)    20.00 (25.00%)    18.00 (12.50%)    17.00 ( 6.25%)
      Data megabytes read per second:        18.80 ( 0.00%)    24.27 (29.10%)    22.26 (18.40%)    20.54 ( 9.26%)
      Data megabytes written per second:     35.83 ( 0.00%)    46.25 (29.08%)    42.42 (18.39%)    39.14 ( 9.24%)
      Files created alone per second:        28.00 ( 0.00%)    38.00 (35.71%)    34.00 (21.43%)    30.00 ( 7.14%)
      Files create/transact per second:       8.00 ( 0.00%)    10.00 (25.00%)     9.00 (12.50%)     8.00 ( 0.00%)
      Files deleted alone per second:       556.00 ( 0.00%)  1224.00 (120.14%)  3062.00 (450.72%)  6124.00 (1001.44%)
      Files delete/transact per second:       8.00 ( 0.00%)    10.00 (25.00%)     9.00 (12.50%)     8.00 ( 0.00%)
    
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             113.34    107.99    109.73    108.72
      User+Sys Time Running Test (seconds)        145.51    139.81    143.32    143.55
      Total Elapsed Time (seconds)               1159.16    899.23    980.17   1062.27
    
      MMTests Statistics: vmstat
      Page Ins                                    13710192    13729032    13727944    13760136
      Page Outs                                   43071140    42987228    42733684    42931624
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                               0           0           0           0
      Kswapd pages scanned                         9941613     9937443     9939085     9929154
      Kswapd pages reclaimed                       9940926     9936751     9938397     9928465
      Direct pages reclaimed                             0           0           0           0
      Kswapd efficiency                                99%         99%         99%         99%
      Kswapd velocity                             8576.567   11051.058   10140.164    9347.109
      Direct efficiency                               100%        100%        100%        100%
      Direct velocity                                0.000       0.000       0.000       0.000
    
    It looks like here that the full series regresses performance but as
    ftrace showed no usage of wait_iff_congested() or sync reclaim I am
    assuming it's a disruption due to monitoring.  Other data such as memory
    usage, page IO, swap IO all looked similar.
    
    Running a benchmark with a plain DD showed nothing very interesting.
    The full series stalled in wait_iff_congested() slightly less but stall
    times on vanilla kernels were marginal.
    
    Running a benchmark that hammered on file-backed mappings showed stalls
    due to congestion but not in sync writebacks
    
      MICRO
                                           3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             308.13    294.50    298.75    299.53
      User+Sys Time Running Test (seconds)        330.45    316.28    318.93    320.79
      Total Elapsed Time (seconds)               1814.90   1833.88   1821.14   1832.91
    
      MMTests Statistics: vmstat
      Page Ins                                      108712      120708       97224      110344
      Page Outs                                  155514576   156017404   155813676   156193256
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                         2599253     1550480     2512822     2414760
      Kswapd pages scanned                        69742364    71150694    68839041    69692533
      Kswapd pages reclaimed                      34824488    34773341    34796602    34799396
      Direct pages reclaimed                         53693       94750       61792       75205
      Kswapd efficiency                                49%         48%         50%         49%
      Kswapd velocity                            38427.662   38797.901   37799.972   38022.889
      Direct efficiency                                 2%          6%          2%          3%
      Direct velocity                             1432.174     845.464    1379.807    1317.446
      Percentage direct scans                           3%          2%          3%          3%
      Page writes by reclaim                             0           0           0           0
      Page writes file                                   0           0           0           0
      Page writes anon                                   0           0           0           0
      Page reclaim immediate                             0           0           0        1218
      Page rescued immediate                             0           0           0           0
      Slabs scanned                                  15360       16384       13312       16384
      Direct inode steals                                0           0           0           0
      Kswapd inode steals                             4340        4327        1630        4323
    
      FTrace Reclaim Statistics: congestion_wait
      Direct number congest     waited                 0          0          0          0
      Direct time   congest     waited               0ms        0ms        0ms        0ms
      Direct full   congest     waited                 0          0          0          0
      Direct number conditional waited               900        870        754        789
      Direct time   conditional waited               0ms        0ms        0ms       20ms
      Direct full   conditional waited                 0          0          0          0
      KSwapd number congest     waited              2106       2308       2116       1915
      KSwapd time   congest     waited          139924ms   157832ms   125652ms   132516ms
      KSwapd full   congest     waited              1346       1530       1202       1278
      KSwapd number conditional waited             12922      16320      10943      14670
      KSwapd time   conditional waited               0ms        0ms        0ms        0ms
      KSwapd full   conditional waited                 0          0          0          0
    
    Reclaim statistics are not radically changed.  The stall times in kswapd
    are massive but it is clear that it is due to calls to congestion_wait()
    and that is almost certainly the call in balance_pgdat().  Otherwise
    stalls due to dirty pages are non-existant.
    
    I ran a benchmark that stressed high-order allocation.  This is very
    artifical load but was used in the past to evaluate lumpy reclaim and
    compaction.  Generally I look at allocation success rates and latency
    figures.
    
      STRESS-HIGHALLOC
                       3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      Pass 1          81.00 ( 0.00%)    28.00 (-53.00%)    24.00 (-57.00%)    28.00 (-53.00%)
      Pass 2          82.00 ( 0.00%)    39.00 (-43.00%)    38.00 (-44.00%)    43.00 (-39.00%)
      while Rested    88.00 ( 0.00%)    87.00 (-1.00%)    88.00 ( 0.00%)    88.00 ( 0.00%)
    
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             740.93    681.42    685.14    684.87
      User+Sys Time Running Test (seconds)       2922.65   3269.52   3281.35   3279.44
      Total Elapsed Time (seconds)               1161.73   1152.49   1159.55   1161.44
    
      MMTests Statistics: vmstat
      Page Ins                                     4486020     2807256     2855944     2876244
      Page Outs                                    7261600     7973688     7975320     7986120
      Swap Ins                                       31694           0           0           0
      Swap Outs                                      98179           0           0           0
      Direct pages scanned                           53494       57731       34406      113015
      Kswapd pages scanned                         6271173     1287481     1278174     1219095
      Kswapd pages reclaimed                       2029240     1281025     1260708     1201583
      Direct pages reclaimed                          1468       14564       16649       92456
      Kswapd efficiency                                32%         99%         98%         98%
      Kswapd velocity                             5398.133    1117.130    1102.302    1049.641
      Direct efficiency                                 2%         25%         48%         81%
      Direct velocity                               46.047      50.092      29.672      97.306
      Percentage direct scans                           0%          4%          2%          8%
      Page writes by reclaim                       1616049           0           0           0
      Page writes file                             1517870           0           0           0
      Page writes anon                               98179           0           0           0
      Page reclaim immediate                        103778       27339        9796       17831
      Page rescued immediate                             0           0           0           0
      Slabs scanned                                1096704      986112      980992      998400
      Direct inode steals                              223      215040      216736      247881
      Kswapd inode steals                           175331       61548       68444       63066
      Kswapd skipped wait                            21991           0           1           0
      THP fault alloc                                    1         135         125         134
      THP collapse alloc                               393         311         228         236
      THP splits                                        25          13           7           8
      THP fault fallback                                 0           0           0           0
      THP collapse fail                                  3           5           7           7
      Compaction stalls                                865        1270        1422        1518
      Compaction success                               370         401         353         383
      Compaction failures                              495         869        1069        1135
      Compaction pages moved                        870155     3828868     4036106     4423626
      Compaction move failure                        26429       23865       29742       27514
    
    Success rates are completely hosed for 3.4-rc2 which is almost certainly
    due to commit fe2c2a106663 ("vmscan: reclaim at order 0 when compaction
    is enabled").  I expected this would happen for kswapd and impair
    allocation success rates (https://lkml.org/lkml/2012/1/25/166) but I did
    not anticipate this much a difference: 80% less scanning, 37% less
    reclaim by kswapd
    
    In comparison, reclaim/compaction is not aggressive and gives up easily
    which is the intended behaviour.  hugetlbfs uses __GFP_REPEAT and would
    be much more aggressive about reclaim/compaction than THP allocations
    are.  The stress test above is allocating like neither THP or hugetlbfs
    but is much closer to THP.
    
    Mainline is now impaired in terms of high order allocation under heavy
    load although I do not know to what degree as I did not test with
    __GFP_REPEAT.  Keep this in mind for bugs related to hugepage pool
    resizing, THP allocation and high order atomic allocation failures from
    network devices.
    
    In terms of congestion throttling, I see the following for this test
    
      FTrace Reclaim Statistics: congestion_wait
      Direct number congest     waited                 3          0          0          0
      Direct time   congest     waited               0ms        0ms        0ms        0ms
      Direct full   congest     waited                 0          0          0          0
      Direct number conditional waited               957        512       1081       1075
      Direct time   conditional waited               0ms        0ms        0ms        0ms
      Direct full   conditional waited                 0          0          0          0
      KSwapd number congest     waited                36          4          3          5
      KSwapd time   congest     waited            3148ms      400ms      300ms      500ms
      KSwapd full   congest     waited                30          4          3          5
      KSwapd number conditional waited             88514        197        332        542
      KSwapd time   conditional waited            4980ms        0ms        0ms        0ms
      KSwapd full   conditional waited                49          0          0          0
    
    The "conditional waited" times are the most interesting as this is
    directly impacted by the number of dirty pages encountered during scan.
    As lumpy reclaim is no longer scanning contiguous ranges, it is finding
    fewer dirty pages.  This brings wait times from about 5 seconds to 0.
    kswapd itself is still calling congestion_wait() so it'll still stall but
    it's a lot less.
    
    In terms of the type of IO we were doing, I see this
    
      FTrace Reclaim Statistics: mm_vmscan_writepage
      Direct writes anon  sync                         0          0          0          0
      Direct writes anon  async                        0          0          0          0
      Direct writes file  sync                         0          0          0          0
      Direct writes file  async                        0          0          0          0
      Direct writes mixed sync                         0          0          0          0
      Direct writes mixed async                        0          0          0          0
      KSwapd writes anon  sync                         0          0          0          0
      KSwapd writes anon  async                    91682          0          0          0
      KSwapd writes file  sync                         0          0          0          0
      KSwapd writes file  async                   822629          0          0          0
      KSwapd writes mixed sync                         0          0          0          0
      KSwapd writes mixed async                        0          0          0          0
    
    In 3.2, kswapd was doing a bunch of async writes of pages but
    reclaim/compaction was never reaching a point where it was doing sync
    IO.  This does not guarantee that reclaim/compaction was not calling
    wait_on_page_writeback() but I would consider it unlikely.  It indicates
    that merging patches 2 and 3 to stop reclaim/compaction calling
    wait_on_page_writeback() should be safe.
    
    This patch:
    
    Lumpy reclaim had a purpose but in the mind of some, it was to kick the
    system so hard it trashed.  For others the purpose was to complicate
    vmscan.c.  Over time it was giving softer shoes and a nicer attitude but
    memory compaction needs to step up and replace it so this patch sends
    lumpy reclaim to the farm.
    
    The tracepoint format changes for isolating LRU pages with this patch
    applied.  Furthermore reclaim/compaction can no longer queue dirty pages
    in pageout() if the underlying BDI is congested.  Lumpy reclaim used
    this logic and reclaim/compaction was using it in error.
    
    Change-Id: Ib2992962c9e99cf250a7f859bb2a67034051e4d4
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c53919adc045bf803252e912f23028a68525753d
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    
    Conflicts:
    	mm/vmscan.c

commit 5455b2a2d8e814a7ebd126e7a99c40c37200e86d
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:53 2012 -0700

    mm: push lru index into shrink_[in]active_list()
    
    Let's toss lru index through call stack to isolate_lru_pages(), this is
    better than its reconstructing from individual bits.
    
    Change-Id: Id07444ba97af9699ecfff1750db13cb2fee147fc
    [akpm@linux-foundation.org: fix kerneldoc, per Minchan]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3cb9945179bd04e9282f31a1173ac11b1300c462
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 3f2f4d6763dcc2807e7868f68d27bb8c582e841a
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Mon Jun 3 16:15:48 2013 +0530

    msm: pm: send notification only for SPC and PC
    
    Today the pm notification is sent for all the power
    modes of each CPU. This is not necessary for shallow
    power modes.
    
    This change is to send the notification for only
    standalone power collapse for each core and idle
    power collapse of core0.
    
    Change-Id: I9b3df3d72e43d645387c94b1b9c5da3bcf2f2e5f
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit 59924f3840e48decd01ad7b0568ea37cf5a23017
Author: Sameer Thalappil <sameert@codeaurora.org>
Date:   Tue Mar 19 14:28:37 2013 -0700

    nl80211/cfg80211: add VHT MCS support
    
    Add support for reporting and calculating VHT MCSes.
    
    Note that I'm not completely sure that the bitrate
    calculations are correct, nor that they can't be
    simplified.
    
    Change-Id: Id4c132850a85ff59f0fc16396763ed717689bec0
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Git-commit: db9c64cf8d9d3fcbc34b09d037f266d1fc9f928c
    Git-repo:
    git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Sameer Thalappil <sameert@codeaurora.org>

commit ddb8ea1f4ed6bf33fb85967d7c404baa34d9d040
Author: Iliyan Malchev <malchev@google.com>
Date:   Sat May 18 15:43:50 2013 -0700

    timed_gpio: fix order of destruction
    
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 58e1c9a31755b4bc6047b70d0736543a2772bc45
Author: Stephen Smalley <sds@tycho.nsa.gov>
Date:   Fri May 10 10:16:19 2013 -0400

    Enable setting security contexts on rootfs inodes.
    
    rootfs (ramfs) can support setting of security contexts
    by userspace due to the vfs fallback behavior of calling
    the security module to set the in-core inode state
    for security.* attributes when the filesystem does not
    provide an xattr handler.  No xattr handler required
    as the inodes are pinned in memory and have no backing
    store.
    
    This is useful in allowing early userspace to label individual
    files within a rootfs while still providing a policy-defined
    default via genfs.
    
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Change-Id: I3436cf9ae27ade445e37376d7b9125746b1e506f

commit 1445a54c00cec82b3af8a0af93761f5a8077b8e3
Author: FrozenCow <frozencow@gmail.com>
Date:   Tue Jul 16 20:08:28 2013 -0500

    usb: gadget: mass_storage: added sysfs entry for cdrom to LUNs
    
    This patch adds a "cdrom" sysfs entry for each mass_storage LUN, just
    like "ro" sysfs entry. This allows switching between USB and CD-ROM
    emulation without reinserting the module or recompiling the kernel.
    
    Change-Id: I115e3c8b5cf6e473cb081ef84f7be451ecdc1f0f

commit 5bb5399fa8536eb126789701b873bbaf723f47b7
Author: JP Abgrall <jpa@google.com>
Date:   Thu May 30 15:31:17 2013 -0700

    misc: uidstat: avoid create_stat() race and blockage.
    
    * create_stat() race would lead to:
      [   58.132324] proc_dir_entry 'uid_stat/10061' already registered
    
    * blocking kmalloc reported by sbranden
     tcp_read_sock()
      uid_stat_tcp_rcv()
        create_stat()
          kmalloc(GFP_KERNEL)
    
    Signed-off-by: JP Abgrall <jpa@google.com>

commit bff3e4d860332286b0a8c03757b6b4ef44357822
Author: Caleb Johnson <spidercaleb@gmail.com>
Date:   Sat Jun 29 12:12:31 2013 -0500

    The names to the functions that acquire and release a console lock
    have changed, but kernel/power/consoleearlysuspend.c still uses the old
    names. See http://goo.gl/pxgOK
    
    Change-Id: Ie972e51e1acfcec3a12c8fdc86b9287104346a99

commit f10a1422905e623f26e6f6240fac4f462be7938d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 22 14:04:28 2012 +0200

    sched: Make sure to not re-read variables after validation
    
    We could re-read rq->rt_avg after we validated it was smaller than
    total, invalidating the check and resulting in an unintended negative.
    
    Change-Id: I8543974aad539107768e9e513ca3a8c4cb79b2ff
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: David Rientjes <rientjes@google.com>
    Link: http://lkml.kernel.org/r/1337688268.9698.29.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    CRs-Fixed: 497236
    Git-commit: b654f7de41b0e3903ee2b51d3b8db77fe52ce728
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit e6279ad94bfb180b47419a59d638838fa72db20c
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Tue Jun 11 17:43:09 2013 -0700

    sched: re-calculate a cpu's next_balance point upon sched domain changes
    
    Commit 55ddeb0f (sched: Reset rq->next_interval before going idle) reset
    a cpu's rq->next_balance when pulled_task = 0, which will be true when
    the cpu failed to pull any task, causing it go idle. However that patch
    relied on next_balance being calculated as a result of traversing cpu's
    sched domain hierarchy.
    
    A cpu that is the only online cpu will however not be attached to any
    sched domain hierarchy. When such a cpu calls into idle_balance(), we
    will end up initializing next_balance to be 1sec away! Such a CPU will
    defer load balance check for another 1sec, even though we may bring up
    more cpus in the meantime requiring it to check for load imbalance more
    frequently. This could then lead to increased scheduling latency for
    some tasks.
    
    This patch results in a cpu's next_balance being re-calculated when its
    attaching to a new sched domain hierarchy.  This should let cpus call
    load balance checks at the right time we expect them to!
    
    Change-Id: I855cff8da5ca28d278596c3bb0163b839d4704bc
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 9d69101d3ac2ada49a84007366abb7f08d5a12bd
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Thu May 23 15:24:57 2013 -0700

    sched: remove migration notification from RT class
    
    Commit 88a7e37d265 (sched: provide per cpu-cgroup option to
    notify on migrations) added a notifier call when a task is moved
    to a different CPU. Unfortunately the two call sites in the RT
    sched class where this occurs happens with a runqueue lock held.
    This can result in a deadlock if the notifier call attempts to do
    something like wake up a task.
    
    Fortunately the benefit of 88a7e37d265 comes mainly from notifying
    on migration of non-RT tasks, so we can simply ignore the movements
    of RT tasks.
    
    CRs-Fixed: 491370
    Change-Id: I8849d826bf1eeaf85a6f6ad872acb475247c5926
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 0575ffc7e8a64634c1144b0d605caa6c2a8dd6bd
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Mon Mar 11 16:33:42 2013 -0700

    sched: provide per cpu-cgroup option to notify on migrations
    
    On systems where CPUs may run asynchronously, task migrations
    between CPUs running at grossly different speeds can cause
    problems.
    
    This change provides a mechanism to notify a subsystem
    in the kernel if a task in a particular cgroup migrates to a
    different CPU. Other subsystems (such as cpufreq) may then
    register for this notifier to take appropriate action when
    such a task is migrated.
    
    The cgroup attribute to set for this behavior is
    "notify_on_migrate" .
    
    Change-Id: Ie1868249e53ef901b89c837fdc33b0ad0c0a4590
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 3ea363cea05bf0838d056e13441031588a6b2e29
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu May 16 14:36:58 2013 +0300

    block: Remove "requeuing urgent req" error messages
    
    It is possible for URGENT request to be requeued/reinserted if it was
    fetched during the creation of a packed list. This end case is rare and is
    not handled at the moment.
    This patch changes the messages notifying of the above to debug level
    (instead of error) in order to clear the dmesg log.
    
    Change-Id: Ie8bc067e61559a6f702077b95c5dbcc426404232
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 3c5a70e08061cbb23e82b7097bd83592531af613
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:08 2013 +0000

    lockdep: remove task argument from debug_check_no_locks_held
    
    The only existing caller to debug_check_no_locks_held calls it
    with 'current' as the task, and the freezer needs to call
    debug_check_no_locks_held but doesn't already have a current
    task pointer, so remove the argument.  It is already assuming
    that the current task is relevant by dumping the current stack
    trace as part of the warning.
    
    This was originally part of 6aa9707099c (lockdep: check that
    no locks held at freeze time) which was reverted in
    dbf520a9d7d4.
    
    Change-Id: Idbaf1332ce6c80dc49c1d31c324c7fbf210657c5
    Original-author: Mandeep Singh Baines <msb@chromium.org>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e3fb502d3ce7b007933f3af72a50a3952f29e757
Author: Colin Cross <ccross@android.com>
Date:   Wed Aug 15 13:10:04 2012 -0700

    HACK: ARM: disable sleeping while atomic warning in do_signal
    
    ARM disables interrupts in do_signal, which triggers a warning in
    try_to_freeze, see details at https://lkml.org/lkml/2011/8/23/221.
    To prevent the warnings, add try_to_freeze_nowarn and call it from
    do_signal.
    
    Change-Id: If7482de21c386adc705fa1ac4ecb8c7ece5bb356
    Signed-off-by: Colin Cross <ccross@android.com>

commit 4fa6838990d461dbd29e33b63f7ddf2d60a11ecc
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat Jun 1 01:52:05 2013 -0700

    asoc: Kill logspam in compressed audio driver
    
    Change-Id: I415036d4d8fd6729f1ee14a29dafc5efd6b98a4a

commit 13615e0edca1168fe562fbc3c1d0247bc3eb7b0b
Author: Mike Kasick <mike@kasick.org>
Date:   Sat Jul 7 19:00:47 2012 -0400

    Add and enable kexec hardboot support.
    
    Consists of squashed commits from:
    https://github.com/mkasick/android_kernel_samsung_jfltespr.git
    
    commit 750bb80f2854d6af5273e55ea179a4c60b2d9efc
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sun Jun 2 23:11:24 2013 -0400
    
        Clear download mode flag on kexec hardboot.
    
    commit 138ba851ee949af291eae914f410aea2d85ed9f5
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sun May 12 22:39:26 2013 -0400
    
        Support hard booting to a kexec kernel.
    
        See KEXEC_HARDBOOT config option help for details.
    
    commit 3ab41019a7af08395c233bebb605c2f1ea49c8e0
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sat Jul 7 23:10:24 2012 -0400
    
        Support copying of kernel tagged list (atags) in the decompressor.
    
        This is needed to hardboot kexec a kernel with a new tags list (including a
        new kernel command line), since the new atags would otherwise be lost with
        the limited kernel memory mapping.  Without this patch, a hardboot-kexec'd
        kernel uses the atags provided by the bootloader.
    
    commit 446c3d7a6631877f7dc9142359cf0eae300e9a08
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sat Jul 7 23:09:39 2012 -0400
    
        Enable caching and buffering for all of physical RAM in the decompressor.
    
        Old method is to enable caching and buffering only for the 256 MB at the
        start of the decompressor image.  This makes a hardboot-kexec'd kernel very
        slow to decompress since the decompressor is located far above the kernel
        destination.  This patch reduces boot time from 35 to 8 seconds on epicmtd.
    
    commit d3646be88f5b1f9a8ae714c922ec50c681b7157f
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sat Jul 7 19:00:47 2012 -0400
    
        Enable and fix kexec syscall support.
    
        Use mem_text_write_kernel_word when assigning reboot_code_buffer parameters
        to avoid protection faults (writes to read-only kernel memory) when
        CONFIG_STRICT_MEMORY_RWX is enabled.
    
    Change-Id: I0400ad00fcf02b8ad017f8aa371724a659b930b4
    
    Conflicts:
    	arch/arm/configs/msm8930_melius_defconfig

commit 58b3436f6a74daaf401ed978a65cddb2092d1455
Author: David Ferguson <ferguson.david@gmail.com>
Date:   Mon May 27 01:29:43 2013 -0500

    fix compiling on Mac with non-GZIP kernel compressions
    
      * Non-GZIP compressions do not include the size in the file. Makefile.lib
        uses the stat command to determine the size of the file and then append
        the file size to the end of the image.
      * On BSD systems like Mac, stat does not recognize -c "%s" instead it
        needs -f "%z". This resulted in a non-booting kernel since the appended
        file size was either corrupt or 0.
      * Run both forms of stat, using whichever does not error out.
      * Allow the 2nd form to print to stderr if something goes wrong to help
        with debug.
    
    Change-Id: I3295d4f59a3f43af476f891aad47f7962ed34a3e

commit d0cdebb54ca71bf186ddf332b9ed95287a58c250
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 23:50:08 2014 +0100

    Fixed zcache!

commit 624812995cd822379d026fc2d19ab1633a79fbec
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 14:42:57 2013 -0700

    drivers: staging: Fix Zcache
    
     * Remove dependency on obsolete qcache
     * Remove X86 dependency as this version of zsmalloc supports ARM
     * Fix args for updated zsmalloc
     * Enable for JF
    
    Change-Id: I7a6daf84246df54bb8030268d07a96256db4fc42
    
    Conflicts:
    	arch/arm/configs/msm8930_melius_defconfig
    	drivers/gpu/ion/msm/msm_ion.c
    	drivers/staging/zcache/Kconfig
    	drivers/staging/zcache/zcache-main.c
    	drivers/staging/zsmalloc/Kconfig

commit 04f1b1da24cbbf44400e3ebed9244445f92cd008
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 01:20:04 2013 -0700

    jf: Add sleep status device
    
    Change-Id: Ie3a28f36bcdaf0b89d20776e9fba3cbd11813a7d

commit c895b7c0ce89282e043dc44ad2266b52a6785715
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Mon Dec 17 18:51:12 2012 -0800

    msm: SSR: Remove useless warning
    
    This is a warning so that developers know to add more restart
    order lists to SSR when a new chip is added. This is mostly
    irrelevant now because we assume either entire SoC restart on SSR
    or independent restart on SSR, not group restarts. Remove this
    warning as it is mostly a reminder that nobody is listening for.
    
    Change-Id: Icbf955cb18395d8d5d086b2167c5c329588b9256
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 9648a6e6b540ad992f9898592332d817cc8a8dbd
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Mon Feb 4 13:35:16 2013 -0800

    arm: Remove no-longer-required RCU_NONIDLE wrapper
    
    Commit 21111be8 "ARM: Fix negative idle stats for offline cpu" moved call
    to cpu_die() to occur outside of rcu_idle_enter()/rcu_idle_exit() section.
    As a result, the RCU_NONIDLE() wrapper to complete() call in
    arch/arm/kernel/process.c:cpu_die() is no longer required (and is
    technically incorrect to have). Removing RCU_NONIDLE() wrapper also removes
    this warning seen during CPU offline:
    
    [Note: Below message has been edited to fit 75-char per line limit.
    Insignificant portions of warning message has been removed in each line.]
    
    ------------[ cut here ]------------
    WARNING: at kernel/rcutree.c:456 rcu_idle_exit_common+0x4c/0xe0()
    Modules linked in:
    (unwind_backtrace+0x0/0x120) from (warn_slowpath_common+0x4c/0x64)
    (warn_slowpath_common+0x4c/0x64) from (warn_slowpath_null+0x18/0x1c)
    (warn_slowpath_null+0x18/0x1c) from (rcu_idle_exit_common+0x4c/0xe0)
    (rcu_idle_exit_common+0x4c/0xe0) from (rcu_idle_exit+0xa8/0xc0)
    (rcu_idle_exit+0xa8/0xc0) from (cpu_die+0x24/0x5c)
    (cpu_die+0x24/0x5c) from (cpu_idle+0xdc/0xf0)
    (cpu_idle+0xdc/0xf0) from (0x8160)
    ---[ end trace 61bf21937a496a37 ]---
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Change-Id: I721e6b20651674e6f6f584cf8d814af00b688c91

commit ee67c80bf12971e7f4670e5cca17037d08e21c3d
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Wed Nov 28 15:50:20 2012 -0800

    sched: fix rq->lock recursion
    
    Enabling SCHED_HRTICK currently results in rq->lock recursion and a hard
    hang at bootup.  Essentially try_to_wakeup() grabs rq->lock and tries
    arming a hrtimer via hrtimer_restart(), which deep down tries waking up
    ksoftirqd, which leads to a recursive call to try_to_wakeup() and thus
    attempt to take rq->lock recursively!!
    
    This is fixed by having scheduler queue hrtimer via
    __hrtimer_start_range_ns() which avoids waking up ksoftirqd.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Change-Id: I11a13be1d9db3a749614ccf3d4f5fb7bf6f18fa1

commit b7f4ef90bb9d39f4ad81407d8ff71c0f5cae19a3
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Thu Mar 7 12:14:53 2013 -0800

    sched: Reset rq->next_interval before going idle
    
    next_balance, the point in jiffy time scale when a cpu will next load
    balance, could have been calculated when the cpu was busy. A busy cpu
    will apply its sched domain's busy_factor (usually > 1) in computing
    next_balance for that sched domain, which causes the (busy) cpu to load
    balance less frequently in its sched domains. However when the same cpu
    is going idle, its next_balance needs to be reset without consideration
    of busy_factor. Failure to do so would not trigger nohz idle balancer on
    that cpu for unnecessarily long time (introducing additional scheduling
    latencies for tasks). Fix bug in scheduler which aims to reset
    next_balance before a cpu goes idle (as per existing comment) but is
    clearly not doing so.
    
    Change-Id: I7e027a51686528c4092d770c7d33c874d38f5df4
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 7b0d41c4de2be1d1c33a53b3f2fa3e8825b254f7
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun May 5 23:26:05 2013 -0700

    mmc: Fix warning during bootup
    
     * Don't call get_task_io_context with irqs off (this happens during the
       probe).
    
    Change-Id: Ib56a38dfee7fed2632476fe729332111a22a7e8a

commit 485dd33b9a16080e04bc9f91565c589f74a795a1
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Nov 8 11:10:01 2012 -0800

    ARM: SoC: add per-platform SMP operations
    
    This adds a 'struct smp_operations' to abstract the CPU initialization
    and hot plugging functions on SMP systems, which otherwise conflict
    in a multiplatform kernel. This also helps shmobile and potentially
    others that have more than one method to do these.
    
    To allow the kernel to continue building, the platform hooks are
    defined as weak symbols which are overrided by the platform code.
    Once all platforms are converted, the "weak" attribute will be
    removed and the function made static.
    
    Unlike the original version from Marc, this new version from Arnd
    does not use a generalized abstraction for per-soc data structures
    but only tries to solve the problem for the SMP operations. This
    way, we can collapse the previous four data structures into a
    single struct, which is less systematic but also easier to follow
    as a causal reader.
    
    Change-Id: I9d022d4e789f8f87f26a237fa2e9e410be070976
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    [rameezmustafa@codeaurora.org: backport to kernel 3.4]
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
    
    Conflicts:
    	arch/arm/kernel/smp.c

commit 733154d3a1b82f2984dffad997a723a73dcaf0e3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 19:32:12 2014 +0100

    Merged with cm kernel!

commit 40a19cd42463b62a6d84e2ab48c7f33e09bafd26
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 18:50:14 2014 +0100

    Fixed mmc block!

commit 7cd772b0b4e2ead4dac5741a5dbbc1655ecef37e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 18:20:02 2014 +0100

    New kernel released!

commit 22b07e891e5f31870c72760a715db731728cbdab
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 14:07:57 2014 +0100

    reduced wifi wakelocks!

commit 51022ce0b21db219976ab7bd2c189e2e89674287
Author: Deepak Katragadda <dkatraga@codeaurora.org>
Date:   Mon Feb 10 12:23:47 2014 -0800

    msm: peripheral-loader: Change IOMAP_SIZE from SZ_4M to SZ_1M
    
    The ioremap call made for zeroing out trailing memory is currently
    requesting for a 4M memory region. This is resulting in vmalloc
    allocation failures on some platforms.
    Instead, request for 1M memory locations and zero out the area
    in smaller chunks to make this failure less probable.
    
    CRs-Fixed: 613426
    
    Change-Id: I554dc42cbb37ec893306bd1cc625230216b9b6c9
    Signed-off-by: Deepak Katragadda <dkatraga@codeaurora.org>

commit 3f19463568e57f11140a51acd87ede7b3caf2559
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 21:25:03 2011 +0200

    locking: Various static lock initializer fixes
    
    The static lock initializers want to be fed the proper name of the
    lock and not some random string. In mainline random strings are
    obfuscating the readability of debug output, but for RT they prevent
    the spinlock substitution. Fix it up.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 3b87bd56b2ce41e6b4518e13e5cbf81620860d44
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 14:02:25 2014 +0100

    Updated configuration!

commit fb8862b82e44764fcf9e74a60be82002a8d846b1
Author: ausdim <koronaios@gmail.com>
Date:   Thu Mar 6 02:46:57 2014 +0200

    Disable some debuggers and update my kernel config file
    
    Conflicts:
    	arch/arm/configs/ausdim_jf_tw_4.4.2_defconfig

commit bd27fdf3cbf87a24207a0faca568754d7dc8c022
Author: ausdim <koronaios@gmail.com>
Date:   Wed Mar 5 23:16:42 2014 +0200

    logger: Add an enable toggle for entry storage (Thanks to AndreiLux)

commit 65a7201586557f836d47f60c53bf7aaa444fcdb5
Author: ausdim <koronaios@gmail.com>
Date:   Wed Mar 5 23:13:41 2014 +0200

    Remove "+" from kernel info

commit 2c3e86d61e7f55cd3198f7411268f4ab92683809
Author: ausdim <koronaios@gmail.com>
Date:   Wed Mar 5 00:40:31 2014 +0200

    Completely remove Tima

commit af8d69567eb409730bab45a198c0f259097afa15
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 00:30:20 2014 +0100

    New kernel configuration!

commit f306bb06170d0c0e2c4e71f215a47422786c2da3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 8 00:10:42 2014 +0100

    Removed INTELLIDEMAND GOVERNOR && RELATED objects!

commit 17d731ce60e1d5dab61c19f7f101ca6a48d8f694
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 7 23:50:44 2014 +0100

     msm: cpufreq: Ensure cpufreq change happens on corresponding CPU
    
    * This removes stuttering a few ppl were experiencing -imoseyon
    
    Checking the cpus_allowed mask of the current thread before changing the
    frequency doesn't guarantee that the rest of the execution will continue on
    the same CPU. The only way to guarantee this is to schedule a work on the
    specific CPU and also prevent hotplug  of that CPU (already done by
    existing code).
    
    Change-Id: I51a02fcc777a47d3c16f2d83c47e96f2c59f7ae6
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 7b4e06d4c313bd9f2a2429a49d7798f572a3ca6e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 7 23:49:16 2014 +0100

    Fixed previous commit!

commit 79f58a0fcab8dbf6e4599c549c7efe0c00e065bd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 7 18:15:47 2014 +0100

    Governors tuning!

commit 9a224bb537352bdd9aaaabd8acbfd309a4f88d53
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Mar 7 16:13:40 2014 +0100

    nohz: stat: Fix CPU idle time accounting Since cpustat[CPUTIME_IDLE] is
    never connected to ts->idle_sleeptime,# Please enter the commit message
    for your changes. Lines starting never read from cpustat[CPUTIME_IDLE]
    when reporting stats in# with '#' will be ignored, and an empty message
    aborts the commit. /proc/stat.# On branch my-tw-4.4
    Note this was rejected by Michal Hocko when it was initially proposed#
    (use "git reset HEAD <file>..." to unstage) by Martin Schwidefsky in
    LKML, so if you want to upstream it, better# find an alternative (either
    completely disable cpustat[CPUTIME_IDLE]# modified:
    kernel/time/tick-sched.c for CONFIG_NO_HZ or somehow connect them to
    keep them in sync.)# bug 1190321 Change-Id:
    Idc92488910b826aff850a010016d8326c7ab9e6c Signed-off-by: Bo Yan
    <byan@nvidia.com> Reviewed-on: http://git-master/r/214638 Reviewed-by:
    Automatic_Commit_Validation_User GVS: Gerrit_Virtual_Submit Reviewed-by:
    Liang Cheng (SW) <licheng@nvidia.com>
    Tested-by: Liang Cheng (SW) <licheng@nvidia.com>

commit cb89a1f9b98efbedc1eb1395da1a462c44c75c01
Author: savoca <adeddo27@gmail.com>
Date:   Fri Feb 21 20:02:53 2014 -0500

    Add support for single core eco-mode

commit c7d30e862f04dda2ad1c56ca4c0b0b7e373cf90e
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Jan 12 02:13:18 2014 -0800

    ARM: Disable unaligned instructions due to unexplanable unaligned exceptions
    
    There's a bug where when unaligned access was enabled and with compiler
    generating unaligned instructions, unaligned exception was occurring.  This
    shouldn't happen but it did.  So disable for now until further analysis.
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 87e3d93efeb95f62dea8b6f143f5c1f08348fdb4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Mar 5 21:39:15 2014 +0100

    Revert "ARM: 7587/1: implement optimized percpu variable access"
    
    This reverts commit be9592eacd6890aad500e6bcde80a240044bacd5.

commit 3db94e5aa71af2cb34550a85cc4b1745911e37a0
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 17 19:50:11 2013 +0100

    ARM: 7927/1: dcache: select DCACHE_WORD_ACCESS for big-endian CPUs
    
    With commit 11ec50caedb5 ("word-at-a-time: provide generic big-endian
    zero_bytemask implementation"), the asm-generic word-at-a-time code now
    provides a zero_bytemask implementation, allowing us to make use of
    DCACHE_WORD_ACCESS on big-endian CPUs, providing our
    load_unaligned_zeropad function is endianness-clean.
    
    This patch reworks the load_unaligned_zeropad fixup code to work for
    both big- and little-endian CPUs, then removes the !CPU_BIG_ENDIAN check
    when selecting DCACHE_WORD_ACCESS.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit c629c4f5fcc41c7a4ba95ab350c596d6ccfbed69
Author: myfluxi <linflux@arcor.de>
Date:   Sat Feb 15 22:18:54 2014 +0100

    ipv4: Dump stack on attempt to release alive inet socket
    
    We hide the bug, that will make SELinux bomb out. Let's dump the stack
    when it happens to have at least a chance to find the cause.

commit e518ba6b45653543fd6ffacbbdcd0c5fb307770a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 21:05:29 2014 +0100

    New kernel test version!

commit e0f99a6476f888066909b4370110aea7ca99cf8e
Author: Asutosh Das <asutoshd@codeaurora.org>
Date:   Fri Jan 24 11:36:07 2014 +0530

    mmc: block: check for NULL pointer before dereferencing
    
    mmc block data can be NULL. Hence, check for NULL before
    dereferencing md.
    
    CRs-Fixed: 562259
    Change-Id: I0182c216ec73347cdd2ea464f593839fffd242a9
    Signed-off-by: Asutosh Das <asutoshd@codeaurora.org>
    
    Conflicts:
    	drivers/mmc/card/block.c

commit 8187671eaae2cac7dc78743e555cd393c9ed1afb
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sun Jan 26 12:05:57 2014 +0200

    block: do not notify urgent request, when flush with data in flight
    
    MMC device driver implements URGENT request execution with priority
    (using stop flow), as a result currently running (and prepared) request
    may be reinserted back into I/O scheduler. This will break block layer
    logic of flushes (flush request should not be inserted into I/O scheduler).
    
    Block layer flush machinery keep q->flush_data_in_flight list updated with
    started but not completed flush requests with data (REQ_FUA).
    
    This change will not notify underling block device driver about pending
    urgent request during flushes in flight.
    
    Change-Id: I8b654925a3c989250fcb8f4f7c998795fb203923
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 31a127276c82d607dde876a1b8e253cafb56bdd2
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Tue Jun 11 22:36:08 2013 -0700

    ARM: Flush the caches for non panicking CPUs in case of a kernel panic
    
    In case of a kernel panic, only the panicking CPU does an entire
    cache flush. This means that certain dirty cache lines in the L1
    caches of the other CPUs may never get flushed. This gives us
    improper RAM dumps. Add cache flushing for all the online CPUs.
    The outer domain is not flushed since it is already being done by
    the panicking CPU.
    
    Change-Id: Ibf844ecf6b4dbc3c623789f72a26936aeb4a7306
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit b8a0706283054c7449b54da38d055a13bb406a75
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Tue Sep 17 19:57:46 2013 -0700

    ARM: smp: fix incorrect per-cpu definition of regs_before_stop
    
    The commit efd002896a3636a4d1606ac0630aaafbbfdea9aa introduced changes
    to store CPU registers for all CPUs that handle IPI_CPU_STOP. The
    structure to save the registers was intended to be a per-cpu variable.
    However, the patch did not allocate a per-cpu structure and instead only
    ended up providing a compiler per-cpu directive. Fix this bug by actually
    defining a static per-cpu variable.
    
    Change-Id: Ifd192223c135025897e10312f61b108757575068
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit 84c04cee1e09f667d07051c1b9e28376f7aa591c
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Tue Oct 23 01:30:54 2012 +0200

    ACPI / processor: prevent cpu from becoming online
    
    Even if acpi_processor_handle_eject() offlines cpu, there is a chance
    to online the cpu after that. So the patch closes the window by using
    get/put_online_cpus().
    
    Why does the patch change _cpu_up() logic?
    
    The patch cares the race of hot-remove cpu and _cpu_up(). If the patch
    does not change it, there is the following race.
    
    hot-remove cpu                         |  _cpu_up()
    ------------------------------------- ------------------------------------
    call acpi_processor_handle_eject()     |
         call cpu_down()                   |
         call get_online_cpus()            |
                                           | call cpu_hotplug_begin() and stop here
         call arch_unregister_cpu()        |
         call acpi_unmap_lsapic()          |
         call put_online_cpus()            |
                                           | start and continue _cpu_up()
         return acpi_processor_remove()    |
    continue hot-remove the cpu            |
    
    So _cpu_up() can continue to itself. And hot-remove cpu can also continue
    itself. If the patch changes _cpu_up() logic, the race disappears as below:
    
    hot-remove cpu                         | _cpu_up()
    -----------------------------------------------------------------------
    call acpi_processor_handle_eject()     |
         call cpu_down()                   |
         call get_online_cpus()            |
                                           | call cpu_hotplug_begin() and stop here
         call arch_unregister_cpu()        |
         call acpi_unmap_lsapic()          |
              cpu's cpu_present is set     |
              to false by set_cpu_present()|
         call put_online_cpus()            |
                                           | start _cpu_up()
                                           | check cpu_present() and return -EINVAL
         return acpi_processor_remove()    |
    continue hot-remove the cpu            |
    
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit ef1881f935aa624394b7802afebd71b0928c8557
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue Nov 13 11:32:43 2012 -0800

    kernel/cpu.c: Add comment for priority in cpu_hotplug_pm_callback
    
    cpu_hotplug_pm_callback should have higher priority than
    bsp_pm_callback which depends on cpu_hotplug_pm_callback to disable cpu hotplug
    to avoid race during bsp online checking.
    
    This is to hightlight the priorities between the two callbacks in case people
    may overlook the order.
    
    Ideally the priorities should be defined in macro/enum instead of fixed values.
    To do that, a seperate patchset may be pushed which will touch serveral other
    generic files and is out of scope of this patchset.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352835171-3958-7-git-send-email-fenghua.yu@intel.com
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit ddcd3f739fb7d637f92644aa1eda3dde97fe2cd3
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Tue Jul 31 16:43:28 2012 -0700

    mm/hotplug: correctly setup fallback zonelists when creating new pgdat
    
    When hotadd_new_pgdat() is called to create new pgdat for a new node, a
    fallback zonelist should be created for the new node.  There's code to try
    to achieve that in hotadd_new_pgdat() as below:
    
    	/*
    	 * The node we allocated has no zone fallback lists. For avoiding
    	 * to access not-initialized zonelist, build here.
    	 */
    	mutex_lock(&zonelists_mutex);
    	build_all_zonelists(pgdat, NULL);
    	mutex_unlock(&zonelists_mutex);
    
    But it doesn't work as expected.  When hotadd_new_pgdat() is called, the
    new node is still in offline state because node_set_online(nid) hasn't
    been called yet.  And build_all_zonelists() only builds zonelists for
    online nodes as:
    
            for_each_online_node(nid) {
                    pg_data_t *pgdat = NODE_DATA(nid);
    
                    build_zonelists(pgdat);
                    build_zonelist_cache(pgdat);
            }
    
    Though we hope to create zonelist for the new pgdat, but it doesn't.  So
    add a new parameter "pgdat" the build_all_zonelists() to build pgdat for
    the new pgdat too.
    
    Signed-off-by: Jiang Liu <liuj97@gmail.com>
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Keping Chen <chenkeping@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6be9685c23cf2501ed5c56221d00893b697f25e1
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Thu May 31 16:26:26 2012 -0700

    kernel/cpu.c: document clear_tasks_mm_cpumask()
    
    Add more comments on clear_tasks_mm_cpumask, plus adds a runtime check:
    the function is only suitable for offlined CPUs, and if called
    inappropriately, the kernel should scream aloud.
    
    [akpm@linux-foundation.org: tweak comment: s/walks up/walks/, use 80 cols]
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 4adfe9592d877e64f739fb0e2b3e6ece848d7e27
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Thu May 31 16:26:22 2012 -0700

    cpu: introduce clear_tasks_mm_cpumask() helper
    
    Many architectures clear tasks' mm_cpumask like this:
    
    	read_lock(&tasklist_lock);
    	for_each_process(p) {
    		if (p->mm)
    			cpumask_clear_cpu(cpu, mm_cpumask(p->mm));
    	}
    	read_unlock(&tasklist_lock);
    
    Depending on the context, the code above may have several problems,
    such as:
    
    1. Working with task->mm w/o getting mm or grabing the task lock is
       dangerous as ->mm might disappear (exit_mm() assigns NULL under
       task_lock(), so tasklist lock is not enough).
    
    2. Checking for process->mm is not enough because process' main
       thread may exit or detach its mm via use_mm(), but other threads
       may still have a valid mm.
    
    This patch implements a small helper function that does things
    correctly, i.e.:
    
    1. We take the task's lock while whe handle its mm (we can't use
       get_task_mm()/mmput() pair as mmput() might sleep);
    
    2. To catch exited main thread case, we use find_lock_task_mm(),
       which walks up all threads and returns an appropriate task
       (with task lock held).
    
    Also, Per Peter Zijlstra's idea, now we don't grab tasklist_lock in
    the new helper, instead we take the rcu read lock. We can do this
    because the function is called after the cpu is taken down and marked
    offline, so no new tasks will get this cpu set in their mm mask.
    
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit be9592eacd6890aad500e6bcde80a240044bacd5
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Thu Nov 29 20:39:54 2012 +0100

    ARM: 7587/1: implement optimized percpu variable access
    
    Use the previously unused TPIDRPRW register to store percpu offsets.
    TPIDRPRW is only accessible in PL1, so it can only be used in the kernel.
    
    This replaces 2 loads with a mrc instruction for each percpu variable
    access. With hackbench, the performance improvement is 1.4% on Cortex-A9
    (highbank). Taking an average of 30 runs of "hackbench -l 1000" yields:
    
    Before: 6.2191
    After: 6.1348
    
    Will Deacon reported similar delta on v6 with 11MPCore.
    
    The asm "memory clobber" are needed here to ensure the percpu offset
    gets reloaded. Testing by Will found that this would not happen in
    __schedule() which is a bit of a special case as preemption is disabled
    but the execution can move cores.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 8a4dac3ab61bbcddf1c9b165c7992f064da45a7e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 20:34:21 2014 +0100

    Finish lost patch for arch/arm/kernel/smp.c and SMP_IDLE_THREAD

commit 933cd78f08e2cc06e375ade7c1888983a6342fcf
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 17:40:19 2014 +0100

    Updated config!

commit 72063db3b03095aa32e1fe04894e67fc0dbee4cb
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Jul 3 18:46:35 2012 -0700

    ARM: smp: Fix suspicious RCU originating from cpu_die()
    
    While running hotplug tests I ran into this RCU splat
    
    ===============================
    [ INFO: suspicious RCU usage. ]
    3.4.0 #3275 Tainted: G        W
    -------------------------------
    include/linux/rcupdate.h:729 rcu_read_lock() used illegally while idle!
    
    other info that might help us debug this:
    
    RCU used illegally from idle CPU!
    rcu_scheduler_active = 1, debug_locks = 0
    RCU used illegally from extended quiescent state!
    4 locks held by swapper/2/0:
     #0:  ((cpu_died).wait.lock){......}, at: [<c00ab128>] complete+0x1c/0x5c
     #1:  (&p->pi_lock){-.-.-.}, at: [<c00b275c>] try_to_wake_up+0x2c/0x388
     #2:  (&rq->lock){-.-.-.}, at: [<c00b2860>] try_to_wake_up+0x130/0x388
     #3:  (rcu_read_lock){.+.+..}, at: [<c00abe5c>] cpuacct_charge+0x28/0x1f4
    
    stack backtrace:
    [<c001521c>] (unwind_backtrace+0x0/0x12c) from [<c00abec8>] (cpuacct_charge+0x94/0x1f4)
    [<c00abec8>] (cpuacct_charge+0x94/0x1f4) from [<c00b395c>] (update_curr+0x24c/0x2c8)
    [<c00b395c>] (update_curr+0x24c/0x2c8) from [<c00b59c4>] (enqueue_task_fair+0x50/0x194)
    [<c00b59c4>] (enqueue_task_fair+0x50/0x194) from [<c00afea4>] (enqueue_task+0x30/0x34)
    [<c00afea4>] (enqueue_task+0x30/0x34) from [<c00b0908>] (ttwu_activate+0x14/0x38)
    [<c00b0908>] (ttwu_activate+0x14/0x38) from [<c00b28a8>] (try_to_wake_up+0x178/0x388)
    [<c00b28a8>] (try_to_wake_up+0x178/0x388) from [<c00a82a0>] (__wake_up_common+0x34/0x78)
    [<c00a82a0>] (__wake_up_common+0x34/0x78) from [<c00ab154>] (complete+0x48/0x5c)
    [<c00ab154>] (complete+0x48/0x5c) from [<c07db7cc>] (cpu_die+0x2c/0x58)
    [<c07db7cc>] (cpu_die+0x2c/0x58) from [<c000f954>] (cpu_idle+0x64/0xfc)
    [<c000f954>] (cpu_idle+0x64/0xfc) from [<80208160>] (0x80208160)
    
    When a cpu is marked offline during its idle thread it calls
    cpu_die() during an RCU idle period. cpu_die() calls complete()
    to notify the killing process that the cpu has died. complete()
    calls into the scheduler code and eventually grabs an RCU read
    lock in cpuacct_charge().
    
    Mark complete() as RCU_NONIDLE so that RCU pays attention to this
    CPU for the duration of the complete() function even though it's
    in idle.
    
    Change-Id: I548a278e595737390bbc2c97bddda06a0725ecbd
    Suggested-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 668fb066713416e7a6344518fc88a4f4dc85ca2e
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jan 29 14:05:39 2014 -0800

    mm/page-writeback.c: fix dirty_balance_reserve subtraction from dirtyable memory
    
    Tejun reported stuttering and latency spikes on a system where random
    tasks would enter direct reclaim and get stuck on dirty pages.  Around
    50% of memory was occupied by tmpfs backed by an SSD, and another disk
    (rotating) was reading and writing at max speed to shrink a partition.
    
    : The problem was pretty ridiculous.  It's a 8gig machine w/ one ssd and 10k
    : rpm harddrive and I could reliably reproduce constant stuttering every
    : several seconds for as long as buffered IO was going on on the hard drive
    : either with tmpfs occupying somewhere above 4gig or a test program which
    : allocates about the same amount of anon memory.  Although swap usage was
    : zero, turning off swap also made the problem go away too.
    :
    : The trigger conditions seem quite plausible - high anon memory usage w/
    : heavy buffered IO and swap configured - and it's highly likely that this
    : is happening in the wild too.  (this can happen with copying large files
    : to usb sticks too, right?)
    
    This patch (of 2):
    
    The dirty_balance_reserve is an approximation of the fraction of free
    pages that the page allocator does not make available for page cache
    allocations.  As a result, it has to be taken into account when
    calculating the amount of "dirtyable memory", the baseline to which
    dirty_background_ratio and dirty_ratio are applied.
    
    However, currently the reserve is subtracted from the sum of free and
    reclaimable pages, which is non-sensical and leads to erroneous results
    when the system is dominated by unreclaimable pages and the
    dirty_balance_reserve is bigger than free+reclaimable.  In that case, at
    least the already allocated cache should be considered dirtyable.
    
    Fix the calculation by subtracting the reserve from the amount of free
    pages, then adding the reclaimable pages on top.
    
    [akpm@linux-foundation.org: fix CONFIG_HIGHMEM build]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: Tejun Heo <tj@kernel.org>
    Tested-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 7d4b0dbc968fa8745278bb4e2879747cf881c830
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Feb 19 19:27:00 2014 +0200

    Revert "CHROMIUM: mm: Fix calculation of dirtyable memory"
    
    This reverts commit e2cf9dd031f2db6a1d69d973a10bb444331a1142.

commit af1f5dd3da6830dce7161c18be32a5a37c65859e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 16 10:42:38 2012 +0000

    rcu: Use smp_hotplug_thread facility for RCUs per-CPU kthread
    
    Bring RCU into the new-age CPU-hotplug fold by modifying RCU's per-CPU
    kthread code to use the new smp_hotplug_thread facility.
    
    [ tglx: Adapted it to use callbacks and to the simplified rcu yield ]
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.673354828@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 413cf4630f023ad6656f9b957bc5b41a1fcc68fc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:35 2012 +0000

    rcu: Yield simpler
    
    The rcu_yield() code is amazing. It's there to avoid starvation of the
    system when lots of (boosting) work is to be done.
    
    Now looking at the code it's functionality is:
    
     Make the thread SCHED_OTHER and very nice, i.e. get it out of the way
     Arm a timer with 2 ticks
     schedule()
    
    Now if the system goes idle the rcu task returns, regains SCHED_FIFO
    and plugs on. If the systems stays busy the timer fires and wakes a
    per node kthread which in turn makes the per cpu thread SCHED_FIFO and
    brings it back on the cpu. For the boosting thread the "make it FIFO"
    bit is missing and it just runs some magic boost checks. Now this is a
    lot of code with extra threads and complexity.
    
    It's way simpler to let the tasks when they detect overload schedule
    away for 2 ticks and defer the normal wakeup as long as they are in
    yielded state and the cpu is not idle.
    
    That solves the same problem and the only difference is that when the
    cpu goes idle it's not guaranteed that the thread returns right away,
    but it won't be longer out than two ticks, so no harm is done. If
    that's an issue than it is way simpler just to wake the task from
    idle as RCU has callbacks there anyway.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120716103948.131256723@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit a6917f9781f1bf04c68844ade512a2ac63f3efae
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:37 2012 +0000

    softirq: Use hotplug thread infrastructure
    
    [ paulmck: Call rcu_note_context_switch() with interrupts enabled. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.456416747@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 3040e68c9658c464ea0ddd2ea359b12caa60f5f7
Author: mrg666 <drgungor@hotmail.com>
Date:   Fri Dec 20 19:25:09 2013 -0500

    af_unix: speedup /proc/net/unix
    /proc/net/unix has quadratic behavior, and can hold unix_table_lock for
    a while if high number of unix sockets are alive. (90 ms for 200k
    sockets...)
    
    We already have a hash table, so its quite easy to use it.
    
    Problem is unbound sockets are still hashed in a single hash slot
    (unix_socket_table[UNIX_HASH_TABLE])
    
    This patch also spreads unbound sockets to 256 hash slots, to speedup
    both /proc/net/unix and unix_diag.
    
    Time to read /proc/net/unix with 200k unix sockets :
    (time dd if=/proc/net/unix of=/dev/null bs=4k)
    
    before : 520 secs
    after : 2 secs
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 1f3537fbc2a44e73d79421e48924ecc35872a551
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue May 29 15:06:24 2012 -0700

    kernel: cgroup: push rcu read locking from css_is_ancestor() to callsite
    
    Library functions should not grab locks when the callsites can do it,
    even if the lock nests like the rcu read-side lock does.
    
    Push the rcu_read_lock() from css_is_ancestor() to its single user,
    mem_cgroup_same_or_subtree() in preparation for another user that may
    already hold the rcu read-side lock.
    
    Change-Id: I0ec7ec0059ed588d8f85bc9be8fdc42ce0ca7f5d
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Git-commit: 91c63734f6908425903aed69c04035592f18d398
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 3b3cda1880c6cb442d49e1310a16add4f8e08dfc
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:47 2012 -0700

    mm: methods for teaching filesystems about PG_swapcache pages
    
    In order to teach filesystems to handle swap cache pages, three new page
    functions are introduced:
    
      pgoff_t page_file_index(struct page *);
      loff_t page_file_offset(struct page *);
      struct address_space *page_file_mapping(struct page *);
    
    page_file_index() - gives the offset of this page in the file in
    PAGE_CACHE_SIZE blocks.  Like page->index is for mapped pages, this
    function also gives the correct index for PG_swapcache pages.
    
    page_file_offset() - uses page_file_index(), so that it will give the
    expected result, even for PG_swapcache pages.
    
    page_file_mapping() - gives the mapping backing the actual page; that is
    for swap cache pages it will give swap_file->f_mapping.
    
    Change-Id: I13d18bb25be606760eac26cc842eb7c9fc9e4766
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f981c5950fa85916ba49bea5d9a7a5078f47e569
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [ohaugan@codeaurora.org: Resolved merge issues]
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 73e3e2f50614bc8a5259bac66a13b610abd15047
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 11 23:15:42 2014 +0100

    CPU hotplug, debug: detect imbalance between get_online_cpus() and pu…
    ¦t_online_cpus()# Please enter the commit message for your changes.
    Lines starting
    The synchronization between CPU hotplug readers and writers is achieved#
    (use "git reset HEAD <file>..." to unstage) by means of refcounting,
    safeguarded by the cpu_hotplug.lock.#
    get_online_cpus() increments the refcount, whereas put_online_cpus()#
    decrements it.  If we ever hit an imbalance between the two, we end up
    compromising the guarantees of the hotplug synchronization i.e, for
    example, an extra call to put_online_cpus() can end up allowing a
    hotplug reader to execute concurrently with a hotplug writer. So, add a
    WARN_ON() in put_online_cpus() to detect such cases where the refcount
    can go negative, and also attempt to fix it up, so that we can continue
    to run. Change-Id: I144efeaa5899a2e8a3cddd21f010679cbaaa2459
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com> Cc: Jiri
    Kosina <jkosina@suse.cz> Cc: Thomas Gleixner <tglx@linutronix.de> Cc:
    Ingo Molnar <mingo@kernel.org> Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by:
    Linus Torvalds <torvalds@linux-foundation.org> Git-commit: 075663d
    Git-repo:
    git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Osvaldo Banuelos <osvaldob@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 18ca08d6b706f0aad3c4f6f4cc256d2b2bcb0acc
Author: Jan Kara <jack@suse.cz>
Date:   Fri Jun 28 21:32:27 2013 +0200

    block: Reserve only one queue tag for sync IO if only 3 tags are available
    
    In case a device has three tags available we still reserve two of them
    for sync IO. That leaves only a single tag for async IO such as
    writeback from flusher thread which results in poor performance.
    
    Allow async IO to consume two tags in case queue has three tag availabe
    to get a decent async write performance.
    
    This patch improves streaming write performance on a machine with such disk
    from ~21 MB/s to ~52 MB/s. Also postmark throughput in presence of
    streaming writer improves from 8 to 12 transactions per second so sync
    IO doesn't seem to be harmed in presence of heavy async writer.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit b863533c5563a5c1a3d66fe61dbf9f9008077b4a
Author: Fengguang Wu <fengguang.wu@intel.com>
Date:   Wed Sep 11 14:21:47 2013 -0700

    readahead: make context readahead more conservative
    
    This helps performance on moderately dense random reads on SSD.
    
    Transaction-Per-Second numbers provided by Taobao:
    
    		QPS	case
    		-------------------------------------------------------
    		7536	disable context readahead totally
    w/ patch:	7129	slower size rampup and start RA on the 3rd read
    		6717	slower size rampup
    w/o patch:	5581	unmodified context readahead
    
    Before, readahead will be started whenever reading page N+1 when it happen
    to read N recently.  After patch, we'll only start readahead when *three*
    random reads happen to access pages N, N+1, N+2.  The probability of this
    happening is extremely low for pure random reads, unless they are very
    dense, which actually deserves some readahead.
    
    Also start with a smaller readahead window.  The impact to interleaved
    sequential reads should be small, because for a long run stream, the the
    small readahead window rampup phase is negletable.
    
    The context readahead actually benefits clustered random reads on HDD
    whose seek cost is pretty high.  However as SSD is increasingly used for
    random read workloads it's better for the context readahead to concentrate
    on interleaved sequential reads.
    
    Another SSD rand read test from Miao
    
            # file size:        2GB
            # read IO amount: 625MB
            sysbench --test=fileio          \
                    --max-requests=10000    \
                    --num-threads=1         \
                    --file-num=1            \
                    --file-block-size=64K   \
                    --file-test-mode=rndrd  \
                    --file-fsync-freq=0     \
                    --file-fsync-end=off    run
    
    shows the performance of btrfs grows up from 69MB/s to 121MB/s, ext4 from
    104MB/s to 121MB/s.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Tested-by: Tao Ma <tm@tao.ma>
    Tested-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 6da7d9e6313483cce5ca90ab55ee0c2c83d5dfc2
Author: Felix Fietkau <nbd@openwrt.org>
Date:   Fri May 4 21:08:33 2012 -0700

    timer: optimize apply_slack()
    
    __fls(mask) is equivalent to find_last_bit(&mask, BITS_PER_LONG), but cheaper.
    find_last_bit was showing up high on the list when I was profiling for stalls
    on icache misses on a system with very small cache size (MIPS).
    
    Signed-off-by: Felix Fietkau <nbd@openwrt.org>
    Signed-off-by: edoko <r_data@naver.com>
    
    Change-Id: I8a5021a2fb2936c00ffd456663a76cb1b23e3100

commit ef4f1008132364713150cfa91f004f113bcafbfb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 26 18:44:33 2013 +0100

    stop_machine: Mark per cpu stopper enabled early
    
    commit 14e568e78 (stop_machine: Use smpboot threads) introduced the
    following regression:
    
    Before this commit the stopper enabled bit was set in the online
    notifier.
    
    CPU0				CPU1
    cpu_up
    				cpu online
    hotplug_notifier(ONLINE)
      stopper(CPU1)->enabled = true;
    ...
    stop_machine()
    
    The conversion to smpboot threads moved the enablement to the wakeup
    path of the parked thread. The majority of users seem to have the
    following working order:
    
    CPU0				CPU1
    cpu_up
    				cpu online
    unpark_threads()
      wakeup(stopper[CPU1])
    ....
    				stopper thread runs
    				  stopper(CPU1)->enabled = true;
    stop_machine()
    
    But Konrad and Sander have observed:
    
    CPU0				CPU1
    cpu_up
    				cpu online
    unpark_threads()
      wakeup(stopper[CPU1])
    ....
    stop_machine()
    				stopper thread runs
    				  stopper(CPU1)->enabled = true;
    
    Now the stop machinery kicks CPU0 into the stop loop, where it gets
    stuck forever because the queue code saw stopper(CPU1)->enabled ==
    false, so CPU0 waits for CPU1 to enter stomp_machine, but the CPU1
    stopper work got discarded due to enabled == false.
    
    Add a pre_unpark function to the smpboot thread descriptor and call it
    before waking the thread.
    
    This fixes the problem at hand, but the stop_machine code should be
    more robust. The stopper->enabled flag smells fishy at best.
    
    Thanks to Konrad for going through a loop of debug patches and
    providing the information to decode this issue.
    
    Reported-and-tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reported-and-tested-by: Sander Eikelenboom <linux@eikelenboom.it>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1302261843240.22263@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: Iaff8824879eb21552fc9e46e259b604dfce113bc

commit caaa10698537f1a8c156aebf0ee8a09b82d89331
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 31 12:11:14 2013 +0000

    stop_machine: Use smpboot threads
    
    Use the smpboot thread infrastructure. Mark the stopper thread
    selfparking and park it after it has finished the take_cpu_down()
    work.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Richard Weinberger <rw@linutronix.de>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130131120741.686315164@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: I30771810f2cbb2a64ca090864156edc79d338dfd

commit ecddb86b73f4d3884da39ca83dc94c2398a67b55
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 31 12:11:13 2013 +0000

    stop_machine: Store task reference in a separate per cpu variable
    
    To allow the stopper thread being managed by the smpboot thread
    infrastructure separate out the task storage from the stopper data
    structure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Richard Weinberger <rw@linutronix.de>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130131120741.626690384@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: Ibfe2389e42fcf2e236940bbc223a36da571ed6e9

commit c8089a6549170ba98db84036879edf91352d4f54
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 9 09:33:34 2013 +0200

    kthread: Prevent unpark race which puts threads on the wrong cpu
    
    The smpboot threads rely on the park/unpark mechanism which binds per
    cpu threads on a particular core. Though the functionality is racy:
    
    CPU0	       	 	CPU1  	     	    CPU2
    unpark(T)				    wake_up_process(T)
      clear(SHOULD_PARK)	T runs
    			leave parkme() due to !SHOULD_PARK
      bind_to(CPU2)		BUG_ON(wrong CPU)
    
    We cannot let the tasks move themself to the target CPU as one of
    those tasks is actually the migration thread itself, which requires
    that it starts running on the target cpu right away.
    
    The solution to this problem is to prevent wakeups in park mode which
    are not from unpark(). That way we can guarantee that the association
    of the task to the target cpu is working correctly.
    
    Add a new task state (TASK_PARKED) which prevents other wakeups and
    use this state explicitly for the unpark wakeup.
    
    Peter noticed: Also, since the task state is visible to userspace and
    all the parked tasks are still in the PID space, its a good hint in ps
    and friends that these tasks aren't really there for the moment.
    
    The migration thread has another related issue.
    
    CPU0	      	     	 CPU1
    Bring up CPU2
    create_thread(T)
    park(T)
     wait_for_completion()
    			 parkme()
    			 complete()
    sched_set_stop_task()
    			 schedule(TASK_PARKED)
    
    The sched_set_stop_task() call is issued while the task is on the
    runqueue of CPU1 and that confuses the hell out of the stop_task class
    on that cpu. So we need the same synchronizaion before
    sched_set_stop_task().
    
    Reported-by: Dave Jones <davej@redhat.com>
    Reported-and-tested-by: Dave Hansen <dave@sr71.net>
    Reported-and-tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Peter Ziljstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: dhillf@gmail.com
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1304091635430.21884@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: If1e9993951c4ad1f6f35ad0698f6ccd05a67e81f

commit 8f4db41e2ff95a2e3b28eecf80c8fa7588dbf881
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 31 12:11:12 2013 +0000

    smpboot: Allow selfparking per cpu threads
    
    The stop machine threads are still killed when a cpu goes offline. The
    reason is that the thread is used to bring the cpu down, so it can't
    be parked along with the other per cpu threads.
    
    Allow a per cpu thread to be excluded from automatic parking, so it
    can park itself once it's done
    
    Add a create callback function as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Richard Weinberger <rw@linutronix.de>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130131120741.553993267@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: I864f39336a2cb648c518526459929c081f831216

commit 0137c7eb9763f9da636eb6ad5f39c4105f066a5b
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Jul 12 01:55:54 2012 -0700

    hotplug: Fix UP bug in smpboot hotplug code
    
    Because kernel subsystems need their per-CPU kthreads on UP systems as
    well as on SMP systems, the smpboot hotplug kthread functions must be
    provided in UP builds as well as in SMP builds.  This commit therefore
    adds smpboot.c to UP builds and excludes irrelevant code via #ifdef.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: I7b570d6c241c513227c3fdc1d843bf369bed036c

commit 90c47fed513016b537451640ae69c5b1069f4628
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:36 2012 +0000

    smpboot: Provide infrastructure for percpu hotplug threads
    
    Provide a generic interface for setting up and tearing down percpu
    threads.
    
    On registration the threads for already online cpus are created and
    started. On deregistration (modules) the threads are stoppped.
    
    During hotplug operations the threads are created, started, parked and
    unparked. The datastructure for registration provides a pointer to
    percpu storage space and optional setup, cleanup, park, unpark
    functions. These functions are called when the thread state changes.
    
    Each implementation has to provide a function which is queried and
    returns whether the thread should run and the thread function itself.
    
    The core code handles all state transitions and avoids duplicated code
    in the call sites.
    
    [ paulmck: Preemption leak fix ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.352501068@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: Ib2ac667cd13cf26a042d65c1b3f20fe7e4b02423

commit 712be8a208962475fa5c807b7130ffaac58a11df
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:36 2012 +0000

    kthread: Implement park/unpark facility
    
    To avoid the full teardown/setup of per cpu kthreads in the case of
    cpu hot(un)plug, provide a facility which allows to put the kthread
    into a park position and unpark it when the cpu comes online again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120716103948.236618824@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: I05d28788540b666349bafecf6cb3fdc873b6cdde

commit 2b7cfa244bb9a67cabc2f8d73d12ae3024fc32ed
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 20:41:00 2012 +0530

    smpboot, idle: Fix comment mismatch over idle_threads_init()
    
    The comment over idle_threads_init() really talks about the functionality
    of idle_init(). Move that comment to idle_init(), and add a suitable
    comment over idle_threads_init().
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: suresh.b.siddha@intel.com
    Cc: venki@google.com
    Cc: nikunj@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20120524151100.2549.66501.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: Ib0cd6d6e19e0c64868a42a77101b080a5f3b04f8

commit dcd2e24c4efa776fdb5a77afd6cb08ffd5aeeeca
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 20:40:55 2012 +0530

    smpboot, idle: Optimize calls to smp_processor_id() in idle_threads_init()
    
    While trying to initialize idle threads for all cpus, idle_threads_init()
    calls smp_processor_id() in a loop, which is unnecessary. The intent
    is to initialize idle threads for all non-boot cpus. So just use a variable
    to note the boot cpu and use it in the loop.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: suresh.b.siddha@intel.com
    Cc: venki@google.com
    Cc: nikunj@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20120524151055.2549.64309.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: Ib65df4c31e93e1622c26f2c2a4946ffd28c1839d

commit cff6de55328f2a098921ec42eff4912ed31dfbef
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Apr 20 17:08:50 2012 -0700

    smp, idle: Allocate idle thread for each possible cpu during boot
    
    percpu areas are already allocated during boot for each possible cpu.
    percpu idle threads can be considered as an extension of the percpu areas,
    and allocate them for each possible cpu during boot.
    
    This will eliminate the need for workqueue based idle thread allocation.
    In future we can move the idle thread area into the percpu area too.
    
    [ tglx: Moved the loop into smpboot.c and added an error check when
      the init code failed to allocate an idle thread for a cpu which
      should be onlined ]
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: venki@google.com
    Link: http://lkml.kernel.org/r/1334966930.28674.245.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Change-Id: I36828165fc08b7c0a8a0fe6a2aa24d358e623dd2

commit bdfe3b55fecb126d8f1138cd4d12c56632033581
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:45 2012 +0000

    smp: Provide generic idle thread allocation
    
    All SMP architectures have magic to fork the idle task and to store it
    for reusage when cpu hotplug is enabled. Provide a generic
    infrastructure for it.
    
    Create/reinit the idle thread for the cpu which is brought up in the
    generic code and hand the thread pointer to the architecture code via
    __cpu_up().
    
    Note, that fork_idle() is called via a workqueue, because this
    guarantees that the idle thread does not get a reference to a user
    space VM. This can happen when the boot process did not bring up all
    possible cpus and a later cpu_up() is initiated via the sysfs
    interface. In that case fork_idle() would be called in the context of
    the user space task and take a reference on the user space VM.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Acked-by: Venkatesh Pallipadi <venki@google.com>
    Link: http://lkml.kernel.org/r/20120420124557.102478630@linutronix.de
    
    Change-Id: Ie2d32789f3a69ee15f38ba704aaa84d6be85bcd4

commit 36e1ac51b57f47a36724a7459f50469fb2f2c457
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:44 2012 +0000

    smp: Add generic smpboot facility
    
    Start a new file, which will hold SMP and CPU hotplug related generic
    infrastructure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124557.035417523@linutronix.de
    
    Change-Id: Ia1ad435435aa12c47ac0d381ae031ebf6edcff1f

commit c0adeb576b62dd8f63ee12efde2dda528bcb1aa4
Author: Arun Bharadwaj <abharadw@codeaurora.org>
Date:   Wed Jul 3 10:35:02 2013 -0700

    tracing/sched: Add trace events to track cpu hotplug.
    
    Add ftrace event trace_sched_cpu_hotplug to track cpu
    hot-add and hot-remove events.
    
    This is useful in a variety of power, performance and
    debug analysis scenarios.
    
    Change-Id: I5d202c7a229ffacc3aafb7cf9afee0b0ee7b0931
    Signed-off-by: Arun Bharadwaj <abharadw@codeaurora.org>

commit 65cca973d4034c11c171f7294bc09c42182da7e3
Author: dorimanx <yuri@bynet.co.il>
Date:   Tue Feb 11 22:08:31 2014 +0200

    cpufreq: Fix policy getting stuck when user & kernel min/max don't ov.
    
    .erlap
    
    Every __cpufreq_set_policy starts with checking the new policy min/max has
    some overlap with the current policy min/max. This works out fine until we
    end up with the policy min/max being set to a range that doesn't overlap
    with the user policy min/max. Once we get into this situation, the check at
    the start of __cpufreq_set_policy fails and prevents us from getting out of
    this state.
    
    This only happens when one of the CPUFREQ_ADJUST/CPUFREQ_INCOMPATIBLE
    notifiers called inside __cpufreq_set_policy pick a min/max outside the
    range of user policy min/max.
    
    The real intent of the check at the start of __cpufreq_set_policy is to
    make sure userspace can't set user policy min > user policy max. Since
    __cpufreq_set_policy always gets called only with current user policy
    min/max except when the actual user space policy min/max is changed, we can
    fix the issue by simply checking the new policy min/max against current
    user policy min/max.
    
    Change-Id: Iaac805825e64d7985c41fb9052bd96baacdf3d6f
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    
    https://github.com/franciscofranco/mako/commit/b81d1cd3ce7c8b3feaed06d5628ecbf93812156a

commit b22c5f5a00c29463b9712dda8194ae3b94a40026
Author: dorimanx <yuri@bynet.co.il>
Date:   Tue Feb 11 22:12:39 2014 +0200

    cpufreq: Optimize cpufreq_frequency_table_verify()
    
    cpufreq_frequency_table_verify() is rewritten here to make it more logical
    and efficient.
     - merge multiple lines for variable declarations together.
     - quit early if any frequency between min/max is found.
     - don't call cpufreq_verify_within_limits() in case any valid freq is
       found as it is of no use.
     - rename the count variable as found and change its type to boolean.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    
    https://github.com/franciscofranco/mako/commit/cf3e70824fc6fe931ecd06d82fbd724bc70cf3d7

commit 16b3f14610a49e8867e00aa753cd28ee5ee91f2a
Author: Alistair Strachan <alistair.strachan@imgtec.com>
Date:   Wed Apr 10 16:35:14 2013 -0700

    sync: Fix a race condition between release_obj and print_obj
    
    Before this change, a timeline would only be removed from the timeline
    list *after* the sync driver had its release_obj() called. However, the
    driver's release_obj() may free resources needed by print_obj().
    
    Although the timeline list is locked when print_obj() is called, it is
    not locked when release_obj() is called. If one CPU was in print_obj()
    when another was in release_obj(), the print_obj() may make unsafe
    accesses.
    
    It is not actually necessary to hold the timeline list lock when calling
    release_obj() if the call is made after the timeline is unlinked from
    the list, since there is no possibility another thread could be in --
    or enter -- print_obj() for that timeline.
    
    This change moves the release_obj() call to after the timeline is
    unlinked, preventing the above race from occurring.
    
    Signed-off-by: Alistair Strachan <alistair.strachan@imgtec.com>

commit a5509dd88aaeb04e313c549c00ce27173d47fa93
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Mon Feb 4 15:33:44 2013 -0600

    cfg80211: Fix memory leak
    
    When a driver requests a specific regulatory domain after cfg80211 already
    has one, a struct ieee80211_regdomain is leaked.
    
    Change-Id: Id28fc9861b9c911a97bd242439eabca097d76258
    Reported-by: Larry Finger <Larry.Finger@lwfinger.net>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Git-commit: b7566fc363e23f0efd3fa1e1460f9421cdc0d77e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [mattw@codeaurora.org: trivially backport to the msm-3.4 kernel]
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 038c2e10e200baa12815cbef299bae1b9ea18d0a
Author: Shaohua Li <shli@fusionio.com>
Date:   Fri Nov 9 08:44:27 2012 +0100

    block: recursive merge requests
    
    In a workload, thread 1 accesses a, a+2, ..., thread 2 accesses a+1, a+3,....
    When the requests are flushed to queue, a and a+1 are merged to (a, a+1), a+2
    and a+3 too to (a+2, a+3), but (a, a+1) and (a+2, a+3) aren't merged.
    
    If we do recursive merge for such interleave access, some workloads throughput
    get improvement. A recent worload I'm checking on is swap, below change
    boostes the throughput around 5% ~ 10%.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 1be20dccdbc9da0f2d550cc82ec3638681088b83
Author: Namjae Jeon <linkinjeon@gmail.com>
Date:   Tue Dec 11 16:00:21 2012 -0800

    writeback: remove nr_pages_dirtied arg from balance_dirty_pages_ratelimited_nr()
    
    There is no reason to pass the nr_pages_dirtied argument, because
    nr_pages_dirtied value from the caller is unused in
    balance_dirty_pages_ratelimited_nr().
    
    Signed-off-by: Namjae Jeon <linkinjeon@gmail.com>
    Signed-off-by: Vivek Trivedi <vtrivedi018@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit f6bcfa4f2d3a50ed2fdc22feb6e6eed298da3797
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Fri Sep 28 20:27:49 2012 +0800

    CPU hotplug, writeback: Don't call writeback_set_ratelimit() too often during hotplug
    
    The CPU hotplug callback related to writeback calls writeback_set_ratelimit()
    during every state change in the hotplug sequence. This is unnecessary
    since num_online_cpus() changes only once during the entire hotplug operation.
    
    So invoke the function only once per hotplug, thereby avoiding the
    unnecessary repetition of those costly calculations.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 6e3f289ed53b8a803b4cb66567014151b5ec5ebe
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 14 18:19:58 2013 +0400

    sched: add wait_for_completion_io[_timeout]
    
    The only difference between wait_for_completion[_timeout]() and
    wait_for_completion_io[_timeout]() is that the latter calls
    io_schedule_timeout() instead of schedule_timeout() so that the caller
    is accounted as waiting for IO, not just sleeping.
    
    These functions can be used for correct iowait time accounting when the
    completion struct is actually used for waiting for IO (e.g. completion
    of a bio request in the block layer).
    
    Modified to compile with 3.4.0 on the Nexus 4 (Mako)
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 0fc34f3d8de091e96d934dd5d9c454e25bc2a128
Author: Davidlohr Bueso <davidlohr.bueso@hp.com>
Date:   Mon Apr 29 16:18:09 2013 -0700

    lib/int_sqrt.c: optimize square root algorithm
    
    Optimize the current version of the shift-and-subtract (hardware)
    algorithm, described by John von Newmann[1] and Guy L Steele.
    
    Iterating 1,000,000 times, perf shows for the current version:
    
     Performance counter stats for './sqrt-curr' (10 runs):
    
             27.170996 task-clock                #    0.979 CPUs utilized            ( +-  3.19% )
                     3 context-switches          #    0.103 K/sec                    ( +-  4.76% )
                     0 cpu-migrations            #    0.004 K/sec                    ( +-100.00% )
                   104 page-faults               #    0.004 M/sec                    ( +-  0.16% )
            64,921,199 cycles                    #    2.389 GHz                      ( +-  0.03% )
            28,967,789 stalled-cycles-frontend   #   44.62% frontend cycles idle     ( +-  0.18% )
       <not supported> stalled-cycles-backend
           104,502,623 instructions              #    1.61  insns per cycle
                                                 #    0.28  stalled cycles per insn  ( +-  0.00% )
            34,088,368 branches                  # 1254.587 M/sec                    ( +-  0.00% )
                 4,901 branch-misses             #    0.01% of all branches          ( +-  1.32% )
    
           0.027763015 seconds time elapsed                                          ( +-  3.22% )
    
    And for the new version:
    
    Performance counter stats for './sqrt-new' (10 runs):
    
              0.496869 task-clock                #    0.519 CPUs utilized            ( +-  2.38% )
                     0 context-switches          #    0.000 K/sec
                     0 cpu-migrations            #    0.403 K/sec                    ( +-100.00% )
                   104 page-faults               #    0.209 M/sec                    ( +-  0.15% )
               590,760 cycles                    #    1.189 GHz                      ( +-  2.35% )
               395,053 stalled-cycles-frontend   #   66.87% frontend cycles idle     ( +-  3.67% )
       <not supported> stalled-cycles-backend
               398,963 instructions              #    0.68  insns per cycle
                                                 #    0.99  stalled cycles per insn  ( +-  0.39% )
                70,228 branches                  #  141.341 M/sec                    ( +-  0.36% )
                 3,364 branch-misses             #    4.79% of all branches          ( +-  5.45% )
    
           0.000957440 seconds time elapsed                                          ( +-  2.42% )
    
    Furthermore, this saves space in instruction text:
    
       text    data     bss     dec     hex filename
        111       0       0     111      6f lib/int_sqrt-baseline.o
         89       0       0      89      59 lib/int_sqrt.o
    
    [1] http://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC
    
    Signed-off-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Reviewed-by: Jonathan Gonzalez <jgonzlez@linets.cl>
    Tested-by: Jonathan Gonzalez <jgonzlez@linets.cl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 7a5f22f67f75e30a460416c0e6296132764ebd7f
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Wed Jan 2 20:00:40 2013 -0600

    writeback: fix writeback cache thrashing
    
    Consider Process A: huge I/O on sda
            doing heavy write operation - dirty memory becomes more
            than dirty_background_ratio
            on HDD - flusher thread flush-8:0
    
    Consider Process B: small I/O on sdb
            doing while [1]; read 1024K + rewrite 1024K + sleep 2sec
            on Flash device - flusher thread flush-8:16
    
    As Process A is a heavy dirtier, dirty memory becomes more
    than dirty_background_thresh. Due to this, below check becomes
    true(checking global_page_state in over_bground_thresh)
    for all bdi devices(even for very small dirtied bdi - sdb):
    
    In this case, even small cached data on 'sdb' is forced to flush
    and writeback cache thrashing happens.
    
    When we added debug prints inside above 'if' condition and ran
    above Process A(heavy dirtier on bdi with flush-8:0) and
    Process B(1024K frequent read/rewrite on bdi with flush-8:16)
    we got below prints:
    
    [Test setup: ARM dual core CPU, 512 MB RAM]
    
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56064 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56704 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 84720 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 94720 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   384 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   960 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =    64 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92160 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   256 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   768 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =    64 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   256 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   320 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =     0 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92032 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 91968 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   192 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =  1024 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =    64 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   192 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   576 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =     0 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 84352 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   192 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   512 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =     0 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92608 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92544 KB
    
    As mentioned in above log, when global dirty memory > global background_thresh
    small cached data is also forced to flush by flush-8:16.
    If removing global background_thresh checking code, we can reduce cache
    thrashing of frequently used small data.
    And It will be great if we can reserve a portion of writeback cache using
    min_ratio.
    
    After applying patch:
    $ echo 5 > /sys/block/sdb/bdi/min_ratio
    $ cat /sys/block/sdb/bdi/min_ratio
    5
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56064 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56704 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  84160 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  96960 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  94080 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93120 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93120 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  91520 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  89600 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93696 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93696 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  72960 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  90624 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  90624 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  90688 KB
    
    As mentioned in the above logs, once cache is reserved for Process B,
    and patch is applied there is less writeback cache thrashing on sdb
    by frequent forced writeback by flush-8:16 in over_bground_thresh.
    
    After all, small cached data will be flushed by periodic writeback
    once every dirty_writeback_interval.
    
    Suggested-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Vivek Trivedi <t.vivek@samsung.com>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 782ecc95314ca5b4329a044fa9642dae78756e9a
Author: Junxiao Bi <junxiao.bi@oracle.com>
Date:   Wed Sep 11 14:23:04 2013 -0700

    writeback: fix race that cause writeback hung
    
    There is a race between mark inode dirty and writeback thread, see the
    following scenario.  In this case, writeback thread will not run though
    there is dirty_io.
    
    __mark_inode_dirty()                                          bdi_writeback_workfn()
    	...                                                       	...
    	spin_lock(&inode->i_lock);
    	...
    	if (bdi_cap_writeback_dirty(bdi)) {
    	    <<< assume wb has dirty_io, so wakeup_bdi is false.
    	    <<< the following inode_dirty also have wakeup_bdi false.
    	    if (!wb_has_dirty_io(&bdi->wb))
    		    wakeup_bdi = true;
    	}
    	spin_unlock(&inode->i_lock);
    	                                                            <<< assume last dirty_io is removed here.
    	                                                            pages_written = wb_do_writeback(wb);
    	                                                            ...
    	                                                            <<< work_list empty and wb has no dirty_io,
    	                                                            <<< delayed_work will not be queued.
    	                                                            if (!list_empty(&bdi->work_list) ||
    	                                                                (wb_has_dirty_io(wb) && dirty_writeback_interval))
    	                                                                queue_delayed_work(bdi_wq, &wb->dwork,
    	                                                                    msecs_to_jiffies(dirty_writeback_interval * 10));
    	spin_lock(&bdi->wb.list_lock);
    	inode->dirtied_when = jiffies;
    	<<< new dirty_io is added.
    	list_move(&inode->i_wb_list, &bdi->wb.b_dirty);
    	spin_unlock(&bdi->wb.list_lock);
    
    	<<< though there is dirty_io, but wakeup_bdi is false,
    	<<< so writeback thread will not be waked up and
    	<<< the new dirty_io will not be flushed.
    	if (wakeup_bdi)
    	    bdi_wakeup_thread_delayed(bdi);
    
    Writeback will run until there is a new flush work queued.  This may cause
    a lot of dirty pages stay in memory for a long time.
    
    Signed-off-by: Junxiao Bi <junxiao.bi@oracle.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 870026671d04dd396ea91655f47053da2035e5c4
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Tue Aug 13 09:31:32 2013 +0530

    PM / QoS: Fix deadlock during PM QoS vote to default value
    
    pm_qos_work_fn() calls pm_qos_update_request() which does
    cancel_delayed_work_sync() on the same pm_qos_work_fn()
    causing deadlock. Fix this by updating the target with
    default timeout value directly instead of calling
    pm_qos_update_request().
    
    CRs-Fixed: 526216
    Change-Id: I7de2fb1c89f87b0ebf7427116b8920aec50d5b2b
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 1749bd6118f8ae85e1c744c96ab04a51dd3276c3
Author: Francisco Franco <franciscofranco.1990@gmail.com>
Date:   Wed Sep 25 03:03:01 2013 +0100

    audit: kiss goodbye you stupid piece of crap logging messages.
    
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 6da867c78ef46a715fd04cba51138fc7b0323fc1
Author: Feng Tang <feng.tang@intel.com>
Date:   Tue Mar 12 11:56:46 2013 +0800

    clocksource: Add new feature flag CLOCK_SOURCE_SUSPEND_NONSTOP
    
    Some x86 processors have a TSC clocksource, which continues to run
    even when system is suspended. Also most OMAP platforms have a
    32 KHz timer which has similar capability. Add a feature flag so that
    it could be utilized.
    
    CRs-fixed: 536881
    Change-Id: I863678fec5a3021158240f2c5b144c552ba0b16a
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Git-commit: 5caf4636259ae3af0efbb9bfc4cd97874b547c7d
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 63fd844de8fa83c87ef00476b238de36ae2cb4b4
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Jan 15 15:24:30 2014 +0200

    Alwas set noatime no diratime on all partitions! I/O Boost! V2

commit a261792549e7aabb12cd643d96bcd061ab8f8a29
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Jan 15 14:29:38 2014 +0200

    Alwas set noatime no diratime on all partitions! I/O Boost!
    
    Also mod read_ahead to 512. as seems that is faster! in test apps.

commit 7976b5670d755129e1446345dcf6303343c5ddb3
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Jan 15 14:34:51 2014 +0200

    binfmt_elf.c: use get_random_int() to fix entropy depleting
    
    Changes:
    --------
    v4->v3:
    - s/random_stack_user()/get_atrandom_bytes()/
    - Move this function to ahead of its use to avoid the predeclaration.
    
    v3->v2:
    - Tweak code comments of random_stack_user().
    - Remove redundant bits mask and shift upon the random variable.
    
    v2->v1:
    - Fix random copy to check up buffer length that are not 4-byte multiples.
    
    v3 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59597.html
    v2 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59418.html
    v1 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59128.html
    
    Thanks,
    -Jeff
    
    Entropy is quickly depleted under normal operations like ls(1), cat(1),
    etc...  between 2.6.30 to current mainline, for instance:
    
    $ cat /proc/sys/kernel/random/entropy_avail
    3428
    $ cat /proc/sys/kernel/random/entropy_avail
    2911
    $cat /proc/sys/kernel/random/entropy_avail
    2620
    
    We observed this problem has been occurring since 2.6.30 with
    fs/binfmt_elf.c: create_elf_tables()->get_random_bytes(), introduced by
    f06295b ("ELF: implement AT_RANDOM for glibc PRNG seeding").
    
    /*
     * Generate 16 random bytes for userspace PRNG seeding.
     */
    get_random_bytes(k_rand_bytes, sizeof(k_rand_bytes));
    
    The patch introduces a wrapper around get_random_int() which has lower
    overhead than calling get_random_bytes() directly.
    
    With this patch applied:
    $ cat /proc/sys/kernel/random/entropy_avail
    2731
    $ cat /proc/sys/kernel/random/entropy_avail
    2802
    $ cat /proc/sys/kernel/random/entropy_avail
    2878
    
    Analyzed by John Sobecki.
    
    Signed-off-by: Jie Liu <jeff.liu@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andreas Dilger <aedilger@gmail.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Arnd Bergmann <arnn@arndb.de>
    Cc: John Sobecki <john.sobecki@oracle.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Cc: Ted Ts'o <tytso@mit.edu>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Ulrich Drepper <drepper@redhat.com>

commit 4a5e809dc382b2b951b540cf17c4bb2e4023a7e9
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 15:30:31 2014 +0100

    New kernel test version!

commit 198713146ed02d462fa3d9fc327639d68df5fe8f
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Feb 10 17:46:48 2014 -0600

    zram: add Crypto API support
    
    Current version of zram does not allow any substitution of a default
    compression algorithm. Therefore, I decided to change the existing
    implementation of page compression by adding Crypto API compability.
    
    All direct calls to lzo1x compression/decompression methods are now
    replaced by calls consistent with Crypto. Also, I removed "workmem"
    field from struct zram_meta, as it was there for lzo1x purposes only
    and is no longer needed. Finally, I added a set of functions required
    by Crypto API to work properly.
    
    In order to substitute the default algorithm (lzo), change the value
    of zram.compressor module parameter to a proper name (e.g. lz4).
    
    Signed-off-by: Piotr Sarna <p.sarna@partner.samsung.com>
    Acked-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    
    backported to work with new zram driver
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4af1a5b7ce56c52ea4250998964dafa245173292
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Feb 10 17:17:31 2014 -0600

    ZRAM/ZSMALLOC: Linux 3.4 API backport
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fc8b5798e17997cb4195f0a8781d2ba5542263e2
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:46:06 2014 -0800

    zram: remove zram->lock in read path and change it with mutex
    
    Finally, we separated zram->lock dependency from 32bit stat/ table
    handling so there is no reason to use rw_semaphore between read and
    write path so this patch removes the lock from read path totally and
    changes rw_semaphore with mutex.  So, we could do
    
    old:
    
      read-read: OK
      read-write: NO
      write-write: NO
    
    Now:
    
      read-read: OK
      read-write: OK
      write-write: NO
    
    The below data proves mixed workload performs well 11 times and there is
    also enhance on write-write path because current rw-semaphore doesn't
    support SPIN_ON_OWNER.  It's side effect but anyway good thing for us.
    
    Write-related tests perform better (from 61% to 1058%) but read path has
    good/bad(from -2.22% to 1.45%) but they are all marginal within stddev.
    
      CPU 12
      iozone -t -T -l 12 -u 12 -r 16K -s 60M -I +Z -V 0
    
      ==Initial write                ==Initial write
      records: 10                    records: 10
      avg:  516189.16                avg:  839907.96
      std:   22486.53 (4.36%)        std:   47902.17 (5.70%)
      max:  546970.60                max:  909910.35
      min:  481131.54                min:  751148.38
      ==Rewrite                      ==Rewrite
      records: 10                    records: 10
      avg:  509527.98                avg: 1050156.37
      std:   45799.94 (8.99%)        std:   40695.44 (3.88%)
      max:  611574.27                max: 1111929.26
      min:  443679.95                min:  980409.62
      ==Read                         ==Read
      records: 10                    records: 10
      avg: 4408624.17                avg: 4472546.76
      std:  281152.61 (6.38%)        std:  163662.78 (3.66%)
      max: 4867888.66                max: 4727351.03
      min: 4058347.69                min: 4126520.88
      ==Re-read                      ==Re-read
      records: 10                    records: 10
      avg: 4462147.53                avg: 4363257.75
      std:  283546.11 (6.35%)        std:  247292.63 (5.67%)
      max: 4912894.44                max: 4677241.75
      min: 4131386.50                min: 4035235.84
      ==Reverse Read                 ==Reverse Read
      records: 10                    records: 10
      avg: 4565865.97                avg: 4485818.08
      std:  313395.63 (6.86%)        std:  248470.10 (5.54%)
      max: 5232749.16                max: 4789749.94
      min: 4185809.62                min: 3963081.34
      ==Stride read                  ==Stride read
      records: 10                    records: 10
      avg: 4515981.80                avg: 4418806.01
      std:  211192.32 (4.68%)        std:  212837.97 (4.82%)
      max: 4889287.28                max: 4686967.22
      min: 4210362.00                min: 4083041.84
      ==Random read                  ==Random read
      records: 10                    records: 10
      avg: 4410525.23                avg: 4387093.18
      std:  236693.22 (5.37%)        std:  235285.23 (5.36%)
      max: 4713698.47                max: 4669760.62
      min: 4057163.62                min: 3952002.16
      ==Mixed workload               ==Mixed workload
      records: 10                    records: 10
      avg:  243234.25                avg: 2818677.27
      std:   28505.07 (11.72%)       std:  195569.70 (6.94%)
      max:  288905.23                max: 3126478.11
      min:  212473.16                min: 2484150.69
      ==Random write                 ==Random write
      records: 10                    records: 10
      avg:  555887.07                avg: 1053057.79
      std:   70841.98 (12.74%)       std:   35195.36 (3.34%)
      max:  683188.28                max: 1096125.73
      min:  437299.57                min:  992481.93
      ==Pwrite                       ==Pwrite
      records: 10                    records: 10
      avg:  501745.93                avg:  810363.09
      std:   16373.54 (3.26%)        std:   19245.01 (2.37%)
      max:  518724.52                max:  833359.70
      min:  464208.73                min:  765501.87
      ==Pread                        ==Pread
      records: 10                    records: 10
      avg: 4539894.60                avg: 4457680.58
      std:  197094.66 (4.34%)        std:  188965.60 (4.24%)
      max: 4877170.38                max: 4689905.53
      min: 4226326.03                min: 4095739.72
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Tested-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 801fc65aef2be4f6cb43b1388d3e2c9f0603c8ce
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:46:04 2014 -0800

    zram: remove workqueue for freeing removed pending slot
    
    Commit a0c516cbfc74 ("zram: don't grab mutex in zram_slot_free_noity")
    introduced free request pending code to avoid scheduling by mutex under
    spinlock and it was a mess which made code lenghty and increased
    overhead.
    
    Now, we don't need zram->lock any more to free slot so this patch
    reverts it and then, tb_lock should protect it.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Tested-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 406bdeb80d6e8944fe41616bc0780c54e35a71bd
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:46:03 2014 -0800

    zram: introduce zram->tb_lock
    
    Currently, the zram table is protected by zram->lock but it's rather
    coarse-grained lock and it makes hard for scalibility.
    
    Let's use own rwlock instead of depending on zram->lock.  This patch
    adds new locking so obviously, it would make slow but this patch is just
    prepartion for removing coarse-grained rw_semaphore(ie, zram->lock)
    which is hurdle about zram scalability.
    
    Final patch in this patchset series will remove the lock from read-path
    and change rw_semaphore with mutex in write path.  With bonus, we could
    drop pending slot free mess in next patch.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Tested-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit aff35e78860b1dd4504822236bd6dafcebd771a8
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:46:02 2014 -0800

    zram: use atomic operation for stat
    
    Some of fields in zram->stats are protected by zram->lock which is
    rather coarse-grained so let's use atomic operation without explict
    locking.
    
    This patch is ready for removing dependency of zram->lock in read path
    which is very coarse-grained rw_semaphore.  Of course, this patch adds
    new atomic operation so it might make slow but my 12CPU test couldn't
    spot any regression.  All gain/lose is marginal within stddev.
    
      iozone -t -T -l 12 -u 12 -r 16K -s 60M -I +Z -V 0
    
      ==Initial write                ==Initial write
      records: 50                    records: 50
      avg:  412875.17                avg:  415638.23
      std:   38543.12 (9.34%)        std:   36601.11 (8.81%)
      max:  521262.03                max:  502976.72
      min:  343263.13                min:  351389.12
      ==Rewrite                      ==Rewrite
      records: 50                    records: 50
      avg:  416640.34                avg:  397914.33
      std:   60798.92 (14.59%)       std:   46150.42 (11.60%)
      max:  543057.07                max:  522669.17
      min:  304071.67                min:  316588.77
      ==Read                         ==Read
      records: 50                    records: 50
      avg: 4147338.63                avg: 4070736.51
      std:  179333.25 (4.32%)        std:  223499.89 (5.49%)
      max: 4459295.28                max: 4539514.44
      min: 3753057.53                min: 3444686.31
      ==Re-read                      ==Re-read
      records: 50                    records: 50
      avg: 4096706.71                avg: 4117218.57
      std:  229735.04 (5.61%)        std:  171676.25 (4.17%)
      max: 4430012.09                max: 4459263.94
      min: 2987217.80                min: 3666904.28
      ==Reverse Read                 ==Reverse Read
      records: 50                    records: 50
      avg: 4062763.83                avg: 4078508.32
      std:  186208.46 (4.58%)        std:  172684.34 (4.23%)
      max: 4401358.78                max: 4424757.22
      min: 3381625.00                min: 3679359.94
      ==Stride read                  ==Stride read
      records: 50                    records: 50
      avg: 4094933.49                avg: 4082170.22
      std:  185710.52 (4.54%)        std:  196346.68 (4.81%)
      max: 4478241.25                max: 4460060.97
      min: 3732593.23                min: 3584125.78
      ==Random read                  ==Random read
      records: 50                    records: 50
      avg: 4031070.04                avg: 4074847.49
      std:  192065.51 (4.76%)        std:  206911.33 (5.08%)
      max: 4356931.16                max: 4399442.56
      min: 3481619.62                min: 3548372.44
      ==Mixed workload               ==Mixed workload
      records: 50                    records: 50
      avg:  149925.73                avg:  149675.54
      std:    7701.26 (5.14%)        std:    6902.09 (4.61%)
      max:  191301.56                max:  175162.05
      min:  133566.28                min:  137762.87
      ==Random write                 ==Random write
      records: 50                    records: 50
      avg:  404050.11                avg:  393021.47
      std:   58887.57 (14.57%)       std:   42813.70 (10.89%)
      max:  601798.09                max:  524533.43
      min:  325176.99                min:  313255.34
      ==Pwrite                       ==Pwrite
      records: 50                    records: 50
      avg:  411217.70                avg:  411237.96
      std:   43114.99 (10.48%)       std:   33136.29 (8.06%)
      max:  530766.79                max:  471899.76
      min:  320786.84                min:  317906.94
      ==Pread                        ==Pread
      records: 50                    records: 50
      avg: 4154908.65                avg: 4087121.92
      std:  151272.08 (3.64%)        std:  219505.04 (5.37%)
      max: 4459478.12                max: 4435857.38
      min: 3730512.41                min: 3101101.67
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Tested-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit a0db95c6b846263c6f3187a78d25f9bf4e684a8f
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:46:01 2014 -0800

    zram: remove unnecessary free
    
    Commit a0c516cbfc74 ("zram: don't grab mutex in zram_slot_free_noity")
    introduced pending zram slot free in zram's write path in case of
    missing slot free by memory allocation failure in zram_slot_free_notify
    but it is not necessary because we have already freed the slot right
    before overwriting.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Tested-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f7a549a45c99930ded236a1a34c34e1dcb7df476
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:46:00 2014 -0800

    zram: delay pending free request in read path
    
    Sergey reported we don't need to handle pending free request every I/O
    so that this patch removes it in read path while we remain it in write
    path.
    
    Let's consider below example.
    
    Swap subsystem ask to zram "A" block free by swap_slot_free_notify but
    zram had been pended it without real freeing.  Swap subsystem allocates
    "A" block for new data but request pended for a long time just handled
    and zram blindly free new data on the "A" block.  :(
    
    That's why we couldn't remove handle pending free request right before
    zram-write.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reported-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Tested-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 30f9c2910788e83882e5a0639b6cd624e1546440
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:45:58 2014 -0800

    zram: fix race between reset and flushing pending work
    
    Dan and Sergey reported that there is a racy between reset and flushing
    of pending work so that it could make oops by freeing zram->meta in
    reset while zram_slot_free can access zram->meta if new request is
    adding during the race window.
    
    This patch moves flush after taking init_lock so it prevents new request
    so that it closes the race.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Tested-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 918ede9d5dcbcb6f6f50a107073645d5a3b105c7
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:45:56 2014 -0800

    zram: add zram maintainers
    
    Add maintainer information for zram into the MAINTAINERS file.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6b5171fef0be909e555b931ab8874f4977793044
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:45:55 2014 -0800

    zsmalloc: add copyright
    
    Add my copyright to the zsmalloc source code which I maintain.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ed9a64aa4f67888e968b9f93e0e98b2b1f1f3bfe
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:45:55 2014 -0800

    zram: add copyright
    
    Add my copyright to the zram source code which I maintain.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 703e3d9dcd18d8cc723dbd1e49f43787aedaa675
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:45:54 2014 -0800

    zram: remove old private project comment
    
    Remove the old private compcache project address so upcoming patches
    should be sent to LKML because we Linux kernel community will take care.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 16562be958e5cf5c2f7f47eabfa658a1d163c89a
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Feb 10 16:42:55 2014 -0600

    ZRAM/ZSMALLOC: initial file import from Linux 3.14 rc1 source
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	drivers/block/zram/Kconfig
    	drivers/block/zram/zram_drv.h
    	drivers/staging/Kconfig
    	drivers/staging/Makefile
    	drivers/staging/zram/zram_drv.c
    	drivers/staging/zram/zram_sysfs.c
    	drivers/staging/zsmalloc/Kconfig
    	drivers/staging/zsmalloc/zsmalloc-main.c
    	drivers/staging/zsmalloc/zsmalloc.h

commit dfdf0a84f61cd7bfd2034fcd175cd825ec66d0ea
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 11:25:57 2014 +0100

    Code style

commit 6cf7fd4148145ff307fb31ac494d869b235e45db
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 11:20:17 2014 +0100

    Revert "overall_stats: add overall stats for all available cores"
    
    This reverts commit a0444ec7c704e898fe4c5eb79c6eb4a76bcbcd3c.

commit 89a8290ba95707c0a473a606e3303c440dcbbe00
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 11:19:58 2014 +0100

    Revert "overall_stats: make dual/quad core stats configurable via meunconfig"
    
    This reverts commit 0fb0314da6748df52e971e7705c89babfeefb7e6.

commit 481d567e1003804428f5256319e33378e1bc3bee
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 11:19:36 2014 +0100

    Revert "overall_stats: forward port to kernel 3.4+"
    
    This reverts commit c34b22c91d4bd748d54741f16941beca8faa7503.

commit b6c96dd7ea17a23f8ce2140bc766cc24acb1cae7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 10:46:43 2014 +0100

    Revert "Revert "msm: pm-data: Enable powercollapse/suspend_enabled for non-boot cpus""
    
    This reverts commit fa0ce3f573f736fb18756cbb061474f56f3aeede.

commit 8ff74d844c7e81f27da352e09048d0f8174d14cc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:56:06 2014 +0100

    Revert "slub: use __SetPageSlab function to set PG_slab flag"
    
    This reverts commit 3f9fc50e349a210f17eb571c1947911dbe47db86.

commit f57eac182d49d75ce1c27594724bffff99ac014f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:35 2014 +0100

    Revert "slub: Use freelist instead of "object" in __slab_alloc"
    
    This reverts commit caa45468a06897e8868408f68bf32603c1f2e5cf.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 0e970d23421e56da372c0a17895e4c3a120c0499
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:35 2014 +0100

    Revert "slub: Add frozen check in __slab_alloc"
    
    This reverts commit ce131bceb2d149e08e4cfea43164f09620c6f01d.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 90abaae774d13adf43f28f2f246949eb777044d4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:34 2014 +0100

    Revert "slub: Acquire_slab() avoid loop"
    
    This reverts commit c596daac5186af36e79b16fd73a80939b499ec00.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 4cd2e1b772640cf09a2e0758da1c55ae7d3281cb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:34 2014 +0100

    Revert "slub: Simplify control flow in __slab_alloc()"
    
    This reverts commit 7c9cd6f6db5e5d6a2e243938c1395a0106901abb.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 7b178f1465177e14e1e4e736a5003549b7b172fb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:33 2014 +0100

    Revert "slub: new_slab_objects() can also get objects from partial list"
    
    This reverts commit 291b2022b4c7b85223427d4f57f15266f1aae08b.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 1210415620d5366f66fe28c829534a512722d3e6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:32 2014 +0100

    Revert "slub: Get rid of the node field"
    
    This reverts commit 7f175bb2e340eb1003f2828e478c459c5fd769f4.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 6ac29af715bc0f66e0570a13f6be19a83061ebfd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:31 2014 +0100

    Revert "slub: Separate out kmem_cache_cpu processing from deactivate_slab"
    
    This reverts commit 961ea3c5bffad5406341a9a593c4a8e489280efd.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit b6010e2d5693d243a2181073ad6b5962ec4477f9
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:31 2014 +0100

    Revert "slub: Use page variable instead of c->page."
    
    This reverts commit 73e94060e52f6e1849bb1ff07ae5f0769c1757ab.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit f6184cb084ca1dcdeed3e270488f1a1e0ea1f4d2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:30 2014 +0100

    Revert "slub: pass page to node_match() instead of kmem_cache_cpu structure"
    
    This reverts commit ee39de41a9247e75d207382f685b1daaba03f7af.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 9a52e731ae0883273de18248f4230a36913fcf1c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:29 2014 +0100

    Revert "slob: Define page struct fields used in mm_types.h"
    
    This reverts commit 47f1096419f8adeaa1556ce751b773df310a93b5.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 981c89e0c7fdd7ba473e64961d514313a671aa10
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:29 2014 +0100

    Revert "slab: Use page struct fields instead of casting"
    
    This reverts commit 1508398e481db814bd167c4debab0c31c413d8c8.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 92812a700722367b81a4d2291570712b1e8c67bd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:28 2014 +0100

    Revert "slab: Remove some accessors"
    
    This reverts commit 82d018d9213ae1f425acc5fcc1a1baeb90ff876a.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 99e011c6d1defcb5d493c282738f2f227fb4dc97
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:28 2014 +0100

    Revert "slob: No need to zero mapping since it is no longer in use"
    
    This reverts commit ca185538afb07e753c63efd1b8808dc485628f46.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit c79130b9b70e92ddd2e8f8a44fae96cb80f13183
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:27 2014 +0100

    Revert "slob: Remove various small accessors"
    
    This reverts commit 7dab32e75b473d367734beb82bf54dfa0a106d08.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 01510907ee4e2cb3aec96e2cfca435d0dd0ad0cb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:26 2014 +0100

    Revert "mm, sl[aou]b: Extract common fields from struct kmem_cache"
    
    This reverts commit 9b51336831ce44cf405fb6a457f5c566ce90240d.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 7ec82c1d095864360835fbc101cf40d234c87838
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:26 2014 +0100

    Revert "mm, sl[aou]b: Extract common code for kmem_cache_create()"
    
    This reverts commit 352a840a4f4ecce1b6e73ec0ce916125912f2d7f.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 617cec0dea1892fc1750c0c4ed3dc142a1be47bb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:25 2014 +0100

    Revert "slob: Fix early boot kernel crash"
    
    This reverts commit 330738dadf6eda4ef62e0c77af46c96056af16ae.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit d480365bd81843ad9ff8d1f14b4d2ed68b08f02e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:24 2014 +0100

    Revert "slab: Get rid of obj_size macro"
    
    This reverts commit c635b9e76314c17dcf3f47e4838685dd667a41b0.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit e20f1db3e175aa452caaa11dd7f2c7aa9e0c4094
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:24 2014 +0100

    Revert "slab/mempolicy: always use local policy from interrupt context"
    
    This reverts commit c7e0d5512321cbfb48048b490b4a487b91b92f9c.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 0ed748f61d979b9d2895731c5f705ac8fa8e8b67
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:23 2014 +0100

    Revert "slab: rename gfpflags to allocflags"
    
    This reverts commit f216379554a671eff5fd75b9d08520528f6ad2da.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 24df133070f1207ac0a7b8ea85d9aa5a941c29de
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:22 2014 +0100

    Revert "mm, slab: Build fix for recent kmem_cache changes"
    
    This reverts commit b24d96d11693d8639044807ed1707e224f4ac540.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit fd3c2ba6ae52dd4a3c0760cb9f9952e474b2eb3c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:22 2014 +0100

    Revert "slab: Fix a typo in commit 8c138b "slab: Get rid of obj_size macro""
    
    This reverts commit 4c6d6016514a3cf14141773d9faf2f7fd7762148.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 606b4b1097789a8b91013a6d4547a99915df9373
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:21 2014 +0100

    Revert "slab: move FULL state transition to an initcall"
    
    This reverts commit e281bdfb798380f71b29b9701b68d3ba7bd6f3b7.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit dd06a2c1673be3c4cacfe5bebae64657f91ed66d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:20 2014 +0100

    Revert "slub: use __cmpxchg_double_slab() at interrupt disabled place"
    
    This reverts commit a49021b01664b186b21180adee781a905a1c3778.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit a669c53bcd4960aedbad18e2a60e17eb20394579
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:20 2014 +0100

    Revert "slub: refactoring unfreeze_partials()"
    
    This reverts commit ca6a466d6beb8ee945ef2ee863f94b290f360ddb.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 3384be5108b283f130fb3cf867e76660d471342f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:19 2014 +0100

    Revert "slub: remove invalid reference to list iterator variable"
    
    This reverts commit 2ec1e06b819ffcb1a57b0f039f1bba52c1691bea.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 22ae64214d1cfb14bed7d840d6fa9aad57ed6890
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:19 2014 +0100

    Revert "mm, sl[aou]b: Common definition for boot state of the slab allocators"
    
    This reverts commit f28aa18b69a3baf94b3279ad3e61a5f147439ccb.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 212bf8cfd07766dd1e7b9d99c076960f0575e9b7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:18 2014 +0100

    Revert "mm, sl[aou]b: Use a common mutex definition"
    
    This reverts commit 289d0c263e878d08069e6b9b0a42be2f2b145346.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 689704ca91149472bbf7bc477ef4d7dc7462bdf5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:17 2014 +0100

    Revert "mm, sl[aou]b: Move kmem_cache_create mutex handling to common code"
    
    This reverts commit 42691437c126d1e8395edf46a7b8a376603306fa.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit f0da9afb070e1e011eb3a7e870573c47caab9086
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:16 2014 +0100

    Revert "mm, slub: ensure irqs are enabled for kmemcheck"
    
    This reverts commit fd79c8798130fa276a2ec46d390fc138d0771928.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit d11a6d88ef071744d7f9ca2092e945fad65fe619
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:16 2014 +0100

    Revert "mm: sl[au]b: add knowledge of PFMEMALLOC reserve pages"
    
    This reverts commit 3cb70c35d64c3c315e89c84d3ff25a7cd87c2fd0.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 6fcd30c0265d78765a5d9ffef0850bc61ac07cb9
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:15 2014 +0100

    Revert "mm: slub: optimise the SLUB fast path to avoid pfmemalloc checks"
    
    This reverts commit cc4be881d5e546f6e931ff290b286d2b78d65a36.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 9ceb1f48ab37aef8434d498b418050ecb72d5902
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:15 2014 +0100

    Revert "slub: use free_page instead of put_page for freeing kmalloc allocation"
    
    This reverts commit 21304b51f6119fc6987275fc31577c1a93f264fa.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 96fad5f839e6a49142ad3c0f6ce3e30c69b4e3ca
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:14 2014 +0100

    Revert "slub: Take node lock during object free checks"
    
    This reverts commit 19df4ffcb4689cbe83033113697475c216f0b7c3.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit de5977f26dcce0df2059ab7c969881f6bd4e109b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:13 2014 +0100

    Revert "slub: reduce failure of this_cpu_cmpxchg in put_cpu_partial() after unfreezing"
    
    This reverts commit 53521bb525d9c016197dc028949a7903355a8095.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit a389f9dd11818fbc5ab030d33ecc1d8397e62ad7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:13 2014 +0100

    Revert "mm/slub: Add debugging to verify correct cache use on kmem_cache_free()"
    
    This reverts commit d595d95b1524a8c15b10ed53d0aa6a7be7300ff2.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit ea1e61c57dbe132e5c3933337bb9f536b61af86a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:12 2014 +0100

    Revert "mm/slub: Use kmem_cache for the kmem_cache structure"
    
    This reverts commit b8812c93774a14d76c50647991a72e47562e35f6.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 970ba7b89e00519d3b8526919c6f912633023de9
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:11 2014 +0100

    Revert "mm: introduce __GFP_MEMALLOC to allow access to emergency reserves"
    
    This reverts commit 5b1502f50f922af30b243308bc98cf4e5c72fefa.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit b62b922b66f48ab9742c4dfced6f5c22710e7ad0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:11 2014 +0100

    Revert "mm: micro-optimise slab to avoid a function call"
    
    This reverts commit dd1413c018102732740d585a792414475c01ad63.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit f86d15949f6d7b7aa10de6c3c2b14d704412f91b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:10 2014 +0100

    Revert "slab: do not call compound_head() in page_get_cache()"
    
    This reverts commit 22f813a9da0fd40d0403f155c94ef0c22a337cf8.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit f067655f446794c9ee9b5dcdf021790a0aecafe2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:10 2014 +0100

    Revert "mm, slab: remove page_get_cache"
    
    This reverts commit 1032dd9dfeb51e04a6e753614473227456b0bccb.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 4f7bb40e313b3c1a254cc1b5dea173ca128df936
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:09 2014 +0100

    Revert "mm, slab: lock the correct nodelist after reenabling irqs"
    
    This reverts commit 00f130eb520ad5efc638936463ba2acccdb9172b.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 3f5bd6776399074c3e4ad590b85dbad39e60b1f0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:08 2014 +0100

    Revert "mm: Fix build warning in kmem_cache_create()"
    
    This reverts commit 06292a7fd7d9ec92e1107dc765c2a5f407a31435.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit c356102b862b9d807047b35a6766f2c749909c5b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:08 2014 +0100

    Revert "mm/slab_common.c: cleanup"
    
    This reverts commit 80ff2f9993036eb01a5de977e817c4f014e5403c.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit d29619056620aec4ad6f78efd52f542b8e64d7d2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:07 2014 +0100

    Revert "Revert "mm/slab_common.c: cleanup""
    
    This reverts commit b1084a266073bd26999b157d17644c6ad1ad14ea.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 6a3be79b073c6053b22fcb825e7b15154dde1710
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:06 2014 +0100

    Revert "mm/slab: restructure kmem_cache_create() debug checks"
    
    This reverts commit 4e78718d9f5f3851834a8506d90200629ad7f21a.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 4a2e8be5e488e6e6e79e410669d10c77e0fb2ff5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:06 2014 +0100

    Revert "mm/slab_common: Improve error handling in kmem_cache_create"
    
    This reverts commit 60a543ef17d2c9c4a5dba650652a23d04fe1e279.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 716526df9a913ec1f5c71dbdb4d599dbd6d561ae
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:05 2014 +0100

    Revert "mm/sl[aou]b: Move list_add() to slab_common.c"
    
    This reverts commit 9f08aba5f66bab592e224369cf083a7fcba9b2e0.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 55a90e4fe64959e259e192b8f8eb0a8c3cc9643e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:04 2014 +0100

    Revert "mm/sl[aou]b: Use "kmem_cache" name for slab cache with kmem_cache struct"
    
    This reverts commit 1b59786fd6bd79188fbaf58f6a5b73ff90153029.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit ccbaba2d9659312c848e451d2013ae25acb3a1b1
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:03 2014 +0100

    Revert "mm/sl[aou]b: Extract a common function for kmem_cache_destroy"
    
    This reverts commit 7b3f24fe07eaa346581feda6f4b17b8ffa80deeb.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 0198f5d166264f57de4a59776abc940815b17ff3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:03 2014 +0100

    Revert "mm/sl[aou]b: Move freeing of kmem_cache structure to common code"
    
    This reverts commit 7195d6eeecf2f6dd4d69fccf63c16c5e0744433d.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit a9fec16a93c969e1f477857053b69d7a373a8330
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:02 2014 +0100

    Revert "mm/sl[aou]b: Get rid of __kmem_cache_destroy"
    
    This reverts commit b3c54b23a2f69b153f0b876d1257635436208bab.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 84beebe96768df15f5ba5fea9ff7e092c25ad1c0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:01 2014 +0100

    Revert "mm/sl[aou]b: Move duping of slab name to slab_common.c"
    
    This reverts commit cb41de24d7e97686d509bdd71d4f42c7ea03d118.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit b58f161defe3faa1317172404e22d9aeae1ca250
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:01 2014 +0100

    Revert "mm/sl[aou]b: Do slab aliasing call from common code"
    
    This reverts commit 5b68b6f4e1a535b6205428dd85f10f00132d054e.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 10ef87b20a6cd617775d5f51651cd3e510903797
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:55:00 2014 +0100

    Revert "mm/sl[aou]b: Move sysfs_slab_add to common"
    
    This reverts commit f5ae7a6d38baa3ed131240e3570424eb150332b7.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit b9ffbc3b179376970c01e3829f068485a519691e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:59 2014 +0100

    Revert "mm/sl[aou]b: Move kmem_cache allocations into common code"
    
    This reverts commit 2a6850829158130d7dc706cf4761ae3099a46b34.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit edad867ecb302151382d1988d2df6caa188333c3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:59 2014 +0100

    Revert "mm/sl[aou]b: Shrink __kmem_cache_create() parameter lists"
    
    This reverts commit 5c7efaa1e893d66434b8cb74be59fb97d07876fc.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 89b9c07db10dca7afcb293facd664a564b2e0d46
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:58 2014 +0100

    Revert "mm/sl[aou]b: Move kmem_cache refcounting to common code"
    
    This reverts commit eec7af2d7fd06db43a6220ec9fbf925618521577.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit c8d1bc9d151f659bd58bd974ca98a4990dc08f1d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:57 2014 +0100

    Revert "Revert "mm/sl[aou]b: Move sysfs_slab_add to common""
    
    This reverts commit bbd42965eb6f70de1e68a8f652143cc0c0a17ab7.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit b81b3cc34a2a9a81efd6c33ac4c05d66bf50e5ea
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:57 2014 +0100

    Revert "slub: Zero initial memory segment for kmem_cache and kmem_cache_node"
    
    This reverts commit b2e8c010bf1d5177ed5b196c222985614760505c.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 4f73885c066a4d26ba279b6217ab3eacc74f594e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:56 2014 +0100

    Revert "slub: consider pfmemalloc_match() in get_partial_node()"
    
    This reverts commit ba34dc39d437de9b63fa23ceb3ecd0b364e70553.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 5e24eef63e56787d85eaa262e98c92bc6e8ec96c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:56 2014 +0100

    Revert "slab: fix the DEADLOCK issue on l3 alien lock"
    
    This reverts commit 5806108ceb32a2a1ddaecdbdbcd0a47a54645551.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 34568cd58a805894329cdcb613347fccd7920714
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:55 2014 +0100

    Revert "slab: do ClearSlabPfmemalloc() for all pages of slab"
    
    This reverts commit 6262c390172c6dab4d0ee34d1288ecffb2d6cbd4.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit fd0a6ad0cee9a70119a888f272e600c0fd66951e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:54 2014 +0100

    Revert "slab: fix starting index for finding another object"
    
    This reverts commit 8b80ebe169f2fafbd69157029134b60c6d0ff49a.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 682422dd4e6c4ade007d0226fede0bbef34db234
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:54 2014 +0100

    Revert "slab: Only define slab_error for DEBUG"
    
    This reverts commit 5f2e80ed65b7c9c0e81f19c6af0a32486f6f8280.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit e678285b1c10e650bb1083a94b6cc019952ba68a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:53 2014 +0100

    Revert "mm, sl[au]b: Taint kernel when we detect a corrupted slab"
    
    This reverts commit 04833181f14f8c5ae1ff345b1c77e77913eae116.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit f3a39d1dd4cbce3d853484e0071f87964473ffae
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:52 2014 +0100

    Revert "mm, slub: Rename slab_alloc() -> slab_alloc_node() to match SLAB"
    
    This reverts commit ce21dc4d77dcc437ab1350d4aec0562a62727fe3.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 3ad8b6f2d64539ff15812590fb72a3a26f94322c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:52 2014 +0100

    Revert "slub: init_kmem_cache_cpus() and put_cpu_partial() can be static"
    
    This reverts commit ce5f5d1435c1e07c95cfb4fe92d59a6df114d703.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 73f7942fbe4f1ca3b8de2bf9225b555895b48ea7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:51 2014 +0100

    Revert "slub: remove one code path and reduce lock contention in __slab_free()"
    
    This reverts commit e54ff566304d8a59df4de242e138d46d543d6c29.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit dbd39faccbfca07e8b1bf29d24ddceafc416d608
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:51 2014 +0100

    Revert "mm, slab: Remove silly function slab_buffer_size()"
    
    This reverts commit 1c2b245ca133a79af8228a513bd4d1c1fd668b2c.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 96a96696581570a1cc6f2b7fa8e7a5ace067ddbc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:50 2014 +0100

    Revert "mm, slab: Replace 'caller' type, void* -> unsigned long"
    
    This reverts commit 8480f3fab5b2e43ecdb854a909a3ac434b4fc95a.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 4528816c00a6d047fd28dd7944b91c14dcb6777a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:49 2014 +0100

    Revert "mm, slab: Match SLAB and SLUB kmem_cache_alloc_xxx_trace() prototype"
    
    This reverts commit 487ab7ade9f41d28452ac252167eeb7fb28d712a.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 639657b759bdeaebff5cfcec39dcfb9a1d8fbb0c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:49 2014 +0100

    Revert "mm, slab: Rename __cache_alloc() -> slab_alloc()"
    
    This reverts commit d0c08b04848ce710d3e86ba28a4858f8299e0e49.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit c090dd633c410811e701a8508c785d7e2d9a033a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:48 2014 +0100

    Revert "mm/slab: Fix typo _RET_IP -> _RET_IP_"
    
    This reverts commit 48ccdf215ef2fd2dcf4f6fa01e71cdaf9847cae8.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 098ff1f52ecfa0fe533aba2be78a879b1491bfcd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:48 2014 +0100

    Revert "mm/slab: Fix kmem_cache_alloc_node_trace() declaration"
    
    This reverts commit e062cebf81cb946704ef4ddf0d245f9e66dc7941.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 82c9a993fe17e596d5a98ca83c562b1f4b493c33
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:47 2014 +0100

    Revert "Revert "mm/slab: Fix kmem_cache_alloc_node_trace() declaration""
    
    This reverts commit e05a258f824e04a12bc07297a5784088c83ccd9e.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit dab993b0e1d73a8a0aee28999dbf54f33ca162d4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:46 2014 +0100

    Revert "slab: Fix build failure in __kmem_cache_create()"
    
    This reverts commit eb75ea4ac09ff5882d2b9d02e0f5be99a9705cb6.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 98548fb3d78bf6086f07603de481549c3c96d3e8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:46 2014 +0100

    Revert "mm, slab: release slab_mutex earlier in kmem_cache_destroy()"
    
    This reverts commit e2b8d02dc509193e091f479b74559207efc0e0f1.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 91fb7457e09b2640faa1ed5a5b3fb2b5819950be
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:45 2014 +0100

    Revert "mm/sl[au]b: Move slabinfo processing to slab_common.c"
    
    This reverts commit 161c91842725268d4711e34a608b265897b57e39.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 89a5b178f3aca34e2bdc3e6856b086ee0612ec30
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:44 2014 +0100

    Revert "mm/sl[au]b: Move print_slabinfo_header to slab_common.c"
    
    This reverts commit c9acd7ce5ca53e121a71c0e8f3f189a0fd21f6ac.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 3fed5676a6d46de8738aafdbf5f28fc8b103c8b3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:44 2014 +0100

    Revert "sl[au]b: Process slabinfo_show in common code"
    
    This reverts commit ca536c187a8f51054ae10496e903e7257faf0e9c.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit e431f0efd3b87d79a917bbaa7151775ab3940d2c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:43 2014 +0100

    Revert "slub: Commonize slab_cache field in struct page"
    
    This reverts commit ddcfb93a6ee2695d58b147884d368f80cebf73a5.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 2bd8b37d04c2b7185e12d65f5c9f580d3c5e4c18
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:42 2014 +0100

    Revert "mm, slob: Use NUMA_NO_NODE instead of -1"
    
    This reverts commit fa6eff7ed688a9c288a18e45cf59bd3279524082.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 275c03990df5fa49d16f3e3dceabfe47223fe0f7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:42 2014 +0100

    Revert "mm, slob: Add support for kmalloc_track_caller()"
    
    This reverts commit 92d95eb3d80c8f4c6dec3b2d8592b862c8d6c57b.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 15976d31d2c89f158fa9b5ea6ead6d2f759e0c71
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:41 2014 +0100

    Revert "mm, slob: fix build breakage in __kmalloc_node_track_caller"
    
    This reverts commit c0fbc121b56496a7bad4bf54fe7be423e17106a4.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit d28e27d4c92dc7f84f812b98ccb63f8f9b937e17
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:40 2014 +0100

    Revert "mm/slob: Drop usage of page->private for storing page-sized allocations"
    
    This reverts commit 4b0bcee9ca871f42f4000aed9e33fe64c5b55589.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit a1a49ca640ab6869964dd12d676d52753b100480
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:40 2014 +0100

    Revert "mm/slob: Use object_size field in kmem_cache_size()"
    
    This reverts commit 37b3ae63b1a47b0de5546cb4050576a766483b2b.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 9253ced4988321e7e7d3aa9543b4aa7a02c73c2a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:39 2014 +0100

    Revert "mm/sl[aou]b: Move common kmem_cache_size() to slab.h"
    
    This reverts commit 9a49aea0b92f67eca778242a1de93a528a88c72d.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 0ba74c00afecba7affaaaf07d7f45c664669d544
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:39 2014 +0100

    Revert "slab: Ignore internal flags in cache creation"
    
    This reverts commit 3d88e6017936338ad57f84bcc4f8c52dae651979.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit f82f8846ddd72a4829cd86346b1a134b1c103731
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:38 2014 +0100

    Revert "slub: Use correct cpu_slab on dead cpu"
    
    This reverts commit 19d3fb00ce439e89f8c7d128c031b567c9b823df.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 416e250e2049986f43b92b854023d3dad5b6c29c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:37 2014 +0100

    Revert "mm: fix slab.c kernel-doc warnings"
    
    This reverts commit 5c1e12ce290c2355ca9fbdad25267ba90c89e482.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 14df686c3421e1c0604aa663cbb1599298d28ff4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:37 2014 +0100

    Revert "slab: Simplify bootstrap"
    
    This reverts commit 45468bad9d1e6c518755027369660549dff443d4.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 7a4723c6c1dde52424e36817767dc14e93f9b9f5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:36 2014 +0100

    Revert "mm, sl[au]b: create common functions for boot slab creation"
    
    This reverts commit 85a656ef6ce182146917bc1128b6e2de51798199.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit d00315bb2b5be6b237424aa3054986ba01312b93
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:35 2014 +0100

    Revert "slub: Use statically allocated kmem_cache boot structure for bootstrap"
    
    This reverts commit 9c8305f110a8eee59c27776c5c2aea27eb2d7e15.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit a39a4b07e6d1a580e925a97f6df7deee5a2fec86
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:35 2014 +0100

    Revert "slab: Use the new create_boot_cache function to simplify bootstrap"
    
    This reverts commit a03babf6b2e4c60178a064791008c354557803d2.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 41c444df5e264267d3769b5c630e20120bfd93cf
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:34 2014 +0100

    Revert "mm/slob: Use free_page instead of put_page for page-size kmalloc allocations"
    
    This reverts commit 71dd2aa0687e177eac13022a4efcdcf8954f5302.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 4901284300da1efa7ef8a07a132257b731d0affa
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:33 2014 +0100

    Revert "mm/slob: use min_t() to compare ARCH_SLAB_MINALIGN"
    
    This reverts commit 0fc2fdec0a91d1cffae6effbf842dfbe42049977.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 4d5aff69e2e5c052d8333cc80b4de6333b11985e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:33 2014 +0100

    Revert "mm/sl[aou]b: Common alignment code"
    
    This reverts commit 94bb4416d022b64fcdf91ce8cfb78e54cd5b2bb7.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit a0fad7664cef9ce2c7cf2427a06de8e033219629
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:32 2014 +0100

    Revert "slub, hotplug: ignore unrelated node's hot-adding and hot-removing"
    
    This reverts commit 38df84030153426207c4e516b9088c895fa2bd0e.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit af20566af4b21e1b1bb7c71c0a56579fb2a4b74d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:31 2014 +0100

    Revert "slab: annotate on-slab caches nodelist locks"
    
    This reverts commit b3ebc07994159ddca3d7b3a7719ce70af5aeeb18.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit ca022e09cd844a46abecd034472ca373522951be
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:31 2014 +0100

    Revert "sl[au]b: always get the cache from its page in kmem_cache_free()"
    
    This reverts commit d4125aed939e482f12e77abbddd0bead9c686c0e.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 972d3385fe6600f4f844f4887ea71dade1f962cc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:30 2014 +0100

    Revert "Revert "mm: micro-optimise slab to avoid a function call""
    
    This reverts commit 145715d3a586109898c1c66aaabbefc50a74167d.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit f649501fbe739ae178a00346b1c092b23a549671
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 09:54:30 2014 +0100

    Revert "drivers/tty/smux_private.h: SLAB/SLOB compatibility fixup"
    
    This reverts commit 8e42c1474ab6f5b3035d45f1798bf132e7f21b55.
    
    Signed-off-by: Alucard24 <dmbaoh2@gmail.com>

commit 899cc7e817b6ec5d32ca62454ba00581db782671
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Mar 4 00:21:52 2014 +0100

    Fixed previous commits!

commit 39dcb3ba848f1e2f00477341f5d3cfaa4d543541
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 3 23:39:24 2014 +0100

    Merged random with faux code!

commit 89f64fc11126a3a97dc0ad71073b5eea99576154
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 3 23:33:54 2014 +0100

    Imported missing I/O schedulers!

commit d460331b8beae538dc0f513befb3c248b9e5bc63
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Feb 15 13:26:45 2014 -0800

    intelliactive: fix race condition when unregistering input handlers
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit c50467409c4f15ac4bdaf3e6b53721357e1c666a
Author: myfluxi <linflux@arcor.de>
Date:   Sat Feb 15 19:56:05 2014 -0800

    arm: vfpmodule: Fix warning procfs vfp_bounce reporting failed
    
    Creation of procfs cpu/vfp_bounce fails because we're initialized too early. Fix
    this by creating it on rootfs_initcall as before the NEON patches.
    
    <6>[    0.130770] VFP support v0.3: implementor 51 architecture 64 part 6f varia
    nt 2 rev 0
    <4>[    0.130795] ------------[ cut here ]------------
    <4>[    0.130813] WARNING: at fs/proc/generic.c:323 __xlate_proc_name+0xac/0xcc(
    )
    <4>[    0.130822] name 'cpu/vfp_bounce'
    <4>[    0.130855] [<c010e26c>] (unwind_backtrace+0x0/0x144) from [<c0a20f58>] (d
    ump_stack+0x20/0x24)
    <4>[    0.130879] [<c0a20f58>] (dump_stack+0x20/0x24) from [<c019b670>] (warn_sl
    owpath_common+0x58/0x70)
    <4>[    0.130899] [<c019b670>] (warn_slowpath_common+0x58/0x70) from [<c019b704>
    ] (warn_slowpath_fmt+0x40/0x48)
    <4>[    0.130919] [<c019b704>] (warn_slowpath_fmt+0x40/0x48) from [<c02c2ad8>] (
    __xlate_proc_name+0xac/0xcc)
    <4>[    0.130938] [<c02c2ad8>] (__xlate_proc_name+0xac/0xcc) from [<c02c2b50>] (
    __proc_create+0x58/0x100)
    <4>[    0.130956] [<c02c2b50>] (__proc_create+0x58/0x100) from [<c02c2ed0>] (pro
    c_create_data+0x5c/0xc0)
    <4>[    0.130979] [<c02c2ed0>] (proc_create_data+0x5c/0xc0) from [<c0f03484>] (v
    fp_init+0x19c/0x200)
    <4>[    0.131000] [<c0f03484>] (vfp_init+0x19c/0x200) from [<c0f00c98>] (do_one_
    initcall+0x98/0x168)
    <4>[    0.131020] [<c0f00c98>] (do_one_initcall+0x98/0x168) from [<c0f00e60>] (k
    ernel_init+0xf8/0x1b4)
    <4>[    0.131043] [<c0f00e60>] (kernel_init+0xf8/0x1b4) from [<c01081a0>] (kerne
    l_thread_exit+0x0/0x8)
    <4>[    0.131076] ---[ end trace ea6d9a9b5e947151 ]---
    <3>[    0.131086] Failed to create procfs node for VFP bounce reporting
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit c4d5c5af7fa8f6019ae44de08c4251cfdc7e3ab5
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Sep 22 10:08:50 2013 +0000

    ARM: only allow kernel mode neon with AEABI
    
    This prevents the linker erroring with:
    
    arm-linux-ld: error: arch/arm/lib/xor-neon.o uses VFP instructions, whereas arch/arm/lib/built-in.o does not
    arm-linux-ld: failed to merge target specific data of file arch/arm/lib/xor-neon.o
    
    This is due to the non-neon files being marked as containing FPA data/
    instructions (even though they do not) being mixed with files which
    contain VFP, which is an incompatible floating point format.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 5c106cd7794709d2ae003a6a75049d03bbf50b8b
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Sep 9 14:08:38 2013 +0000

    ARM: 7835/2: fix modular build of xor_blocks() with NEON enabled
    
    Commit 0195659 introduced a NEON accelerated version of the xor_blocks()
    function, but it needs the changes in this patch to allow it to be built
    as a module rather than statically into the kernel.
    
    This patch creates a separate module xor-neon.ko which exports the NEON
    inner xor_blocks() functions depended upon by the regular xor.ko if it
    is built with CONFIG_KERNEL_MODE_NEON=y
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit bdf1935b83b4d3549289bd9eeda71ee2448f8cc2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri May 17 16:51:23 2013 +0000

    ARM: crypto: add NEON accelerated XOR implementation
    
    Add a source file xor-neon.c (which is really just the reference
    C implementation passed through the GCC vectorizer) and hook it
    up to the XOR framework.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

commit 644b7d1f0022a75d849b0f07c9d81b6840b80878
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Feb 13 17:10:32 2014 -0800

    ARM: add support for kernel mode NEON
    
    In order to safely support the use of NEON instructions in
    kernel mode, some precautions need to be taken:
    - the userland context that may be present in the registers (even
      if the NEON/VFP is currently disabled) must be stored under the
      correct task (which may not be 'current' in the UP case),
    - to avoid having to keep track of additional vfpstates for the
      kernel side, disallow the use of NEON in interrupt context
      and run with preemption disabled,
    - after use, re-enable preemption and re-enable the lazy restore
      machinery by disabling the NEON/VFP unit.
    
    This patch adds the functions kernel_neon_begin() and
    kernel_neon_end() which take care of the above. It also adds
    the Kconfig symbol KERNEL_MODE_NEON to enable it.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 8111240a406a79be05e8b157fe557b30982f28a2
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Jan 9 01:22:04 2014 -0800

    ARM: be strict about FP exceptions in kernel mode
    
    The support code in vfp_support_entry does not care whether the
    exception that caused it to be invoked occurred in kernel mode or
    in user mode. However, neither condition that could trigger this
    exception (lazy restore and VFP bounce to support code) is
    currently allowable in kernel mode.
    
    In either case, print a message describing the condition before
    letting the undefined instruction handler run its course and trigger
    an oops.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel at linaro.org>
    Acked-by: Nicolas Pitre <nico at linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 776791274c78d6058aef2c49206fa59a9c6501de
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Feb 13 17:08:38 2014 -0800

    ARM: move VFP init to an earlier boot stage
    
    In order to use the NEON unit in the kernel, we should
    initialize it a bit earlier in the boot process so NEON users
    that like to do a quick benchmark at load time (like the
    xor_blocks or RAID-6 code) find the NEON/VFP unit already
    enabled.
    
    Replaced late_initcall() with core_initcall().
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel at linaro.org>
    Acked-by: Nicolas Pitre <nico at linaro.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1a016cdedc1d1349c4fcbad9234cbf55c65ff58b
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Jan 4 10:54:24 2014 -0800

    kernel/futex.c: Linux 3.4 compatibility fix up
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 3c86ab3c1af9c59178009a12ce893b68ad590124
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sat Jan 4 10:47:26 2014 -0800

    futex: Document ordering guarantees
    
    Date	Thu, 2 Jan 2014 07:05:19 -0800
    
    From: Thomas Gleixner <tglx@linutronix.de>
    
    That's essential, if you want to hack on futexes.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Darren Hart <dvhart@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: Tom Vaden <tom.vaden@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: Jason Low <jason.low2@hp.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit a1151167dd324e6839ea1966d7fabc675094d39b
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sat Jan 4 10:42:17 2014 -0800

    futex: Larger hash table
    
    Date	Thu, 2 Jan 2014 07:05:18 -0800
    
    From: Davidlohr Bueso <davidlohr@hp.com>
    
    Currently, the futex global hash table suffers from it's fixed, smallish
    (for today's standards) size of 256 entries, as well as its lack of NUMA
    awareness. Large systems, using many futexes, can be prone to high amounts
    of collisions; where these futexes hash to the same bucket and lead to
    extra contention on the same hb->lock. Furthermore, cacheline bouncing is a
    reality when we have multiple hb->locks residing on the same cacheline and
    different futexes hash to adjacent buckets.
    This patch keeps the current static size of 16 entries for small systems,
    or otherwise, 256 * ncpus (or larger as we need to round the number to a
    power of 2). Note that this number of CPUs accounts for all CPUs that can
    ever be available in the system, taking into consideration things like
    hotpluging. While we do impose extra overhead at bootup by making the hash
    table larger, this is a one time thing, and does not shadow the benefits
    of this patch.
    
    Furthermore, as suggested by tglx, by cache aligning the hash buckets we can
    avoid access across cacheline boundaries and also avoid massive cache line
    bouncing if multiple cpus are hammering away at different hash buckets which
    happen to reside in the same cache line.
    
    Also, similar to other core kernel components (pid, dcache, tcp), by using
    alloc_large_system_hash() we benefit from its NUMA awareness and thus the
    table is distributed among the nodes instead of in a single one.
    
    For a custom microbenchmark that pounds on the uaddr hashing -- making the wait
    path fail at futex_wait_setup() returning -EWOULDBLOCK for large amounts of
    futexes, we can see the following benefits on a 80-core, 8-socket 1Tb server:
    
    +---------+--------------------+------------------------+-----------------------+-------------------------------+
    | threads | baseline (ops/sec) | aligned-only (ops/sec) | large table (ops/sec) | large table+aligned (ops/sec) |
    +---------+--------------------+------------------------+-----------------------+-------------------------------+
    |     512 |		 32426 | 50531  (+55.8%)	| 255274  (+687.2%)	| 292553  (+802.2%)		|
    |     256 |		 65360 | 99588  (+52.3%)	| 443563  (+578.6%)	| 508088  (+677.3%)		|
    |     128 |		125635 | 200075 (+59.2%)	| 742613  (+491.1%)	| 835452  (+564.9%)		|
    |      80 |		193559 | 323425 (+67.1%)	| 1028147 (+431.1%)	| 1130304 (+483.9%)		|
    |      64 |		247667 | 443740 (+79.1%)	| 997300  (+302.6%)	| 1145494 (+362.5%)		|
    |      32 |		628412 | 721401 (+14.7%)	| 965996  (+53.7%)	| 1122115 (+78.5%)		|
    +---------+--------------------+------------------------+-----------------------+-------------------------------+
    Cc: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Darren Hart <dvhart@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: Tom Vaden <tom.vaden@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Reviewed-by: Waiman Long <Waiman.Long@hp.com>
    Reviewed-and-tested-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	kernel/futex.c

commit 7a0bcd10f0153e96b9a2152a1c585438d945d99f
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sat Jan 4 10:40:57 2014 -0800

    futex: Misc cleanups
    
    Date	Thu, 2 Jan 2014 07:05:17 -0800
    
    From: Jason Low <jason.low2@hp.com>
    
    - Remove unnecessary head variables.
    - Delete unused parameter in queue_unlock().
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Darren Hart <dvhart@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: Tom Vaden <tom.vaden@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 485c48e1832b9ea5f0ff6f05bc1e241a5840baa1
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Feb 13 16:59:56 2014 -0800

    intelliactive: tune for APQ8064 devices
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 6febcf2ce853fa1391c29f1f6bf75d6fc98247cb
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Feb 27 12:07:55 2014 -0600

    cpufreq: intelliactive: initial coding and introduction!
    
    intelliactive is based on Latest Google Interactive cpufreq governor from
    Google.  The original Interactive provides smooth UI operations but suffers
    slight battery drain due to its aggressive policies.  Intelliactive is an
    improvement built on top of it.
    
    It has the following enhancements added:
    1. self-boost capability from input drivers (no need for PowerHAL assist)
    2. two phase scheduling (idle/busy phases to prevent from jumping directly to
       max freq
    3. Checks for offline cpus and short circuits some unnecessary checks to
       improve code execution paths
    4. removed trace calls
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	drivers/cpufreq/Kconfig
    	drivers/cpufreq/Makefile
    	include/linux/cpufreq.h

commit e0ed2b53e3fb6c296994e570ddfc568f0866940c
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Feb 27 18:57:18 2014 -0600

    fastcharge: initial GT9505 adaptation
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 5c29daf0838327d46ee8e2f4736fdd0e3ca7cf55
Author: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
Date:   Thu Dec 26 18:23:08 2013 -0800

    ARM: mm: update __v7_setup() to the new LoUIS cache maintenance API
    
    The ARMv7 processor setup function __v7_setup() cleans and invalidates the
    CPU cache before enabling MMU to start the CPU with a clean CPU local cache.
    
    But on ARMv7 architectures like Cortex-[A15/A8], this code will end
    up flushing the L2 caches(up to level of Coherency) which is undesirable
    and expensive. The setup functions are used in the CPU hotplug scenario too
    and hence flushing all cache levels should be avoided.
    
    This patch replaces the cache flushing call with the newly introduced
    v7 dcache LoUIS API where only cache levels up to LoUIS are cleaned and
    invalidated when a processors executes __v7_setup which is the expected
    behavior.
    
    For processors like A9 and A5 where the L2 cache is an outer one the
    behavior should be unchanged.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit a87b829d9481934f639500f0ff5c98c32f588ac4
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Dec 26 18:22:21 2013 -0800

    ARM: kernel: update __cpu_disable to use cache LoUIS maintenance API
    
    When a CPU is hotplugged out caches that reside in its power domain
    lose their contents and so must be cleaned to the next memory level.
    
    Currently, __cpu_disable calls flush_cache_all() that for new generation
    processor like A15/A7 ends up cleaning and invalidating all cache levels
    up to Level of Coherency, which includes the unified L2.
    
    This ends up being a waste of cycles since the L2 cache contents are not
    lost on power down.
    
    This patch updates __cpu_disable to use the new LoUIS API cache operations.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b7d2327f957a2d3af273844c51b78876fa2247ba
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Dec 26 18:21:32 2013 -0800

    ARM: kernel: update cpu_suspend code to use cache LoUIS operations
    
    In processors like A15/A7 L2 cache is unified and integrated within the
    processor cache hierarchy, so that it is not considered an outer cache
    anymore. For processors like A15/A7 flush_cache_all() ends up cleaning
    all cache levels up to Level of Coherency (LoC) that includes
    the L2 unified cache.
    
    When a single CPU is suspended (CPU idle) a complete L2 clean is not
    required, so generic cpu_suspend code must clean the data cache using the
    newly introduced cache LoUIS function.
    
    The context and stack pointer (context pointer) are cleaned to main memory
    using cache area functions that operate on MVA and guarantee that the data
    is written back to main memory (perform cache cleaning up to the Point of
    Coherency - PoC) so that the processor can fetch the context when the MMU
    is off in the cpu_resume code path.
    
    outer_cache management remains unchanged.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 7318e2750a54bd232b1e3ef7e7bc8a2648fa4355
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Dec 26 18:20:41 2013 -0800

    ARM: mm: rename jump labels in v7_flush_dcache_all function
    
    This patch renames jump labels in v7_flush_dcache_all in order to define
    a specific flush cache levels entry point.
    
    TODO: factor out the level flushing loop if considered worthwhile and
          define the input registers requirements.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 41e03063773f97007d514383bfb6f6b995189cd5
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Dec 26 18:19:25 2013 -0800

    ARM: mm: implement LoUIS API for cache maintenance ops
    
    ARM v7 architecture introduced the concept of cache levels and related
    control registers. New processors like A7 and A15 embed an L2 unified cache
    controller that becomes part of the cache level hierarchy. Some operations in
    the kernel like cpu_suspend and __cpu_disable do not require a flush of the
    entire cache hierarchy to DRAM but just the cache levels belonging to the
    Level of Unification Inner Shareable (LoUIS), which in most of ARM v7 systems
    correspond to L1.
    
    The current cache flushing API used in cpu_suspend and __cpu_disable,
    flush_cache_all(), ends up flushing the whole cache hierarchy since for
    v7 it cleans and invalidates all cache levels up to Level of Coherency
    (LoC) which cripples system performance when used in hot paths like hotplug
    and cpuidle.
    
    Therefore a new kernel cache maintenance API must be added to cope with
    latest ARM system requirements.
    
    This patch adds flush_cache_louis() to the ARM kernel cache maintenance API.
    
    This function cleans and invalidates all data cache levels up to the
    Level of Unification Inner Shareable (LoUIS) and invalidates the instruction
    cache for processors that support it (> v7).
    
    This patch also creates an alias of the cache LoUIS function to flush_kern_all
    for all processor versions prior to v7, so that the current cache flushing
    behaviour is unchanged for those processors.
    
    v7 cache maintenance code implements a cache LoUIS function that cleans and
    invalidates the D-cache up to LoUIS and invalidates the I-cache, according
    to the new API.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit cac1e05e65fb41779c11ae281fa9210697e614f2
Author: Paul Reioux <reioux@gmail.com>
Date:   Tue Nov 12 00:31:09 2013 -0600

    Cpufreq: create/remove percpu sysfs nodes once
    
    On cpu hotplug on/off, percpu "cpufreq" sysfs node and it's
    sub nodes are freshly created and removed. This increases
    cpu up/down latency.
    
    Create percpu "cpufreq" sysfs node and it's sub nodes in driver
    registration and remove it in unregistration. This decreases
    cpu up/down latency significantly.
    
    Change-Id: I5c2b02e74de0ae4fe08dedc959bd7402cd9b7aa5
    Signed-off-by: Puneet Saxena <puneets@nvidia.com>
    Reviewed-on: http://git-master/r/146812
    (cherry picked from commit 298382285677fa791098d117ccc0a0a3d9ab0f9a)
    Reviewed-on: http://git-master/r/219379
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Sachin Nikam <snikam@nvidia.com>
    
    modified for use on Mako by faux123
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4b891ade5fdf29bfe66150957573302f62035d77
Author: Ajay Nandakumar <anandakumarm@nvidia.com>
Date:   Tue Jan 29 17:16:47 2013 +0530

    timekeeping: Fix time moving backwards
    
    Changed the calculation logic that sometimes calculates
    the time wrong.
    
    Sometimes there is an overflow when the tv_nsec field in
    the timespec structure is being added since it is 32-bit.
    
    To resolve this issue nsec variable is being added first
    so that the addition is performed in 64 bit signed format.
    
    Bug 1217429
    
    Change-Id: I9c65da88f02596ba73c47be6342ed909e650db22
    Signed-off-by: Ajay Nandakumar <anandakumarm@nvidia.com>
    Reviewed-on: http://git-master/r/195092
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Prashant Gaikwad <pgaikwad@nvidia.com>
    Reviewed-by: Shridhar Rasal <srasal@nvidia.com>
    Reviewed-by: Bo Yan <byan@nvidia.com>
    GVS: Gerrit_Virtual_Submit
    Reviewed-by: Bharat Nihalani <bnihalani@nvidia.com>
    Tested-by: Shridhar Rasal <srasal@nvidia.com>
    
    Conflicts:
    	kernel/time/timekeeping.c

commit a5ed849c76da6289a6acffc839d9d3bce3d528ca
Author: Paul Walmsley <pwalmsley@nvidia.com>
Date:   Wed Mar 6 19:02:56 2013 -0800

    sched: reinitialize rq->next_balance when a CPU is hot-added
    
    Reinitialize rq->next_balance when a CPU is hot-added.  Otherwise,
    scheduler domain rebalancing may be skipped if rq->next_balance was
    set to a future time when the CPU was last active, and the
    newly-re-added CPU is in idle_balance().  As a result, the
    newly-re-added CPU will remain idle with no tasks scheduled until the
    softlockup watchdog runs - potentially 4 seconds later.  This can
    waste energy and reduce performance.
    
    This behavior can be observed in some SoC kernels, which use CPU
    hotplug to dynamically remove and add CPUs in response to load.  In
    one case that triggered this behavior,
    
    0. the system started with all cores enabled, running multi-threaded
       CPU-bound code;
    
    1. the system entered some single-threaded code;
    
    2. a CPU went idle and was hot-removed;
    
    3. the system started executing a multi-threaded CPU-bound task;
    
    4. the CPU from event 2 was re-added, to respond to the load.
    
    The time interval between events 2 and 4 was approximately 300
    milliseconds.
    
    Of course, ideally CPU hotplug would not be used in this manner,
    but this patch does appear to fix a real bug.
    
    Nvidia folks: this patch is submitted as at least a partial fix for
    bug 1243368 ("[sched] Load-balancing not happening correctly after
    cores brought online")
    
    Change-Id: Iabac21e110402bb581b7db40c42babc951d378d0
    Signed-off-by: Paul Walmsley <pwalmsley@nvidia.com>
    Cc: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-on: http://git-master/r/208927
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Peter Zu <pzu@nvidia.com>
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    GVS: Gerrit_Virtual_Submit
    Tested-by: Peter Zu <pzu@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>

commit 08fdce96cece62aa4c0fae2c63842aa8a2b87853
Author: Neil Zhang <zhangwm@marvell.com>
Date:   Fri Dec 28 10:00:26 2012 +0000

    sched: remove redundant update_runtime notifier
    
    migration_call() will do all the things that update_runtime() does.
    So it seems update_runtime() is a redundant notifier, remove it.
    
    Furthermore, there is potential risk that the current code will catch
    BUG_ON at line 687 of rt.c when do cpu hotplug while there are realtime
    threads running because of enable runtime twice.
    
    Change-Id: I0fdad8d5a1cebb845d3f308b205dbd6517c3e4de
    Cc: bitbucket@online.de
    Signed-off-by: Neil Zhang <zhangwm@marvell.com>
    Reviewed-on: http://git-master/r/215596
    (cherry picked from commit 8f646de983f24361814d9a6ca679845fb2265807)
    Reviewed-on: http://git-master/r/223067
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Tested-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Paul Walmsley <pwalmsley@nvidia.com>
    Reviewed-by: Automatic_Commit_Validation_User
    GVS: Gerrit_Virtual_Submit
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit b7e22b849e92d01ae0363ecea06e27a52a9c8444
Author: Mike Galbraith <bitbucket@online.de>
Date:   Thu May 30 14:01:25 2013 -0700

    sched,rt: disable rt_runtime borrowing by default
    
    Make the default RT_RUNTIME_SHARE setting reflect the most common
    throttle role, that of safety mechanism to protect the box.
    
    Bug 1269903
    
    Change-Id: Id4ccf0095ea254f2e15fddc7ab02069f7f60a7c0
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Reviewed-on: http://git-master/r/234274
    (cherry picked from commit be74a12c8d8b987f569cdd0eec2aead3dcbdfa31)
    Reviewed-on: http://git-master/r/237266
    Tested-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>
    Reviewed-by: Paul Walmsley <pwalmsley@nvidia.com>
    GVS: Gerrit_Virtual_Submit

commit 6c0ab82939380ef57fbb22e542aad3d478e64f71
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Nov 11 17:25:02 2013 -0600

    softirq: reduce latencies
    
    Date	Thu, 03 Jan 2013 23:49:40 -0800
    
    In various network workloads, __do_softirq() latencies can be up
    to 20 ms if HZ=1000, and 200 ms if HZ=100.
    
    This is because we iterate 10 times in the softirq dispatcher,
    and some actions can consume a lot of cycles.
    
    This patch changes the fallback to ksoftirqd condition to :
    
    - A time limit of 2 ms.
    - need_resched() being set on current task
    
    When one of this condition is met, we wakeup ksoftirqd for further
    softirq processing if we still have pending softirqs.
    
    Using need_resched() as the only condition can trigger RCU stalls,
    as we can keep BH disabled for too long.
    
    I ran several benchmarks and got no significant difference in
    throughput, but a very significant reduction of latencies (one order
    of magnitude) :
    
    In following bench, 200 antagonist "netperf -t TCP_RR" are started in
    background, using all available cpus.
    
    Then we start one "netperf -t TCP_RR", bound to the cpu handling the NIC
    IRQ (hard+soft)
    
    Before patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=550110.424
    MIN_LATENCY=146858
    MAX_LATENCY=997109
    P50_LATENCY=305000
    P90_LATENCY=550000
    P99_LATENCY=710000
    MEAN_LATENCY=376989.12
    STDDEV_LATENCY=184046.92
    After patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=40545.492
    MIN_LATENCY=9834
    MAX_LATENCY=78366
    P50_LATENCY=33583
    P90_LATENCY=59000
    P99_LATENCY=69000
    MEAN_LATENCY=38364.67
    STDDEV_LATENCY=12865.26
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 73e56c7c8ba5af04086b073e0365673271f2fe71
Author: Joe Perches <joe@perches.com>
Date:   Mon Nov 11 17:22:41 2013 -0600

    jiffies conversions: Use compile time constants when possible
    
    Date	Fri, 04 Jan 2013 13:15:43 -0800
    
    Do the multiplications and divisions at compile time
    instead of runtime when the converted value is a constant.
    
    Make the calculation functions static __always_inline to jiffies.h.
    
    Add #defines with __builtin_constant_p to test and use the
    static inline or the runtime functions as appropriate.
    
    Prefix the old exported symbols/functions with __
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit cc97d77167283f6f8b079e0e5e44b0c243abd6b1
Author: Jan Kara <jack@suse.cz>
Date:   Wed Sep 11 21:22:22 2013 +0000

    writeback: fix occasional slow sync(1)
    
    In case when system contains no dirty pages, wakeup_flusher_threads() will
    submit WB_SYNC_NONE writeback for 0 pages so wb_writeback() exits
    immediately without doing anything, even though there are dirty inodes in
    the system.  Thus sync(1) will write all the dirty inodes from a
    WB_SYNC_ALL writeback pass which is slow.
    
    Fix the problem by using get_nr_dirty_pages() in wakeup_flusher_threads()
    instead of calculating number of dirty pages manually.  That function also
    takes number of dirty inodes into account.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Reported-by: Paul Taysom <taysom@chromium.org>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d4291fb095901aaca8b46a766accfebd0273e89b
Author: Eric Paris <eparis@redhat.com>
Date:   Fri Jul 6 14:13:29 2012 -0400

    SELinux: include definition of new capabilities
    
    The kernel has added CAP_WAKE_ALARM and CAP_EPOLLWAKEUP.  We need to
    define these in SELinux so they can be mediated by policy.
    
    Change-Id: I8a3e0db15ec5f4eb05d455a57e8446a8c2b484c2
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Signed-off-by: James Morris <james.l.morris@oracle.com>
    [sds: rename epollwakeup to block_suspend to match upstream merge]
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>

commit d9eca5552e06f67bd87fe21fa81283e81f08b104
Author: Puneet Kumar <puneetster@chromium.org>
Date:   Wed Nov 7 23:47:01 2012 -0800

    CHROMIUM: mm: Fix calculation of dirtyable memory
    
    The system uses global_dirtyable_memory() to calculate
    number of dirtyable pages/pages that can be allocated
    to the page cache.  A bug causes an underflow thus making
    the page count look like a big unsigned number.  This in turn
    confuses the dirty writeback throttling to aggressively write
    back pages as they become dirty (usually 1 page at a time).
    
    Fix is to ensure there is no underflow while doing the math.
    
    Signed-off-by: Sonny Rao <sonnyrao@chromium.org>
    Signed-off-by: Puneet Kumar <puneetster@chromium.org>
    
    BUG=chrome-os-partner:16011
    TEST=Manual; boot kernel, powerwash, login with testaccount and
    make sure no jank occurs on sync of applications
    
    Change-Id: I614e7c3156e014f0f28a4ef9bdd8cb8a2cd07b2a
    Reviewed-on: https://gerrit.chromium.org/gerrit/37612
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Olof Johansson <olofj@chromium.org>
    Commit-Ready: Puneet Kumar <puneetster@chromium.org>
    Reviewed-by: Puneet Kumar <puneetster@chromium.org>
    Tested-by: Puneet Kumar <puneetster@chromium.org>

commit 497c1254dd15748970473b464f044edb37cf8fa9
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Oct 20 01:36:56 2013 -0500

    msm_kcal: delay late resume to be last to resume
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 5dcbe1518649a238699f8fc7e095187a779f88b5
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Oct 19 17:52:16 2013 -0500

    msm_kcal: add late resume drivers to autoload the lut
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fcb7c1b9d55eabf0c58135586b694e8cf90384b5
Author: Shawn Bohrer <sbohrer@rgmadvisors.com>
Date:   Sat Oct 5 12:20:29 2013 -0500

    sched/rt: Remove redundant nr_cpus_allowed test
    
    Date	Fri, 4 Oct 2013 14:24:53 -0500
    
    From: Shawn Bohrer <sbohrer@rgmadvisors.com>
    
    In 76854c7e8f3f4172fef091e78d88b3b751463ac6 "sched: Use
    rt.nr_cpus_allowed to recover select_task_rq() cycles" an optimization
    was added to select_task_rq_rt() that immediately returns when
    p->nr_cpus_allowed == 1 at the beginning of the function.  This makes
    the latter p->nr_cpus_allowed > 1 check redundant and can be removed.
    Signed-off-by: Shawn Bohrer <sbohrer@rgmadvisors.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 0d40456bf4bfd9c793b30f69974bd093f1f722c8
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Thu Dec 15 15:44:45 2011 -0600

    AIO: Don't plug the I/O queue in do_io_submit() Asynchronous I/O latency to a solid-state disk greatly increased between the 2.6.32 and 3.0 kernels. By removing the plug from do_io_submit(), we observed a 34% improvement in the I/O latency.
    
    Unfortunately, at this level, we don't know if the request is to
    a rotating disk or not.
    
    Change-Id: I7101df956473ed9fd5dcff18e473dd93b688a5c1
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: linux-aio@kvack.org
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>

commit 231554eddd4e145e0295ad301abb7c0474fa5b3b
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Sep 23 01:03:53 2013 -0500

    MSM KCAL: Add generic MSK KCAL Color/Gamma Control
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mach-msm/Makefile

commit c679e3942aa3ceeb66c05e7a4923b4caa1f3f48a
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Sep 23 00:59:33 2013 -0500

    drivers/video/msm: restore update_lcdc_lut capabilities for MDP devices
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 905cb2f253cc6b362febec386dac990f0d300142
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Sep 7 19:33:51 2013 -0500

    sched: Micro-optimize the smart wake-affine logic
    
    Smart wake-affine is using node-size as the factor currently, but the overhead
    of the mask operation is high.
    
    Thus, this patch introduce the 'sd_llc_size' percpu variable, which will record
    the highest cache-share domain size, and make it to be the new factor, in order
    to reduce the overhead and make it more reasonable.
    
    Tested-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Tested-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Link: http://lkml.kernel.org/r/51D5008E.6030102@linux.vnet.ibm.com
    [ Tidied up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 8ae1ca1b0524d895463b048cc1004b5206e5ee85
Author: Michael Wang <wangyun@linux.vnet.ibm.com>
Date:   Sat Sep 7 19:32:00 2013 -0500

    sched: Implement smarter wake-affine logic
    
    The wake-affine scheduler feature is currently always trying to pull
    the wakee close to the waker. In theory this should be beneficial if
    the waker's CPU caches hot data for the wakee, and it's also beneficial
    in the extreme ping-pong high context switch rate case.
    
    Testing shows it can benefit hackbench up to 15%.
    
    However, the feature is somewhat blind, from which some workloads
    such as pgbench suffer. It's also time-consuming algorithmically.
    
    Testing shows it can damage pgbench up to 50% - far more than the
    benefit it brings in the best case.
    
    So wake-affine should be smarter and it should realize when to
    stop its thankless effort at trying to find a suitable CPU to wake on.
    
    This patch introduces 'wakee_flips', which will be increased each
    time the task flips (switches) its wakee target.
    
    So a high 'wakee_flips' value means the task has more than one
    wakee, and the bigger the number, the higher the wakeup frequency.
    
    Now when making the decision on whether to pull or not, pay attention to
    the wakee with a high 'wakee_flips', pulling such a task may benefit
    the wakee. Also imply that the waker will face cruel competition later,
    it could be very cruel or very fast depends on the story behind
    'wakee_flips', waker therefore suffers.
    
    Furthermore, if waker also has a high 'wakee_flips', that implies that
    multiple tasks rely on it, then waker's higher latency will damage all
    of them, so pulling wakee seems to be a bad deal.
    
    Thus, when 'waker->wakee_flips / wakee->wakee_flips' becomes
    higher and higher, the cost of pulling seems to be worse and worse.
    
    The patch therefore helps the wake-affine feature to stop its pulling
    work when:
    
    	wakee->wakee_flips > factor &&
    	waker->wakee_flips > (factor * wakee->wakee_flips)
    The 'factor' here is the number of CPUs in the current CPU's NUMA node,
    so a bigger node will lead to more pulling since the trial becomes more
    severe.
    
    After applying the patch, pgbench shows up to 40% improvements and no regressions.
    
    Tested with 12 cpu x86 server and tip 3.10.0-rc7.
    
    The percentages in the final column highlight the areas with the biggest wins,
    all other areas improved as well:
    
    	pgbench		    base	smart
    
    	| db_size | clients |  tps  |	|  tps  |
    	+---------+---------+-------+   +-------+
    	| 22 MB   |       1 | 10598 |   | 10796 |
    	| 22 MB   |       2 | 21257 |   | 21336 |
    	| 22 MB   |       4 | 41386 |   | 41622 |
    	| 22 MB   |       8 | 51253 |   | 57932 |
    	| 22 MB   |      12 | 48570 |   | 54000 |
    	| 22 MB   |      16 | 46748 |   | 55982 | +19.75%
    	| 22 MB   |      24 | 44346 |   | 55847 | +25.93%
    	| 22 MB   |      32 | 43460 |   | 54614 | +25.66%
    	| 7484 MB |       1 |  8951 |   |  9193 |
    	| 7484 MB |       2 | 19233 |   | 19240 |
    	| 7484 MB |       4 | 37239 |   | 37302 |
    	| 7484 MB |       8 | 46087 |   | 50018 |
    	| 7484 MB |      12 | 42054 |   | 48763 |
    	| 7484 MB |      16 | 40765 |   | 51633 | +26.66%
    	| 7484 MB |      24 | 37651 |   | 52377 | +39.11%
    	| 7484 MB |      32 | 37056 |   | 51108 | +37.92%
    	| 15 GB   |       1 |  8845 |   |  9104 |
    	| 15 GB   |       2 | 19094 |   | 19162 |
    	| 15 GB   |       4 | 36979 |   | 36983 |
    	| 15 GB   |       8 | 46087 |   | 49977 |
    	| 15 GB   |      12 | 41901 |   | 48591 |
    	| 15 GB   |      16 | 40147 |   | 50651 | +26.16%
    	| 15 GB   |      24 | 37250 |   | 52365 | +40.58%
    	| 15 GB   |      32 | 36470 |   | 50015 | +37.14%
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/51D50057.9000809@linux.vnet.ibm.com
    [ Improved the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 73a9bf23c51e06690cfd17f5eb5f593e989b770a
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Jul 2 22:38:35 2013 +1000

    sync: don't block the flusher thread waiting on IO
    
    When sync does it's WB_SYNC_ALL writeback, it issues data Io and
    then immediately waits for IO completion. This is done in the
    context of the flusher thread, and hence completely ties up the
    flusher thread for the backing device until all the dirty inodes
    have been synced. On filesystems that are dirtying inodes constantly
    and quickly, this means the flusher thread can be tied up for
    minutes per sync call and hence badly affect system level write IO
    performance as the page cache cannot be cleaned quickly.
    
    We already have a wait loop for IO completion for sync(2), so cut
    this out of the flusher thread and delegate it to wait_sb_inodes().
    Hence we can do rapid IO submission, and then wait for it all to
    complete.
    
    Effect of sync on fsmark before the patch:
    
    FSUse%        Count         Size    Files/sec     App Overhead
    .....
         0       640000         4096      35154.6          1026984
         0       720000         4096      36740.3          1023844
         0       800000         4096      36184.6           916599
         0       880000         4096       1282.7          1054367
         0       960000         4096       3951.3           918773
         0      1040000         4096      40646.2           996448
         0      1120000         4096      43610.1           895647
         0      1200000         4096      40333.1           921048
    
    And a single sync pass took:
    
      real    0m52.407s
      user    0m0.000s
      sys     0m0.090s
    
    After the patch, there is no impact on fsmark results, and each
    individual sync(2) operation run concurrently with the same fsmark
    workload takes roughly 7s:
    
      real    0m6.930s
      user    0m0.000s
      sys     0m0.039s
    
    IOWs, sync is 7-8x faster on a busy filesystem and does not have an
    adverse impact on ongoing async data write operations.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8555b32a7210eac17c30794a830d2d8c58d7c18d
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Tue Aug 6 01:03:22 2013 +0000

    watchdog: Fix warning caused by use of smp_processor_id()
    
    watchdog driver creates per-cpu kernel threads that are bound to
    separate cpus. Use of smp_processor_id() by such threads in
    __touch_watchdog() is perfectly fine, as they are generally pinned to
    the cpu on which they are running.
    
    During cpu offline event, however, affinity for the watchdog thread
    bound to dying cpu is broken before it is killed. There is a small
    time window between affinity broken and thread dying in which thread
    can run on a cpu with its affinity not set to that cpu exclusively.
    This will trigger a warning from use of smp_processor_id(), which is
    harmless in this case as it will provide the required heartbeat on cpu
    where it is running and moreover that thread will be shortly killed.
    
    Mute the harmless warning by use of raw_smp_processor_id() in
    __touch_watchdog(). This seems less intrusive fix than killing threads
    in CPU_DOWN_PREPARE event handler.
    
    Change-Id: I7fa22ff529aeea0ec3d5610cfec87aea92cf95a0
    CRs-Fixed: 517188
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit c3cc011b3baab5b7fa8304b4c3947ba407cdc64b
Author: Tingting Yang <tingting@codeaurora.org>
Date:   Tue Aug 6 11:12:52 2013 +0800

    msm: move printk out of spin lock low_water_lock
    
    cpu3 stuck in printk more time in spin lock low_water_lock cause cpu0
    get spin lock fail and system crashed.
    
    CRs-Fixed: 521570
    Change-Id: I75356a4b4171ae2888ce6cce792f569b5ca8cdcf
    Signed-off-by: Tingting Yang <tingting@codeaurora.org>

commit 158e9404b1b99dd6365e66900a4384bd84a4d281
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Mon May 13 15:11:55 2013 -0700

    sched: fix reference to wrong cfs_rq
    
    Commit 7db16c8c (sched: Fix SCHED_HRTICK bug leading to late preemption
    of tasks) introduced a bug in sched_slice() calculation by using wrong
    cfs_rq for tasks. rq->cfs was incorrectly used as task's cfs_rq, rather
    than the correct one to which they belonged.
    
    Fix the bug by using correct cfs_rq for tasks.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 701c3fc473e201f48e4b8506c2d62df36b37d771
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Thu Apr 18 11:42:22 2013 -0700

    sched: Fix SCHED_HRTICK bug leading to late preemption of tasks
    
    SCHED_HRTICK feature is useful to preempt SCHED_FAIR tasks on-the-dot
    (just when they would have exceeded their ideal_runtime). It makes use
    of a a per-cpu hrtimer resource and hence alarming that hrtimer should
    be based on total SCHED_FAIR tasks a cpu has across its various cfs_rqs,
    rather than being based on number of tasks in a particular cfs_rq (as
    implemented currently). As a result, with current code, its possible for
    a running task (which is the sole task in its cfs_rq) to be preempted
    much after its ideal_runtime has elapsed, resulting in increased latency
    for tasks in other cfs_rq on same cpu.
    
    Fix this by alarming sched hrtimer based on total number of SCHED_FAIR
    tasks a CPU has across its various cfs_rqs.
    
    Change-Id: I1f23680a64872f8ce0f451ac4bcae28e8967918f
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 0d5c0e880c109c94338a14d1a7533834e5882ee0
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:51:07 2013 -0500

    sched: Unthrottle rt runqueues in __disable_runtime()
    
    migrate_tasks() uses _pick_next_task_rt() to get tasks from the
    real-time runqueues to be migrated. When rt_rq is throttled
    _pick_next_task_rt() won't return anything, in which case
    migrate_tasks() can't move all threads over and gets stuck in an
    infinite loop.
    
    Instead unthrottle rt runqueues before migrating tasks.
    
    Additionally: move unthrottle_offline_cfs_rqs() to rq_offline_fair()
    
    Change-Id: If8a4a399f1a14b7f4789c1b205dcfadbde555214
    Signed-off-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/5FBF8E85CA34454794F0F7ECBA79798F379D3648B7@HQMAIL04.nvidia.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Git-commit: a4c96ae319b8047f62dedbe1eac79e321c185749
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
    backported to Linux 3.4
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	kernel/sched/core.c
    	kernel/sched/sched.h

commit 13f9ffb6902303b8476dfa0b4f668b7437c74de9
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Tue Jul 30 21:22:33 2013 +0000

    sched: change WARN_ON_ONCE to WARN_ON in try_to_wake_up_local()
    
    The WARN_ON_ONCE() calls at the beginning of try_to_wake_up_local()
    were recently converted from BUG_ON() calls. If these hit it indicates
    something is wrong and that may contribute to other system instability.
    To eliminate the risk of an instance of one of these errors going
    un-noticed because there was an earlier instance that occured long ago,
    change to WARN_ON(). If there ever is a flood of these there are bigger
    problems.
    
    Change-Id: I392832e2b6ec24b3569b001b1af9ecd4ed6828e7
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 9c1c34be429fa1fb899f98355a54e8cb49f4181c
Author: Tingting Yang <tingting@codeaurora.org>
Date:   Thu Aug 8 01:57:42 2013 +0000

    ARM: smp: Save CPU registers before IPI_CPU_STOP processing
    
    When a kernel panic occurs on one CPU, other CPUs are instructed to stop
    execution via the IPI_CPU_STOP message. These other CPUs dump their stack,
    which may not be good enough to reconstruct their context to perform
    post-mortem analysis. Dump each CPU's context
    (before it started procesing the IPI) into a globally accessible structure
    to allow for easier post-mortem debugging.
    
    Change-Id: I68ac75f73d7ddaebaff9122b23e341bcb00e8fb9
    Signed-off-by: Tingting Yang <tingting@codeaurora.org>

commit aaa7e6f34e660a071b3c58f1d7e615b10471b998
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Tue Jun 11 00:14:21 2013 +0000

    msm: Allow lowmem to be non contiguous and mixed.
    
    Any image that is expected to have a lifetime of
    the entire system can give the virtual address
    space back for use in vmalloc.
    
    Change-Id: I81ce848cd37e8573d706fa5d1aa52147b3c8da12
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit 8e42c1474ab6f5b3035d45f1798bf132e7f21b55
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Sep 2 14:29:47 2013 -0500

    drivers/tty/smux_private.h: SLAB/SLOB compatibility fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 145715d3a586109898c1c66aaabbefc50a74167d
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Sep 2 14:22:25 2013 -0500

    Revert "mm: micro-optimise slab to avoid a function call"
    
    This reverts commit 381760eadc393bcb1bb328510ad75cf13431806d.

commit d4125aed939e482f12e77abbddd0bead9c686c0e
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:46 2012 -0800

    sl[au]b: always get the cache from its page in kmem_cache_free()
    
    struct page already has this information.  If we start chaining caches,
    this information will always be more trustworthy than whatever is passed
    into the function.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    backported to Android Linux 3.4 by faux123
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	include/linux/memcontrol.h
    	mm/slab.h

commit b3ebc07994159ddca3d7b3a7719ce70af5aeeb18
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:31 2012 -0800

    slab: annotate on-slab caches nodelist locks
    
    We currently provide lockdep annotation for kmalloc caches, and also
    caches that have SLAB_DEBUG_OBJECTS enabled.  The reason for this is that
    we can quite frequently nest in the l3->list_lock lock, which is not
    something trivial to avoid.
    
    My proposal with this patch, is to extend this to caches whose slab
    management object lives within the slab as well ("on_slab").  The need for
    this arose in the context of testing kmemcg-slab patches.  With such
    patchset, we can have per-memcg kmalloc caches.  So the same path that led
    to nesting between kmalloc caches will could then lead to in-memcg
    nesting.  Because they are not annotated, lockdep will trigger.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 38df84030153426207c4e516b9088c895fa2bd0e
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Dec 11 16:01:05 2012 -0800

    slub, hotplug: ignore unrelated node's hot-adding and hot-removing
    
    SLUB only focuses on the nodes which have normal memory and it ignores the
    other node's hot-adding and hot-removing.
    
    Aka: if some memory of a node which has no onlined memory is online, but
    this new memory onlined is not normal memory (for example, highmem), we
    should not allocate kmem_cache_node for SLUB.
    
    And if the last normal memory is offlined, but the node still has memory,
    we should remove kmem_cache_node for that node.  (The current code delays
    it when all of the memory is offlined)
    
    So we only do something when marg->status_change_nid_normal > 0.
    marg->status_change_nid is not suitable here.
    
    The same problem doesn't exist in SLAB, because SLAB allocates kmem_list3
    for every node even the node don't have normal memory, SLAB tolerates
    kmem_list3 on alien nodes.  SLUB only focuses on the nodes which have
    normal memory, it don't tolerate alien kmem_cache_node.  The patch makes
    SLUB become self-compatible and avoids WARNs and BUGs in rare conditions.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Rob Landley <rob@landley.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Cc: Greg Kroah-Hartman <gregkh@suse.de>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 94bb4416d022b64fcdf91ce8cfb78e54cd5b2bb7
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:16 2012 +0000

    mm/sl[aou]b: Common alignment code
    
    Extract the code to do object alignment from the allocators.
    Do the alignment calculations in slab_common so that the
    __kmem_cache_create functions of the allocators do not have
    to deal with alignment.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 0fc2fdec0a91d1cffae6effbf842dfbe42049977
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Oct 5 16:55:20 2012 +0200

    mm/slob: use min_t() to compare ARCH_SLAB_MINALIGN
    
    The definition of ARCH_SLAB_MINALIGN is architecture dependent
    and can be either of type size_t or int. Comparing that value
    with ARCH_KMALLOC_MINALIGN can cause harmless warnings on
    platforms where they are different. Since both are always
    small positive integer numbers, using the size_t type to compare
    them is safe and gets rid of the warning.
    
    Without this patch, building ARM collie_defconfig results in:
    
    mm/slob.c: In function '__kmalloc_node':
    mm/slob.c:431:152: warning: comparison of distinct pointer types lacks a cast [enabled by default]
    mm/slob.c: In function 'kfree':
    mm/slob.c:484:153: warning: comparison of distinct pointer types lacks a cast [enabled by default]
    mm/slob.c: In function 'ksize':
    mm/slob.c:503:153: warning: comparison of distinct pointer types lacks a cast [enabled by default]
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    [ penberg@kernel.org: updates for master ]
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 71dd2aa0687e177eac13022a4efcdcf8954f5302
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Mon Oct 22 09:04:31 2012 -0300

    mm/slob: Use free_page instead of put_page for page-size kmalloc allocations
    
    When freeing objects, the slob allocator currently free empty pages
    calling __free_pages(). However, page-size kmallocs are disposed
    using put_page() instead.
    
    It makes no sense to call put_page() for kernel pages that are provided
    by the object allocator, so we shouldn't be doing this ourselves.
    
    This is based on:
    commit d9b7f22623b5fa9cc189581dcdfb2ac605933bf4
    Author: Glauber Costa <glommer@parallels.com>
    slub: use free_page instead of put_page for freeing kmalloc allocation
    
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit a03babf6b2e4c60178a064791008c354557803d2
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:09 2012 +0000

    slab: Use the new create_boot_cache function to simplify bootstrap
    
    Simplify setup and reduce code in kmem_cache_init(). This allows us to
    get rid of initarray_cache as well as the manual setup code for
    the kmem_cache and kmem_cache_node arrays during bootstrap.
    
    We introduce a new bootstrap state "PARTIAL" for slab that signals the
    creation of a kmem_cache boot cache.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 9c8305f110a8eee59c27776c5c2aea27eb2d7e15
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:07 2012 +0000

    slub: Use statically allocated kmem_cache boot structure for bootstrap
    
    Simplify bootstrap by statically allocated two kmem_cache structures. These are
    freed after bootup is complete. Allows us to no longer worry about calculations
    of sizes of kmem_cache structures during bootstrap.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 85a656ef6ce182146917bc1128b6e2de51798199
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:07 2012 +0000

    mm, sl[au]b: create common functions for boot slab creation
    
    Use a special function to create kmalloc caches and use that function in
    SLAB and SLUB.
    
    Acked-by: Joonsoo Kim <js1304@gmail.com>
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 45468bad9d1e6c518755027369660549dff443d4
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:01 2012 +0000

    slab: Simplify bootstrap
    
    The nodelists field in kmem_cache is pointing to the first unused
    object in the array field when bootstrap is complete.
    
    A problem with the current approach is that the statically sized
    kmem_cache structure use on boot can only contain NR_CPUS entries.
    If the number of nodes plus the number of cpus is greater then we
    would overwrite memory following the kmem_cache_boot definition.
    
    Increase the size of the array field to ensure that also the node
    pointers fit into the array field.
    
    Once we do that we no longer need the kmem_cache_nodelists
    array and we can then also use that structure elsewhere.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5c1e12ce290c2355ca9fbdad25267ba90c89e482
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Tue Nov 6 17:10:10 2012 -0800

    mm: fix slab.c kernel-doc warnings
    
    Fix new kernel-doc warnings in mm/slab.c:
    
    Warning(mm/slab.c:2358): No description found for parameter 'cachep'
    Warning(mm/slab.c:2358): Excess function parameter 'name' description in '__kmem_cache_create'
    Warning(mm/slab.c:2358): Excess function parameter 'size' description in '__kmem_cache_create'
    Warning(mm/slab.c:2358): Excess function parameter 'align' description in '__kmem_cache_create'
    Warning(mm/slab.c:2358): Excess function parameter 'ctor' description in '__kmem_cache_create'
    
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Cc:	Christoph Lameter <cl@linux-foundation.org>
    Cc:	Pekka Enberg <penberg@kernel.org>
    Cc:	Matt Mackall <mpm@selenic.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 19d3fb00ce439e89f8c7d128c031b567c9b823df
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:00 2012 +0000

    slub: Use correct cpu_slab on dead cpu
    
    Pass a kmem_cache_cpu pointer into unfreeze partials so that a different
    kmem_cache_cpu structure than the local one can be specified.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 3d88e6017936338ad57f84bcc4f8c52dae651979
Author: Glauber Costa <glommer@parallels.com>
Date:   Wed Oct 17 15:36:51 2012 +0400

    slab: Ignore internal flags in cache creation
    
    Some flags are used internally by the allocators for management
    purposes. One example of that is the CFLGS_OFF_SLAB flag that slab uses
    to mark that the metadata for that cache is stored outside of the slab.
    
    No cache should ever pass those as a creation flags. We can just ignore
    this bit if it happens to be passed (such as when duplicating a cache in
    the kmem memcg patches).
    
    Because such flags can vary from allocator to allocator, we allow them
    to make their own decisions on that, defining SLAB_AVAILABLE_FLAGS with
    all flags that are valid at creation time.  Allocators that doesn't have
    any specific flag requirement should define that to mean all flags.
    
    Common code will mask out all flags not belonging to that set.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 9a49aea0b92f67eca778242a1de93a528a88c72d
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Fri Oct 19 09:33:12 2012 -0300

    mm/sl[aou]b: Move common kmem_cache_size() to slab.h
    
    This function is identically defined in all three allocators
    and it's trivial to move it to slab.h
    
    Since now it's static, inline, header-defined function
    this patch also drops the EXPORT_SYMBOL tag.
    
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 37b3ae63b1a47b0de5546cb4050576a766483b2b
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Fri Oct 19 09:33:11 2012 -0300

    mm/slob: Use object_size field in kmem_cache_size()
    
    Fields object_size and size are not the same: the latter might include
    slab metadata. Return object_size field in kmem_cache_size().
    Also, improve trace accuracy by correctly tracing reported size.
    
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 4b0bcee9ca871f42f4000aed9e33fe64c5b55589
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Fri Oct 19 09:33:10 2012 -0300

    mm/slob: Drop usage of page->private for storing page-sized allocations
    
    This field was being used to store size allocation so it could be
    retrieved by ksize(). However, it is a bad practice to not mark a page
    as a slab page and then use fields for special purposes.
    There is no need to store the allocated size and
    ksize() can simply return PAGE_SIZE << compound_order(page).
    
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c0fbc121b56496a7bad4bf54fe7be423e17106a4
Author: David Rientjes <rientjes@google.com>
Date:   Tue Sep 25 12:53:51 2012 -0700

    mm, slob: fix build breakage in __kmalloc_node_track_caller
    
    On Sat, 8 Sep 2012, Ezequiel Garcia wrote:
    
    > @@ -454,15 +455,35 @@ void *__kmalloc_node(size_t size, gfp_t gfp, int node)
    >  			gfp |= __GFP_COMP;
    >  		ret = slob_new_pages(gfp, order, node);
    >
    > -		trace_kmalloc_node(_RET_IP_, ret,
    > +		trace_kmalloc_node(caller, ret,
    >  				   size, PAGE_SIZE << order, gfp, node);
    >  	}
    >
    >  	kmemleak_alloc(ret, size, 1, gfp);
    >  	return ret;
    >  }
    > +
    > +void *__kmalloc_node(size_t size, gfp_t gfp, int node)
    > +{
    > +	return __do_kmalloc_node(size, gfp, node, _RET_IP_);
    > +}
    >  EXPORT_SYMBOL(__kmalloc_node);
    >
    > +#ifdef CONFIG_TRACING
    > +void *__kmalloc_track_caller(size_t size, gfp_t gfp, unsigned long caller)
    > +{
    > +	return __do_kmalloc_node(size, gfp, NUMA_NO_NODE, caller);
    > +}
    > +
    > +#ifdef CONFIG_NUMA
    > +void *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,
    > +					int node, unsigned long caller)
    > +{
    > +	return __do_kmalloc_node(size, gfp, node, caller);
    > +}
    > +#endif
    
    This breaks Pekka's slab/next tree with this:
    
    mm/slob.c: In function '__kmalloc_node_track_caller':
    mm/slob.c:488: error: 'gfp' undeclared (first use in this function)
    mm/slob.c:488: error: (Each undeclared identifier is reported only once
    mm/slob.c:488: error: for each function it appears in.)
    
    mm, slob: fix build breakage in __kmalloc_node_track_caller
    
    "mm, slob: Add support for kmalloc_track_caller()" breaks the build
    because gfp is undeclared.  Fix it.
    
    Acked-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 92d95eb3d80c8f4c6dec3b2d8592b862c8d6c57b
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:53 2012 -0300

    mm, slob: Add support for kmalloc_track_caller()
    
    Currently slob falls back to regular kmalloc for this case.
    With this patch kmalloc_track_caller() is correctly implemented,
    thus tracing the specified caller.
    
    This is important to trace accurately allocations performed by
    krealloc, kstrdup, kmemdup, etc.
    
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit fa6eff7ed688a9c288a18e45cf59bd3279524082
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:51 2012 -0300

    mm, slob: Use NUMA_NO_NODE instead of -1
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	mm/slob.c

commit ddcfb93a6ee2695d58b147884d368f80cebf73a5
Author: Glauber Costa <glommer@parallels.com>
Date:   Mon Oct 22 18:05:36 2012 +0400

    slub: Commonize slab_cache field in struct page
    
    Right now, slab and slub have fields in struct page to derive which
    cache a page belongs to, but they do it slightly differently.
    
    slab uses a field called slab_cache, that lives in the third double
    word. slub, uses a field called "slab", living outside of the
    doublewords area.
    
    Ideally, we could use the same field for this. Since slub heavily makes
    use of the doubleword region, there isn't really much room to move
    slub's slab_cache field around. Since slab does not have such strict
    placement restrictions, we can move it outside the doubleword area.
    
    The naming used by slab, "slab_cache", is less confusing, and it is
    preferred over slub's generic "slab".
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ca536c187a8f51054ae10496e903e7257faf0e9c
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Oct 19 18:20:27 2012 +0400

    sl[au]b: Process slabinfo_show in common code
    
    With all the infrastructure in place, we can now have slabinfo_show
    done from slab_common.c. A cache-specific function is called to grab
    information about the cache itself, since that is still heavily
    dependent on the implementation. But with the values produced by it, all
    the printing and handling is done from common code.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    CC: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c9acd7ce5ca53e121a71c0e8f3f189a0fd21f6ac
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Oct 19 18:20:26 2012 +0400

    mm/sl[au]b: Move print_slabinfo_header to slab_common.c
    
    The header format is highly similar between slab and slub. The main
    difference lays in the fact that slab may optionally have statistics
    added here in case of CONFIG_SLAB_DEBUG, while the slub will stick them
    somewhere else.
    
    By making sure that information conditionally lives inside a
    globally-visible CONFIG_DEBUG_SLAB switch, we can move the header
    printing to a common location.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 161c91842725268d4711e34a608b265897b57e39
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Oct 19 18:20:25 2012 +0400

    mm/sl[au]b: Move slabinfo processing to slab_common.c
    
    This patch moves all the common machinery to slabinfo processing
    to slab_common.c. We can do better by noticing that the output is
    heavily common, and having the allocators to just provide finished
    information about this. But after this first step, this can be done
    easier.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e2b8d02dc509193e091f479b74559207efc0e0f1
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Oct 8 09:26:01 2012 +0200

    mm, slab: release slab_mutex earlier in kmem_cache_destroy()
    
    Commit 1331e7a1bbe1 ("rcu: Remove _rcu_barrier() dependency on
    __stop_machine()") introduced slab_mutex -> cpu_hotplug.lock dependency
    through kmem_cache_destroy() -> rcu_barrier() -> _rcu_barrier() ->
    get_online_cpus().
    
    Lockdep thinks that this might actually result in ABBA deadlock,
    and reports it as below:
    
    === [ cut here ] ===
     ======================================================
     [ INFO: possible circular locking dependency detected ]
     3.6.0-rc5-00004-g0d8ee37 #143 Not tainted
     -------------------------------------------------------
     kworker/u:2/40 is trying to acquire lock:
      (rcu_sched_state.barrier_mutex){+.+...}, at: [<ffffffff810f2126>] _rcu_barrier+0x26/0x1e0
    
     but task is already holding lock:
      (slab_mutex){+.+.+.}, at: [<ffffffff81176e15>] kmem_cache_destroy+0x45/0xe0
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #2 (slab_mutex){+.+.+.}:
            [<ffffffff810ae1e2>] validate_chain+0x632/0x720
            [<ffffffff810ae5d9>] __lock_acquire+0x309/0x530
            [<ffffffff810ae921>] lock_acquire+0x121/0x190
            [<ffffffff8155d4cc>] __mutex_lock_common+0x5c/0x450
            [<ffffffff8155d9ee>] mutex_lock_nested+0x3e/0x50
            [<ffffffff81558cb5>] cpuup_callback+0x2f/0xbe
            [<ffffffff81564b83>] notifier_call_chain+0x93/0x140
            [<ffffffff81076f89>] __raw_notifier_call_chain+0x9/0x10
            [<ffffffff8155719d>] _cpu_up+0xba/0x14e
            [<ffffffff815572ed>] cpu_up+0xbc/0x117
            [<ffffffff81ae05e3>] smp_init+0x6b/0x9f
            [<ffffffff81ac47d6>] kernel_init+0x147/0x1dc
            [<ffffffff8156ab44>] kernel_thread_helper+0x4/0x10
    
     -> #1 (cpu_hotplug.lock){+.+.+.}:
            [<ffffffff810ae1e2>] validate_chain+0x632/0x720
            [<ffffffff810ae5d9>] __lock_acquire+0x309/0x530
            [<ffffffff810ae921>] lock_acquire+0x121/0x190
            [<ffffffff8155d4cc>] __mutex_lock_common+0x5c/0x450
            [<ffffffff8155d9ee>] mutex_lock_nested+0x3e/0x50
            [<ffffffff81049197>] get_online_cpus+0x37/0x50
            [<ffffffff810f21bb>] _rcu_barrier+0xbb/0x1e0
            [<ffffffff810f22f0>] rcu_barrier_sched+0x10/0x20
            [<ffffffff810f2309>] rcu_barrier+0x9/0x10
            [<ffffffff8118c129>] deactivate_locked_super+0x49/0x90
            [<ffffffff8118cc01>] deactivate_super+0x61/0x70
            [<ffffffff811aaaa7>] mntput_no_expire+0x127/0x180
            [<ffffffff811ab49e>] sys_umount+0x6e/0xd0
            [<ffffffff81569979>] system_call_fastpath+0x16/0x1b
    
     -> #0 (rcu_sched_state.barrier_mutex){+.+...}:
            [<ffffffff810adb4e>] check_prev_add+0x3de/0x440
            [<ffffffff810ae1e2>] validate_chain+0x632/0x720
            [<ffffffff810ae5d9>] __lock_acquire+0x309/0x530
            [<ffffffff810ae921>] lock_acquire+0x121/0x190
            [<ffffffff8155d4cc>] __mutex_lock_common+0x5c/0x450
            [<ffffffff8155d9ee>] mutex_lock_nested+0x3e/0x50
            [<ffffffff810f2126>] _rcu_barrier+0x26/0x1e0
            [<ffffffff810f22f0>] rcu_barrier_sched+0x10/0x20
            [<ffffffff810f2309>] rcu_barrier+0x9/0x10
            [<ffffffff81176ea1>] kmem_cache_destroy+0xd1/0xe0
            [<ffffffffa04c3154>] nf_conntrack_cleanup_net+0xe4/0x110 [nf_conntrack]
            [<ffffffffa04c31aa>] nf_conntrack_cleanup+0x2a/0x70 [nf_conntrack]
            [<ffffffffa04c42ce>] nf_conntrack_net_exit+0x5e/0x80 [nf_conntrack]
            [<ffffffff81454b79>] ops_exit_list+0x39/0x60
            [<ffffffff814551ab>] cleanup_net+0xfb/0x1b0
            [<ffffffff8106917b>] process_one_work+0x26b/0x4c0
            [<ffffffff81069f3e>] worker_thread+0x12e/0x320
            [<ffffffff8106f73e>] kthread+0x9e/0xb0
            [<ffffffff8156ab44>] kernel_thread_helper+0x4/0x10
    
     other info that might help us debug this:
    
     Chain exists of:
       rcu_sched_state.barrier_mutex --> cpu_hotplug.lock --> slab_mutex
    
      Possible unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(slab_mutex);
                                    lock(cpu_hotplug.lock);
                                    lock(slab_mutex);
       lock(rcu_sched_state.barrier_mutex);
    
      *** DEADLOCK ***
    === [ cut here ] ===
    
    This is actually a false positive. Lockdep has no way of knowing the fact
    that the ABBA can actually never happen, because of special semantics of
    cpu_hotplug.refcount and its handling in cpu_hotplug_begin(); the mutual
    exclusion there is not achieved through mutex, but through
    cpu_hotplug.refcount.
    
    The "neither cpu_up() nor cpu_down() will proceed past cpu_hotplug_begin()
    until everyone who called get_online_cpus() will call put_online_cpus()"
    semantics is totally invisible to lockdep.
    
    This patch therefore moves the unlock of slab_mutex so that rcu_barrier()
    is being called with it unlocked. It has two advantages:
    
    - it slightly reduces hold time of slab_mutex; as it's used to protect
      the cachep list, it's not necessary to hold it over kmem_cache_free()
      call any more
    - it silences the lockdep false positive warning, as it avoids lockdep ever
      learning about slab_mutex -> cpu_hotplug.lock dependency
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit eb75ea4ac09ff5882d2b9d02e0f5be99a9705cb6
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Sun Sep 30 17:28:25 2012 +0900

    slab: Fix build failure in __kmem_cache_create()
    
    Fix build failure with CONFIG_DEBUG_SLAB=y && CONFIG_DEBUG_PAGEALLOC=y caused
    by commit 8a13a4cc "mm/sl[aou]b: Shrink __kmem_cache_create() parameter lists".
    
    mm/slab.c: In function '__kmem_cache_create':
    mm/slab.c:2474: error: 'align' undeclared (first use in this function)
    mm/slab.c:2474: error: (Each undeclared identifier is reported only once
    mm/slab.c:2474: error: for each function it appears in.)
    make[1]: *** [mm/slab.o] Error 1
    make: *** [mm] Error 2
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e05a258f824e04a12bc07297a5784088c83ccd9e
Author: Pekka Enberg <penberg@kernel.org>
Date:   Sat Sep 29 10:00:59 2012 +0300

    Revert "mm/slab: Fix kmem_cache_alloc_node_trace() declaration"
    
    This reverts commit 1e5965bf1f018cc30a4659fa3f1a40146e4276f6. Ezequiel
    Garcia has a better fix.

commit e062cebf81cb946704ef4ddf0d245f9e66dc7941
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Tue Sep 25 08:07:09 2012 -0300

    mm/slab: Fix kmem_cache_alloc_node_trace() declaration
    
    The bug was introduced in commit 4052147c0afa ("mm, slab: Match SLAB
    and SLUB kmem_cache_alloc_xxx_trace() prototype").
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 48ccdf215ef2fd2dcf4f6fa01e71cdaf9847cae8
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Tue Sep 25 08:07:08 2012 -0300

    mm/slab: Fix typo _RET_IP -> _RET_IP_
    
    The bug was introduced by commit 7c0cb9c64f83 ("mm, slab: Replace
    'caller' type, void* -> unsigned long").
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit d0c08b04848ce710d3e86ba28a4858f8299e0e49
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:57 2012 -0300

    mm, slab: Rename __cache_alloc() -> slab_alloc()
    
    This patch does not fix anything and its only goal is to
    produce common code between SLAB and SLUB.
    
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 487ab7ade9f41d28452ac252167eeb7fb28d712a
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:56 2012 -0300

    mm, slab: Match SLAB and SLUB kmem_cache_alloc_xxx_trace() prototype
    
    This long (seemingly unnecessary) patch does not fix anything and
    its only goal is to produce common code between SLAB and SLUB.
    
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 8480f3fab5b2e43ecdb854a909a3ac434b4fc95a
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:55 2012 -0300

    mm, slab: Replace 'caller' type, void* -> unsigned long
    
    This allows to use _RET_IP_ instead of builtin_address(0), thus
    achiveing implementation consistency in all three allocators.
    Though maybe a nitpick, the real goal behind this patch is
    to be able to obtain common code between SLAB and SLUB.
    
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 1c2b245ca133a79af8228a513bd4d1c1fd668b2c
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:52 2012 -0300

    mm, slab: Remove silly function slab_buffer_size()
    
    This function is seldom used, and can be simply replaced with cachep->size.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e54ff566304d8a59df4de242e138d46d543d6c29
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Thu Aug 16 00:02:40 2012 +0900

    slub: remove one code path and reduce lock contention in __slab_free()
    
    When we try to free object, there is some of case that we need
    to take a node lock. This is the necessary step for preventing a race.
    After taking a lock, then we try to cmpxchg_double_slab().
    But, there is a possible scenario that cmpxchg_double_slab() is failed
    with taking a lock. Following example explains it.
    
    CPU A               CPU B
    need lock
    ...                 need lock
    ...                 lock!!
    lock..but spin      free success
    spin...             unlock
    lock!!
    free fail
    
    In this case, retry with taking a lock is occured in CPU A.
    I think that in this case for CPU A,
    "release a lock first, and re-take a lock if necessary" is preferable way.
    
    There are two reasons for this.
    
    First, this makes __slab_free()'s logic somehow simple.
    With this patch, 'was_frozen = 1' is "always" handled without taking a lock.
    So we can remove one code path.
    
    Second, it may reduce lock contention.
    When we do retrying, status of slab is already changed,
    so we don't need a lock anymore in almost every case.
    "release a lock first, and re-take a lock if necessary" policy is
    helpful to this.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ce5f5d1435c1e07c95cfb4fe92d59a6df114d703
Author: Fengguang Wu <fengguang.wu@intel.com>
Date:   Fri Sep 28 16:34:05 2012 +0800

    slub: init_kmem_cache_cpus() and put_cpu_partial() can be static
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ce21dc4d77dcc437ab1350d4aec0562a62727fe3
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:58 2012 -0300

    mm, slub: Rename slab_alloc() -> slab_alloc_node() to match SLAB
    
    This patch does not fix anything, and its only goal is to enable us
    to obtain some common code between SLAB and SLUB.
    Neither behavior nor produced code is affected.
    
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 04833181f14f8c5ae1ff345b1c77e77913eae116
Author: Dave Jones <davej@redhat.com>
Date:   Tue Sep 18 15:54:12 2012 -0400

    mm, sl[au]b: Taint kernel when we detect a corrupted slab
    
    It doesn't seem worth adding a new taint flag for this, so just re-use
    the one from 'bad page'
    
    Acked-by: Christoph Lameter <cl@linux.com> # SLUB
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5f2e80ed65b7c9c0e81f19c6af0a32486f6f8280
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 11 19:49:38 2012 +0000

    slab: Only define slab_error for DEBUG
    
    On Tue, 11 Sep 2012, Stephen Rothwell wrote:
    > After merging the final tree, today's linux-next build (sparc64 defconfig)
    > produced this warning:
    >
    > mm/slab.c:808:13: warning: '__slab_error' defined but not used [-Wunused-function]
    >
    > Introduced by commit 945cf2b6199b ("mm/sl[aou]b: Extract a common
    > function for kmem_cache_destroy").  All uses of slab_error() are now
    > guarded by DEBUG.
    
    There is no use case left for slab builds without DEBUG.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 8b80ebe169f2fafbd69157029134b60c6d0ff49a
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Mon Sep 17 14:09:06 2012 -0700

    slab: fix starting index for finding another object
    
    In array cache, there is a object at index 0, check it.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6262c390172c6dab4d0ee34d1288ecffb2d6cbd4
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Sep 17 14:09:03 2012 -0700

    slab: do ClearSlabPfmemalloc() for all pages of slab
    
    Right now, we call ClearSlabPfmemalloc() for first page of slab when we
    clear SlabPfmemalloc flag.  This is fine for most swap-over-network use
    cases as it is expected that order-0 pages are in use.  Unfortunately it
    is possible that that __ac_put_obj() checks SlabPfmemalloc on a tail
    page and while this is harmless, it is sloppy.  This patch ensures that
    the head page is always used.
    
    This problem was originally identified by Joonsoo Kim.
    
    [js1304@gmail.com: Original implementation and problem identification]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5806108ceb32a2a1ddaecdbdbcd0a47a54645551
Author: Michael Wang <wangyun@linux.vnet.ibm.com>
Date:   Wed Sep 5 10:33:18 2012 +0800

    slab: fix the DEADLOCK issue on l3 alien lock
    
    DEADLOCK will be report while running a kernel with NUMA and LOCKDEP enabled,
    the process of this fake report is:
    
    	   kmem_cache_free()	//free obj in cachep
    	-> cache_free_alien()	//acquire cachep's l3 alien lock
    	-> __drain_alien_cache()
    	-> free_block()
    	-> slab_destroy()
    	-> kmem_cache_free()	//free slab in cachep->slabp_cache
    	-> cache_free_alien()	//acquire cachep->slabp_cache's l3 alien lock
    
    Since the cachep and cachep->slabp_cache's l3 alien are in the same lock class,
    fake report generated.
    
    This should not happen since we already have init_lock_keys() which will
    reassign the lock class for both l3 list and l3 alien.
    
    However, init_lock_keys() was invoked at a wrong position which is before we
    invoke enable_cpucache() on each cache.
    
    Since until set slab_state to be FULL, we won't invoke enable_cpucache()
    on caches to build their l3 alien while creating them, so although we invoked
    init_lock_keys(), the l3 alien lock class won't change since we don't have
    them until invoked enable_cpucache() later.
    
    This patch will invoke init_lock_keys() after we done enable_cpucache()
    instead of before to avoid the fake DEADLOCK report.
    
    Michael traced the problem back to a commit in release 3.0.0:
    
    commit 30765b92ada267c5395fc788623cb15233276f5c
    Author: Peter Zijlstra <peterz@infradead.org>
    Date:   Thu Jul 28 23:22:56 2011 +0200
    
        slab, lockdep: Annotate the locks before using them
    
        Fernando found we hit the regular OFF_SLAB 'recursion' before we
        annotate the locks, cure this.
    
        The relevant portion of the stack-trace:
    
        > [    0.000000]  [<c085e24f>] rt_spin_lock+0x50/0x56
        > [    0.000000]  [<c04fb406>] __cache_free+0x43/0xc3
        > [    0.000000]  [<c04fb23f>] kmem_cache_free+0x6c/0xdc
        > [    0.000000]  [<c04fb2fe>] slab_destroy+0x4f/0x53
        > [    0.000000]  [<c04fb396>] free_block+0x94/0xc1
        > [    0.000000]  [<c04fc551>] do_tune_cpucache+0x10b/0x2bb
        > [    0.000000]  [<c04fc8dc>] enable_cpucache+0x7b/0xa7
        > [    0.000000]  [<c0bd9d3c>] kmem_cache_init_late+0x1f/0x61
        > [    0.000000]  [<c0bba687>] start_kernel+0x24c/0x363
        > [    0.000000]  [<c0bba0ba>] i386_start_kernel+0xa9/0xaf
    
        Reported-by: Fernando Lopez-Lezcano <nando@ccrma.Stanford.EDU>
        Acked-by: Pekka Enberg <penberg@kernel.org>
        Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
        Link: http://lkml.kernel.org/r/1311888176.2617.379.camel@laptop
        Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    The commit moved init_lock_keys() before we build up the alien, so we
    failed to reclass it.
    
    Cc: <stable@vger.kernel.org> # 3.0+
    Acked-by: Christoph Lameter <cl@linux.com>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Conflicts:
    	mm/slab.c

commit ba34dc39d437de9b63fa23ceb3ecd0b364e70553
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Mon Sep 17 14:09:09 2012 -0700

    slub: consider pfmemalloc_match() in get_partial_node()
    
    get_partial() is currently not checking pfmemalloc_match() meaning that
    it is possible for pfmemalloc pages to leak to non-pfmemalloc users.
    This is a problem in the following situation.  Assume that there is a
    request from normal allocation and there are no objects in the per-cpu
    cache and no node-partial slab.
    
    In this case, slab_alloc enters the slow path and new_slab_objects() is
    called which may return a PFMEMALLOC page.  As the current user is not
    allowed to access PFMEMALLOC page, deactivate_slab() is called
    ([5091b74a: mm: slub: optimise the SLUB fast path to avoid pfmemalloc
    checks]) and returns an object from PFMEMALLOC page.
    
    Next time, when we get another request from normal allocation,
    slab_alloc() enters the slow-path and calls new_slab_objects().  In
    new_slab_objects(), we call get_partial() and get a partial slab which
    was just deactivated but is a pfmemalloc page.  We extract one object
    from it and re-deactivate.
    
      "deactivate -> re-get in get_partial -> re-deactivate" occures repeatedly.
    
    As a result, access to PFMEMALLOC page is not properly restricted and it
    can cause a performance degradation due to frequent deactivation.
    deactivation frequently.
    
    This patch changes get_partial_node() to take pfmemalloc_match() into
    account and prevents the "deactivate -> re-get in get_partial()
    scenario.  Instead, new_slab() is called.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b2e8c010bf1d5177ed5b196c222985614760505c
Author: Christoph Lameter <cl@linux.com>
Date:   Sat Sep 8 18:27:10 2012 +0000

    slub: Zero initial memory segment for kmem_cache and kmem_cache_node
    
    Tony Luck reported the following problem on IA-64:
    
      Worked fine yesterday on next-20120905, crashes today. First sign of
      trouble was an unaligned access, then a NULL dereference. SL*B related
      bits of my config:
    
      CONFIG_SLUB_DEBUG=y
      # CONFIG_SLAB is not set
      CONFIG_SLUB=y
      CONFIG_SLABINFO=y
      # CONFIG_SLUB_DEBUG_ON is not set
      # CONFIG_SLUB_STATS is not set
    
      And he console log.
    
      PID hash table entries: 4096 (order: 1, 32768 bytes)
      Dentry cache hash table entries: 262144 (order: 7, 2097152 bytes)
      Inode-cache hash table entries: 131072 (order: 6, 1048576 bytes)
      Memory: 2047920k/2086064k available (13992k code, 38144k reserved,
      6012k data, 880k init)
      kernel unaligned access to 0xca2ffc55fb373e95, ip=0xa0000001001be550
      swapper[0]: error during unaligned kernel access
       -1 [1]
      Modules linked in:
    
      Pid: 0, CPU 0, comm:              swapper
      psr : 00001010084a2018 ifs : 800000000000060f ip  :
      [<a0000001001be550>]    Not tainted (3.6.0-rc4-zx1-smp-next-20120906)
      ip is at new_slab+0x90/0x680
      unat: 0000000000000000 pfs : 000000000000060f rsc : 0000000000000003
      rnat: 9666960159966a59 bsps: a0000001001441c0 pr  : 9666960159965a59
      ldrs: 0000000000000000 ccv : 0000000000000000 fpsr: 0009804c8a70433f
      csd : 0000000000000000 ssd : 0000000000000000
      b0  : a0000001001be500 b6  : a00000010112cb20 b7  : a0000001011660a0
      f6  : 0fff7f0f0f0f0e54f0000 f7  : 0ffe8c5c1000000000000
      f8  : 1000d8000000000000000 f9  : 100068800000000000000
      f10 : 10005f0f0f0f0e54f0000 f11 : 1003e0000000000000078
      r1  : a00000010155eef0 r2  : 0000000000000000 r3  : fffffffffffc1638
      r8  : e0000040600081b8 r9  : ca2ffc55fb373e95 r10 : 0000000000000000
      r11 : e000004040001646 r12 : a000000101287e20 r13 : a000000101280000
      r14 : 0000000000004000 r15 : 0000000000000078 r16 : ca2ffc55fb373e75
      r17 : e000004040040000 r18 : fffffffffffc1646 r19 : e000004040001646
      r20 : fffffffffffc15f8 r21 : 000000000000004d r22 : a00000010132fa68
      r23 : 00000000000000ed r24 : 0000000000000000 r25 : 0000000000000000
      r26 : 0000000000000001 r27 : a0000001012b8500 r28 : a00000010135f4a0
      r29 : 0000000000000000 r30 : 0000000000000000 r31 : 0000000000000001
      Unable to handle kernel NULL pointer dereference (address
      0000000000000018)
      swapper[0]: Oops 11003706212352 [2]
      Modules linked in:
    
      Pid: 0, CPU 0, comm:              swapper
      psr : 0000121008022018 ifs : 800000000000cc18 ip  :
      [<a0000001004dc8f1>]    Not tainted (3.6.0-rc4-zx1-smp-next-20120906)
      ip is at __copy_user+0x891/0x960
      unat: 0000000000000000 pfs : 0000000000000813 rsc : 0000000000000003
      rnat: 0000000000000000 bsps: 0000000000000000 pr  : 9666960159961765
      ldrs: 0000000000000000 ccv : 0000000000000000 fpsr: 0009804c0270033f
      csd : 0000000000000000 ssd : 0000000000000000
      b0  : a00000010004b550 b6  : a00000010004b740 b7  : a00000010000c750
      f6  : 000000000000000000000 f7  : 1003e9e3779b97f4a7c16
      f8  : 1003e0a00000010001550 f9  : 100068800000000000000
      f10 : 10005f0f0f0f0e54f0000 f11 : 1003e0000000000000078
      r1  : a00000010155eef0 r2  : a0000001012870b0 r3  : a0000001012870b8
      r8  : 0000000000000298 r9  : 0000000000000013 r10 : 0000000000000000
      r11 : 9666960159961a65 r12 : a000000101287010 r13 : a000000101280000
      r14 : a000000101287068 r15 : a000000101287080 r16 : 0000000000000298
      r17 : 0000000000000010 r18 : 0000000000000018 r19 : a000000101287310
      r20 : 0000000000000290 r21 : 0000000000000000 r22 : 0000000000000000
      r23 : a000000101386f58 r24 : 0000000000000000 r25 : 000000007fffffff
      r26 : a000000101287078 r27 : a0000001013c69b0 r28 : 0000000000000000
      r29 : 0000000000000014 r30 : 0000000000000000 r31 : 0000000000000813
    
    Sedat Dilek and Hugh Dickins reported similar problems as well.
    
    Earlier patches in the common set moved the zeroing of the kmem_cache
    structure into common code. See "Move allocation of kmem_cache into
    common code".
    
    The allocation for the two special structures is still done from SLUB
    specific code but no zeroing is done since the cache creation functions
    used to zero. This now needs to be updated so that the structures are
    zeroed during allocation in kmem_cache_init().  Otherwise random pointer
    values may be followed.
    
    Reported-by: Tony Luck <tony.luck@intel.com>
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Reported-by: Hugh Dickins <hughd@google.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit bbd42965eb6f70de1e68a8f652143cc0c0a17ab7
Author: Pekka Enberg <penberg@kernel.org>
Date:   Wed Sep 5 12:07:44 2012 +0300

    Revert "mm/sl[aou]b: Move sysfs_slab_add to common"
    
    This reverts commit 96d17b7be0a9849d381442030886211dbb2a7061 which
    caused the following errors at boot:
    
      [    1.114885] kobject (ffff88001a802578): tried to init an initialized object, something is seriously wrong.
      [    1.114885] Pid: 1, comm: swapper/0 Tainted: G        W    3.6.0-rc1+ #6
      [    1.114885] Call Trace:
      [    1.114885]  [<ffffffff81273f37>] kobject_init+0x87/0xa0
      [    1.115555]  [<ffffffff8127426a>] kobject_init_and_add+0x2a/0x90
      [    1.115555]  [<ffffffff8127c870>] ? sprintf+0x40/0x50
      [    1.115555]  [<ffffffff81124c60>] sysfs_slab_add+0x80/0x210
      [    1.115555]  [<ffffffff81100175>] kmem_cache_create+0xa5/0x250
      [    1.115555]  [<ffffffff81cf24cd>] ? md_init+0x144/0x144
      [    1.115555]  [<ffffffff81cf25b6>] local_init+0xa4/0x11b
      [    1.115555]  [<ffffffff81cf24e1>] dm_init+0x14/0x45
      [    1.115836]  [<ffffffff810001ba>] do_one_initcall+0x3a/0x160
      [    1.116834]  [<ffffffff81cc2c90>] kernel_init+0x133/0x1b7
      [    1.117835]  [<ffffffff81cc25c4>] ? do_early_param+0x86/0x86
      [    1.117835]  [<ffffffff8171aff4>] kernel_thread_helper+0x4/0x10
      [    1.118401]  [<ffffffff81cc2b5d>] ? start_kernel+0x33f/0x33f
      [    1.119832]  [<ffffffff8171aff0>] ? gs_change+0xb/0xb
      [    1.120325] ------------[ cut here ]------------
      [    1.120835] WARNING: at fs/sysfs/dir.c:536 sysfs_add_one+0xc1/0xf0()
      [    1.121437] sysfs: cannot create duplicate filename '/kernel/slab/:t-0000016'
      [    1.121831] Modules linked in:
      [    1.122138] Pid: 1, comm: swapper/0 Tainted: G        W    3.6.0-rc1+ #6
      [    1.122831] Call Trace:
      [    1.123074]  [<ffffffff81195ce1>] ? sysfs_add_one+0xc1/0xf0
      [    1.123833]  [<ffffffff8103adfa>] warn_slowpath_common+0x7a/0xb0
      [    1.124405]  [<ffffffff8103aed1>] warn_slowpath_fmt+0x41/0x50
      [    1.124832]  [<ffffffff81195ce1>] sysfs_add_one+0xc1/0xf0
      [    1.125337]  [<ffffffff81195eb3>] create_dir+0x73/0xd0
      [    1.125832]  [<ffffffff81196221>] sysfs_create_dir+0x81/0xe0
      [    1.126363]  [<ffffffff81273d3d>] kobject_add_internal+0x9d/0x210
      [    1.126832]  [<ffffffff812742a3>] kobject_init_and_add+0x63/0x90
      [    1.127406]  [<ffffffff81124c60>] sysfs_slab_add+0x80/0x210
      [    1.127832]  [<ffffffff81100175>] kmem_cache_create+0xa5/0x250
      [    1.128384]  [<ffffffff81cf24cd>] ? md_init+0x144/0x144
      [    1.128833]  [<ffffffff81cf25b6>] local_init+0xa4/0x11b
      [    1.129831]  [<ffffffff81cf24e1>] dm_init+0x14/0x45
      [    1.130305]  [<ffffffff810001ba>] do_one_initcall+0x3a/0x160
      [    1.130831]  [<ffffffff81cc2c90>] kernel_init+0x133/0x1b7
      [    1.131351]  [<ffffffff81cc25c4>] ? do_early_param+0x86/0x86
      [    1.131830]  [<ffffffff8171aff4>] kernel_thread_helper+0x4/0x10
      [    1.132392]  [<ffffffff81cc2b5d>] ? start_kernel+0x33f/0x33f
      [    1.132830]  [<ffffffff8171aff0>] ? gs_change+0xb/0xb
      [    1.133315] ---[ end trace 2703540871c8fab7 ]---
      [    1.133830] ------------[ cut here ]------------
      [    1.134274] WARNING: at lib/kobject.c:196 kobject_add_internal+0x1f5/0x210()
      [    1.134829] kobject_add_internal failed for :t-0000016 with -EEXIST, don't try to register things with the same name in the same directory.
      [    1.135829] Modules linked in:
      [    1.136135] Pid: 1, comm: swapper/0 Tainted: G        W    3.6.0-rc1+ #6
      [    1.136828] Call Trace:
      [    1.137071]  [<ffffffff81273e95>] ? kobject_add_internal+0x1f5/0x210
      [    1.137830]  [<ffffffff8103adfa>] warn_slowpath_common+0x7a/0xb0
      [    1.138402]  [<ffffffff8103aed1>] warn_slowpath_fmt+0x41/0x50
      [    1.138830]  [<ffffffff811955a3>] ? release_sysfs_dirent+0x73/0xf0
      [    1.139419]  [<ffffffff81273e95>] kobject_add_internal+0x1f5/0x210
      [    1.139830]  [<ffffffff812742a3>] kobject_init_and_add+0x63/0x90
      [    1.140429]  [<ffffffff81124c60>] sysfs_slab_add+0x80/0x210
      [    1.140830]  [<ffffffff81100175>] kmem_cache_create+0xa5/0x250
      [    1.141829]  [<ffffffff81cf24cd>] ? md_init+0x144/0x144
      [    1.142307]  [<ffffffff81cf25b6>] local_init+0xa4/0x11b
      [    1.142829]  [<ffffffff81cf24e1>] dm_init+0x14/0x45
      [    1.143307]  [<ffffffff810001ba>] do_one_initcall+0x3a/0x160
      [    1.143829]  [<ffffffff81cc2c90>] kernel_init+0x133/0x1b7
      [    1.144352]  [<ffffffff81cc25c4>] ? do_early_param+0x86/0x86
      [    1.144829]  [<ffffffff8171aff4>] kernel_thread_helper+0x4/0x10
      [    1.145405]  [<ffffffff81cc2b5d>] ? start_kernel+0x33f/0x33f
      [    1.145828]  [<ffffffff8171aff0>] ? gs_change+0xb/0xb
      [    1.146313] ---[ end trace 2703540871c8fab8 ]---
    
    Conflicts:
    
    	mm/slub.c
    
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit eec7af2d7fd06db43a6220ec9fbf925618521577
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:38:33 2012 +0000

    mm/sl[aou]b: Move kmem_cache refcounting to common code
    
    Get rid of the refcount stuff in the allocators and do that part of
    kmem_cache management in the common code.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5c7efaa1e893d66434b8cb74be59fb97d07876fc
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:18:33 2012 +0000

    mm/sl[aou]b: Shrink __kmem_cache_create() parameter lists
    
    Do the initial settings of the fields in common code. This will allow us
    to push more processing into common code later and improve readability.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 2a6850829158130d7dc706cf4761ae3099a46b34
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:20:34 2012 +0000

    mm/sl[aou]b: Move kmem_cache allocations into common code
    
    Shift the allocations to common code. That way the allocation and
    freeing of the kmem_cache structures is handled by common code.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit f5ae7a6d38baa3ed131240e3570424eb150332b7
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:18:32 2012 +0000

    mm/sl[aou]b: Move sysfs_slab_add to common
    
    Simplify locking by moving the slab_add_sysfs after all locks have been
    dropped. Eases the upcoming move to provide sysfs support for all
    allocators.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5b68b6f4e1a535b6205428dd85f10f00132d054e
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:18:32 2012 +0000

    mm/sl[aou]b: Do slab aliasing call from common code
    
    The slab aliasing logic causes some strange contortions in slub. So add
    a call to deal with aliases to slab_common.c but disable it for other
    slab allocators by providng stubs that fail to create aliases.
    
    Full general support for aliases will require additional cleanup passes
    and more standardization of fields in kmem_cache.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit cb41de24d7e97686d509bdd71d4f42c7ea03d118
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:45:41 2013 -0500

    mm/sl[aou]b: Move duping of slab name to slab_common.c
    
    Duping of the slabname has to be done by each slab. Moving this code to
    slab_common avoids duplicate implementations.
    
    With this patch we have common string handling for all slab allocators.
    Strings passed to kmem_cache_create() are copied internally. Subsystems
    can create temporary strings to create slab caches.
    
    Slabs allocated in early states of bootstrap will never be freed (and
    those can never be freed since they are essential to slab allocator
    operations).  During bootstrap we therefore do not have to worry about
    duping names.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b3c54b23a2f69b153f0b876d1257635436208bab
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:38:33 2012 +0000

    mm/sl[aou]b: Get rid of __kmem_cache_destroy
    
    What is done there can be done in __kmem_cache_shutdown.
    
    This affects RCU handling somewhat. On rcu free all slab allocators do
    not refer to other management structures than the kmem_cache structure.
    Therefore these other structures can be freed before the rcu deferred
    free to the page allocator occurs.
    
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 7195d6eeecf2f6dd4d69fccf63c16c5e0744433d
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:18:32 2012 +0000

    mm/sl[aou]b: Move freeing of kmem_cache structure to common code
    
    The freeing action is basically the same in all slab allocators.
    Move to the common kmem_cache_destroy() function.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 7b3f24fe07eaa346581feda6f4b17b8ffa80deeb
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:18:33 2012 +0000

    mm/sl[aou]b: Extract a common function for kmem_cache_destroy
    
    kmem_cache_destroy does basically the same in all allocators.
    
    Extract common code which is easy since we already have common mutex
    handling.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 1b59786fd6bd79188fbaf58f6a5b73ff90153029
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:20:33 2012 +0000

    mm/sl[aou]b: Use "kmem_cache" name for slab cache with kmem_cache struct
    
    Make all allocators use the "kmem_cache" slabname for the "kmem_cache"
    structure.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 9f08aba5f66bab592e224369cf083a7fcba9b2e0
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:43:07 2013 -0500

    mm/sl[aou]b: Move list_add() to slab_common.c
    
    Move the code to append the new kmem_cache to the list of slab caches to
    the kmem_cache_create code in the shared code.
    
    This is possible now since the acquisition of the mutex was moved into
    kmem_cache_create().
    
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 60a543ef17d2c9c4a5dba650652a23d04fe1e279
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:20:33 2012 +0000

    mm/slab_common: Improve error handling in kmem_cache_create
    
    Instead of using s == NULL use an errorcode. This allows much more
    detailed diagnostics as to what went wrong. As we add more functionality
    from the slab allocators to the common kmem_cache_create() function we will
    also add more error conditions.
    
    Print the error code during the panic as well as in a warning if the module
    can handle failure. The API for kmem_cache_create() currently does not allow
    the returning of an error code. Return NULL but log the cause of the problem
    in the syslog.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 4e78718d9f5f3851834a8506d90200629ad7f21a
Author: Shuah Khan <shuah.khan@hp.com>
Date:   Thu Aug 16 00:09:46 2012 -0700

    mm/slab: restructure kmem_cache_create() debug checks
    
    kmem_cache_create() does cache integrity checks when CONFIG_DEBUG_VM is
    defined.  These checks interspersed with the regular code path has lead
    to compile time warnings when compiled without CONFIG_DEBUG_VM defined.
    Restructuring the code to move the integrity checks in to a new function
    would eliminate the current compile warning problem and also will allow
    for future changes to the debug only code to evolve without introducing
    new warnings in the regular path.
    
    This restructuring work is based on the discussion in the following
    thread:
    
    https://lkml.org/lkml/2012/7/13/424
    
    [akpm@linux-foundation.org: fix build, cleanup]
    Signed-off-by: Shuah Khan <shuah.khan@hp.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit b1084a266073bd26999b157d17644c6ad1ad14ea
Author: Pekka Enberg <penberg@kernel.org>
Date:   Thu Aug 16 10:12:18 2012 +0300

    Revert "mm/slab_common.c: cleanup"
    
    This reverts commit 455ce9eb1cfa083da0def023094190aeb133855a. Andrew
    sent a better version.
    
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 80ff2f9993036eb01a5de977e817c4f014e5403c
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Aug 14 14:53:22 2012 -0700

    mm/slab_common.c: cleanup
    
    Eliminate an ifdef and a label by moving all the CONFIG_DEBUG_VM checking
    inside the locked region.
    
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 06292a7fd7d9ec92e1107dc765c2a5f407a31435
Author: Shuah Khan <shuah.khan@hp.com>
Date:   Fri Jul 13 17:12:05 2012 -0600

    mm: Fix build warning in kmem_cache_create()
    
    The label oops is used in CONFIG_DEBUG_VM ifdef block and is defined
    outside ifdef CONFIG_DEBUG_VM block. This results in the following
    build warning when built with CONFIG_DEBUG_VM disabled. Fix to move
    label oops definition to inside a CONFIG_DEBUG_VM block.
    
    mm/slab_common.c: In function ‘kmem_cache_create’:
    mm/slab_common.c:101:1: warning: label ‘oops’ defined but not used
    [-Wunused-label]
    
    Signed-off-by: Shuah Khan <shuah.khan@hp.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	mm/slab_common.c

commit 00f130eb520ad5efc638936463ba2acccdb9172b
Author: David Rientjes <rientjes@google.com>
Date:   Tue Aug 28 19:57:21 2012 -0700

    mm, slab: lock the correct nodelist after reenabling irqs
    
    cache_grow() can reenable irqs so the cpu (and node) can change, so ensure
    that we take list_lock on the correct nodelist.
    
    This fixes an issue with commit 072bb0aa5e06 ("mm: sl[au]b: add
    knowledge of PFMEMALLOC reserve pages") where list_lock for the wrong
    node was taken after growing the cache.
    
    Reported-and-tested-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1032dd9dfeb51e04a6e753614473227456b0bccb
Author: David Rientjes <rientjes@google.com>
Date:   Thu Aug 16 12:25:31 2012 -0700

    mm, slab: remove page_get_cache
    
    page_get_cache() isn't called from anything, so remove it.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 22f813a9da0fd40d0403f155c94ef0c22a337cf8
Author: Michel Lespinasse <walken@google.com>
Date:   Tue Aug 14 14:53:20 2012 -0700

    slab: do not call compound_head() in page_get_cache()
    
    page_get_cache() does not need to call compound_head(), as its unique
    caller virt_to_slab() already makes sure to return a head page.
    
    Additionally, removing the compound_head() call makes page_get_cache()
    consistent with page_get_slab().
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit dd1413c018102732740d585a792414475c01ad63
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:30 2012 -0700

    mm: micro-optimise slab to avoid a function call
    
    Getting and putting objects in SLAB currently requires a function call but
    the bulk of the work is related to PFMEMALLOC reserves which are only
    consumed when network-backed storage is critical.  Use an inline function
    to determine if the function call is required.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5b1502f50f922af30b243308bc98cf4e5c72fefa
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:03 2012 -0700

    mm: introduce __GFP_MEMALLOC to allow access to emergency reserves
    
    __GFP_MEMALLOC will allow the allocation to disregard the watermarks, much
    like PF_MEMALLOC.  It allows one to pass along the memalloc state in
    object related allocation flags as opposed to task related flags, such as
    sk->sk_allocation.  This removes the need for ALLOC_PFMEMALLOC as callers
    using __GFP_MEMALLOC can get the ALLOC_NO_WATERMARK flag which is now
    enough to identify allocations related to page reclaim.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    backported to Android Linux 3.4 by faux123
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	include/linux/gfp.h
    	mm/page_alloc.c

commit b8812c93774a14d76c50647991a72e47562e35f6
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:41:26 2013 -0500

    mm/slub: Use kmem_cache for the kmem_cache structure
    
    Do not use kmalloc() but kmem_cache_alloc() for the allocation
    of the kmem_cache structures in slub.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    backported for Linux 3.4
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit d595d95b1524a8c15b10ed53d0aa6a7be7300ff2
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:06:14 2012 +0000

    mm/slub: Add debugging to verify correct cache use on kmem_cache_free()
    
    Add additional debugging to check that the objects is actually from the cache
    the caller claims. Doing so currently trips up some other debugging code. It
    takes a lot to infer from that what was happening.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    [ penberg@kernel.org: Use pr_err() ]
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 53521bb525d9c016197dc028949a7903355a8095
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Jun 23 03:22:38 2012 +0900

    slub: reduce failure of this_cpu_cmpxchg in put_cpu_partial() after unfreezing
    
    In current implementation, after unfreezing, we doesn't touch oldpage,
    so it remain 'NOT NULL'. When we call this_cpu_cmpxchg()
    with this old oldpage, this_cpu_cmpxchg() is mostly be failed.
    
    We can change value of oldpage to NULL after unfreezing,
    because unfreeze_partial() ensure that all the cpu partial slabs is removed
    from cpu partial list. In this time, we could expect that
    this_cpu_cmpxchg is mostly succeed.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 19df4ffcb4689cbe83033113697475c216f0b7c3
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 30 12:54:46 2012 -0500

    slub: Take node lock during object free checks
    
    Only applies to scenarios where debugging is on:
    
    Validation of slabs can currently occur while debugging
    information is updated from the fast paths of the allocator.
    This results in various races where we get false reports about
    slab metadata not being in order.
    
    This patch makes the fast paths take the node lock so that
    serialization with slab validation will occur. Causes additional
    slowdown in debug scenarios.
    
    Reported-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 21304b51f6119fc6987275fc31577c1a93f264fa
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Aug 3 22:51:37 2012 +0400

    slub: use free_page instead of put_page for freeing kmalloc allocation
    
    When freeing objects, the slub allocator will most of the time free
    empty pages by calling __free_pages(). But high-order kmalloc will be
    diposed by means of put_page() instead. It makes no sense to call
    put_page() in kernel pages that are provided by the object allocators,
    so we shouldn't be doing this ourselves. Aside from the consistency
    change, we don't change the flow too much. put_page()'s would call its
    dtor function, which is __free_pages. We also already do all of the
    Compound page tests ourselves, and the Mlock test we lose don't really
    matter.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    CC: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit cc4be881d5e546f6e931ff290b286d2b78d65a36
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Jul 31 16:44:00 2012 -0700

    mm: slub: optimise the SLUB fast path to avoid pfmemalloc checks
    
    This patch removes the check for pfmemalloc from the alloc hotpath and
    puts the logic after the election of a new per cpu slab.  For a pfmemalloc
    page we do not use the fast path but force the use of the slow path which
    is also used for the debug case.
    
    This has the side-effect of weakening pfmemalloc processing in the
    following way;
    
    1. A process that is allocating for network swap calls __slab_alloc.
       pfmemalloc_match is true so the freelist is loaded and c->freelist is
       now pointing to a pfmemalloc page.
    
    2. A process that is attempting normal allocations calls slab_alloc,
       finds the pfmemalloc page on the freelist and uses it because it did
       not check pfmemalloc_match()
    
    The patch allows non-pfmemalloc allocations to use pfmemalloc pages with
    the kmalloc slabs being the most vunerable caches on the grounds they
    are most likely to have a mix of pfmemalloc and !pfmemalloc requests. A
    later patch will still protect the system as processes will get throttled
    if the pfmemalloc reserves get depleted but performance will not degrade
    as smoothly.
    
    [mgorman@suse.de: Expanded changelog]
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 3cb70c35d64c3c315e89c84d3ff25a7cd87c2fd0
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:36:34 2013 -0500

    mm: sl[au]b: add knowledge of PFMEMALLOC reserve pages
    
    When a user or administrator requires swap for their application, they
    create a swap partition and file, format it with mkswap and activate it
    with swapon.  Swap over the network is considered as an option in diskless
    systems.  The two likely scenarios are when blade servers are used as part
    of a cluster where the form factor or maintenance costs do not allow the
    use of disks and thin clients.
    
    The Linux Terminal Server Project recommends the use of the Network Block
    Device (NBD) for swap according to the manual at
    https://sourceforge.net/projects/ltsp/files/Docs-Admin-Guide/LTSPManual.pdf/download
    There is also documentation and tutorials on how to setup swap over NBD at
    places like https://help.ubuntu.com/community/UbuntuLTSP/EnableNBDSWAP The
    nbd-client also documents the use of NBD as swap.  Despite this, the fact
    is that a machine using NBD for swap can deadlock within minutes if swap
    is used intensively.  This patch series addresses the problem.
    
    The core issue is that network block devices do not use mempools like
    normal block devices do.  As the host cannot control where they receive
    packets from, they cannot reliably work out in advance how much memory
    they might need.  Some years ago, Peter Zijlstra developed a series of
    patches that supported swap over an NFS that at least one distribution is
    carrying within their kernels.  This patch series borrows very heavily
    from Peter's work to support swapping over NBD as a pre-requisite to
    supporting swap-over-NFS.  The bulk of the complexity is concerned with
    preserving memory that is allocated from the PFMEMALLOC reserves for use
    by the network layer which is needed for both NBD and NFS.
    
    Patch 1 adds knowledge of the PFMEMALLOC reserves to SLAB and SLUB to
            preserve access to pages allocated under low memory situations
            to callers that are freeing memory.
    
    Patch 2 optimises the SLUB fast path to avoid pfmemalloc checks
    
    Patch 3 introduces __GFP_MEMALLOC to allow access to the PFMEMALLOC
            reserves without setting PFMEMALLOC.
    
    Patch 4 opens the possibility for softirqs to use PFMEMALLOC reserves
            for later use by network packet processing.
    
    Patch 5 only sets page->pfmemalloc when ALLOC_NO_WATERMARKS was required
    
    Patch 6 ignores memory policies when ALLOC_NO_WATERMARKS is set.
    
    Patches 7-12 allows network processing to use PFMEMALLOC reserves when
            the socket has been marked as being used by the VM to clean pages. If
            packets are received and stored in pages that were allocated under
            low-memory situations and are unrelated to the VM, the packets
            are dropped.
    
            Patch 11 reintroduces __skb_alloc_page which the networking
            folk may object to but is needed in some cases to propogate
            pfmemalloc from a newly allocated page to an skb. If there is a
            strong objection, this patch can be dropped with the impact being
            that swap-over-network will be slower in some cases but it should
            not fail.
    
    Patch 13 is a micro-optimisation to avoid a function call in the
            common case.
    
    Patch 14 tags NBD sockets as being SOCK_MEMALLOC so they can use
            PFMEMALLOC if necessary.
    
    Patch 15 notes that it is still possible for the PFMEMALLOC reserve
            to be depleted. To prevent this, direct reclaimers get throttled on
            a waitqueue if 50% of the PFMEMALLOC reserves are depleted.  It is
            expected that kswapd and the direct reclaimers already running
            will clean enough pages for the low watermark to be reached and
            the throttled processes are woken up.
    
    Patch 16 adds a statistic to track how often processes get throttled
    
    Some basic performance testing was run using kernel builds, netperf on
    loopback for UDP and TCP, hackbench (pipes and sockets), iozone and
    sysbench.  Each of them were expected to use the sl*b allocators
    reasonably heavily but there did not appear to be significant performance
    variances.
    
    For testing swap-over-NBD, a machine was booted with 2G of RAM with a
    swapfile backed by NBD.  8*NUM_CPU processes were started that create
    anonymous memory mappings and read them linearly in a loop.  The total
    size of the mappings were 4*PHYSICAL_MEMORY to use swap heavily under
    memory pressure.
    
    Without the patches and using SLUB, the machine locks up within minutes
    and runs to completion with them applied.  With SLAB, the story is
    different as an unpatched kernel run to completion.  However, the patched
    kernel completed the test 45% faster.
    
    MICRO
                                             3.5.0-rc2 3.5.0-rc2
                                             vanilla     swapnbd
    Unrecognised test vmscan-anon-mmap-write
    MMTests Statistics: duration
    Sys Time Running Test (seconds)             197.80    173.07
    User+Sys Time Running Test (seconds)        206.96    182.03
    Total Elapsed Time (seconds)               3240.70   1762.09
    
    This patch: mm: sl[au]b: add knowledge of PFMEMALLOC reserve pages
    
    Allocations of pages below the min watermark run a risk of the machine
    hanging due to a lack of memory.  To prevent this, only callers who have
    PF_MEMALLOC or TIF_MEMDIE set and are not processing an interrupt are
    allowed to allocate with ALLOC_NO_WATERMARKS.  Once they are allocated to
    a slab though, nothing prevents other callers consuming free objects
    within those slabs.  This patch limits access to slab pages that were
    alloced from the PFMEMALLOC reserves.
    
    When this patch is applied, pages allocated from below the low watermark
    are returned with page->pfmemalloc set and it is up to the caller to
    determine how the page should be protected.  SLAB restricts access to any
    page with page->pfmemalloc set to callers which are known to able to
    access the PFMEMALLOC reserve.  If one is not available, an attempt is
    made to allocate a new page rather than use a reserve.  SLUB is a bit more
    relaxed in that it only records if the current per-CPU page was allocated
    from PFMEMALLOC reserve and uses another partial slab if the caller does
    not have the necessary GFP or process flags.  This was found to be
    sufficient in tests to avoid hangs due to SLUB generally maintaining
    smaller lists than SLAB.
    
    In low-memory conditions it does mean that !PFMEMALLOC allocators can fail
    a slab allocation even though free objects are available because they are
    being preserved for callers that are freeing pages.
    
    [a.p.zijlstra@chello.nl: Original implementation]
    [sebastian@breakpoint.cc: Correct order of page flag clearing]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    backported for Android Linux 3.4 by faux123
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fd79c8798130fa276a2ec46d390fc138d0771928
Author: David Rientjes <rientjes@google.com>
Date:   Mon Jul 9 14:00:38 2012 -0700

    mm, slub: ensure irqs are enabled for kmemcheck
    
    kmemcheck_alloc_shadow() requires irqs to be enabled, so wait to disable
    them until after its called for __GFP_WAIT allocations.
    
    This fixes a warning for such allocations:
    
    	WARNING: at kernel/lockdep.c:2739 lockdep_trace_alloc+0x14e/0x1c0()
    
    Acked-by: Fengguang Wu <fengguang.wu@intel.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 42691437c126d1e8395edf46a7b8a376603306fa
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:35:51 2013 -0500

    mm, sl[aou]b: Move kmem_cache_create mutex handling to common code
    
    Move the mutex handling into the common kmem_cache_create()
    function.
    
    Then we can also move more checks out of SLAB's kmem_cache_create()
    into the common code.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 289d0c263e878d08069e6b9b0a42be2f2b145346
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:31:53 2013 -0500

    mm, sl[aou]b: Use a common mutex definition
    
    Use the mutex definition from SLAB and make it the common way to take a sleeping lock.
    
    This has the effect of using a mutex instead of a rw semaphore for SLUB.
    
    SLOB gains the use of a mutex for kmem_cache_create serialization.
    Not needed now but SLOB may acquire some more features later (like slabinfo
    / sysfs support) through the expansion of the common code that will
    need this.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit f28aa18b69a3baf94b3279ad3e61a5f147439ccb
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Jul 6 15:25:11 2012 -0500

    mm, sl[aou]b: Common definition for boot state of the slab allocators
    
    All allocators have some sort of support for the bootstrap status.
    
    Setup a common definition for the boot states and make all slab
    allocators use that definition.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Conflicts:
    	mm/slab.c

commit 2ec1e06b819ffcb1a57b0f039f1bba52c1691bea
Author: Julia Lawall <Julia.Lawall@lip6.fr>
Date:   Sun Jul 8 13:37:40 2012 +0200

    slub: remove invalid reference to list iterator variable
    
    If list_for_each_entry, etc complete a traversal of the list, the iterator
    variable ends up pointing to an address at an offset from the list head,
    and not a meaningful structure.  Thus this value should not be used after
    the end of the iterator.  The patch replaces s->name by al->name, which is
    referenced nearby.
    
    This problem was found using Coccinelle (http://coccinelle.lip6.fr/).
    
    Signed-off-by: Julia Lawall <Julia.Lawall@lip6.fr>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ca6a466d6beb8ee945ef2ee863f94b290f360ddb
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Jun 9 02:23:16 2012 +0900

    slub: refactoring unfreeze_partials()
    
    Current implementation of unfreeze_partials() is so complicated,
    but benefit from it is insignificant. In addition many code in
    do {} while loop have a bad influence to a fail rate of cmpxchg_double_slab.
    Under current implementation which test status of cpu partial slab
    and acquire list_lock in do {} while loop,
    we don't need to acquire a list_lock and gain a little benefit
    when front of the cpu partial slab is to be discarded, but this is a rare case.
    In case that add_partial is performed and cmpxchg_double_slab is failed,
    remove_partial should be called case by case.
    
    I think that these are disadvantages of current implementation,
    so I do refactoring unfreeze_partials().
    
    Minimizing code in do {} while loop introduce a reduced fail rate
    of cmpxchg_double_slab. Below is output of 'slabinfo -r kmalloc-256'
    when './perf stat -r 33 hackbench 50 process 4000 > /dev/null' is done.
    
    ** before **
    Cmpxchg_double Looping
    ------------------------
    Locked Cmpxchg Double redos   182685
    Unlocked Cmpxchg Double redos 0
    
    ** after **
    Cmpxchg_double Looping
    ------------------------
    Locked Cmpxchg Double redos   177995
    Unlocked Cmpxchg Double redos 1
    
    We can see cmpxchg_double_slab fail rate is improved slightly.
    
    Bolow is output of './perf stat -r 30 hackbench 50 process 4000 > /dev/null'.
    
    ** before **
     Performance counter stats for './hackbench 50 process 4000' (30 runs):
    
         108517.190463 task-clock                #    7.926 CPUs utilized            ( +-  0.24% )
             2,919,550 context-switches          #    0.027 M/sec                    ( +-  3.07% )
               100,774 CPU-migrations            #    0.929 K/sec                    ( +-  4.72% )
               124,201 page-faults               #    0.001 M/sec                    ( +-  0.15% )
       401,500,234,387 cycles                    #    3.700 GHz                      ( +-  0.24% )
       <not supported> stalled-cycles-frontend
       <not supported> stalled-cycles-backend
       250,576,913,354 instructions              #    0.62  insns per cycle          ( +-  0.13% )
        45,934,956,860 branches                  #  423.297 M/sec                    ( +-  0.14% )
           188,219,787 branch-misses             #    0.41% of all branches          ( +-  0.56% )
    
          13.691837307 seconds time elapsed                                          ( +-  0.24% )
    
    ** after **
     Performance counter stats for './hackbench 50 process 4000' (30 runs):
    
         107784.479767 task-clock                #    7.928 CPUs utilized            ( +-  0.22% )
             2,834,781 context-switches          #    0.026 M/sec                    ( +-  2.33% )
                93,083 CPU-migrations            #    0.864 K/sec                    ( +-  3.45% )
               123,967 page-faults               #    0.001 M/sec                    ( +-  0.15% )
       398,781,421,836 cycles                    #    3.700 GHz                      ( +-  0.22% )
       <not supported> stalled-cycles-frontend
       <not supported> stalled-cycles-backend
       250,189,160,419 instructions              #    0.63  insns per cycle          ( +-  0.09% )
        45,855,370,128 branches                  #  425.436 M/sec                    ( +-  0.10% )
           169,881,248 branch-misses             #    0.37% of all branches          ( +-  0.43% )
    
          13.596272341 seconds time elapsed                                          ( +-  0.22% )
    
    No regression is found, but rather we can see slightly better result.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit a49021b01664b186b21180adee781a905a1c3778
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Fri May 18 22:01:17 2012 +0900

    slub: use __cmpxchg_double_slab() at interrupt disabled place
    
    get_freelist(), unfreeze_partials() are only called with interrupt disabled,
    so __cmpxchg_double_slab() is suitable.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e281bdfb798380f71b29b9701b68d3ba7bd6f3b7
Author: Glauber Costa <glommer@parallels.com>
Date:   Thu Jun 21 00:59:18 2012 +0400

    slab: move FULL state transition to an initcall
    
    During kmem_cache_init_late(), we transition to the LATE state,
    and after some more work, to the FULL state, its last state
    
    This is quite different from slub, that will only transition to
    its last state (previously SYSFS), in a (late)initcall, after a lot
    more of the kernel is ready.
    
    This means that in slab, we have no way to taking actions dependent
    on the initialization of other pieces of the kernel that are supposed
    to start way after kmem_init_late(), such as cgroups initialization.
    
    To achieve more consistency in this behavior, that patch only
    transitions to the UP state in kmem_init_late. In my analysis,
    setup_cpu_cache() should be happy to test for >= UP, instead of
    == FULL. It also has passed some tests I've made.
    
    We then only mark FULL state after the reap timers are in place,
    meaning that no further setup is expected.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Conflicts:
    	mm/slab.c

commit 4c6d6016514a3cf14141773d9faf2f7fd7762148
Author: Feng Tang <feng.tang@intel.com>
Date:   Mon Jul 2 14:29:10 2012 +0800

    slab: Fix a typo in commit 8c138b "slab: Get rid of obj_size macro"
    
    Commit  8c138b only sits in Pekka's and linux-next tree now, which tries
    to replace obj_size(cachep) with cachep->object_size, but has a typo in
    kmem_cache_free() by using "size" instead of "object_size", which casues
    some regressions.
    
    Reported-and-tested-by: Fengguang Wu <wfg@linux.intel.com>
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Cc: Christoph Lameter <cl@linux.com>
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit b24d96d11693d8639044807ed1707e224f4ac540
Author: Thierry Reding <thierry.reding@avionic-design.de>
Date:   Fri Jun 22 19:42:49 2012 +0200

    mm, slab: Build fix for recent kmem_cache changes
    
    Commit 3b0efdf ("mm, sl[aou]b: Extract common fields from struct
    kmem_cache") renamed the kmem_cache structure's "next" field to "list"
    but forgot to update one instance in leaks_show().
    
    Signed-off-by: Thierry Reding <thierry.reding@avionic-design.de>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit f216379554a671eff5fd75b9d08520528f6ad2da
Author: Glauber Costa <glommer@parallels.com>
Date:   Thu Jun 14 16:17:21 2012 +0400

    slab: rename gfpflags to allocflags
    
    A consistent name with slub saves us an acessor function.
    In both caches, this field represents the same thing. We would
    like to use it from the mem_cgroup code.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c7e0d5512321cbfb48048b490b4a487b91b92f9c
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sat Jun 9 02:40:03 2012 -0700

    slab/mempolicy: always use local policy from interrupt context
    
    slab_node() could access current->mempolicy from interrupt context.
    However there's a race condition during exit where the mempolicy
    is first freed and then the pointer zeroed.
    
    Using this from interrupts seems bogus anyways. The interrupt
    will interrupt a random process and therefore get a random
    mempolicy. Many times, this will be idle's, which noone can change.
    
    Just disable this here and always use local for slab
    from interrupts. I also cleaned up the callers of slab_node a bit
    which always passed the same argument.
    
    I believe the original mempolicy code did that in fact,
    so it's likely a regression.
    
    v2: send version with correct logic
    v3: simplify. fix typo.
    Reported-by: Arun Sharma <asharma@fb.com>
    Cc: penberg@kernel.org
    Cc: cl@linux.com
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    [tdmackey@twitter.com: Rework control flow based on feedback from
    cl@linux.com, fix logic, and cleanup current task_struct reference]
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: David Mackey <tdmackey@twitter.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c635b9e76314c17dcf3f47e4838685dd667a41b0
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:58 2012 -0500

    slab: Get rid of obj_size macro
    
    The size of the slab object is frequently needed. Since we now
    have a size field directly in the kmem_cache structure there is no
    need anymore of the obj_size macro/function.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 330738dadf6eda4ef62e0c77af46c96056af16ae
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Jul 10 18:31:05 2012 -0500

    slob: Fix early boot kernel crash
    
    Commit fd3142a59af2012a7c5dc72ec97a4935ff1c5fc6 broke
    slob since a piece of a change for a later patch slipped into
    it.
    
    Fengguang Wu writes:
    
      The commit crashes the kernel w/o any dmesg output (the attached one is
      created by the script as a summary for that run). This is very
      reproducible in kvm for the attached config.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 352a840a4f4ecce1b6e73ec0ce916125912f2d7f
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:30:44 2013 -0500

    mm, sl[aou]b: Extract common code for kmem_cache_create()
    
    Kmem_cache_create() does a variety of sanity checks but those
    vary depending on the allocator. Use the strictest tests and put them into
    a slab_common file. Make the tests conditional on CONFIG_DEBUG_VM.
    
    This patch has the effect of adding sanity checks for SLUB and SLOB
    under CONFIG_DEBUG_VM and removes the checks in SLAB for !CONFIG_DEBUG_VM.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 9b51336831ce44cf405fb6a457f5c566ce90240d
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:57 2012 -0500

    mm, sl[aou]b: Extract common fields from struct kmem_cache
    
    Define a struct that describes common fields used in all slab allocators.
    A slab allocator either uses the common definition (like SLOB) or is
    required to provide members of kmem_cache with the definition given.
    
    After that it will be possible to share code that
    only operates on those fields of kmem_cache.
    
    The patch basically takes the slob definition of kmem cache and
    uses the field namees for the other allocators.
    
    It also standardizes the names used for basic object lengths in
    allocators:
    
    object_size	Struct size specified at kmem_cache_create. Basically
    		the payload expected to be used by the subsystem.
    
    size		The size of memory allocator for each object. This size
    		is larger than object_size and includes padding, alignment
    		and extra metadata for each object (f.e. for debugging
    		and rcu).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 7dab32e75b473d367734beb82bf54dfa0a106d08
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:54 2012 -0500

    slob: Remove various small accessors
    
    Those have become so simple that they are no longer needed.
    
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: David Rientjes <rientjes@google.com>
    signed-off-by: Christoph Lameter <cl@linux.com>
    
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ca185538afb07e753c63efd1b8808dc485628f46
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:53 2012 -0500

    slob: No need to zero mapping since it is no longer in use
    
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 82d018d9213ae1f425acc5fcc1a1baeb90ff876a
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:56 2012 -0500

    slab: Remove some accessors
    
    Those are rather trivial now and its better to see inline what is
    really going on.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 1508398e481db814bd167c4debab0c31c413d8c8
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:55 2012 -0500

    slab: Use page struct fields instead of casting
    
    Add fields to the page struct so that it is properly documented that
    slab overlays the lru fields.
    
    This cleans up some casts in slab.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 47f1096419f8adeaa1556ce751b773df310a93b5
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:52 2012 -0500

    slob: Define page struct fields used in mm_types.h
    
    Define the fields used by slob in mm_types.h and use struct page instead
    of struct slob_page in slob. This cleans up numerous of typecasts in slob.c and
    makes readers aware of slob's use of page struct fields.
    
    [Also cleans up some bitrot in slob.c. The page struct field layout
    in slob.c is an old layout and does not match the one in mm_types.h]
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ee39de41a9247e75d207382f685b1daaba03f7af
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:59 2012 -0500

    slub: pass page to node_match() instead of kmem_cache_cpu structure
    
    Avoid passing the kmem_cache_cpu pointer to node_match. This makes the
    node_match function more generic and easier to understand.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 73e94060e52f6e1849bb1ff07ae5f0769c1757ab
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:58 2012 -0500

    slub: Use page variable instead of c->page.
    
    Store the value of c->page to avoid additional fetches
    from per cpu data.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 961ea3c5bffad5406341a9a593c4a8e489280efd
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:57 2012 -0500

    slub: Separate out kmem_cache_cpu processing from deactivate_slab
    
    Processing on fields of kmem_cache_cpu is cleaner if code working on fields
    of this struct is taken out of deactivate_slab().
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 7f175bb2e340eb1003f2828e478c459c5fd769f4
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:56 2012 -0500

    slub: Get rid of the node field
    
    The node field is always page_to_nid(c->page). So its rather easy to
    replace. Note that there maybe slightly more overhead in various hot paths
    due to the need to shift the bits from page->flags. However, that is mostly
    compensated for by a smaller footprint of the kmem_cache_cpu structure (this
    patch reduces that to 3 words per cache) which allows better caching.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 291b2022b4c7b85223427d4f57f15266f1aae08b
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:55 2012 -0500

    slub: new_slab_objects() can also get objects from partial list
    
    Moving the attempt to get a slab page from the partial lists simplifies
    __slab_alloc which is rather complicated.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 7c9cd6f6db5e5d6a2e243938c1395a0106901abb
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:54 2012 -0500

    slub: Simplify control flow in __slab_alloc()
    
    Simplify control flow a bit avoiding nesting.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c596daac5186af36e79b16fd73a80939b499ec00
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:29:21 2013 -0500

    slub: Acquire_slab() avoid loop
    
    Avoid the loop in acquire slab and simply fail if there is a conflict.
    
    This will cause the next page on the list to be considered.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	mm/slub.c

commit ce131bceb2d149e08e4cfea43164f09620c6f01d
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:52 2012 -0500

    slub: Add frozen check in __slab_alloc
    
    Verify that objects returned from __slab_alloc come from slab pages
    in the correct state.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit caa45468a06897e8868408f68bf32603c1f2e5cf
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:51 2012 -0500

    slub: Use freelist instead of "object" in __slab_alloc
    
    The variable "object" really refers to a list of objects that we
    are handling.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 3f9fc50e349a210f17eb571c1947911dbe47db86
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Fri May 18 00:47:47 2012 +0900

    slub: use __SetPageSlab function to set PG_slab flag
    
    To set page-flag, using SetPageXXXX() and __SetPageXXXX() is more
    understandable and maintainable. So change it.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit f2c01a10cba73d7fae362f53402d5bac58397c3e
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:24:39 2013 -0500

    sched: document the difference between nr_running and h_nr_running
    
    Date    Sun, 18 Aug 2013 16:25:22 +0800
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit e4da3dc461dee61000794f8dbd05f47511f9a992
Author: Lei Wen <leiwen@marvell.com>
Date:   Sun Sep 1 18:00:30 2013 -0500

    sched: change active_load_balance_cpu_stop to use h_nr_running
    
    Date	Sun, 18 Aug 2013 16:25:21 +0800
    
    We should only avoid do the active load balance when there is no
    cfs type task. If just use rq->nr_running, it is possible for the
    source cpu has multiple rt task, while zero cfs task, so that it
    would confuse the active load balance function that try to move,
    but find no task it could move.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit d6ec3e6f307f6280286f30cc7b74e85b40e2865b
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:20:33 2013 -0500

    sched: change find_busiest_queue to h_nr_running
    
    Date    Sun, 18 Aug 2013 16:25:20 +0800
    
    Since find_busiest_queue try to avoid do load balance for runqueue
    which has only one cfs task and its load is above the imbalance
    value calculated, we should use h_nr_running of cfs instead of
    nr_running of rq.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1585d07dded567a6ec7a3dc48c558c6ae44eb4bd
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:16:42 2013 -0500

    sched: change update_sg_lb_stats to h_nr_running
    
    Date    Sun, 18 Aug 2013 16:25:19 +0800
    
    Since update_sg_lb_stats is used to calculate sched_group load
    difference of cfs type task, it should use h_nr_running instead of
    nr_running of rq.
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    bacported to Linux 3.4 by faux123
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ed1b729263cdf4caa5fc84b5598cb20e4b2a471b
Author: Lei Wen <leiwen@marvell.com>
Date:   Sun Sep 1 17:48:08 2013 -0500

    sched: change pick_next_task_fair to h_nr_running
    
    Date	Sun, 18 Aug 2013 16:25:18 +0800
    
    Since pick_next_task_fair only want to ensure there is some task in the
    run queue to be picked up, it should use the h_nr_running instead of
    nr_running, since nr_running cannot present all tasks if group existed.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit d9a02106120c73e727571772dbde931a861005e5
Author: Lei Wen <leiwen@marvell.com>
Date:   Sun Sep 1 17:44:00 2013 -0500

    sched: change cpu_avg_load_per_task using h_nr_running
    
    Date	Sun, 18 Aug 2013 16:25:16 +0800
    
    Since cpu_avg_load_per_task is used only by cfs scheduler, its meaning
    should present the average cfs type task load in the current run queue.
    Thus we change it to h_nr_running for well presenting its meaning.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit d94e54be1c4e6fd470f7085449020d568e3aab9e
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 18:29:35 2013 -0500

    kernel/sched/fair.c: fix merge derp
    
     from patch sched: change load balance number to h_nr_running of run queue
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit dc8c2a896837a88ebc08a5a27c1c73c95492a85c
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:14:16 2013 -0500

    sched: change load balance number to h_nr_running of run queue
    
    Date    Sun, 18 Aug 2013 16:25:15 +0800
    
    Since rq->nr_running would include both migration and rt task, it is not
    reasonable to seek to move nr_running number of task in the load_balance
    function, since it only apply to cfs type.
    
    Change it to cfs's h_nr_running, which could well present the task
    number in current cfs queue.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    backported to Linux 3.4 by faux123
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 03495cf544f6b3dbc8b78c336e41df19fa29c570
Author: Kees Cook <keescook@chromium.org>
Date:   Sat Aug 31 02:41:20 2013 -0500

    hwrng: add randomness to system from rng sources
    
    When bringing a new RNG source online, it seems like it would make sense
    to use some of its bytes to make the system entropy pool more random,
    as done with all sorts of other devices that contain per-device or
    per-boot differences.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	drivers/char/hw_random/core.c

commit 7e5a9429966ee02557eef5d6cfdf62a39ca9167f
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:10:10 2013 -0500

    sched: Periodically decay max cost of idle balance
    
    This RFC patch builds on patch 2 and periodically decays that max value to
    do idle balancing per sched domain.
    
    Though we want to decay it fairly consistently, we may not want to lower it by
    too much each time, especially since avg_idle is capped based on that value.
    So I thought that decaying the value every second and lowering it by half a
    percent each time appeared to be fairly reasonable.
    
    This change would allow us to remove the limit we set on each domain's max cost
    to idle balance. Also, since the max can be reduced now, we now have to
    update rq->max_idle balance_cost more frequently. So after every idle balance,
    we loop through the sched domain to find the max sd's newidle load balance cost
    for any one domain. Then we will set rq->max_idle_balance_cost to that value.
    
    Since we are now decaying the max cost to do idle balancing, that max cost can
    also become not high enough. One possible explanation for why is that
    besides the time spent on each newidle load balance, there are other costs
    associated with attempting idle balancing. Idle balance also releases and
    reacquires a spin lock. That cost is not counted when we keep track of each
    domain's cost to do newidle load balance. Also, acquiring the rq locks can
    potentially prevent other CPUs from running something useful. And after
    migrating tasks, it might potentially have to pay the costs of cache misses and
    refreshing tasks' cache.
    
    Because of that, this patch also compares avg_idle with max cost to do idle
    balancing + sched_migration_cost. While using the max cost helps reduce
    overestimating the average idle, the sched_migration_cost can help account
    for those additional costs of idle balancing.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    [peterz: rewrote the logic, but kept the spirit]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    backported to Linux 3.4
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 9911d1e946c66d4a24fb22eba680930d4b77d491
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:05:40 2013 -0500

    sched: Consider max cost of idle balance per sched domain
    
    Date    Thu, 29 Aug 2013 13:05:35 -0700
    
    In this patch, we keep track of the max cost we spend doing idle load balancing
    for each sched domain. If the avg time the CPU remains idle is less then the
    time we have already spent on idle balancing + the max cost of idle balancing
    in the sched domain, then we don't continue to attempt the balance. We also
    keep a per rq variable, max_idle_balance_cost, which keeps track of the max
    time spent on newidle load balances throughout all its domains. Additionally,
    we swap sched_migration_cost used in idle_balance for rq->max_idle_balance_cost.
    
    By using the max, we avoid overrunning the average. This further reduces the chance
    we attempt balancing when the CPU is not idle for longer than the cost to balance.
    
    I also limited the max cost of each domain to 5*sysctl_sched_migration_cost as
    a way to prevent the max from becoming too inflated.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    
    backported for Linux 3.4
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 8c31b475098b58e27d96a1ca76d5b1bfde95d09d
Author: Jason Low <jason.low2@hp.com>
Date:   Sat Aug 31 01:47:10 2013 -0500

    sched: Reduce overestimating rq->avg_idle
    
    Date	Thu, 29 Aug 2013 13:05:34 -0700
    
    When updating avg_idle, if the delta exceeds some max value, then avg_idle
    gets set to the max, regardless of what the previous avg was. This can cause
    avg_idle to often be overestimated.
    
    This patch modifies the way we update avg_idle by always updating it with the
    function call to update_avg() first. Then, if avg_idle exceeds the max, we set
    it to the max.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit e16137253e10c708165710d67a0659c71767a5d5
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Aug 26 02:04:17 2013 -0500

    fs/ecryptfs/keystore.c: GCC 4.7 compiler warning fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 692965458e0e491d7bfc44b4fb527d2091808966
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Aug 26 01:10:18 2013 -0500

    exfat: AIO Optimization compatibility fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4623b6534f84923cf8dc0b8a832fa86415e70297
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Aug 26 01:09:26 2013 -0500

    drivers/Makefile: fix exfat driver path
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 61a1fdaf396e100625dfcf112d211917765e4679
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Aug 12 13:27:45 2013 -0500

    net/netfilter/xt_socket.c: GCC 4.7 fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1fdbba8f36dcd54c9ff704d5c61e4455919e2bcd
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Aug 12 13:26:46 2013 -0500

    fs/cifs/transport.c: GCC 4.7 fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 285d67e3a31289300e04576b2bafa247cf7c6bf7
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Aug 12 13:26:25 2013 -0500

    drivers/media/common/tuners/xc4000.c: GCC 4.7 fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 7c499a22f30938da0fcbee2c53392164acbbfa9a
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Aug 10 03:02:06 2013 -0500

    zcache-main: switch to lz4 as default
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 26d588b4b49df3d6949168597a00baaa26ec5095
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Aug 10 03:01:05 2013 -0500

    zcache-main.c: use MACRO define instead of hardcoded default of lzo
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit f30340237659dee61e5337249f2d412263633651
Author: Paul Reioux <reioux@gmail.com>
Date:   Fri Aug 9 13:59:28 2013 -0500

    zram_drv: switch default compressor to new lz4
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 293c75e43776aeb0e5c42130faaea9132fc97e6c
Author: Paul Reioux <reioux@gmail.com>
Date:   Fri Aug 9 13:28:14 2013 -0500

    staging: zram: add per-cpu support to Crypto
    
    Date	Tue, 30 Jul 2013 14:30:49 +0200
    
    Since original zram code did not implement any per-cpu operations,
    my previous patch (staging: zram: add Crypto API support) did not
    include them either.
    
    This patch complements the first one with per-cpu support for Crypto,
    allocating tfms buffer separately for each online processor.
    Changes are based on zswap and zcache per-cpu code.
    
    Basic tests (concurrent writing several 10-40MB chunks to zram) performed
    on an ARM-based EXYNOS4412 Quad-Core showed that per-cpu code provides
    noticeable time saving, ranging between 30-40% for LZO and LZ4 compressors.
    Sample data (LZO): writing 160MB, 40MB per thread took 0.60s with per-cpu
    code included and approximately 0.80s without per-cpu support.
    
    Signed-off-by: Piotr Sarna <p.sarna@partner.samsung.com>
    Acked-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    
    backported to Linux 3.4 by faux123
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 94fcb94700dc3eb37aba43d6a1c39529eb043b85
Author: Paul Reioux <reioux@gmail.com>
Date:   Fri Aug 9 13:25:02 2013 -0500

    staging: zram: add Crypto API support
    
    Date	Tue, 30 Jul 2013 14:30:48 +0200
    
    Current version of zram does not allow any substitution of a default
    compression algorithm. Therefore, I decided to change the existing
    implementation of page compression by adding Crypto API compability.
    
    All direct calls to lzo1x compression/decompression methods are now
    replaced by calls consistent with Crypto. Also, I removed "workmem"
    field from struct zram_meta, as it was there for lzo1x purposes only
    and is no longer needed. Finally, I added a set of functions required
    by Crypto API to work properly.
    
    In order to substitute the default algorithm (lzo), change the value
    of zram.compressor module parameter to a proper name (e.g. lz4).
    
    Signed-off-by: Piotr Sarna <p.sarna@partner.samsung.com>
    Acked-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    
    backported to Linux 3.4 by faux123
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1a9920c703f87360b7e2ed7765d0d62c06a7cd56
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Fri Aug 9 13:07:32 2013 -0500

    LZ4: compression/decompression signedness mismatch (v2)
    
    LZ4 compression and decompression functions require different
    in signedness input/output parameters: unsigned char for
    compression and signed char for decompression.
    
    Change decompression API to require "(const) unsigned char *".
    
    v2: minor coding style fix.
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit c6edd8d0dd57ff20cc6a6e8b6b142077ba424c5f
Author: Paul Reioux <reioux@gmail.com>
Date:   Fri Aug 9 13:05:22 2013 -0500

    lib/lz4/lz4_compress: fix macro usage error
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4b143dbea880ec82bbc0f40967a9e2b6ac643fe8
Author: Chanho Min <chanho.min@lge.com>
Date:   Fri Aug 9 12:58:40 2013 -0500

    crypto: Add lz4 Cryptographic API
    
    From	Chanho Min <>
    
    This patch adds support for lz4 and lz4hc compression algorithm
    using the lib/lz4/* codebase.
    Signed-off-by: Chanho Min <chanho.min@lge.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit d2dc714ddbf2c7c155ee808cef47a23c8d10d474
Author: Chanho Min <chanho.min@lge.com>
Date:   Fri Aug 9 12:56:51 2013 -0500

    lib: Add lz4 compressor module
    
    From	Chanho Min <>
    
    This patch adds support for LZ4 compression in the Linux Kernel.
    LZ4 Compression APIs for kernel are based on LZ4 implementation
    by Yann Collet and changed with kernel coding style.
    
    LZ4 homepage : http://fastcompression.blogspot.com/p/lz4.html
    LZ4 source repository : http://code.google.com/p/lz4/
    svn revision : r90
    
    Two APIs are added:
    lz4_compress() support basic lz4 compression whereas lz4hc_compress() support
    high compression or CPU performance get lower but compression ratio get higher.
    Also, we require the pre-allocated working memory with the defined size and
    destination buffer must be allocated with the size of lz4_compressbound.
    
    Signed-off-by: Chanho Min <chanho.min@lge.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 78e0d8a61031ace072ced577dc72355873e95451
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Fri Aug 9 12:47:57 2013 -0500

    arm: Add support for LZ4-compressed kernel
    
    Date	Tue, 26 Feb 2013 15:24:29 +0900
    
    This patch integrates the LZ4 decompression code to the arm pre-boot code.
    And it depends on two patchs below
    
    lib: Add support for LZ4-compressed kernel
    decompressor: Add LZ4 decompressor module
    
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    
    v2:
    - Apply CFLAGS, -Os to decompress.o to improve decompress
      performance during boot-up process
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ff724dd8370eb5738924ced627af5c9818f6059d
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Fri Aug 9 12:46:09 2013 -0500

    lib: Add support for LZ4-compressed kernel
    
    Date	Tue, 26 Feb 2013 15:24:28 +0900
    
    This patch adds support for extracting LZ4-compressed kernel images,
    as well as LZ4-compressed ramdisk images in the kernel boot process.
    
    This depends on the patch below
    decompressor: Add LZ4 decompressor module
    
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    
    v2:
    - Clean up code
    - Use lz4_decompress() for LZ4-compressed kernel during boot-process
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit a9e9e3036209d0d5d6920e4ddae1681443406a6e
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Fri Aug 9 12:44:46 2013 -0500

    decompressor: Add LZ4 decompressor module
    
    Date	Tue, 26 Feb 2013 15:24:27 +0900
    
    This patch adds support for LZ4 decompression in the Linux Kernel.
    LZ4 Decompression APIs for kernel are based on LZ4 implementation
    by Yann Collet.
    
    LZ4 homepage : http://fastcompression.blogspot.com/p/lz4.html
    LZ4 source repository : http://code.google.com/p/lz4/
    
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    
    v2:
    - Clean up code
    - Enable unaligned access for ARM v6 and above with
      CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
    - Add lz4_decompress() for faster decompression with
      uncompressed output size
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b1b7a63e3eafa5e6c82930864f90d0999aeaacf7
Author: Paul Reioux <reioux@gmail.com>
Date:   Wed Aug 7 16:28:59 2013 -0500

    Makefile: update to work with GCC 4.7 toolchain
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 56fc5e921b0f8053157de31df75ed6a8d3cadfe0
Author: Paul Reioux <reioux@gmail.com>
Date:   Tue Jun 4 02:36:19 2013 -0500

    Intellidemand: limit min cpu perflock to boot cpu only
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit da7e6cfbd970387dbe1a4b8221fc6266dcd18d19
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Jun 24 14:20:57 2013 -0500

    intellidemand: re-enable performance lock option
    
    with low freq enabled kernels, minimum performance lock will help music
    playback to be smooth and glitch free
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fa0ce3f573f736fb18756cbb061474f56f3aeede
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Jun 24 01:17:53 2013 -0500

    Revert "msm: pm-data: Enable powercollapse/suspend_enabled for non-boot cpus"
    
    This reverts commit d5ac51b7bd43225ecfaf57dcc7a0564f48cb6249.

commit 12b578de11b549173c35cfec2bc6463a13387b3d
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Tue Jun 18 17:05:19 2013 -0500

    SELinux: Reduce overhead of mls_level_isvalid() function call
    
    Date	Mon, 10 Jun 2013 13:55:08 -0400
    
    v4->v5:
      - Fix scripts/checkpatch.pl warning.
    
    v3->v4:
      - Merge the 2 separate while loops in ebitmap_contains() into
        a single one.
    
    v2->v3:
      - Remove unused local variables i, node from mls_level_isvalid().
    
    v1->v2:
     - Move the new ebitmap comparison logic from mls_level_isvalid()
       into the ebitmap_contains() helper function.
     - Rerun perf and performance tests on the latest v3.10-rc4 kernel.
    
    While running the high_systime workload of the AIM7 benchmark on
    a 2-socket 12-core Westmere x86-64 machine running 3.10-rc4 kernel
    (with HT on), it was found that a pretty sizable amount of time was
    spent in the SELinux code. Below was the perf trace of the "perf
    record -a -s" of a test run at 1500 users:
    
      5.04%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      1.96%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      1.95%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    The ebitmap_get_bit() was the hottest function in the perf-report
    output.  Both the ebitmap_get_bit() and find_next_bit() functions
    were, in fact, called by mls_level_isvalid(). As a result, the
    mls_level_isvalid() call consumed 8.95% of the total CPU time of
    all the 24 virtual CPUs which is quite a lot. The majority of the
    mls_level_isvalid() function invocations come from the socket creation
    system call.
    
    Looking at the mls_level_isvalid() function, it is checking to see
    if all the bits set in one of the ebitmap structure are also set in
    another one as well as the highest set bit is no bigger than the one
    specified by the given policydb data structure. It is doing it in
    a bit-by-bit manner. So if the ebitmap structure has many bits set,
    the iteration loop will be done many times.
    
    The current code can be rewritten to use a similar algorithm as the
    ebitmap_contains() function with an additional check for the
    highest set bit. The ebitmap_contains() function was extended to
    cover an optional additional check for the highest set bit, and the
    mls_level_isvalid() function was modified to call ebitmap_contains().
    
    With that change, the perf trace showed that the used CPU time drop
    down to just 0.08% (ebitmap_contains + mls_level_isvalid) of the
    total which is about 100X less than before.
    
      0.07%            ls  [kernel.kallsyms]     [k] ebitmap_contains
      0.05%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      0.01%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      0.01%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    The remaining ebitmap_get_bit() and find_next_bit() functions calls
    are made by other kernel routines as the new mls_level_isvalid()
    function will not call them anymore.
    
    This patch also improves the high_systime AIM7 benchmark result,
    though the improvement is not as impressive as is suggested by the
    reduction in CPU time spent in the ebitmap functions. The table below
    shows the performance change on the 2-socket x86-64 system (with HT
    on) mentioned above.
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | high_systime |     +0.1%     |     +0.9%      |     +2.6%       |
    +--------------+---------------+----------------+-----------------+
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 815da9d1e335d9b7a4e6076eea9cde8615d99da3
Author: Paul Reioux <reioux@gmail.com>
Date:   Tue Jun 18 17:04:09 2013 -0500

    Revert "SELinux: reduce overhead of mls_level_isvalid() function call"
    
    This reverts commit ada60871076634915c357cfa9640beb4e7730da7.

commit d7d8550869a32f4f0b2b6897685dbdf15b5a8149
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Jun 17 12:49:48 2013 -0500

    sched: scale cpu load for judgment of group imbalance
    
    Date	Mon, 17 Jun 2013 21:00:24 +0800
    
    We cannot compare two load directly from two cpus, since the cpu power
    over two cpu may vary largely.
    
    Suppose we meet such two kind of cpus.
    CPU A:
    	No real time work, and there are 3 task, with rq->load.weight
    being 512.
    CPU B:
    	Has real time work, and it take 3/4 of the cpu power, which
    makes CFS only take 1/4, that is 1024/4=256 cpu power. And over its CFS
    runqueue, there is only one task with weight as 128.
    
    Since both cpu's CFS task take for half of the CFS's cpu power, it
    should be considered as balanced in such case.
    
    But original judgment like:
            if ((max_cpu_load - min_cpu_load) >= avg_load_per_task &&
                (max_nr_running - min_nr_running) > 1)
    It makes (512-128)>=((512+128)/4), and lead to imbalance conclusion...
    Make the load as scaled, to avoid such case.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    
    modified for Mako kernel from LKML reference
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b07767992921b0ffeda232c5bd11fab0aa6a84d3
Author: Lei Wen <leiwen@marvell.com>
Date:   Mon Jun 17 12:45:40 2013 -0500

    sched: scale the busy and this queue's per-task load before compare
    
    Date	Mon, 17 Jun 2013 21:00:23 +0800
    
    Since for max_load and this_load, they are the value that already be
    scaled. It is not reasonble to get a minimum value between the scaled
    and non-scaled value, like below example.
    	min(sds->busiest_load_per_task, sds->max_load);
    
    Also add comment over in what condition, there would be cpu power gain
    in move the load.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fa925c44f772fd39691d0b186a21f23f5bf94968
Author: Lei Wen <leiwen@marvell.com>
Date:   Mon Jun 17 12:44:18 2013 -0500

    sched: reduce calculation effort in fix_small_imbalance
    
    Date	Mon, 17 Jun 2013 21:00:22 +0800
    
    Actually all below item could be repalced by scaled_busy_load_per_task
    	(sds->busiest_load_per_task * SCHED_POWER_SCALE)
    		/sds->busiest->sgp->power;
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 52b298cdb3e02cd3f1bbebfa0a53cc31a27a3700
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Jun 17 12:42:02 2013 -0500

    sched: remove WARN_ON(!sd) from init_sched_groups_power()
    
    Date	Tue, 11 Jun 2013 16:32:45 +0530
    
    sd can't be NULL in init_sched_groups_power() and so checking it for NULL isn't
    useful. In case it is required, then also we need to rearrange the code a bit as
    we already accessed invalid pointer sd to get sg: sg = sd->groups.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ad57856aa62f66dd6b1ee823deba47c9393c68bb
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Jun 17 12:41:16 2013 -0500

    sched: don't call get_group() for covered cpus
    
    Date	Tue, 11 Jun 2013 16:32:44 +0530
    
    In build_sched_groups() we don't need to call get_group() for cpus which are
    already covered in previous iterations. So, call get_group() after checking if
    cpu is covered or not.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 0e535dd6f8f07cfc07b60456aba81ce42449d005
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Jun 17 12:39:50 2013 -0500

    sched: Use cached value of span instead of calling sched_domain_span()
    
    Date	Tue, 11 Jun 2013 16:32:43 +0530
    
    In the beginning of build_sched_groups() we called sched_domain_span() and
    cached its return value in span. Few statements later we are calling it again to
    get the same pointer.
    
    Lets use the cached value instead as it hasn't changed in between.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 26c397c6b177cbf8e94f6b2d6fa61e01872c1b5a
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Jun 17 12:26:33 2013 -0500

    mm: Clear page active before releasing pages
    
    Active pages should not be freed to the page allocator as it triggers
    a bad page state warning. Fengguang Wu reported the following
    bug and bisected it to the patch "mm: remove lru parameter from
    __lru_cache_add and lru_cache_add_lru" which is currently in mmotm as
    mm-remove-lru-parameter-from-__lru_cache_add-and-lru_cache_add_lru.patch
    
    [   84.212960] BUG: Bad page state in process rm  pfn:0b0c9
    [   84.214682] page:ffff88000d646240 count:0 mapcount:0 mapping:          (null) index:0x0
    [   84.216883] page flags: 0x20000000004c(referenced|uptodate|active)
    [   84.218697] CPU: 1 PID: 283 Comm: rm Not tainted 3.10.0-rc4-04361-geeb9bfc #49
    [   84.220729]  ffff88000d646240 ffff88000d179bb8 ffffffff82562956 ffff88000d179bd8
    [   84.223242]  ffffffff811333f1 000020000000004c ffff88000d646240 ffff88000d179c28
    [   84.225387]  ffffffff811346a4 ffff880000270000 0000000000000000 0000000000000006
    [   84.227294] Call Trace:
    [   84.227867]  [<ffffffff82562956>] dump_stack+0x27/0x30
    [   84.229045]  [<ffffffff811333f1>] bad_page+0x130/0x158
    [   84.230261]  [<ffffffff811346a4>] free_pages_prepare+0x8b/0x1e3
    [   84.231765]  [<ffffffff8113542a>] free_hot_cold_page+0x28/0x1cf
    [   84.233171]  [<ffffffff82585830>] ? _raw_spin_unlock_irqrestore+0x6b/0xc6
    [   84.234822]  [<ffffffff81135b59>] free_hot_cold_page_list+0x30/0x5a
    [   84.236311]  [<ffffffff8113a4ed>] release_pages+0x251/0x267
    [   84.237653]  [<ffffffff8112a88d>] ? delete_from_page_cache+0x48/0x9e
    [   84.239142]  [<ffffffff8113ad93>] __pagevec_release+0x2b/0x3d
    [   84.240473]  [<ffffffff8113b45a>] truncate_inode_pages_range+0x1b0/0x7ce
    [   84.242032]  [<ffffffff810e76ab>] ? put_lock_stats.isra.20+0x1c/0x53
    [   84.243480]  [<ffffffff810e77f5>] ? lock_release_holdtime+0x113/0x11f
    [   84.244935]  [<ffffffff8113ba8c>] truncate_inode_pages+0x14/0x1d
    [   84.246337]  [<ffffffff8119b3ef>] evict+0x11f/0x232
    [   84.247501]  [<ffffffff8119c527>] iput+0x1a5/0x218
    [   84.248607]  [<ffffffff8118f015>] do_unlinkat+0x19b/0x25a
    [   84.249828]  [<ffffffff810ea993>] ? trace_hardirqs_on_caller+0x210/0x2ce
    [   84.251382]  [<ffffffff8144372e>] ? trace_hardirqs_on_thunk+0x3a/0x3f
    [   84.252879]  [<ffffffff8118f10d>] SyS_unlinkat+0x39/0x4c
    [   84.254174]  [<ffffffff825874d6>] system_call_fastpath+0x1a/0x1f
    [   84.255596] Disabling lock debugging due to kernel taint
    The problem was that a page marked for activation was released via
    pagevec. This patch clears the active bit before freeing in this case.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reported-and-Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 6e9de3703524bedd64571e1bfae458b848f028a4
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Jun 16 19:38:21 2013 -0500

    jflte: relocate sysfs interface for panel_colors
    
    Now at:
      /sys/class/lcd/panel/panel_colors
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4442cb8fce17d07384b1afc0582339a4c1b87fea
Author: Peter Korsgaard <peter.korsgaard@barco.com>
Date:   Thu May 3 12:58:49 2012 +0200

    f_fs: ffs_func_free: cleanup requests allocated by autoconfig
    
    functionfs was leaking request objects created by autoconfig.
    
    Bug: 8659094
    
    Change-Id: I641326cb5cb26e0a2ffa082cd2be2c21c66c38e5
    Signed-off-by: Peter Korsgaard <peter.korsgaard@barco.com>
    Signed-off-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Benoit Goby <benoit@android.com>

commit 3ff0ec445a010276db33197c36b4aaa709759212
Author: JP Abgrall <jpa@google.com>
Date:   Mon Apr 29 16:07:00 2013 -0700

    ARM: fault: assume no context when IRQs are disabled during data abort.
    
    Bail out early if IRQs are disabled in do_page_fault or else
      [14415.157266] BUG: sleeping function called from invalid context at arch/arm/mm/fault.c:301
    
    Russell King's idea from
      http://comments.gmane.org/gmane.linux.ports.arm.omap/59256
    
    Signed-off-by: JP Abgrall <jpa@google.com>

commit ed9a71b8190f4d8f1a467922754fce009cf8b00d
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Jun 9 19:24:51 2013 -0500

    zcache-main.c: enable zcache by default
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ceb5a1b5db6b773bbe03193eac7f8f9262fa32d0
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Jun 9 19:15:24 2013 -0500

    msm_ion.c: fix zcache compatibility issues
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 8a231ffc30695dd1637433146d3b609c5edfc923
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Jun 9 18:07:21 2013 -0500

    zcache-main.c: fix error due to API updates
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 2cd572476766e82af02c07ffb1216dd0114e3832
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Jun 9 18:06:47 2013 -0500

    arch/arm/mach-msm/Kconfig: decouple QCACHE auto selects
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fd8d995ec05bf20e29bed9400bc0462104bcea75
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Jun 9 17:58:18 2013 -0500

    Revert "zram: use zram->lock to protect zram_free_page() in swap free notify path"
    
    This reverts commit 0415710b986a5432d643f5e1f973a2ab12726b1d.

commit 1bc2e28258b69b91c4da8eab9310ea8915ab43d0
Author: Sasha Levin <levinsasha928@gmail.com>
Date:   Thu Jul 19 18:51:22 2012 -0400

    mm: frontswap: remove unneeded headers
    
    Signed-off-by: Sasha Levin <levinsasha928@gmail.com>
    [v1: Rebased with tracing removed]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit e6ad4f1fbc818bb9e716ad1ed56d9426a91655a0
Author: Sasha Levin <levinsasha928@gmail.com>
Date:   Sun Jun 10 12:51:01 2012 +0200

    mm: frontswap: split out __frontswap_curr_pages
    
    Code was duplicated in two functions, clean it up.
    
    Also, assert that the deduplicated code runs under the swap spinlock.
    
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 1faf4c6df561d60c612752f02ed6ac5250972fd2
Author: Dan Magenheimer <dan.magenheimer@oracle.com>
Date:   Mon Apr 9 17:09:27 2012 -0600

    mm: frontswap: core frontswap functionality
    
    This patch, 3of4, provides the core frontswap code that interfaces between
    the hooks in the swap subsystem and a frontswap backend via frontswap_ops.

commit d8699f1c1bd4e6a49f52085d428aaf4bfc2403c3
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat Jun 1 22:34:55 2013 -0700

    panel_colors: Place code in proper files

commit c34ce496a88b54123b3008fb373a916497644ed9
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Fri May 3 08:38:48 2013 -0500

    jf: add support for panel color shifting based on a sysfs interface
    
    Adapted from work by morfic and ktoonsez
    
    Creates a syfs interface here:
      /sys/devices/virtual/sec/tsp/panel_colors
    
    Will accept 0-4 for parameters:
      0 - Cold (more blues)
      1 - Cool
      2 - Normal (stock colors)
      3 - Warm
      4 - Hot (more reds)
    
    Change-Id: I5cc6853d6a33108ece8d0c22fd1866d9fe104efc

commit e29bcc78e0b1b67feac3acc060fbf484e270f9f5
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 15:23:12 2013 -0500

    zram: kill unused zram_get_num_devices()
    
    Date	Fri, 7 Jun 2013 00:07:29 +0800
    
    Now there's no caller of zram_get_num_devices(), so kill it.
    And change zram_devices to static because it's only used in zram_drv.c.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 017b68bba66721f32ce97a071d08f6bd49d4e7df
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 15:20:39 2013 -0500

    zram: simplify and optimize dev_to_zram()
    
    Date	Fri, 7 Jun 2013 00:07:28 +0800
    
    Simplify and optimize dev_to_zram() without walking the zram_devices
    array.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4fdc4c725aa876e97e24d6fc78337058a2967d96
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 15:18:03 2013 -0500

    zram: protect sysfs handler from invalid memory access
    
    Date	Fri, 7 Jun 2013 00:07:27 +0800
    
    Use zram->init_lock to protect access to zram->meta, otherwise it
    may cause invalid memory access if zram->meta has been freed by
    zram_reset_device().
    
    This issue may be triggered by:
    Thread 1:
    while true; do cat mem_used_total; done
    Thread 2:
    while true; do echo 8M > disksize; echo 1 > reset; done
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 67d36746baac60af18d38d2e61891edb7a169322
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 15:10:47 2013 -0500

    zram: avoid access beyond the zram device
    
    Date	Fri, 7 Jun 2013 00:07:26 +0800
    
    Function valid_io_request() should verify the entire request are within
    the zram device address range. Otherwise it may cause invalid memory
    access when accessing/modifying zram->meta->table[index] because the
    'index' is out of range. Then it may access non-exist memory, randomly
    modify memory belong to other subsystems, which is hard to track down.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit d4937272ecdd0a833ae5071fc991804b29fd5e17
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 15:08:24 2013 -0500

    zram: avoid double free in function zram_bvec_write()
    
    Date	Fri, 7 Jun 2013 00:07:25 +0800
    
    When doing a patial write and the whole page is filled with zero,
    zram_bvec_write() will free uncmem twice.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 58e25f1122c81065b10b65a62abdf23b2e22c405
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 14:37:56 2013 -0500

    zram: destroy all devices on error recovery path in zram_init()
    
    Date	Fri, 7 Jun 2013 00:07:24 +0800
    
    On error recovery path of zram_init(), it leaks the zram device object
    causing the failure. So change create_device() to free allocated
    resources on error path.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 40c06df1062970a4b768e92a8099bfc0669dc5ed
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 14:35:38 2013 -0500

    zram: use zram->lock to protect zram_free_page() in swap free notify path
    
    Date	Fri, 7 Jun 2013 00:07:23 +0800
    
    zram_slot_free_notify() is free-running without any protection from
    concurrent operations. So there are race conditions between
    zram_bvec_read()/zram_bvec_write() and zram_slot_free_notify(),
    and possible consequences include:
    1) Trigger BUG_ON(!handle) on zram_bvec_write() side.
    2) Access to freed pages on zram_bvec_read() side.
    3) Break some fields (bad_compress, good_compress, pages_stored)
       in zram->stats if the swap layer makes concurrently call to
       zram_slot_free_notify().
    
    So enhance zram_slot_free_notify() to acquire writer lock on zram->lock
    before calling zram_free_page().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 91e3ecddfea3fcf4a518790af63a785f197d20ff
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Sun Jun 9 14:33:59 2013 -0500

    zram: avoid invalid memory access in zram_exit()
    
    Date	Fri, 7 Jun 2013 00:07:22 +0800
    
    Memory for zram->disk object may have already been freed after returning
    from destroy_device(zram), then it's unsafe for zram_reset_device(zram)
    to access zram->disk again.
    
    We can't solve this bug by flipping the order of destroy_device(zram)
    and zram_reset_device(zram), that will cause deadlock issues to the
    zram sysfs handler.
    
    So fix it by holding an extra reference to zram->disk before calling
    destroy_device(zram).
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit f3a11cdca6e71c47921b2280d351444ad84686c1
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Apr 23 18:30:48 2013 +0200

    staging/zsmalloc: don't use pgtable-mapping from modules
    
    Building zsmalloc as a module does not work on ARM because it uses
    an interface that is not exported:
    
    ERROR: "flush_tlb_kernel_range" [drivers/staging/zsmalloc/zsmalloc.ko] undefined!
    
    Since this is only used as a performance optimization and only on ARM,
    we can avoid the problem simply by not using that optimization when
    building zsmalloc it is a loadable module.
    
    flush_tlb_kernel_range is often an inline function, but out of the
    architectures that use an extern function, only powerpc exports
    it.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7a0cda04ab0a9277b0b76f9c416d547ff4172cb2
Author: Joerg Roedel <joro@8bytes.org>
Date:   Wed Mar 27 01:43:14 2013 +0100

    staging: zsmalloc: Fix link error on ARM
    
    Testing the arm chromebook config against the upstream
    kernel produces a linker error for the zsmalloc module from
    staging. The symbol flush_tlb_kernel_range is not available
    there. Fix this by removing the reimplementation of
    unmap_kernel_range in the zsmalloc module and using the
    function directly. The unmap_kernel_range function is not
    usable by modules, so also disallow building the driver as a
    module for now.
    
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Joerg Roedel <joro@8bytes.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit acda31c9e478777f66607369a05d0cba90dbf900
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Wed Jan 30 09:36:52 2013 -0600

    staging: zsmalloc: remove unused pool name
    
    zs_create_pool() currently takes a name argument which is
    never used in any useful way.
    
    This patch removes it.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f512006adda2ae516a5a31a1cbc5b290b5558b21
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Jan 28 10:00:08 2013 +0900

    staging: zsmalloc: Fix TLB coherency and build problem
    
    Recently, Matt Sealey reported he fail to build zsmalloc caused by
    using of local_flush_tlb_kernel_range which are architecture dependent
    function so !CONFIG_SMP in ARM couldn't implement it so it ends up
    build error following as.
    
      MODPOST 216 modules
      LZMA    arch/arm/boot/compressed/piggy.lzma
      AS      arch/arm/boot/compressed/lib1funcs.o
    ERROR: "v7wbi_flush_kern_tlb_range"
    [drivers/staging/zsmalloc/zsmalloc.ko] undefined!
    make[1]: *** [__modpost] Error 1
    make: *** [modules] Error 2
    make: *** Waiting for unfinished jobs....
    
    The reason we used that function is copy method by [1]
    was really slow in ARM but at that time.
    
    More severe problem is ARM can prefetch speculatively on other CPUs
    so under us, other TLBs can have an entry only if we do flush local
    CPU. Russell King pointed that. Thanks!
    We don't have many choices except using flush_tlb_kernel_range.
    
    My experiment in ARMv7 processor 4 core didn't make any difference with
    zsmapbench[2] between local_flush_tlb_kernel_range and flush_tlb_kernel_range
    but still page-table based is much better than copy-based.
    
    * bigger is better.
    
    1. local_flush_tlb_kernel_range: 3918795 mappings
    2. flush_tlb_kernel_range : 3989538 mappings
    3. copy-based: 635158 mappings
    
    This patch replace local_flush_tlb_kernel_range with
    flush_tlb_kernel_range which are avaialbe in all architectures
    because we already have used it in vmalloc allocator which are
    generic one so build problem should go away and performane loss
    shoud be void.
    
    [1] f553646, zsmalloc: add page table mapping method
    [2] https://github.com/spartacus06/zsmapbench
    
    Cc: stable@vger.kernel.org
    Cc: Dan Magenheimer <dan.magenheimer@oracle.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Konrad Rzeszutek Wilk <konrad@darnok.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Reported-by: Matt Sealey <matt@genesi-usa.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c8c2e9f95eaac13587d1777ede2b6d7330a3c581
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Fri Jan 25 11:46:18 2013 -0600

    staging: zsmalloc: make CLASS_DELTA relative to PAGE_SIZE
    
    Right now ZS_SIZE_CLASS_DELTA is hardcoded to be 16.  This
    creates 254 classes for systems with 4k pages. However, on
    PPC64 with 64k pages, it creates 4095 classes which is far
    too many.
    
    This patch makes ZS_SIZE_CLASS_DELTA relative to PAGE_SIZE
    so that regardless of the page size, there will be the same
    number of classes.
    
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Dan Magenheimer <dan.magenheimer@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c8a20345ed2cbbbb7d77c4adfee4a04f267359df
Author: Davidlohr Bueso <davidlohr.bueso@hp.com>
Date:   Fri Jan 4 12:14:00 2013 -0800

    staging: zsmalloc: comment zs_create_pool function
    
    Just as with zs_malloc() and zs_map_object(), it is worth
    formally commenting the zs_create_pool() function.
    
    Signed-off-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bab116972e5eddf364230c4abe76c9eaed131971
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Wed Aug 8 15:12:17 2012 +0900

    zsmalloc: collapse internal .h into .c
    
    The patch collapses in the internal zsmalloc_int.h into
    the zsmalloc-main.c file.
    
    This is done in preparation for the promotion to mm/ where
    separate internal headers are discouraged.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit be081f96e6f9353def8d13fc5748b859dbebc16b
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Wed Jul 18 11:55:56 2012 -0500

    staging: zsmalloc: add page table mapping method
    
    This patchset provides page mapping via the page table.
    On some archs, most notably ARM, this method has been
    demonstrated to be faster than copying.
    
    The logic controlling the method selection (copy vs page table)
    is controlled by the definition of USE_PGTABLE_MAPPING which
    is/can be defined for any arch that performs better with page
    table mapping.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2bb07d3c88c2028d99b99d4b65c0298045cd75b0
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Wed Jul 18 11:55:55 2012 -0500

    staging: zsmalloc: prevent mappping in interrupt context
    
    Because we use per-cpu mapping areas shared among the
    pools/users, we can't allow mapping in interrupt context
    because it can corrupt another users mappings.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3295aa205f7fd4e4e72db9098a21a04436462caf
Author: Seth Jennings <sjenning@linux.vnet.ibm.com>
Date:   Wed Jul 18 11:55:54 2012 -0500

    staging: zsmalloc: s/firstpage/page in new copy map funcs
    
    firstpage already has precedent and meaning the first page
    of a zspage.  In the case of the copy mapping functions,
    it is the first of a pair of pages needing to be mapped.
    
    This patch just renames the firstpage argument to "page" to
    avoid confusion.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9c154630c2c3d340f9c9e1dae70300da8f0a2ade
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Sun Jun 9 14:19:58 2013 -0500

    SELinux: reduce overhead of mls_level_isvalid() function call
    
    Date	Wed, 10 Apr 2013 14:26:22 -0400
    
    While running the high_systime workload of the AIM7 benchmark on
    a 2-socket 12-core Westmere x86-64 machine running 3.8.2 kernel,
    it was found that a pretty sizable amount of time was spent in the
    SELinux code. Below was the perf trace of the "perf record -a -s"
    of a test run at 1500 users:
    
      3.96%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      1.44%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      1.33%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    The ebitmap_get_bit() was the hottest function in the perf-report
    output.  Both the ebitmap_get_bit() and find_next_bit() functions
    were, in fact, called by mls_level_isvalid(). As a result, the
    mls_level_isvalid() call consumed 6.73% of the total CPU time of all
    the 24 virtual CPUs which is quite a lot.
    
    Looking at the mls_level_isvalid() function, it is checking to see
    if all the bits set in one of the ebitmap structure are also set in
    another one as well as the highest set bit is no bigger than the one
    specified by the given policydb data structure. It is doing it in
    a bit-by-bit manner. So if the ebitmap structure has many bits set,
    the iteration loop will be done many times.
    
    The current code can be rewritten to use a similar algorithm as the
    ebitmap_contains() function with an additional check for the highest
    set bit. With that change, the perf trace showed that the used CPU
    time drop down to just 0.09% of the total which is about 100X less
    than before.
    
      0.04%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      0.04%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      0.01%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    Actually, the remaining ebitmap_get_bit() and find_next_bit() function
    calls are made by other kernel routines as the new mls_level_isvalid()
    function will not call them anymore.
    
    This patch also improves the high_systime AIM7 benchmark result,
    though the improvement is not as impressive as is suggested by the
    reduction in CPU time. The table below shows the performance change
    on the 2-socket x86-64 system mentioned above.
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | high_systime |     +0.2%     |     +1.1%      |     +2.4%       |
    +--------------+---------------+----------------+-----------------+
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 74b07912441fc12e23b0aba6bdaa7cef0ec2b61f
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jun 6 18:31:22 2013 -0500

    sched: Remove unused params of build_sched_domain()
    
    Date	Tue, 4 Jun 2013 16:50:19 +0530
    
    build_sched_domain() never uses parameter struct s_data *d and so passing it is
    useless.
    
    Remove it.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fba97b2de93e3cb1610094dce75e7dc10436e6e9
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jun 6 18:29:01 2013 -0500

    sched: Optimize build_sched_domains() for saving first SD node for a cpu
    
    We are saving first scheduling domain for a cpu in build_sched_domains() by
    iterating over the nested sd->child list. We don't actually need to do it this
    way.
    
    tl will be equal to sched_domain_topology for the first iteration and so we can
    set *per_cpu_ptr(d.sd, i) based on that.  So, save pointer to first SD while
    running the iteration loop over tl's.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 74ed74032c99b56ec1d4363f7ed7b11e999f4b48
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jun 6 18:26:42 2013 -0500

    sched: Optimize build_sched_domains() for saving first SD node for a cpu
    
    Date	Tue, 4 Jun 2013 16:50:18 +0530
    
    We are saving first scheduling domain for a cpu in build_sched_domains() by
    iterating over the nested sd->child list. We don't actually need to do it this
    way.
    
    *per_cpu_ptr(d.sd, i) is guaranteed to be NULL in the beginning as we have
    called __visit_domain_allocation_hell() which does a memset to zero for struct
    s_data.
    
    So, save pointer to first SD while running the iteration loop over tl's.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 8dbaabe6b0f6e35bb4bc756db041edeafb0ce2a1
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Aug 13 17:38:48 2012 +0000

    ARM: mutex: use generic atomic_dec-based implementation for ARMv6+
    
    Commit a76d7bd96d65 ("ARM: 7467/1: mutex: use generic xchg-based
    implementation for ARMv6+") removed the barrier-less, ARM-specific
    mutex implementation in favour of the generic xchg-based code.
    
    Since then, a bug was uncovered in the xchg code when running on SMP
    platforms, due to interactions between the locking paths and the
    MUTEX_SPIN_ON_OWNER code. This was fixed in 0bce9c46bf3b ("mutex: place
    lock in contended state after fastpath_lock failure"), however, the
    atomic_dec-based mutex algorithm is now marginally more efficient for
    ARM (~0.5% improvement in hackbench scores on dual A15).
    
    This patch moves ARMv6+ platforms to the atomic_dec-based mutex code.
    
    Change-Id: I8f64e98ccb61cc1cb9cb68ee15e55d8a792792f5
    Cc: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-on: http://git-master/r/130941
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Bharat Nihalani <bnihalani@nvidia.com>
    Tested-by: Bharat Nihalani <bnihalani@nvidia.com>

commit d7d97837737b4ce13d99eff787f2b832ed485103
Author: Paul Reioux <reioux@gmail.com>
Date:   Fri May 24 16:31:19 2013 -0500

    intellidemand: set default sampling rate to 50k
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 36a9967f27970d1dc896a22d44b65f1398ef39e1
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Apr 4 13:42:51 2013 -0500

    aio: convert the ioctx list to radix tree
    
    Date	Wed, 3 Apr 2013 16:20:48 +0300
    
    When using a large number of threads performing AIO operations the
    IOCTX list may get a significant number of entries which will cause
    significant overhead. For example, when running this fio script:
    
    rw=randrw; size=256k ;directory=/mnt/fio; ioengine=libaio; iodepth=1
    blocksize=1024; numjobs=512; thread; loops=100
    
    on an EXT2 filesystem mounted on top of a ramdisk we can observe up to
    30% CPU time spent by lookup_ioctx:
    
     32.51%  [guest.kernel]  [g] lookup_ioctx
      9.19%  [guest.kernel]  [g] __lock_acquire.isra.28
      4.40%  [guest.kernel]  [g] lock_release
      4.19%  [guest.kernel]  [g] sched_clock_local
      3.86%  [guest.kernel]  [g] local_clock
      3.68%  [guest.kernel]  [g] native_sched_clock
      3.08%  [guest.kernel]  [g] sched_clock_cpu
      2.64%  [guest.kernel]  [g] lock_release_holdtime.part.11
      2.60%  [guest.kernel]  [g] memcpy
      2.33%  [guest.kernel]  [g] lock_acquired
      2.25%  [guest.kernel]  [g] lock_acquire
      1.84%  [guest.kernel]  [g] do_io_submit
    
    This patchs converts the ioctx list to a radix tree. For a performance
    comparison the above FIO script was run on a 2 sockets 8 core
    machine. This are the results for the original list based
    implementation and for the radix tree based implementation:
    
    cores         1         2         4         8         16        32
    list        111025 ms  62219 ms  34193 ms  22998 ms  19335 ms  15956 ms
    radix        75400 ms  42668 ms  23923 ms  17206 ms  15820 ms  13295 ms
    % of radix
    relative      68%       69%       70%       75%       82%       83%
    to list
    
    To consider the impact of the patch on the typical case of having
    only one ctx per process the following FIO script was run:
    
    rw=randrw; size=100m ;directory=/mnt/fio; ioengine=libaio; iodepth=1
    blocksize=1024; numjobs=1; thread; loops=100
    
    on the same system and the results are the following:
    
    list        65241 ms
    radix       65402 ms
    % of radix
    relative    100.25%
    to list
    
    Cc: Andi Kleen <ak@linux.intel.com>
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    modified for Mako hybrid from LKML
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 00ba519dd26592efa20374fb2010cf6562d29cfb
Author: faux123 <reioux@gmail.com>
Date:   Mon Feb 4 23:27:21 2013 -0800

    tmpfs: add support for read_iter and write_iter
    
    Convert tmpfs do_shmem_file_read() to shmem_file_read_iter().
    Make file_read_iter_actor() global so tmpfs can use it too: delete
    file_read_actor(), which was made global in 2.4.4 for use by tmpfs.
    Replace tmpfs generic_file_aio_write() by generic_file_write_iter().
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit a88b4fb2384fd92be4f12f35af99c13605a52e8c
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 19:47:42 2013 -0800

    iov_iter: move into its own file (bug fix)
    
    I found the problem. iov_iter_shorten() wasn't setting i->count to the new
    value.
    
    This fixes it. I'll fix the patchset tomorrow.
    
    Thanks,
    Shaggy

commit 0491dcb2a28306e3fe9c784ca8fb3291e609f907
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 18:00:37 2013 -0800

    ecrpytfs: Convert aio_read/write ops to read/write_iter
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Tyler Hicks <tyhicks@canonical.com>
    Cc: Dustin Kirkland <dustin.kirkland@gazzang.com>
    Cc: ecryptfs@vger.kernel.org

commit cf894db5df8e51ae5f1f037d0762b42b18acd59d
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:58:19 2013 -0800

    ext4: add support for read_iter and write_iter
    
    use the generic_file_read_iter(), create ext4_file_write_iter() based on
    ext4_file_write(), and make ext4_file_write() a wrapper around
    ext4_file_write_iter().
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: linux-ext4@vger.kernel.org
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 6d1b0084deb2e601fd8e432127349855aa71ea7e
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 17:51:05 2013 -0800

    fs: add read_iter and write_iter to several file systems
    
    These are the simple ones.
    
    File systems that use generic_file_aio_read() and generic_file_aio_write()
    can trivially support generic_file_read_iter() and generic_file_write_iter().
    
    This patch adds those file_operations for 9p, adfs, affs, bfs, exofs, ext2,
    ext3, fat, f2fs, hfs, hfsplus, hostfs, hpfs, jfs, jffs2, logfs, minix, nilfs2,
    omfs, ramfs, reiserfs, romfs, sysv, and ufs.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Acked-by: Boaz Harrosh <bharrosh@panasas.com>
    Cc: Zach Brown <zab@zabbo.net>
    Cc: v9fs-developer@lists.sourceforge.net
    Cc: Tigran A. Aivazian <tigran@aivazian.fsnet.co.uk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: linux-ext4@vger.kernel.org
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Benny Halevy <bhalevy@tonian.com>
    Cc: osd-dev@open-osd.org
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Mikulas Patocka <mikulas@artax.karlin.mff.cuni.cz>
    Cc: jfs-discussion@lists.sourceforge.net
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: linux-mtd@lists.infradead.org
    Cc: Joern Engel <joern@logfs.org>
    Cc: Prasad Joshi <prasadjoshi.linux@gmail.com>
    Cc: logfs@logfs.org
    Cc: linux-nilfs@vger.kernel.org
    Cc: Bob Copeland <me@bobcopeland.com>
    Cc: linux-karma-devel@lists.sourceforge.net
    Cc: reiserfs-devel@vger.kernel.org
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Evgeniy Dushistov <dushistov@mail.ru>

commit cabb6683a84d781919b45077bb7605df74719693
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 17:44:20 2013 -0800

    fs: use read_iter and write_iter rather than aio_read and aio_write
    
    File systems implementing read_iter & write_iter should not be required
    to keep aio_read and aio_write as well. The vfs should always call
    read/write_iter if they exist. This will make it easier to remove the
    aio_read/write operation in the future.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-fsdevel@vger.kernel.org

commit eb775ebd067e4bb31ee1a4d27e2025cc2f5bec60
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:41:00 2013 -0800

    fs: create file_readable() and file_writable() functions
    
    Create functions to simplify if file_ops contain either a read
    or aio_read op, or likewise write or aio_write. We will be adding
    read_iter and write_iter and don't need to be complicating the code
    in multiple places.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 1ab5ebd6fc15ae8ae96385421fc8c8e386adc720
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:35:59 2013 -0800

    loop: use aio to perform io on the underlying file
    
    This uses the new kernel aio interface to process loopback IO by
    submitting concurrent direct aio.  Previously loop's IO was serialized
    by synchronous processing in a thread.
    
    The aio operations specify the memory for the IO with the bio_vec arrays
    directly instead of mappings of the pages.
    
    The use of aio operations is enabled when the backing file supports the
    read_iter, write_iter and direct_IO methods.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 9525e3dd65dc3ce03caddca3d7a4ab57485c2bfd
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:22:49 2013 -0800

    bio: add bvec_length(), like iov_length()
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 5d75d95e8101b4cfb23b1b3132e02e5dae10d8c3
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:10:09 2013 -0800

    aio: add aio support for iov_iter arguments
    
    This adds iocb cmds which specify that memory is held in iov_iter
    structures.  This lets kernel callers specify memory that can be
    expressed in an iov_iter, which includes pages in bio_vec arrays.
    
    Only kernel callers can provide an iov_iter so it doesn't make a lot of
    sense to expose the IOCB_CMD values for this as part of the user space
    ABI.
    
    But kernel callers should also be able to perform the usual aio
    operations which suggests using the the existing operation namespace and
    support code.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit e156dc6baa782d0c8a9409358ca45aa6e8083b8c
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:06:57 2013 -0800

    aio: add aio_kernel_() interface
    
    This adds an interface that lets kernel callers submit aio iocbs without
    going through the user space syscalls.  This lets kernel callers avoid
    the management limits and overhead of the context.  It will also let us
    integrate aio operations with other kernel apis that the user space
    interface doesn't have access to.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 5bae765d291f6332a88cf18d5a7d8f55776cd590
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 16:56:45 2013 -0800

    fs: pull iov_iter use higher up the stack
    
    Right now only callers of generic_perform_write() pack their iovec
    arguments into an iov_iter structure.  All the callers higher up in the
    stack work on raw iovec arguments.
    
    This patch introduces the use of the iov_iter abstraction higher up the
    stack.  Private generic path functions are changed to operation on
    iov_iter instead of on raw iovecs.  Exported interfaces that take iovecs
    immediately pack their arguments into an iov_iter and call into the
    shared functions.
    
    File operation struct functions are added with iov_iter as an argument
    so that callers to the generic file system functions can specify
    abstract memory rather than iovec arrays only.
    
    Almost all of this patch only transforms arguments and shouldn't change
    functionality.  The buffered read path is the exception.  We add a
    read_actor function which uses the iov_iter helper functions instead of
    operating on each individual iovec element.  This may improve
    performance as the iov_iter helper can copy multiple iovec elements from
    one mapped page cache page.
    
    As always, the direct IO path is special.  Sadly, it may still be
    cleanest to have it work on the underlying memory structures directly
    instead of working through the iov_iter abstraction.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit fdae1d84c1d83c2a248295c615de6488b63c9739
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 16:48:01 2013 -0800

    dio: add bio_vec support to __blockdev_direct_IO()
    
    The trick here is to initialize the dio state so that do_direct_IO()
    consumes the pages we provide and never tries to map user pages.  This
    is done by making sure that final_block_in_request covers the page that
    we set in the dio.  do_direct_IO() will return before running out of
    pages.
    
    The caller is responsible for dirtying these pages, if needed.  We add
    an option to the dio struct that makes sure we only dirty pages when
    we're operating on iovecs of user addresses.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako kernel from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 164425a5e3039c11926746c02011d6f4af549a59
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 16:39:15 2013 -0800

    dio: Convert direct_IO to use iov_iter
    
    Change the direct_IO aop to take an iov_iter argument rather than an iovec.
    This will get passed down through most filesystems so that only the
    __blockdev_direct_IO helper need be aware of whether user or kernel memory
    is being passed to the function.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    heavily backported for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	fs/gfs2/aops.c

commit 4b7980e78ad82c53fd796403ac85a31cfdb56ee4
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:10 2013 -0600

    iov_iter: let callers extract iovecs and bio_vecs
    
    direct IO treats memory from user iovecs and memory from arrays of
    kernel pages very differently.  User memory is pinned and worked with in
    batches while kernel pages are always pinned and don't require
    additional processing.
    
    Rather than try and provide an abstraction that includes these
    different behaviours we let direct IO extract the memory structs and
    hand them to the existing code.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit 4f7249d014bf8eed6c6b1005e0e8132f3fc058a3
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:09 2013 -0600

    iov_iter: add a shorten call
    
    The generic direct write path wants to shorten its memory vector.  It
    does this when it finds that it has to perform a partial write due to
    LIMIT_FSIZE.  .direct_IO() always performs IO on all of the referenced
    memory because it doesn't have an argument to specify the length of the
    IO.
    
    We add an iov_iter operation for this so that the generic path can ask
    to shorten the memory vector without having to know what kind it is.
    We're happy to shorten the kernel copy of the iovec array, but we refuse
    to shorten the bio_vec array and return an error in this case.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit 2a55542ff2d75273fb4e7e38048a32532e4b1026
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:08 2013 -0600

    iov_iter: add bvec support
    
    This adds a set of iov_iter_ops calls which work with memory which is
    specified by an array of bio_vec structs instead of an array of iovec
    structs.
    
    The big difference is that the pages referenced by the bio_vec elements
    are pinned.  They don't need to be faulted in and we can always use
    kmap_atomic() to map them one at a time.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit 4f14581b7b0f3596d5b76e857dc4290c1f999305
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 15:32:38 2013 -0800

    iov_iter: hide iovec details behind ops function pointers
    
    This moves the current iov_iter functions behind an ops struct of
    function pointers.  The current iov_iter functions all work with memory
    which is specified by iovec arrays of user space pointers.
    
    This patch is part of a series that lets us specify memory with bio_vec
    arrays of page pointers.  By moving to an iov_iter operation struct we
    can add that support in later patches in this series by adding another
    set of function pointers.
    
    I only came to this after having initialy tried to teach the current
    iov_iter functions about bio_vecs by introducing conditional branches
    that dealt with bio_vecs in all the functions.  It wasn't pretty.  This
    approach seems to be the lesser evil.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako kernel from LKML
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit a2faf2b651c225e457bfac3d754574752a871c69
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Mon Jan 28 11:23:07 2013 -0600

    fuse: convert fuse to use iov_iter_copy_[to|from]_user
    
    A future patch hides the internals of struct iov_iter, so fuse should
    be using the supported interface.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Acked-by: Miklos Szeredi <mszeredi@suse.cz>
    Cc: fuse-devel@lists.sourceforge.net

commit 85fad084b60d6af61bb7c53186f0efadc92bcb5b
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:06 2013 -0600

    iov_iter: add copy_to_user support
    
    This adds iov_iter wrappers around copy_to_user() to match the existing
    wrappers around copy_from_user().
    
    This will be used by the generic file system buffered read path.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit 8000641928f7a2e0746bfb7f788a65309f813f0f
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 15:04:23 2013 -0800

    iov_iter: iov_iter_copy_from_user() should use non-atomic copy
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>

commit 20e364deef1213984457859591a9c8e740350a94
Author: Zach Brown <zab@zabbo.net>
Date:   Sat Feb 2 15:03:12 2013 -0800

    iov_iter: move into its own file
    
    This moves the iov_iter functions in to their own file.  We're going to
    be working on them in upcoming patches.  They become sufficiently large,
    and remain self-contained, to justify seperating them from the rest of
    the huge mm/filemap.c.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Acked-by: Jeff Moyer <jmoyer@redhat.com>
    Cc: Zach Brown <zab@zabbo.net>

commit e9b2fda205e9000569f7e0d9cc050ee05023d11e
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 22:07:04 2013 -0500

    intellidemand: enable sec_touchscreen input boosting
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit a27afebe863b284519cbe07d6dadb76620b1ca70
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 20:52:12 2013 -0500

    init/Kconfig: decouple DEBUG_KERNEL from EXPERT settings
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 8b8adddca951d5e433a2fa34d53f2988d833ffb3
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Mar 18 21:50:47 2013 -0700

    msm: acpuclock-krait: Enable HFPLL for init only if switching to it
    
    The existing code left the HFPLL enabled in hfpll_init(), even if
    the CPU was detected to be at a non-HFPLL rate. Correct this for
    power savings between when hfpll_init() and the first runtime CPU
    frequency switch happen. This also ensure votes for HFPLL
    regulators are not left unnecessarily asserted.
    
    Change-Id: Iaca5dc7e4769bdbd494d669726ba9b500256f793
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 920819a394aa2ffcf5e69e080aa20a5bafea55ca
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Oct 15 23:42:55 2012 +0200

    random: make it possible to enable debugging without rebuild
    
    The module parameter that turns debugging mode (which basically means
    printing a few extra lines during runtime) is in '#if 0' block. Forcing
    everyone who would like to see how entropy is behaving on his system to
    rebuild seems to be a little bit too harsh.
    
    If we were concerned about speed, we could potentially turn 'debug' into a
    static key, but I don't think it's necessary.
    
    Drop the '#if 0' block to allow using the 'debug' parameter without rebuilding.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit 68ecb9a7d9a64476470d4d1a696f844284d364df
Author: Jarod Wilson <jarod@redhat.com>
Date:   Tue Nov 6 10:42:42 2012 -0500

    random: prime last_data value per fips requirements
    
    The value stored in last_data must be primed for FIPS 140-2 purposes. Upon
    first use, either on system startup or after an RNDCLEARPOOL ioctl, we
    need to take an initial random sample, store it internally in last_data,
    then pass along the value after that to the requester, so that consistency
    checks aren't being run against stale and possibly known data.
    
    CC: Herbert Xu <herbert@gondor.apana.org.au>
    CC: "David S. Miller" <davem@davemloft.net>
    CC: Matt Mackall <mpm@selenic.com>
    CC: linux-crypto@vger.kernel.org
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: Jarod Wilson <jarod@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit d80527df2b1ab6bad39e309eb69afc0fc8020dc2
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Oct 15 23:43:29 2012 +0200

    random: fix debug format strings
    
    Fix the following warnings in formatting debug output:
    
    drivers/char/random.c: In function ‘xfer_secondary_pool’:
    drivers/char/random.c:827: warning: format ‘%d’ expects type ‘int’, but argument 7 has type ‘size_t’
    drivers/char/random.c: In function ‘account’:
    drivers/char/random.c:859: warning: format ‘%d’ expects type ‘int’, but argument 5 has type ‘size_t’
    drivers/char/random.c:881: warning: format ‘%d’ expects type ‘int’, but argument 5 has type ‘size_t’
    drivers/char/random.c: In function ‘random_read’:
    drivers/char/random.c:1141: warning: format ‘%d’ expects type ‘int’, but argument 5 has type ‘ssize_t’
    drivers/char/random.c:1145: warning: format ‘%d’ expects type ‘int’, but argument 5 has type ‘ssize_t’
    drivers/char/random.c:1145: warning: format ‘%d’ expects type ‘int’, but argument 6 has type ‘long unsigned int’
    
    by using '%zd' instead of '%d' to properly denote ssize_t/size_t conversion.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit 0be6c3acd926ccf058029a0ee6f8914d018e4a7f
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Jul 27 22:26:08 2012 -0400

    random: mix in architectural randomness in extract_buf()
    
    Mix in any architectural randomness in extract_buf() instead of
    xfer_secondary_buf().  This allows us to mix in more architectural
    randomness, and it also makes xfer_secondary_buf() faster, moving a
    tiny bit of additional CPU overhead to process which is extracting the
    randomness.
    
    [ Commit description modified by tytso to remove an extended
      advertisement for the RDRAND instruction. ]
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: DJ Johnston <dj.johnston@intel.com>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Cc: stable@vger.kernel.org

commit d048d913a41e53b4396f8f10f22506a3b22a3c82
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Sat Jul 14 20:27:52 2012 -0400

    random: remove rand_initialize_irq()
    
    With the new interrupt sampling system, we are no longer using the
    timer_rand_state structure in the irq descriptor, so we can stop
    initializing it now.
    
    [ Merged in fixes from Sedat to find some last missing references to
      rand_initialize_irq() ]
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>
    
    Conflicts:
    	drivers/char/random.c

commit 26ac7521b770abbcc5cad4074ee129eebd7d42eb
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Thu Jul 5 10:21:01 2012 -0400

    random: use the arch-specific rng in xfer_secondary_pool
    
    If the CPU supports a hardware random number generator, use it in
    xfer_secondary_pool(), where it will significantly improve things and
    where we can afford it.
    
    Also, remove the use of the arch-specific rng in
    add_timer_randomness(), since the call is significantly slower than
    get_cycles(), and we're much better off using it in
    xfer_secondary_pool() anyway.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: stable@vger.kernel.org
    
    Conflicts:
    	drivers/char/random.c

commit 6b485e5d602a06c8601c1ec582609a15563f71b3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 4 11:16:01 2012 -0400

    random: create add_device_randomness() interface
    
    Add a new interface, add_device_randomness() for adding data to the
    random pool that is likely to differ between two devices (or possibly
    even per boot).  This would be things like MAC addresses or serial
    numbers, or the read-out of the RTC. This does *not* add any actual
    entropy to the pool, but it initializes the pool to different values
    for devices that might otherwise be identical and have very little
    entropy available to them (particularly common in the embedded world).
    
    [ Modified by tytso to mix in a timestamp, since there may be some
      variability caused by the time needed to detect/configure the hardware
      in question. ]
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: stable@vger.kernel.org
    
    Conflicts:
    	include/linux/random.h

commit df9c5d04ea463d66e45c0d2a2d6d3d7ba2888d76
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Fri Jul 6 14:03:18 2012 -0400

    random: fix up sparse warnings
    
    Add extern and static declarations to suppress sparse warnings
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit 42805c7e55a77023df68dfe060a2ac6bd4539b8a
Author: Lance Poore <linuxsociety@gmail.com>
Date:   Sun Aug 12 15:25:06 2012 -0500

    SCHEDULER: Autogroup patch group by current user android UID instead of task ID

commit 23961bc232465312eefb51ca6f2a6d94e2dadcf6
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 13:13:48 2013 -0500

    intellidemand: tweak optimal for krait 600 series
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ab9b6ea999ad277a77aca940d9c239b7290a39e7
Author: Mahesh Sivasubramanian <msivasub@codeaurora.org>
Date:   Fri May 10 09:38:51 2013 -0600

    msm: rpm-smd: Configure WQ for higer priority
    
    When the workqueue runs at a lower priority, it gets starved by higher
    priority threads. Bump the WQ priority to high to ensure rpm smd work
    queue gets a fair chance
    
    Change-Id: I06b864611cf45afe6931d6030327806032894663
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>
    (cherry picked from commit e0eccf8f6dece61cdb639c4b43374c34c3acf3ea)

commit 4c7179d0080e2f76583c4fb6ca139c165cf082d1
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 13 22:16:45 2012 -0700

    workqueue: reimplement WQ_HIGHPRI using a separate worker_pool
    
    WQ_HIGHPRI was implemented by queueing highpri work items at the head
    of the global worklist.  Other than queueing at the head, they weren't
    handled differently; unfortunately, this could lead to execution
    latency of a few seconds on heavily loaded systems.
    
    Now that workqueue code has been updated to deal with multiple
    worker_pools per global_cwq, this patch reimplements WQ_HIGHPRI using
    a separate worker_pool.  NR_WORKER_POOLS is bumped to two and
    gcwq->pools[0] is used for normal pri work items and ->pools[1] for
    highpri.  Highpri workers get -20 nice level and has 'H' suffix in
    their names.  Note that this change increases the number of kworkers
    per cpu.
    
    POOL_HIGHPRI_PENDING, pool_determine_ins_pos() and highpri chain
    wakeup code in process_one_work() are no longer used and removed.
    
    This allows proper prioritization of highpri work items and removes
    high execution latency of highpri work items.
    
    v2: nr_running indexing bug in get_pool_nr_running() fixed.
    
    v3: Refreshed for the get_pool_nr_running() update in the previous
        patch.
    
    Change-Id: Id843c0a425f51f84083786fbf413d999d35771b7
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Josh Hunt <joshhunt00@gmail.com>
    LKML-Reference: <CAKA=qzaHqwZ8eqpLNFjxnO2fX-tgAOjmpvxgBFjv6dJeQaOW1w@mail.gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Git-commit: 3270476a6c0ce322354df8679652f060d66526dc
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    (cherry picked from commit f2a9a769bf4fa185757115007bf94c36c2f29aca)

commit 08c64924f7ab4c3ac5e967efaf7a43c9054549df
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 13 22:16:44 2012 -0700

    workqueue: introduce NR_WORKER_POOLS and for_each_worker_pool()
    
    Introduce NR_WORKER_POOLS and for_each_worker_pool() and convert code
    paths which need to manipulate all pools in a gcwq to use them.
    NR_WORKER_POOLS is currently one and for_each_worker_pool() iterates
    over only @gcwq->pool.
    
    Note that nr_running is per-pool property and converted to an array
    with NR_WORKER_POOLS elements and renamed to pool_nr_running.  Note
    that get_pool_nr_running() currently assumes 0 index.  The next patch
    will make use of non-zero index.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    v2: nr_running indexing bug in get_pool_nr_running() fixed.
    
    v3: Pointer to array is stupid.  Don't use it in get_pool_nr_running()
        as suggested by Linus.
    
    Change-Id: I46e9488601d764d25e4a6c707de129ab68f7064c
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 4ce62e9e30cacc26885cab133ad1de358dd79f21
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    (cherry picked from commit ca6a47d36f578a3c9ae47037413abca8319fd99f)

commit 2042168ba149d25c1d3c1b0a676ef80a396f6957
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: separate out worker_pool flags
    
    GCWQ_MANAGE_WORKERS, GCWQ_MANAGING_WORKERS and GCWQ_HIGHPRI_PENDING
    are per-pool properties.  Add worker_pool->flags and make the above
    three flags per-pool flags.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    Change-Id: I1824fd1c509d8ac6b0619536621a22b15b316256
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 11ebea50dbc1ade5994b2c838a096078d4c02399
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    (cherry picked from commit 1ab4f069d6d1255625a42209a42cf9d6773480d9)

commit b7637ca28990522b72c6137ea24fdf885b35e736
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: use @pool instead of @gcwq or @cpu where applicable
    
    Modify all functions which deal with per-pool properties to pass
    around @pool instead of @gcwq or @cpu.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    Change-Id: I4be6727e1cce6f9aa2a0057b96bdc725c84f1ea8
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 63d95a9150ee3bbd4117fcd609dee40313b454d9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    (cherry picked from commit ae73f6e6eae63e64d552227a109b6ccb802cdd04)

commit 578c540174306010a6e11d175ab832f5fedd0976
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: factor out worker_pool from global_cwq
    
    Move worklist and all worker management fields from global_cwq into
    the new struct worker_pool.  worker_pool points back to the containing
    gcwq.  worker and cpu_workqueue_struct are updated to point to
    worker_pool instead of gcwq too.
    
    This change is mechanical and doesn't introduce any functional
    difference other than rearranging of fields and an added level of
    indirection in some places.  This is to prepare for multiple pools per
    gcwq.
    
    v2: Comment typo fixes as suggested by Namhyung.
    
    Change-Id: Iefae84798c2af580f425b92ed79117935d99f21f
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Git-commit: bd7bdd43dcb81bb08240b9401b36a104f77dc135
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    (cherry picked from commit e3676a67902c18021bd571aa3fcefc74b377b65f)
    
    Conflicts:
    	kernel/workqueue.c

commit f18ae7a756c530fb7e65636383563d177210ea53
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: don't use WQ_HIGHPRI for unbound workqueues
    
    Unbound wqs aren't concurrency-managed and try to execute work items
    as soon as possible.  This is currently achieved by implicitly setting
    %WQ_HIGHPRI on all unbound workqueues; however, WQ_HIGHPRI
    implementation is about to be restructured and this usage won't be
    valid anymore.
    
    Add an explicit chain-wakeup path for unbound workqueues in
    process_one_work() instead of piggy backing on %WQ_HIGHPRI.
    
    Change-Id: Iecd17a9935ee28f856d8b726bb4c296762922bed
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 974271c485a4d8bb801decc616748f90aafb07ec
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    (cherry picked from commit 5b92cea36d8328c17380d7997744284707134397)

commit 99fc2e92543bdd92ec9840bbf24cae46f4121e30
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 11:51:15 2013 -0500

    drivers/sensorhub/ssp_sensorhub.c: reduce dmesg log spam
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 087c082dcb927e844d3879a6a08eec33320ede14
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 11:50:49 2013 -0500

    drivers/motor/immvibespi.c: reduce dmesg log spam
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 459ff49063295bca0e2ca1ce66522ae593e87a73
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:29:07 2013 -0500

    af_unix: use freezable blocking calls in read
    
    Date	Wed, 1 May 2013 18:35:08 -0700
    
    Avoid waking up every thread sleeping in read call on an AF_UNIX
    socket during suspend and resume by calling a freezable blocking
    call.  Previous patches modified the freezer to avoid sending
    wakeups to threads that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit 6916912b172ae08b887ad3b817b8fb1ee9381ca1
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:24:44 2013 -0500

    sigtimedwait: use freezable blocking call
    
    Date	Wed, 1 May 2013 18:35:07 -0700
    
    Avoid waking up every thread sleeping in a sigtimedwait call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit 46c6e2a04dd9b5323bbc6374b1d5ba2959b11218
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:23:32 2013 -0500

    nanosleep: use freezable blocking call
    
    Date	Wed, 1 May 2013 18:35:06 -0700
    
    Avoid waking up every thread sleeping in a nanosleep call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit 4b0674f46693910e9a3e4257bdfde361cf37c08f
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:21:48 2013 -0500

    futex: use freezable blocking call
    
    Date	Wed, 1 May 2013 18:35:05 -0700
    
    Avoid waking up every thread sleeping in a futex_wait call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Signed-off-by: Colin Cross <ccross@android.com>
    
    Conflicts:
    	kernel/futex.c

commit fe241773c029fdd967188ae001cbaed746323bf1
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:19:48 2013 -0500

    select: use freezable blocking call
    
    Date	Wed, 1 May 2013 18:35:04 -0700
    
    Avoid waking up every thread sleeping in a select call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit 037dfc022cca6378307739d83c7b9b106e01924d
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:18:32 2013 -0500

    epoll: use freezable blocking call
    
    Date	Wed, 1 May 2013 18:35:03 -0700
    
    Avoid waking up every thread sleeping in an epoll_wait call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit 1e12f869d1e6d583cbc3264b6f1465514021d2a9
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:17:06 2013 -0500

    binder: use freezable blocking calls
    
    Date	Wed, 1 May 2013 18:35:02 -0700
    
    Avoid waking up every thread sleeping in a binder call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit fb937c0bf2ce66adf8d459ff999965140f87257b
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:16:00 2013 -0500

    freezer: add new freezable helpers using freezer_do_not_count()
    
    Date	Wed, 1 May 2013 18:35:01 -0700
    
    Freezing tasks will wake up almost every userspace task from
    where it is blocking and force it to run until it hits a
    call to try_to_sleep(), generally on the exit path from the syscall
    it is blocking in.  On resume each task will run again, usually
    restarting the syscall and running until it hits the same
    blocking call as it was originally blocked in.
    
    To allow tasks to avoid running on every suspend/resume cycle,
    this patch adds additional freezable wrappers around blocking calls
    that call freezer_do_not_count().  Combined with the previous patch,
    these tasks will not run during suspend or resume unless they wake
    up for another reason, in which case they will run until they hit
    the try_to_freeze() in freezer_count(), and then continue processing
    the wakeup after tasks are thawed.  This patch also converts the
    existing wait_event_freezable* wrappers to use freezer_do_not_count().
    
    Additional patches will convert the most common locations that
    userspace blocks in to use freezable helpers.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit ce58c7fe90883982b1bcea78add568ce52e5b5e7
Author: Colin Cross <ccross@android.com>
Date:   Thu May 2 13:14:01 2013 -0500

    freezer: skip waking up tasks with PF_FREEZER_SKIP set
    
    Date	Wed, 1 May 2013 18:35:00 -0700
    
    Android goes through suspend/resume very often (every few seconds when
    on a busy wifi network with the screen off), and a significant portion
    of the energy used to go in and out of suspend is spent in the
    freezer.  If a task has called freezer_do_not_count(), don't bother
    waking it up.  If it happens to wake up later it will call
    freezer_count() and immediately enter the refrigerator.
    
    Combined with patches to convert freezable helpers to use
    freezer_do_not_count() and convert common sites where idle userspace
    tasks are blocked to use the freezable helpers, this reduces the
    time and energy required to suspend and resume.
    
    Signed-off-by: Colin Cross <ccross@android.com>

commit eadf2d80e380769c1d53d6976a3866baa9e7499f
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 2 13:12:40 2013 -0500

    freezer: shorten freezer sleep time using exponential backoff
    
    Date	Wed, 1 May 2013 18:34:59 -0700
    
    All tasks can easily be frozen in under 10 ms, switch to using
    an initial 1 ms sleep followed by exponential backoff until
    8 ms.  Also convert the printed time to ms instead of centiseconds.
    
    Signed-off-by: Colin Cross <ccross@android.com>
    modified for Mako hybrid from LKML
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4d0f086516eb97311e15e692d10bd713987df32c
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon May 20 01:43:57 2013 -0500

    cpufreq: intellidemand: Fix optimal_freq logic on ramp down
    
    When ondemand governor decides to ramp down the frequency, the missing
    check for frequency being lower than optimal_freq was causing frequency
    to be ramped down to optimal_freq even if the frequency decided by ondemand
    governor was higher.
    
    Change-Id: I00f9ae4c61ccda4039f3338dee2c29b8cb790be4
    Signed-off-by: Veena Sambasivan <veenas@codeaurora.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1e10d62f1aa96da06486033b2140ae0c50bab7af
Author: Veena Sambasivan <veenas@codeaurora.org>
Date:   Tue May 14 12:36:48 2013 -0700

    cpufreq: Fix optimal_freq logic on ramp down
    
    When ondemand governor decides to ramp down the frequency, the missing
    check for frequency being lower than optimal_freq was causing frequency
    to be ramped down to optimal_freq even if the frequency decided by ondemand
    governor was higher.
    
    Change-Id: I00f9ae4c61ccda4039f3338dee2c29b8cb790be4
    Signed-off-by: Veena Sambasivan <veenas@codeaurora.org>

commit 3843c3ea9670fa511b325ba85c8b4f70e84b28bd
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun May 19 10:00:28 2013 -0500

    tspdrv: remove dmesg log spam :p
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 4ab6ebf95d749bcbf121c25c3b469b52e269be96
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sun May 19 13:34:34 2013 -0500

    immvibespi: add sysfs interface for controlling vibe intensity
    
    This is based of codeworkx's work on smdk4412
    
    Useful with something like this:
    http://review.cyanogenmod.org/#/c/35981/
    
    Change-Id: I1c1c16b4cefd600ebc6caa603d92044c466cf792

commit 7794d452d45c114b2acfeb16cb192942afae4601
Author: Paul Reioux <reioux@gmail.com>
Date:   Fri May 17 00:38:56 2013 -0500

    block/Kconfig.iosched: disable test I/O scheduler
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 2b62b0bb7b5139a9520c576c495128147382b0a9
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Apr 15 14:36:55 2013 -0500

    mutex: back out architecture specific check for negative mutex count
    
    Date	Mon, 15 Apr 2013 10:37:59 -0400
    
    If it is confirmed that all the supported architectures can allow a
    negative mutex count without incorrect behavior, we can then back
    out the architecture specific change and allow the mutex count to
    go to any negative number. That should further reduce contention for
    non-x86 architecture.
    
    If this is not the case, this patch should be dropped.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>

commit 74bb236090db48d6e0cfb7c9bb94bd662e752083
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Apr 15 14:34:47 2013 -0500

    mutex: Queue mutex spinners with MCS lock to reduce cacheline contention
    
    Date	Mon, 15 Apr 2013 10:37:58 -0400
    
    The current mutex spinning code (with MUTEX_SPIN_ON_OWNER option turned
    on) allow multiple tasks to spin on a single mutex concurrently. A
    potential problem with the current approach is that when the mutex
    becomes available, all the spinning tasks will try to acquire the
    mutex more or less simultaneously. As a result, there will be a lot of
    cacheline bouncing especially on systems with a large number of CPUs.
    
    This patch tries to reduce this kind of contention by putting the
    mutex spinners into a queue so that only the first one in the queue
    will try to acquire the mutex. This will reduce contention and allow
    all the tasks to move forward faster.
    
    The queuing of mutex spinners is done using an MCS lock based
    implementation which will further reduce contention on the mutex
    cacheline than a similar ticket spinlock based implementation. This
    patch will add a new field into the mutex data structure for holding
    the MCS lock. This expands the mutex size by 8 bytes for 64-bit system
    and 4 bytes for 32-bit system. This overhead will be avoid if the
    MUTEX_SPIN_ON_OWNER option is turned off.
    
    The following table shows the jobs per minute (JPM) scalability data
    on an 8-node 80-core Westmere box with a 3.7.10 kernel. The numactl
    command is used to restrict the running of the fserver workloads to
    1/2/4/8 nodes with hyperthreading off.
    
    +-----------------+-----------+-----------+-------------+----------+
    |  Configuration  | Mean JPM  | Mean JPM  |  Mean JPM   | % Change |
    |		  | w/o patch | patch 1   | patches 1&2 |  1->1&2  |
    +-----------------+------------------------------------------------+
    |		  |              User Range 1100 - 2000		   |
    +-----------------+------------------------------------------------+
    | 8 nodes, HT off |  227972   |  227237   |   305043    |  +34.2%  |
    | 4 nodes, HT off |  393503   |  381558   |   394650    |   +3.4%  |
    | 2 nodes, HT off |  334957   |  325240   |   338853    |   +4.2%  |
    | 1 node , HT off |  198141   |  197972   |   198075    |   +0.1%  |
    +-----------------+------------------------------------------------+
    |		  |              User Range 200 - 1000		   |
    +-----------------+------------------------------------------------+
    | 8 nodes, HT off |  282325   |  312870   |   332185    |   +6.2%  |
    | 4 nodes, HT off |  390698   |  378279   |   393419    |   +4.0%  |
    | 2 nodes, HT off |  336986   |  326543   |   340260    |   +4.2%  |
    | 1 node , HT off |  197588   |  197622   |   197582    |    0.0%  |
    +-----------------+-----------+-----------+-------------+----------+
    At low user range 10-100, the JPM differences were within +/-1%. So
    they are not that interesting.
    
    The fserver workload uses mutex spinning extensively. With just
    the mutex change in the first patch, there is no noticeable change
    in performance.  Rather, there is a slight drop in performance. This
    mutex spinning patch more than recovers the lost performance and show
    a significant increase of +30% at high user load with the full 8 nodes.
    Similar improvements were also seen in a 3.8 kernel.
    
    The table below shows the %time spent by different kernel functions
    as reported by perf when running the fserver workload at 1500 users
    with all 8 nodes.
    
    +-----------------------+-----------+---------+-------------+
    |        Function       |  % time   | % time  |   % time    |
    |                       | w/o patch | patch 1 | patches 1&2 |
    +-----------------------+-----------+---------+-------------+
    | __read_lock_failed    |  34.96%   | 34.91%  |   29.14%    |
    | __write_lock_failed   |  10.14%   | 10.68%  |    7.51%    |
    | mutex_spin_on_owner   |   3.62%   |  3.42%  |    2.33%    |
    | mspin_lock            |    N/A    |   N/A   |    9.90%    |
    | __mutex_lock_slowpath |   1.46%   |  0.81%  |    0.14%    |
    | _raw_spin_lock        |   2.25%   |  2.50%  |    1.10%    |
    +-----------------------+-----------+---------+-------------+
    The fserver workload for an 8-node system is dominated by the
    contention in the read/write lock. Mutex contention also plays a
    role. With the first patch only, mutex contention is down (as shown by
    the __mutex_lock_slowpath figure) which help a little bit. We saw only
    a few percents improvement with that.
    
    By applying patch 2 as well, the single mutex_spin_on_owner figure is
    now split out into an additional mspin_lock figure. The time increases
    from 3.42% to 11.23%. It shows a great reduction in contention among
    the spinners leading to a 30% improvement. The time ratio 9.9/2.33=4.3
    indicates that there are on average 4+ spinners waiting in the spin_lock
    loop for each spinner in the mutex_spin_on_owner loop. Contention in
    other locking functions also go down by quite a lot.
    
    The table below shows the performance change of both patches 1 & 2 over
    patch 1 alone in other AIM7 workloads (at 8 nodes, hyperthreading off).
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | alltests     |      0.0%     |     -0.8%      |     +0.6%       |
    | five_sec     |     -0.3%     |     +0.8%      |     +0.8%       |
    | high_systime |     +0.4%     |     +2.4%      |     +2.1%       |
    | new_fserver  |     +0.1%     |    +14.1%      |    +34.2%       |
    | shared       |     -0.5%     |     -0.3%      |     -0.4%       |
    | short        |     -1.7%     |     -9.8%      |     -8.3%       |
    +--------------+---------------+----------------+-----------------+
    The short workload is the only one that shows a decline in performance
    probably due to the spinner locking and queuing overhead.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>

commit 1e56cec93476cb7b93eb0e05f6884841f15e8384
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Apr 15 14:33:17 2013 -0500

    mutex: Make more scalable by doing less atomic operations
    
    Date	Mon, 15 Apr 2013 10:37:57 -0400
    
    In the __mutex_lock_common() function, an initial entry into
    the lock slow path will cause two atomic_xchg instructions to be
    issued. Together with the atomic decrement in the fast path, a total
    of three atomic read-modify-write instructions will be issued in
    rapid succession. This can cause a lot of cache bouncing when many
    tasks are trying to acquire the mutex at the same time.
    
    This patch will reduce the number of atomic_xchg instructions used by
    checking the counter value first before issuing the instruction. The
    atomic_read() function is just a simple memory read. The atomic_xchg()
    function, on the other hand, can be up to 2 order of magnitude or even
    more in cost when compared with atomic_read(). By using atomic_read()
    to check the value first before calling atomic_xchg(), we can avoid a
    lot of unnecessary cache coherency traffic. The only downside with this
    change is that a task on the slow path will have a tiny bit
    less chance of getting the mutex when competing with another task
    in the fast path.
    
    The same is true for the atomic_cmpxchg() function in the
    mutex-spin-on-owner loop. So an atomic_read() is also performed before
    calling atomic_cmpxchg().
    
    The mutex locking and unlocking code for the x86 architecture can allow
    any negative number to be used in the mutex count to indicate that some
    tasks are waiting for the mutex. I am not so sure if that is the case
    for the other architectures. So the default is to avoid atomic_xchg()
    if the count has already been set to -1. For x86, the check is modified
    to include all negative numbers to cover a larger case.
    
    The following table shows the jobs per minutes (JPM) scalability data
    on an 8-node 80-core Westmere box with a 3.7.10 kernel. The numactl
    command is used to restrict the running of the high_systime workloads
    to 1/2/4/8 nodes with hyperthreading on and off.
    
    +-----------------+-----------+------------+----------+
    |  Configuration  | Mean JPM  |  Mean JPM  | % Change |
    |		  | w/o patch | with patch |	      |
    +-----------------+-----------------------------------+
    |		  |      User Range 1100 - 2000	      |
    +-----------------+-----------------------------------+
    | 8 nodes, HT on  |    36980   |   148590  | +301.8%  |
    | 8 nodes, HT off |    42799   |   145011  | +238.8%  |
    | 4 nodes, HT on  |    61318   |   118445  |  +51.1%  |
    | 4 nodes, HT off |   158481   |   158592  |   +0.1%  |
    | 2 nodes, HT on  |   180602   |   173967  |   -3.7%  |
    | 2 nodes, HT off |   198409   |   198073  |   -0.2%  |
    | 1 node , HT on  |   149042   |   147671  |   -0.9%  |
    | 1 node , HT off |   126036   |   126533  |   +0.4%  |
    +-----------------+-----------------------------------+
    |		  |       User Range 200 - 1000	      |
    +-----------------+-----------------------------------+
    | 8 nodes, HT on  |   41525    |   122349  | +194.6%  |
    | 8 nodes, HT off |   49866    |   124032  | +148.7%  |
    | 4 nodes, HT on  |   66409    |   106984  |  +61.1%  |
    | 4 nodes, HT off |  119880    |   130508  |   +8.9%  |
    | 2 nodes, HT on  |  138003    |   133948  |   -2.9%  |
    | 2 nodes, HT off |  132792    |   131997  |   -0.6%  |
    | 1 node , HT on  |  116593    |   115859  |   -0.6%  |
    | 1 node , HT off |  104499    |   104597  |   +0.1%  |
    +-----------------+------------+-----------+----------+
    At low user range 10-100, the JPM differences were within +/-1%. So
    they are not that interesting.
    
    AIM7 benchmark run has a pretty large run-to-run variance due to random
    nature of the subtests executed. So a difference of less than +-5%
    may not be really significant.
    
    This patch improves high_systime workload performance at 4 nodes
    and up by maintaining transaction rates without significant drop-off
    at high node count.  The patch has practically no impact on 1 and 2
    nodes system.
    
    The table below shows the percentage time (as reported by perf
    record -a -s -g) spent on the __mutex_lock_slowpath() function by
    the high_systime workload at 1500 users for 2/4/8-node configurations
    with hyperthreading off.
    
    +---------------+-----------------+------------------+---------+
    | Configuration | %Time w/o patch | %Time with patch | %Change |
    +---------------+-----------------+------------------+---------+
    |    8 nodes    |      65.34%     |      0.69%       |  -99%   |
    |    4 nodes    |       8.70%	  |      1.02%	     |  -88%   |
    |    2 nodes    |       0.41%     |      0.32%       |  -22%   |
    +---------------+-----------------+------------------+---------+
    It is obvious that the dramatic performance improvement at 8
    nodes was due to the drastic cut in the time spent within the
    __mutex_lock_slowpath() function.
    
    The table below show the improvements in other AIM7 workloads (at 8
    nodes, hyperthreading off).
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | alltests     |     +0.6%     |   +104.2%      |   +185.9%       |
    | five_sec     |     +1.9%     |     +0.9%      |     +0.9%       |
    | fserver      |     +1.4%     |     -7.7%      |     +5.1%       |
    | new_fserver  |     -0.5%     |     +3.2%      |     +3.1%       |
    | shared       |    +13.1%     |   +146.1%      |   +181.5%       |
    | short        |     +7.4%     |     +5.0%      |     +4.2%       |
    +--------------+---------------+----------------+-----------------+
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Reviewed-by: Davidlohr Bueso <davidlohr.bueso@hp.com>

commit 0c3ab62bebd0e7549d803dc994f6f333cd12f329
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Wed May 1 14:35:20 2013 +0300

    block: urgent: Fix dispatching of URGENT mechanism
    
    There are cases when blk_peek_request is called not from blk_fetch_request
    thus the URGENT request may be started but the flag q->dispatched_urgent is
    not updated.
    
    Change-Id: I4fb588823f1b2949160cbd3907f4729767932e12
    CRs-fixed: 471736
    CRs-fixed: 473036
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 38d839eb7cec18ca30b9597ed571925cd63ca9e6
Author: Maya Erez <merez@codeaurora.org>
Date:   Sun Apr 14 15:19:52 2013 +0300

    block: row: Fix starvation tolerance values
    
    The current starvation tolerance values increase the boot time
    since high priority SW requests are delayed by regular priority requests.
    In order to overcome this, increase the starvation tolerance values.
    
    Change-Id: I9947fca9927cbd39a1d41d4bd87069df679d3103
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    Signed-off-by: Maya Erez <merez@codeaurora.org>

commit d51aa6bf0db2567f9356c67e8f9ca8669c57d193
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Apr 11 14:57:15 2013 +0300

    block: urgent request: Update dispatch_urgent in case of requeue/reinsert
    
    The block layer implements a mechanism for verifying that the device
    driver won't be notified of an URGENT request if there is already an
    URGENT request in flight. This is due to the fact that interrupting an
    URGENT request isn't efficient.
    This patch fixes the above described mechanism in case the URGENT request
    was returned back to the block layer from some reason: by requeue or
    reinsert.
    
    CRs-fixed: 473376, 473036, 471736
    Change-Id: Ie8b8208230a302d4526068531616984825f1050d
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit cf4cb936a69414ef538ca84c7dec5dfe0ce3025e
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Mar 21 11:04:02 2013 +0200

    block: row: Update sysfs functions
    
    All ROW (time related) configurable parameters are stored in ms so there
    is no need to convert from/to ms when reading/updating them via sysfs.
    
    Change-Id: Ib6a1de54140b5d25696743da944c076dd6fc02ae
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 81bcbfe54bc2961f2b74ec027bcb6f1354c124a2
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Mar 21 13:02:07 2013 +0200

    block: row: Prevent starvation of regular priority by high priority
    
    At the moment all REGULAR and LOW priority requests are starved as long as
    there are HIGH priority requests to dispatch.
    This patch prevents the above starvation by setting a starvation limit the
    REGULAR\LOW priority requests can tolerate.
    
    Change-Id: Ibe24207982c2c55d75c0b0230f67e013d1106017
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 9812c347e1299eae3c8c473cf9ab364bfc318a43
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Mar 12 21:02:33 2013 +0200

    block: urgent request: remove unnecessary urgent marking
    
    An urgent request is marked by the scheduler in rq->cmd_flags with the
    REQ_URGENT flag. There is no need to add an additional marking by
    the block layer.
    
    Change-Id: I05d5e9539d2f6c1bfa80240b0671db197a5d3b3f
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 18e419c362a108363eb146e2310bd6966fcd54ec
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 16 23:57:10 2013 -0500

    test-iosched: sync with CAF for ease of patching
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 54b58c52cebf48c85686adf655e24729de19e196
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jan 24 14:01:25 2013 +0200

    block: add REQ_URGENT to request flags
    
    This patch adds a new flag to be used in cmd_flags field of struct request
    for marking request as urgent.
    Urgent request is the one that should be given priority currently handled
    (regular) request by the device driver. The decision of a request urgency
    is taken by the scheduler.
    
    Change-Id: Ic20470987ef23410f1d0324f96f00578f7df8717
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 357cd176fab0f6721400b42ad4e552096cceda30
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Feb 21 13:46:29 2013 +0200

    block: test-iosched: Sleep before each test
    
    In order to be sure that the packing statistics collected after the test
    reflect *only* requests issued by the test (and not real request from
    FS) - sleep before each test in order to give an already dispatched
    requests time to complete.
    
    Change-Id: If2f40efad1d79084a8ea85afe93cce58e49ff698
    CRs-Fixed: 453712
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 6b2742425fffddf9406606e203f24e2cad918078
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Mar 12 21:17:18 2013 +0200

    block: row: Re-design urgent request notification mechanism
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. This is done
    in order to reduce the latency of an urgent request. For example:
    long WRITE may be stopped to handle an urgent READ.
    
    This patch updates the ROW URGENT notification policy to apply with the
    below:
    
    - Don't notify URGENT if there is an un-completed URGENT request in driver
    - After notifying that URGENT request is present, the next request
      dispatched is the URGENT one.
    - At every given moment only 1 request can be marked as URGENT.
      Independent of it's location (driver or scheduler)
    
    Other changes to URGENT policy:
    - Only READ queues are allowed to notify of an URGENT request pending.
    
    CR fix:
    If a pending urgent request (A) gets merged with another request (B)
    A is removed from scheduler queue but is not removed from
    rd->pending_urgent_rq.
    
    CRs-Fixed: 453712
    Change-Id: I321e8cf58e12a05b82edd2a03f52fcce7bc9a900
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 740c756edc4d9cd84ec869b693e243eb16b852e0
Author: Shashank Babu Chinta Venkata <sbchin@codeaurora.org>
Date:   Tue Feb 26 17:33:55 2013 -0800

    Revert "block: row: Re-design urgent request notification mechanism"
    
    The revert fixes frequent boot up failures on 8974.
    
    This reverts commit 0c3b048d1fae87db150e9ff729a9608e5346e042.
    
    Change-Id: I181513382a128724ce08980ad2f14cd5943c27bd
    Signed-off-by: Shashank Babu Chinta Venkata <sbchin@codeaurora.org>

commit 000e898d5980ff32461fa2bca0f37cf51bb65b14
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Mon Feb 18 11:38:58 2013 +0200

    block: row: Re-design urgent request notification mechanism
    
    This patch updates the ROW URGENT notification policy to apply with the
    bellow:
    - Don't notify URGENT if there is an un-completed URGENT request in driver
    - After notifying that URGENT request is present, the next request
      dispatched is the URGENT one.
    - At every given moment only 1 request can be marked as URGENT.
      Independent of it's location (driver or scheduler)
    
    Other changes to URGENT policy:
    - Only queues that are allowed to notify of an URGENT request pending
    are READ queues
    
    Change-Id: I17ff73125bc7a760cea1116b466115a2df635a14
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit be19b24b8f270d5614fc7f38ced38468638239f4
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jan 24 16:17:27 2013 +0200

    block: row: Update initial values of ROW data structures
    
    This patch sets the initial values of internal ROW
    parameters.
    
    Change-Id: I38132062a7fcbe2e58b9cc757e55caac64d013dc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    [smuckle@codeaurora.org: ported from msm-3.7]
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit cdc0fa9c6d7a66d487c5f83ec001a818a03f8504
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jan 24 15:08:40 2013 +0200

    block: row: Don't notify URGENT if there are un-completed urgent req
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. If the current
    transmitted request is an urgent request - we don't want it to be
    stopped.
    Due to the above ROW scheduler won't notify of an urgent request if
    there are urgent requests in flight.
    
    Change-Id: I2fa186d911b908ec7611682b378b9cdc48637ac7
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit c074b6a819cc0f339fdb86dc52b6ca4981a281d7
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jan 17 20:56:07 2013 +0200

    block: row: Idling mechanism re-factoring
    
    At the moment idling in ROW is implemented by delayed work that uses
    jiffies granularity which is not very accurate. This patch replaces
    current idling mechanism implementation with hrtime API, which gives
    nanosecond resolution (instead of jiffies).
    
    Change-Id: I86c7b1776d035e1d81571894b300228c8b8f2d92
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit e468658b65f2a74d7211692e718b97be6df6f008
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Wed Jan 23 17:15:49 2013 +0200

    block: row: Dispatch requests according to their io-priority
    
    This patch implements "application-hints" which is a way the issuing
    application can notify the scheduler on the priority of its request.
    This is done by setting the io-priority of the request.
    This patch reuses an already existing mechanism of io-priorities developed
    for CFQ. Please refer to kernel/Documentation/block/ioprio.txt for
    usage example and explanations.
    
    Change-Id: I228ec8e52161b424242bb7bb133418dc8b73925a
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 185d426071895c6a903032790223917ae5109597
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sat Jan 12 16:23:18 2013 +0200

    block: row: Aggregate row_queue parameters to one structure
    
    Each ROW queues has several parameters which default values are defined
    in separate arrays. This patch aggregates all default values into one
    array.
    The values in question are:
     - is idling enabled for the queue
     - queue quantum
     - can the queue notify on urgent request
    
    Change-Id: I3821b0a042542295069b340406a16b1000873ec6
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 6b2a747a71357439fd0a7c6bfd4b27f2c7e00478
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sat Jan 12 16:21:47 2013 +0200

    block: row: fix sysfs functions - idle_time conversion
    
    idle_time was updated to be stored in msec instead of jiffies.
    So there is no need to convert the value when reading from user or
    displaying the value to him.
    
    Change-Id: I58e074b204e90a90536d32199ac668112966e9cf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 61bfd7ae9e29cd90421dfe1f612b40ac3214fa6d
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sat Jan 12 16:21:12 2013 +0200

    block: row: Insert dispatch_quantum into struct row_queue
    
    There is really no point in keeping the dispatch quantum
    of a queue outside of it. By inserting it to the row_queue
    structure we spare extra level in accessing it.
    
    Change-Id: Ic77571818b643e71f9aafbb2ca93d0a92158b199
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit ee565a44e61f170c1393bb1c93d05e66a060cbb6
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sun Jan 13 22:04:59 2013 +0200

    block: row: Add some debug information on ROW queues
    
    1. Add a counter for number of requests on queue.
    2. Add function to print queues status (number requests
       currently on queue and number of already dispatched requests
       in current dispatch cycle).
    
    Change-Id: I1e98b9ca33853e6e6a8ddc53240f6cd6981e6024
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit f4755508f78e2f103f141a177335d52ac52f2b56
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Thu Jan 10 02:15:13 2013 +0530

    block: blk-merge: don't merge the pages with non-contiguous descriptors
    
    blk_rq_map_sg() function merges the physically contiguous pages to use same
    scatter-gather node without checking if their page descriptors are
    contiguous or not.
    
    Now when dma_map_sg() is called on the scatter gather list, it would
    take the base page pointer from each node (one by one) and iterates
    through all of the pages in same sg node by keep incrementing the base
    page pointer with the assumption that physically contiguous pages will
    have their page descriptor address contiguous which may not be true
    if SPARSEMEM config is enabled. So here we may end referring to invalid
    page descriptor.
    
    Following table shows the example of physically contiguous pages but
    their page descriptor addresses non-contiguous.
    -------------------------------------------
    | Page Descriptor    |   Physical Address |
    ------------------------------------------
    | 0xc1e43fdc         |   0xdffff000       |
    | 0xc2052000         |   0xe0000000       |
    -------------------------------------------
    
    With this patch, relevant blk-merge functions will also check if the
    physically contiguous pages are having page descriptors address contiguous
    or not? If not then, these pages are separated to be in different
    scatter-gather nodes.
    
    CRs-Fixed: 392141
    Change-Id: I3601565e5569a69f06fb3af99061c4d4c23af241
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>

commit af12108bd93e2a49482dd5fdf87106075a4b3d69
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Dec 20 19:23:58 2012 +0200

    row: Add support for urgent request handling
    
    This patch adds support for handling urgent requests.
    ROW queue can be marked as "urgent" so if it was un-served in last
    dispatch cycle and a request was added to it - it will trigger
    issuing an urgent-request-notification to the block device driver.
    The block device driver may choose at stop the transmission of current
    ongoing request to handle the urgent one. Foe example: long WRITE may
    be stopped to handle an urgent READ. This decreases READ latency.
    
    Change-Id: I84954c13f5e3b1b5caeadc9fe1f9aa21208cb35e
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 3916627944d3eec99ffdd60cd08fde76e2ee7687
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Dec 6 13:17:19 2012 +0200

    block:row: fix idling mechanism in ROW
    
    This patch addresses the following issues found in the ROW idling
    mechanism:
    1. Fix the delay passed to queue_delayed_work (pass actual delay
       and not the time when to start the work)
    2. Change the idle time and the idling-trigger frequency to be
       HZ dependent (instead of using msec_to_jiffies())
    3. Destroy idle_workqueue() in queue_exit
    
    Change-Id: If86513ad6b4be44fb7a860f29bd2127197d8d5bf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 6fdcb9eedba78ebe4ff67638d594aa995785bfbe
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Oct 30 08:33:06 2012 +0200

    row: Adding support for reinsert already dispatched req
    
    Add support for reinserting already dispatched request back to the
    schedulers internal data structures.
    The request will be reinserted back to the queue (head) it was
    dispatched from as if it was never dispatched.
    
    Change-Id: I70954df300774409c25b5821465fb3aa33d8feb5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 7aa5693cc3dae9f843170f888bffcb9007978b40
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Dec 4 16:04:15 2012 +0200

    block: Add API for urgent request handling
    
    This patch add support in block & elevator layers for handling
    urgent requests. The decision if a request is urgent or not is taken
    by the scheduler. Urgent request notification is passed to the underlying
    block device driver (eMMC for example). Block device driver may decide to
    interrupt the currently running low priority request to serve the new
    urgent request. By doing so READ latency is greatly reduced in read&write
    collision scenarios.
    
    Note that if the current scheduler doesn't implement the urgent request
    mechanism, this code path is never activated.
    
    Change-Id: I8aa74b9b45c0d3a2221bd4e82ea76eb4103e7cfa
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit b9561e04f5a98f47766b7c3782da479b10c67eeb
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Dec 4 15:54:43 2012 +0200

    block: Add support for reinsert a dispatched req
    
    Add support for reinserting a dispatched request back to the
    scheduler's internal data structures.
    This capability is used by the device driver when it chooses to
    interrupt the current request transmission and execute another (more
    urgent) pending request. For example: interrupting long write in order
    to handle pending read. The device driver re-inserts the
    remaining write request back to the scheduler, to be rescheduled
    for transmission later on.
    
    Add API for verifying whether the current scheduler
    supports reinserting requests mechanism. If reinsert mechanism isn't
    supported by the scheduler, this code path will never be activated.
    
    Change-Id: I5c982a66b651ebf544aae60063ac8a340d79e67f
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit f6f16d87ce96be9121dc79af8cef09578d8e8868
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Mon Oct 15 20:56:02 2012 +0200

    block: ROW: Fix forced dispatch
    
    This patch fixes forced dispatch in the ROW scheduling algorithm.
    When the dispatch function is called with the forced flag on, we
    can't delay the dispatch of the requests that are in scheduler queues.
    Thus, when dispatch is called with forced turned on, we need to cancel
    idling, or not to idle at all.
    
    Change-Id: I3aa0da33ad7b59c0731c696f1392b48525b52ddc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 2dc090f9ac5b480c564f49d6c7d0d87851cb2179
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Mon Oct 15 20:50:54 2012 +0200

    block: ROW: Correct minimum values of ROW tunable parameters
    
    The ROW scheduling algorithm exposes several tunable parameters.
    This patch updates the minimum allowed values for those parameters.
    
    Change-Id: I5ec19d54b694e2e83ad5376bd99cc91f084967f5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 23a4d5840e05a2127a849bd7d631de179209d685
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Sep 20 10:46:10 2012 +0300

    block: Adding ROW scheduling algorithm
    
    This patch adds the implementation of a new scheduling algorithm - ROW.
    The policy of this algorithm is to prioritize READ requests over WRITE
    as much as possible without starving the WRITE requests.
    
    Change-Id: I4ed52ea21d43b0e7c0769b2599779a3d3869c519
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 7eb57e055cf608f9ff223961e5642b56b91f1984
Author: Gokhan Moral <gm@alumni.bilkent.edu.tr>
Date:   Mon Jan 21 18:54:43 2013 -0600

    Add SIO I/O scheduler
    
    Conflicts:
    	block/Makefile

commit a32d17c5b883448b9f61e6c71cf7c49cb81f394f
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 15 22:38:11 2012 -0800

    block: fiops add some trace information
    
    Add some trace information, which is helpful when I do debugging.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>

commit 2631f7c38740e7b1739b9467b949da1ed119f1c8
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 15 22:37:22 2012 -0800

    block: fiops bias sync workload
    
    If there are async requests running, delay async workload. Otherwise
    async workload (usually very deep iodepth) will use all queue iodepth
    and later sync requests will get long delayed. The idea is from CFQ.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>

commit 2612d02f1397eed463ffaa1865f17a90af23e713
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 15 22:36:37 2012 -0800

    block: fiops preserve vios key for deep queue depth workload
    
    If the task has running request, even it's added into service tree newly,
    we preserve its vios key, so it will not lost its share. This should work
    for task driving big queue depth. For single depth task, there is no approach
    to preserve its vios key.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>

commit 40f37c8f04e3f3fbfeb94c991995b920d40cbd7a
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 15 22:35:55 2012 -0800

    block: fiops add ioprio support
    
    Add CFQ-like ioprio support. Priority A will get 20% more share than priority
    A+1, which matches CFQ.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>

commit 728638dd048b51650ff030bd52c75681cdc62770
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 15 22:35:23 2012 -0800

    block: fiops sync/async scale
    
    CFQ gives 2.5 times more share to sync workload. This matches CFQ.
    
    Note this is different with the read/write scale. We have 3 types of
    requests:
    1. read
    2. sync write
    3. write
    CFQ doesn't differentitate type 1 and 2, but request cost of 1 and 2
    are usually different for flash based storage. So we have both sync/async
    and read/write scale here.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>

commit 4848ce1a10fd1639b0e248d0d7852963c52eb143
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 15 22:34:26 2012 -0800

    block: fiops read/write request scale
    
    read/write speed of Flash based storage usually is different. For example,
    in my SSD maxium thoughput of read is about 3 times faster than that of
    write. Add a scale to differenate read and write. Also add a tunable, so
    user can assign different scale for read and write.
    
    By default, the scale is 1:1, which means the scale is a noop.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>

commit cd49c52744b5cd2695ddd017350d91b95fda25b7
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 15 22:29:58 2012 -0800

    block: fiops ioscheduler core
    
    FIOPS (Fair IOPS) ioscheduler is IOPS based ioscheduler, so only targets
    for drive without I/O seek. It's quite similar like CFQ, but the dispatch
    decision is made according to IOPS instead of slice.
    
    The algorithm is simple. Drive has a service tree, and each task lives in
    the tree. The key into the tree is called vios (virtual I/O). Every request
    has vios, which is calculated according to its ioprio, request size and so
    on. Task's vios is the sum of vios of all requests it dispatches. FIOPS
    always selects task with minimum vios in the service tree and let the task
    dispatch request. The dispatched request's vios is then added to the task's
    vios and the task is repositioned in the sevice tree.
    
    Unlike CFQ, FIOPS doesn't have separate sync/async queues, because with I/O
    less writeback, usually a task can only dispatch either sync or async requests.
    Bias read or write request can still be done with read/write scale.
    
    One issue is if workload iodepth is lower than drive queue_depth, IOPS
    share of a task might not be strictly according to its priority, request
    size and so on. In this case, the drive is in idle actually. Solving the
    problem need make drive idle, so impact performance. I believe CFQ isn't
    completely fair between tasks in such case too.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    
    Conflicts:
    	block/Makefile

commit 50e05db89bd194dd3c9ae00415802814fb7240b7
Author: Arianna Avanzini <avanzini.arianna@gmail.com>
Date:   Sun Feb 5 01:04:27 2012 +0100

    block: introduce the BFQ-v6 I/O sched for 3.4
    
    Add the BFQ-v6 I/O scheduler to 3.4.
    The general structure is borrowed from CFQ, as much code. A (bfq_)queue is
    associated to each task doing I/O on a device, and each time a scheduling
    decision has to be made a queue is selected and served until it expires.
    
        - Slices are given in the service domain: tasks are assigned budgets,
          measured in number of sectors. Once got the disk, a task must
          however consume its assigned budget within a configurable maximum time
          (by default, the maximum possible value of the budgets is automatically
          computed to comply with this timeout). This allows the desired latency
          vs "throughput boosting" tradeoff to be set.
    
        - Budgets are scheduled according to a variant of WF2Q+, implemented
          using an augmented rb-tree to take eligibility into account while
          preserving an O(log N) overall complexity.
    
        - A low-latency tunable is provided; if enabled, both interactive and soft
          real-time applications are guaranteed very low latency.
    
        - Latency guarantees are preserved also in presence of NCQ.
    
        - Also with flash-based devices, a high throughput is achieved while
          still preserving latency guarantees.
    
        - A useful feature borrowed from CFQ: static fallback queue for OOM.
    
        - Differently from CFQ, BFQ uses a unified mechanism (Early Queue Merge,
          EQM) to get a sequential read pattern, and hence a high throughput,
          with any set of processes performing interleaved I/O. EQM also
          preserves low latency. The code for detecting whether two queues have
          to be merged is a slightly modified version of the CFQ code for
          detecting whether two queues belong to cooperating processes and whether
          the service of a queue should be preempted to boost the throughput.
    
        - BFQ supports full hierarchical scheduling, exporting a cgroups
          interface.  Each node has a full scheduler, so each group can
          be assigned its own ioprio (mapped to a weight, see next point)
          and an ioprio_class.
    
        - If the cgroups interface is used, weights can be explictly assigned,
          otherwise ioprio values are mapped to weights using the relation
          weight = IOPRIO_BE_NR - ioprio.
    
        - ioprio classes are served in strict priority order, i.e., lower
          priority queues are not served as long as there are higher priority
          queues.  Among queues in the same class the bandwidth is distributed
          in proportion to the weight of each queue. A very thin extra bandwidth
          is however guaranteed to the Idle class, to prevent it from starving.
    
    Signed-off-by: Paolo Valente <paolo.valente@unimore.it>
    Signed-off-by: Arianna Avanzini <avanzini.arianna@gmail.com>

commit dd958590646336b7b1a8c2f286cb034e403b4cd6
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:16:58 2013 -0700

    block: cgroups, kconfig, build bits for BFQ-v6-3.4
    
    Update Kconfig.iosched and do the related Makefile changes to include
    kernel configuration options for BFQ. Also add the bfqio controller
    to the cgroups subsystem.
    
    Signed-off-by: Paolo Valente <paolo.valente@unimore.it>
    Signed-off-by: Arianna Avanzini <avanzini.arianna@gmail.com>
    
    Conflicts:
    
    	include/linux/cgroup_subsys.h

commit 76bc30353ee8e4b698005491f618e2bda3ec2caa
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 3 21:17:05 2014 +0100

    Revert "Added bfq, sio, row, vr, fifo, fiops, zen... I/O scheduler!"
    
    This reverts commit 1a9aa1f9a74d0b6c7895d8824c5add0adfeca413.

commit 1e540c8ee75eb23907acb43dfc2538d88f337f01
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 3 21:16:47 2014 +0100

    Revert "block: Adding ROW scheduling algorithm"
    
    This reverts commit bdc49321fa11fbf2e1a05bb1ad53cea94718b5bd.

commit 1a9c969b8f9b742bc782ecdb2c95bf9689b3f682
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 3 21:16:13 2014 +0100

    Revert "block: urgent: Fix dispatching of URGENT mechanism"
    
    This reverts commit 66888f0a886c745e9f629b15bb583640366cfabf.

commit 88807dd7c43e588aab87d19d9d343ae6dd07e832
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 3 21:15:55 2014 +0100

    Revert "block: Remove "requeuing urgent req" error messages"
    
    This reverts commit c6fb591c994ce5c0569b3e146af38f712e520df4.

commit 8fecfcdee7fbaf31dfb34401fa6c0ae44798cdcb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Mar 3 21:15:35 2014 +0100

    Revert "block: Add URGENT request notification support to CFQ scheduler"
    
    This reverts commit e52367a32bd315e087905991164b1822ec640d74.

commit b3cf3944420652accd269063edf0652f54292a5f
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 16:13:03 2013 -0700

    sound control: modified for Samsung Galaxy S4 (GT-9505 variants)
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit cc88b440ff8b835213ccca8c053660f57488d3a4
Author: Paul Reioux <reioux@gmail.com>
Date:   Fri May 3 13:27:30 2013 -0500

    Sound Control: Added headphone poweramp controls
    
    bump version to 2.1 to reflec this new addition
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit af2dbffe5d86cb8a151e7b699714fe5601d9edbb
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 2 20:25:36 2013 -0500

    Sound Control: Updated for HTC One (m7)
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ee7cc3d6c65578264637ccbc8530d7402a220b27
Author: Michael Bohan <mbohan@codeaurora.org>
Date:   Fri Apr 12 13:42:11 2013 -0500

    hrtimer: Prevent enqueue of hrtimer on dead CPU
    
    Date	Wed, 10 Apr 2013 14:07:48 -0700
    
    When switching the hrtimer cpu_base, we briefly allow for
    preemption to become enabled by unlocking the cpu_base lock.
    During this time, the CPU corresponding to the new cpu_base
    that was selected may in fact go offline. In this scenario, the
    hrtimer is enqueued to a CPU that's not online, and therefore
    it never fires.
    
    As an example, consider this example:
    
    CPU #0                          CPU #1
    ----                            ----
    ...                             hrtimer_start()
                                     lock_hrtimer_base()
                                     switch_hrtimer_base()
                                      cpu = hrtimer_get_target() -> 1
                                      spin_unlock(&cpu_base->lock)
                                    <migrate thread to CPU #0>
                                    <offline>
    spin_lock(&new_base->lock)
    this_cpu = 0
    cpu != this_cpu
    enqueue_hrtimer(cpu_base #1)
    To prevent this scenario, verify that the CPU corresponding to
    the new cpu_base is indeed online before selecting it in
    hrtimer_switch_base(). If it's not online, fallback to using the
    base of the current CPU.
    
    Signed-off-by: Michael Bohan <mbohan@codeaurora.org>

commit 8c11f5151aed4bccc21249ce9d157d14d1fc2745
Author: Michael Bohan <mbohan@codeaurora.org>
Date:   Fri Apr 12 13:40:55 2013 -0500

    hrtimer: Consider preemption when migrating hrtimer cpu_bases
    
    Date	Wed, 10 Apr 2013 14:07:47 -0700
    
    When switching to a new cpu_base in switch_hrtimer_base(), we
    briefly enable preemption by unlocking the cpu_base lock in two
    places. During this interval it's possible for the running thread
    to be swapped to a different CPU.
    
    Consider the following example:
    
    CPU #0                                 CPU #1
    ----                                   ----
    hrtimer_start()                        ...
     lock_hrtimer_base()
     switch_hrtimer_base()
      this_cpu = 0;
      target_cpu_base = 0;
      raw_spin_unlock(&cpu_base->lock)
    <migrate to CPU 1>
    ...                                    this_cpu == 0
                                           cpu == this_cpu
                                           timer->base = CPU #0
                                           timer->base != LOCAL_CPU
    Since the cached this_cpu is no longer accurate, we'll skip the
    hrtimer_check_target() check. Once we eventually go to program
    the hardware, we'll decide not to do so since it knows the real
    CPU that we're running on is not the same as the chosen base. As
    a consequence, we may end up missing the hrtimer's deadline.
    
    Fix this by updating the local CPU number each time we retake a
    cpu_base lock in switch_hrtimer_base().
    
    Another possibility is to disable preemption across the whole of
    switch_hrtimer_base. This looks suboptimal since preemption
    would be disabled while waiting for lock(s).
    
    Signed-off-by: Michael Bohan <mbohan@codeaurora.org>

commit 381d26a3d9237093ab038abd2a0c3218ec27414f
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:32:24 2013 -0700

    arch/arm/kernel/armksyms: fix merge derp
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    
    	arch/arm/kernel/armksyms.c

commit 513648466cdb11a994e9efbf5c4713a648bb9b2d
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:30:02 2013 -0700

    Sound Control: Sound control for WCD93xx codec
    
    Fully GPL'ed version.
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	sound/soc/codecs/Kconfig
    	sound/soc/codecs/Makefile
    
    Conflicts:
    	sound/soc/codecs/Kconfig
    	sound/soc/codecs/Makefile
    
    Conflicts:
    
    	sound/soc/codecs/Kconfig
    	sound/soc/codecs/Makefile

commit 858ce9c43885c6c85abf209d09cbd0f1c7ccfe41
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 23 01:00:49 2013 -0600

    Makefile: add some optimization flags for ARM
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 4f142a7437b7c3d0f5e9ea0ad4c52c85337b00e2
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Jul 31 09:28:31 2012 +0400

    switch the protection of percpu_counter list to spinlock
    
    ... making percpu_counter_destroy() non-blocking
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 2b8d102ee5a1418b4dae931d93c2d1779fefa9b9
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 09:25:16 2013 -0800

    decompress_unlzo: fix compilation error
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit b50d2727f6f4db378b3d5f02a7272593c859a7d9
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 09:02:08 2013 -0800

    ARM: 7593/1: nommu: do not enable DCACHE_WORD_ACCESS when !CONFIG_MMU
    
    Commit b9a50f74905a ("ARM: 7450/1: dcache: select DCACHE_WORD_ACCESS for
    little-endian ARMv6+ CPUs") added support for word-at-time path
    comparisons, relying on the ability to perform unaligned loads with
    negligible performance impact in hardware.
    
    For nommu configurations without MPU support, this is unpredictable and
    so we should fall back to the byte-by-byte routines.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Jonathan Austin <jonathan.austin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    modified for Mako from kernel.org
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 20a89bbd20eb107fafbb8cafd2c9ef2a143940e7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 4 12:56:44 2012 +0100

    ARM: 7592/1: nommu: prevent generation of kernel unaligned memory accesses
    
    Recent ARMv7 toolchains assume that unaligned memory accesses will not
    fault and will instead be handled by the processor.
    
    For the nommu case (without an MPU), memory will be treated as
    strongly-ordered and therefore unaligned accesses may fault regardless
    of the SCTLR.A setting.
    
    This patch passes -mno-unaligned-access to GCC when compiling for nommu
    targets, preventing the generation of unaligned memory access in the
    kernel.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Jonathan Austin <jonathan.austin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 883a16584f891fb5faf833117f4297f1f205db6b
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Wed Aug 15 16:28:36 2012 +0100

    ARM: 7492/1: add strstr declaration for decompressors
    
    With the generic unaligned.h, more kernel headers get pulled in including
    dynamic_debug.h which needs strstr. As it is not really used, we only need
    a declaration here.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Tested-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 8531d3196db97c7c6f9d3f7fe79ff80f5038b465
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:27:00 2013 -0700

    ARM: 7493/1: use generic unaligned.h
    
    This moves ARM over to the asm-generic/unaligned.h header. This has the
    benefit of better code generated especially for ARMv7 on gcc 4.7+
    compilers.
    
    As Arnd Bergmann, points out: The asm-generic version uses the "struct"
    version for native-endian unaligned access and the "byteshift" version
    for the opposite endianess. The current ARM version however uses the
    "byteshift" implementation for both.
    
    Thanks to Nicolas Pitre for the excellent analysis:
    
    Test case:
    
    int foo (int *x) { return get_unaligned(x); }
    long long bar (long long *x) { return get_unaligned(x); }
    
    With the current ARM version:
    
    foo:
    	ldrb	r3, [r0, #2]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 2B], MEM[(const u8 *)x_1(D) + 2B]
    	ldrb	r1, [r0, #1]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 1B], MEM[(const u8 *)x_1(D) + 1B]
    	ldrb	r2, [r0, #0]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D)], MEM[(const u8 *)x_1(D)]
    	mov	r3, r3, asl #16	@ tmp154, MEM[(const u8 *)x_1(D) + 2B],
    	ldrb	r0, [r0, #3]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 3B], MEM[(const u8 *)x_1(D) + 3B]
    	orr	r3, r3, r1, asl #8	@, tmp155, tmp154, MEM[(const u8 *)x_1(D) + 1B],
    	orr	r3, r3, r2	@ tmp157, tmp155, MEM[(const u8 *)x_1(D)]
    	orr	r0, r3, r0, asl #24	@,, tmp157, MEM[(const u8 *)x_1(D) + 3B],
    	bx	lr	@
    
    bar:
    	stmfd	sp!, {r4, r5, r6, r7}	@,
    	mov	r2, #0	@ tmp184,
    	ldrb	r5, [r0, #6]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 6B], MEM[(const u8 *)x_1(D) + 6B]
    	ldrb	r4, [r0, #5]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 5B], MEM[(const u8 *)x_1(D) + 5B]
    	ldrb	ip, [r0, #2]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 2B], MEM[(const u8 *)x_1(D) + 2B]
    	ldrb	r1, [r0, #4]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 4B], MEM[(const u8 *)x_1(D) + 4B]
    	mov	r5, r5, asl #16	@ tmp175, MEM[(const u8 *)x_1(D) + 6B],
    	ldrb	r7, [r0, #1]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 1B], MEM[(const u8 *)x_1(D) + 1B]
    	orr	r5, r5, r4, asl #8	@, tmp176, tmp175, MEM[(const u8 *)x_1(D) + 5B],
    	ldrb	r6, [r0, #7]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 7B], MEM[(const u8 *)x_1(D) + 7B]
    	orr	r5, r5, r1	@ tmp178, tmp176, MEM[(const u8 *)x_1(D) + 4B]
    	ldrb	r4, [r0, #0]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D)], MEM[(const u8 *)x_1(D)]
    	mov	ip, ip, asl #16	@ tmp188, MEM[(const u8 *)x_1(D) + 2B],
    	ldrb	r1, [r0, #3]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 3B], MEM[(const u8 *)x_1(D) + 3B]
    	orr	ip, ip, r7, asl #8	@, tmp189, tmp188, MEM[(const u8 *)x_1(D) + 1B],
    	orr	r3, r5, r6, asl #24	@,, tmp178, MEM[(const u8 *)x_1(D) + 7B],
    	orr	ip, ip, r4	@ tmp191, tmp189, MEM[(const u8 *)x_1(D)]
    	orr	ip, ip, r1, asl #24	@, tmp194, tmp191, MEM[(const u8 *)x_1(D) + 3B],
    	mov	r1, r3	@,
    	orr	r0, r2, ip	@ tmp171, tmp184, tmp194
    	ldmfd	sp!, {r4, r5, r6, r7}
    	bx	lr
    
    In both cases the code is slightly suboptimal.  One may wonder why
    wasting r2 with the constant 0 in the second case for example.  And all
    the mov's could be folded in subsequent orr's, etc.
    
    Now with the asm-generic version:
    
    foo:
    	ldr	r0, [r0, #0]	@ unaligned	@,* x
    	bx	lr	@
    
    bar:
    	mov	r3, r0	@ x, x
    	ldr	r0, [r0, #0]	@ unaligned	@,* x
    	ldr	r1, [r3, #4]	@ unaligned	@,
    	bx	lr	@
    
    This is way better of course, but only because this was compiled for
    ARMv7. In this case the compiler knows that the hardware can do
    unaligned word access.  This isn't that obvious for foo(), but if we
    remove the get_unaligned() from bar as follows:
    
    long long bar (long long *x) {return *x; }
    
    then the resulting code is:
    
    bar:
    	ldmia	r0, {r0, r1}	@ x,,
    	bx	lr	@
    
    So this proves that the presumed aligned vs unaligned cases does have
    influence on the instructions the compiler may use and that the above
    unaligned code results are not just an accident.
    
    Still... this isn't fully conclusive without at least looking at the
    resulting assembly fron a pre ARMv6 compilation.  Let's see with an
    ARMv5 target:
    
    foo:
    	ldrb	r3, [r0, #0]	@ zero_extendqisi2	@ tmp139,* x
    	ldrb	r1, [r0, #1]	@ zero_extendqisi2	@ tmp140,
    	ldrb	r2, [r0, #2]	@ zero_extendqisi2	@ tmp143,
    	ldrb	r0, [r0, #3]	@ zero_extendqisi2	@ tmp146,
    	orr	r3, r3, r1, asl #8	@, tmp142, tmp139, tmp140,
    	orr	r3, r3, r2, asl #16	@, tmp145, tmp142, tmp143,
    	orr	r0, r3, r0, asl #24	@,, tmp145, tmp146,
    	bx	lr	@
    
    bar:
    	stmfd	sp!, {r4, r5, r6, r7}	@,
    	ldrb	r2, [r0, #0]	@ zero_extendqisi2	@ tmp139,* x
    	ldrb	r7, [r0, #1]	@ zero_extendqisi2	@ tmp140,
    	ldrb	r3, [r0, #4]	@ zero_extendqisi2	@ tmp149,
    	ldrb	r6, [r0, #5]	@ zero_extendqisi2	@ tmp150,
    	ldrb	r5, [r0, #2]	@ zero_extendqisi2	@ tmp143,
    	ldrb	r4, [r0, #6]	@ zero_extendqisi2	@ tmp153,
    	ldrb	r1, [r0, #7]	@ zero_extendqisi2	@ tmp156,
    	ldrb	ip, [r0, #3]	@ zero_extendqisi2	@ tmp146,
    	orr	r2, r2, r7, asl #8	@, tmp142, tmp139, tmp140,
    	orr	r3, r3, r6, asl #8	@, tmp152, tmp149, tmp150,
    	orr	r2, r2, r5, asl #16	@, tmp145, tmp142, tmp143,
    	orr	r3, r3, r4, asl #16	@, tmp155, tmp152, tmp153,
    	orr	r0, r2, ip, asl #24	@,, tmp145, tmp146,
    	orr	r1, r3, r1, asl #24	@,, tmp155, tmp156,
    	ldmfd	sp!, {r4, r5, r6, r7}
    	bx	lr
    
    Compared to the initial results, this is really nicely optimized and I
    couldn't do much better if I were to hand code it myself.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    modified for Mako from kernel.org reference
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/include/asm/unaligned.h
    
    Conflicts:
    
    	arch/arm/include/asm/unaligned.h

commit a6be386bde407ad8d0a4466847c03b85c31ee711
Author: Will Deacon <will.deacon@arm.com>
Date:   Sat Feb 2 08:49:59 2013 -0800

    ARM: dcache: select DCACHE_WORD_ACCESS for little-endian ARMv6+ CPUs
    
    DCACHE_WORD_ACCESS uses the word-at-a-time API for optimised string
    comparisons in the vfs layer.
    
    This patch implements support for load_unaligned_zeropad for ARM CPUs
    with native support for unaligned memory accesses (v6+) when running
    little-endian.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 16e5f69d8879b97ee7fc6c1faaf3917c4423cf2a
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:26:21 2013 -0700

    ARM: use generic strnlen_user and strncpy_from_user functions
    
    This patch implements the word-at-a-time interface for ARM using the
    same algorithm as x86. We use the fls macro from ARMv5 onwards, where
    we have a clz instruction available which saves us a mov instruction
    when targetting Thumb-2. For older CPUs, we use the magic 0x0ff0001
    constant. Big-endian configurations make use of the implementation from
    asm-generic.
    
    With this implemented, we can replace our byte-at-a-time strnlen_user
    and strncpy_from_user functions with the optimised generic versions.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/include/asm/uaccess.h
    	arch/arm/kernel/armksyms.c
    
    Conflicts:
    
    	arch/arm/include/asm/uaccess.h
    	arch/arm/kernel/armksyms.c

commit b4e2a8704712b9a67576c5413f53c00706f8e5f8
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon May 28 12:59:56 2012 +1000

    lib: Fix generic strnlen_user for 32-bit big-endian machines
    
    The aligned_byte_mask() definition is wrong for 32-bit big-endian
    machines: the "7-(n)" part of the definition assumes a long is 8
    bytes.  This fixes it by using BITS_PER_LONG - 8 instead of 8*7.
    Tested on 32-bit and 64-bit PowerPC.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e9b391e94f510983088c2ffe989bb7cac6064bb6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 2 08:07:29 2013 -0800

    lib: add generic strnlen_user() function
    
    This adds a new generic optimized strnlen_user() function that uses the
    <asm/word-at-a-time.h> infrastructure to portably do efficient string
    handling.
    
    In many ways, strnlen is much simpler than strncpy, and in particular we
    can always pre-align the words we load from memory.  That means that all
    the worries about alignment etc are a non-issue, so this one can easily
    be used on any architecture.  You obviously do have to do the
    appropriate word-at-a-time.h macros.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5c1d6ab9dbb6013e6c40bfcad674563e806c5f56
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:24:32 2013 -0700

    word-at-a-time: make the interfaces truly generic
    
    This changes the interfaces in <asm/word-at-a-time.h> to be a bit more
    complicated, but a lot more generic.
    
    In particular, it allows us to really do the operations efficiently on
    both little-endian and big-endian machines, pretty much regardless of
    machine details.  For example, if you can rely on a fast population
    count instruction on your architecture, this will allow you to make your
    optimized <asm/word-at-a-time.h> file with that.
    
    NOTE! The "generic" version in include/asm-generic/word-at-a-time.h is
    not truly generic, it actually only works on big-endian.  Why? Because
    on little-endian the generic algorithms are wasteful, since you can
    inevitably do better. The x86 implementation is an example of that.
    
    (The only truly non-generic part of the asm-generic implementation is
    the "find_zero()" function, and you could make a little-endian version
    of it.  And if the Kbuild infrastructure allowed us to pick a particular
    header file, that would be lovely)
    
    The <asm/word-at-a-time.h> functions are as follows:
    
     - WORD_AT_A_TIME_CONSTANTS: specific constants that the algorithm
       uses.
    
     - has_zero(): take a word, and determine if it has a zero byte in it.
       It gets the word, the pointer to the constant pool, and a pointer to
       an intermediate "data" field it can set.
    
       This is the "quick-and-dirty" zero tester: it's what is run inside
       the hot loops.
    
     - "prep_zero_mask()": take the word, the data that has_zero() produced,
       and the constant pool, and generate an *exact* mask of which byte had
       the first zero.  This is run directly *outside* the loop, and allows
       the "has_zero()" function to answer the "is there a zero byte"
       question without necessarily getting exactly *which* byte is the
       first one to contain a zero.
    
       If you do multiple byte lookups concurrently (eg "hash_name()", which
       looks for both NUL and '/' bytes), after you've done the prep_zero_mask()
       phase, the result of those can be or'ed together to get the "either
       or" case.
    
     - The result from "prep_zero_mask()" can then be fed into "find_zero()"
       (to find the byte offset of the first byte that was zero) or into
       "zero_bytemask()" (to find the bytemask of the bytes preceding the
       zero byte).
    
       The existence of zero_bytemask() is optional, and is not necessary
       for the normal string routines.  But dentry name hashing needs it, so
       if you enable DENTRY_WORD_AT_A_TIME you need to expose it.
    
    This changes the generic strncpy_from_user() function and the dentry
    hashing functions to use these modified word-at-a-time interfaces.  This
    gets us back to the optimized state of the x86 strncpy that we lost in
    the previous commit when moving over to the generic version.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	fs/namei.c
    
    Conflicts:
    
    	fs/namei.c

commit ce85601ef59d9da500209261b70fe9186d636848
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 07:59:56 2013 -0800

    lib: Sparc's strncpy_from_user is generic enough, move under lib/
    
    To use this, an architecture simply needs to:
    
    1) Provide a user_addr_max() implementation via asm/uaccess.h
    
    2) Add "select GENERIC_STRNCPY_FROM_USER" to their arch Kcnfig
    
    3) Remove the existing strncpy_from_user() implementation and symbol
       exports their architecture had.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: David Howells <dhowells@redhat.com>
    adapted for Mako from kernel.org reference
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    
    	lib/Makefile

commit 2ab36dbfae6a318150329281daf804943779e1aa
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 08:35:08 2013 -0800

    kernel: Move REPEAT_BYTE definition into linux/kernel.h
    
    And make sure that everything using it explicitly includes
    that header file.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    modified for Mako kernel from kernel.org
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit e1fd33beeb2d9f36efafa0e7e694d0ca09a2e7b6
Author: Markus F.X.J. Oberhumer <markus@oberhumer.com>
Date:   Tue Aug 21 16:45:32 2012 +0200

    lib/lzo: Optimize code for CPUs with inefficient unaligned access
    
    Some code paths are only benefical on machines with fast unaligned
    loads, so only use these if CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
    defined.
    
    Signed-off-by: Markus F.X.J. Oberhumer <markus@oberhumer.com>

commit f717983968015d6d9bfd666aefc10b913be6eacd
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:22:02 2013 -0700

    lib/lzo: Update LZO compression to current upstream version
    
    This commit updates the kernel LZO code to the current upsteam version
    which features a significant speed improvement - benchmarking the Calgary
    and Silesia test corpora typically shows a doubled performance in
    both compression and decompression on modern i386/x86_64/powerpc machines.
    
    Signed-off-by: Markus F.X.J. Oberhumer <markus@oberhumer.com>
    
    Conflicts:
    	include/linux/lzo.h
    
    Conflicts:
    
    	include/linux/lzo.h

commit 9158ef1ad8761dba38f9441a120851fbbf4c56cc
Author: Markus F.X.J. Oberhumer <markus@oberhumer.com>
Date:   Mon Aug 13 17:24:24 2012 +0200

    lib/lzo: Rename lzo1x_decompress.c to lzo1x_decompress_safe.c
    
    Rename the source file to match the function name and thereby
    also make room for a possible future even slightly faster
    "non-safe" decompressor version.
    
    Signed-off-by: Markus F.X.J. Oberhumer <markus@oberhumer.com>

commit 7db2e3ae80a60d5c3712f1f2988e9e155b1c5233
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 07:20:16 2013 -0800

    arch/arm/Kconfig: enable unaligned capability for ARM
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 9f3bbf2ad22ffb45289825a1ad71a8cd9f7a5d7f
Author: Dave Martin <dave.martin@linaro.org>
Date:   Thu Jan 31 14:06:35 2013 -0600

    ARM: 7583/1: decompressor: Enable unaligned memory access for v6 and above
    
    Modern GCC can generate code which makes use of the CPU's native
    unaligned memory access capabilities.  This is useful for the C
    decompressor implementations used for unpacking compressed kernels.
    
    This patch disables alignment faults and enables the v6 unaligned
    access model on CPUs which support these features (i.e., v6 and
    later), allowing full unaligned access support for C code in the
    decompressor.
    
    The decompressor C code must not be built to assume that unaligned
    access works if support for v5 or older platforms is included in
    the kernel.
    
    For correct code generation, C decompressor code must always use
    the get_unaligned and put_unaligned accessors when dealing with
    unaligned pointers, regardless of this patch.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit c34b22c91d4bd748d54741f16941beca8faa7503
Author: faux123 <reioux@gmail.com>
Date:   Sun Apr 21 23:42:47 2013 -0700

    overall_stats: forward port to kernel 3.4+
    
    by replacing deprecated functions with native add/sub for 64bit data types
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 0fb0314da6748df52e971e7705c89babfeefb7e6
Author: faux123 <reioux@gmail.com>
Date:   Sun Apr 21 22:31:24 2013 -0700

    overall_stats: make dual/quad core stats configurable via meunconfig
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit a0444ec7c704e898fe4c5eb79c6eb4a76bcbcd3c
Author: faux123 <reioux@gmail.com>
Date:   Sun Apr 21 23:08:51 2013 -0700

    overall_stats: add overall stats for all available cores
    
    this is much more accurate stats tracking than just displaying core 0's
    stats from most other kernels
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 2b91ff5e5884e22c57b0ac80e0e5641c8dd09a07
Author: faux123 <reioux@gmail.com>
Date:   Thu Mar 21 01:42:01 2013 -0500

    cpufreq: conservative: Set MIN_LATENCY_MULTIPLIER to 20
    
    Currently MIN_LATENCY_MULTIPLIER is set defined as 100 and so on a system with
    transition latency of 1 ms, the minimum sampling time comes to be around 100 ms.
    That is quite big if you want to get better performance for your system.
    
    Redefine MIN_LATENCY_MULTIPLIER to 20 so that we can support 20ms sampling rate
    for such platforms.
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 94ed46d543d2f17cfd02360bc03c13c8e264bd5e
Author: faux123 <reioux@gmail.com>
Date:   Thu Mar 21 01:43:13 2013 -0500

    cpufreq: intellidemand: Set MIN_LATENCY_MULTIPLIER to 20
    
    Currently MIN_LATENCY_MULTIPLIER is set defined as 100 and so on a system with
    transition latency of 1 ms, the minimum sampling time comes to be around 100 ms.
    That is quite big if you want to get better performance for your system.
    
    Redefine MIN_LATENCY_MULTIPLIER to 20 so that we can support 20ms sampling rate
    for such platforms.
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 93a2073076ff950442235de88e9b9860ef0ed2c2
Author: faux123 <reioux@gmail.com>
Date:   Thu Mar 21 01:42:46 2013 -0500

    cpufreq: ondemand: Set MIN_LATENCY_MULTIPLIER to 20
    
    Currently MIN_LATENCY_MULTIPLIER is set defined as 100 and so on a system with
    transition latency of 1 ms, the minimum sampling time comes to be around 100 ms.
    That is quite big if you want to get better performance for your system.
    
    Redefine MIN_LATENCY_MULTIPLIER to 20 so that we can support 20ms sampling rate
    for such platforms.
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 382cae2d474d53a5b4650621ff6201bc5d92c00a
Author: faux123 <reioux@gmail.com>
Date:   Thu Mar 14 11:17:57 2013 -0500

    intellidemand: fix permission for sampling_rate as well
    
    to work with Android's PowerHAL
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit df62e69cb9d6587e823860d20cf3f91d1adae980
Author: faux123 <reioux@gmail.com>
Date:   Thu Mar 14 02:21:02 2013 -0500

    intellidemand: change permission for boostpulse from root to system
    
    for use with Android PowerHAL
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit d67a5aeb1b9dc875d53e61d0d7bc4a7d380f78dc
Author: faux123 <reioux@gmail.com>
Date:   Wed Mar 13 18:39:33 2013 -0500

    intellidemand: change boost tunable scope from global to local
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 8b29a4768afd70e89a24c9736f017ddb14458297
Author: faux123 <reioux@gmail.com>
Date:   Wed Mar 6 18:51:19 2013 -0600

    intellidemand: allow clean compilation when LMF is disabled
    
    bumped version to 4.2
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mach-msm/cpufreq.c

commit 2fbf4fde1a4a795c16f7df22f94588f923a225d6
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Apr 28 17:09:24 2013 -0500

    arch/arm/mach-msm/cpufreq: remove deprecated logic
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 65a66652a69742d8637517f2813c00a4569fc379
Author: faux123 <reioux@gmail.com>
Date:   Sun Mar 3 10:21:27 2013 -0600

    msm: cpufreq: make the gov stop part fully atomic
    
    In the function cpufreq_governor_dbs, the case CPUFREQ_GOV_STOP
    is not completely atomic. The removal of sysfs is outside the
    critical path. If a START/STOP done frequently from user space
    then there is chance of adding the sysfs interface before removal.
    This creates a warning from sysfs saying creating an already
    existing entry.
    
    Also once it fails to create, cpufreq_governor_dbs returns with
    error code. The dbs_enable value is incremented which is not
    rolled back and cpufreq core tries to set the old
    governor. This might create further problems.
    
    Hence the removal of sysfs entry should happen in the critical
    path.
    
    CRs-fixed: 440969
    Change-Id: I98891756caf2ce0b85e39e9b4f58034af1ed97ce
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>
    Signed-off-by: faux123 <reioux@gmail.com>

commit 42da35eb36fed7d8834881ae39b3e4ecf1533821
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:14:05 2013 -0700

    msm: cpufreq: make the gov stop part fully atomic
    
    In the function cpufreq_governor_dbs, the case CPUFREQ_GOV_STOP
    is not completely atomic. The removal of sysfs is outside the
    critical path. If a START/STOP done frequently from user space
    then there is chance of adding the sysfs interface before removal.
    This creates a warning from sysfs saying creating an already
    existing entry.
    
    Also once it fails to create, cpufreq_governor_dbs returns with
    error code. The dbs_enable value is incremented which is not
    rolled back and cpufreq core tries to set the old
    governor. This might create further problems.
    
    Hence the removal of sysfs entry should happen in the critical
    path.
    
    CRs-fixed: 440969
    Change-Id: I98891756caf2ce0b85e39e9b4f58034af1ed97ce
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>
    
    Conflicts:
    
    	drivers/cpufreq/cpufreq_ondemand.c

commit 81e219be2002375aca75270349a65622c87c2f41
Author: faux123 <reioux@gmail.com>
Date:   Sun Dec 30 11:52:52 2012 -0800

    intellidemand: Don't update the policy->cur upon cpufreq driver failure
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit d9a6dbef83e8f11d9a0fd851357204831ab6581c
Author: faux123 <reioux@gmail.com>
Date:   Fri Dec 28 20:35:46 2012 -0800

    intellidemand: remove unused min performance lock defines
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 2025345d23d9f689f39534fae6b1c29db81a77bc
Author: faux123 <reioux@gmail.com>
Date:   Fri Dec 28 20:26:35 2012 -0800

    intellidemand: remove eco mode option (replaced by intelli_plug)
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 55adb017635db6c23733fd3ddcd2c9307d4abb4d
Author: faux123 <reioux@gmail.com>
Date:   Fri Dec 28 20:16:30 2012 -0800

    intellidemand: remove cpu cores autoplug capability (replaced by intelli_plug)
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	drivers/cpufreq/Kconfig

commit 993d4538ab525b8812d416500735488643ff368c
Author: faux123 <reioux@gmail.com>
Date:   Wed Dec 19 22:56:46 2012 -0800

    intellidemand: add default values for dbs sync and optimal tuneables
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit aff11d56da35e9de5d94fb78619400bbb3e24b1b
Author: Paul Reioux <reioux@gmail.com>
Date:   Wed Dec 19 20:43:57 2012 -0500

    intellidemand: give intellidemand's cpu_dbs_info its own name
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1dd0a6f74cd6ad44294c6fea9fabb1f623f3d182
Author: faux123 <reioux@gmail.com>
Date:   Tue Dec 18 23:07:55 2012 -0800

    intellidemand: Add support to consider other cpu load when scaling frequencies
    
    Currently only current cpu load is considered when frequency scaling.
    As a side effect of this when scheduler migrates a thread to a slower
    core, the execution time increases unexpectedly.
    
    The new algorithm not only considers current cpu load for frequency
    scaling, but also takes into account load of other online cpus. The
    treshold for this can be controlled using up_threshold_any_cpu_load.
    When the up_threshold_any_cpu_load is crossed in any cpu then the
    current cpu is set to sync_freq.
    
    Also, when more than one cpu is online a two step ramp-up mechanism
    is used. When the up_threshold_multi_core is crossed the cpu is ramped
    up to optimal_freq which is less than policy->max and set to a power
    friendly value.
    
    Change-Id: I64bed1b142cde1f239e73aa1f78981c78aaf59ce
    Signed-off-by: Narayanan Gopalakrishnan <nargop@codeaurora.org>
    
    intellidemand: Fix kernel warning in cpufreq_governor_dbs
    
    GOV_STOP event destroys the timer_mutex and is not initialized back
    upon GOV_START event.
    The Kernel complains at mutex_lock(during GOV_LIMITS event) after
    mutex_destroy.
    
    [<c0014918>] (unwind_backtrace+0x0/0x11c)
    [<c007c004>] (warn_slowpath_common+0x4c/0x64)
    [<c007c034>] (warn_slowpath_null+0x18/0x1c)
    [<c079eef8>] (__mutex_lock_slowpath+0x130/0x3c4)
    [<c079f1ac>] (mutex_lock+0x20/0x3c) from [<c0540900>]
    [<c0540900>] (cpufreq_governor_dbs+0x3c8/0x468)
    [<c053d500>] (__cpufreq_governor+0x90/0xe0)
    [<c053d774>] (__cpufreq_set_policy+0x224/0x258)
    [<c053da58>] (cpufreq_add_dev_interface+0x2b0/0x308)
    [<c053e784>] (cpufreq_add_dev+0x500/0x56c)
    [<c079247c>] (cpufreq_cpu_callback+0x88/0x9c)
    [<c07a2ca8>] (notifier_call_chain+0x38/0x68)
    [<c007ded0>] (__cpu_notify+0x28/0x40)
    [<c0790024>] (_cpu_up+0xe4/0x118)
    [<c0790024>] (cpu_up+0x64/0x80)
    [<c0765840>] (store_online+0x48/0x78)
    [<c038d0c8>] (dev_attr_store+0x18/0x24)
    [<c017be40>] (sysfs_write_file+0x108/0x13c)
    [<c012cdfc>] (vfs_write+0xb0/0x128)
    [<c012d034>](sys_write+0x38/0x64)
    
    Change-Id: Ie2a796b17209243de731ccf2ab04b7f9e7048df8
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>
    
    Adapted for Intellidemand from CAF reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 49797d79e21a0bab58bb6fc6fd55f22ef2478d65
Author: faux123 <reioux@gmail.com>
Date:   Tue Dec 18 22:43:52 2012 -0800

    cpufreq: Avoid using smp_processor_id() in preemptible context
    
    Even though this work item runs on only one cpu at a time (due to
    queue_work_on()) it is possible for the work item to be preempted
    and so use of smp_processor_id() is illegal.
    
    BUG: using smp_processor_id() in preemptible [00000000]
    code: kworker/3:1/4162 caller is dbs_refresh_callback+0xc/0x188
    [<c00151b0>] (unwind_backtrace+0x0/0x120) from [<c0279058>]
    (debug_smp_processor_id+0xbc/0xf0)
    [<c0279058>] (debug_smp_processor_id+0xbc/0xf0) from [<c0454b54>]
    (dbs_refresh_callback+0xc/0x188)
    [<c0454b54>] (dbs_refresh_callback+0xc/0x188) from [<c0087290>]
    (process_one_work+0x354/0x648)
    [<c0087290>] (process_one_work+0x354/0x648) from [<c0089754>]
    (worker_thread+0x1a8/0x2a8)
    [<c0089754>] (worker_thread+0x1a8/0x2a8) from [<c008e480>]
    (kthread+0x90/0xa0)
    [<c008e480>] (kthread+0x90/0xa0) from [<c000f438>]
    (kernel_thread_exit+0x0/0x8)
    
    The intent of the code is to determine which CPU this work item
    is running on, which we can easily do by passing that information
    in a wrapper struct around the work struct. Do this so we avoid
    this problem.
    
    Change-Id: I05ca0ff2b3cbaa239930463ea0760e3e9d75145f
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    adapted for Intellidemand from CAF reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 58962683c7a2ba1bc132b7afcc311f81155e32d8
Author: faux123 <reioux@gmail.com>
Date:   Sun Dec 16 13:11:51 2012 -0800

    Intellidemand: adjust the nr_run hysteresis for better UI response
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit f9730a3720ac878c3724fa5835bb796b2a02e429
Author: faux123 <reioux@gmail.com>
Date:   Sat Dec 8 00:03:05 2012 -0800

    intellidemand: tweak for not using min performance lock
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit f27678ef0ea104dd4716158edf6f37a118984575
Author: faux123 <reioux@gmail.com>
Date:   Fri Dec 7 22:49:17 2012 -0800

    cpufreq_limit: remove unused vars
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 9508ef9849e1eb5156fa3d67c163d2e3392dae3c
Author: faux123 <reioux@gmail.com>
Date:   Thu Dec 6 16:52:07 2012 -0600

    msm_cpufreq_limit: add GPL V2 licensing to access to GPL symbols
    
    fix Makefile to force no memory relocation
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit d066786e6392930476c3d4f4da8d3f3c36465491
Author: faux123 <reioux@gmail.com>
Date:   Thu Dec 6 07:54:34 2012 -0800

    msm_cpu_freq_limit: fixed coding copy and paste error :p
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 2a0bca711cabc042575eec805275db7e0f4c16d9
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:08:08 2013 -0700

    msm_cpu_freq_limit: initial coding for controlling MSM quadcore cpus
    
    this module will allow proper max frequency control across all msm cores
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/mach-msm/Kconfig
    	arch/arm/mach-msm/Makefile
    
    Conflicts:
    
    	arch/arm/mach-msm/Makefile
    
    Conflicts:
    	arch/arm/mach-msm/Kconfig
    	arch/arm/mach-msm/Makefile

commit 475aed9c33692ac14e9dba96f46b89cf567c7e5f
Author: faux123 <reioux@gmail.com>
Date:   Wed Dec 5 19:59:42 2012 -0600

    intellidemand: add eco mode (dual core operations) bump version to 4.1
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 594c766de0e2797c5e938a181af6dc5a227a4693
Author: faux123 <reioux@gmail.com>
Date:   Wed Dec 5 06:19:54 2012 -0800

    intellidemand: increase boost duration to 2.5 seconds from 1.5
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 777b93b42a529ab3f00e07ce6c10e2ea93a12c53
Author: faux123 <reioux@gmail.com>
Date:   Sun Dec 2 10:47:31 2012 -0800

    Intellidemand: put cores 1~3 to sleep while screen is off
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 14525d00513f79c4177445720fcb4c1a35494e7f
Author: faux123 <reioux@gmail.com>
Date:   Sun Nov 25 08:12:27 2012 -0800

    intellidemand: code derps clean up!
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 432b54bb2e2f72a53fa6c983ae25f62b628ae479
Author: faux123 <reioux@gmail.com>
Date:   Sat Nov 24 14:56:18 2012 -0800

    Intellidemand: update for quadcore operations. bump version to 4.0
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit f94d52e7ea0521f916860da5a13c0c5b47fc5b99
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:06:22 2013 -0700

    intellidemand: add minimum cpu performance lock
    
    a minimum cpu frequency performance lock is add to ensure smooth
    operations when only 1 core is active
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	drivers/cpufreq/Kconfig
    
    Conflicts:
    
    	drivers/cpufreq/Kconfig

commit edb80b8f44efe2b37b8982d611383e4047210d97
Author: faux123 <reioux@gmail.com>
Date:   Sat Nov 24 14:20:10 2012 -0800

    intellidemand: add a run queue persistence to determine browsing state
    
    persistence will allow for better browsing state detection rather than just
    purely based on run queue depth alone
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 81bb0bdbf5ff009d56513f5bbbfd7b1b3a42efb3
Author: faux123 <reioux@gmail.com>
Date:   Sat Nov 24 14:17:12 2012 -0800

    intellidemand: add bug fixes
    
     - Fix crash input event handler on governor switch
     - Fix panic when setting sampling rate
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 3baa6767ea23008c84f31676657dfa239fac89a6
Author: faux123 <reioux@gmail.com>
Date:   Sat Nov 24 14:15:27 2012 -0800

    intellidemand: Add boost pulse capability
    
    Boost pulse increases UI performance by increasing CPU freq and
    reduce sampling time.
    
    forward ported for Linux 3.4 for use on Mako
    Signed-off-by: faux123 <reioux@gmail.com>

commit dac40264976114f4e99b2db024603affd290eabd
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:04:53 2013 -0700

    cpufreq/cpufreq_intellidemand: intellidemand cpufreq governor (version 4.0)
    
    Intellidemand is inspired by ondemand governor. Its main features are:
    
     - auto cpu hotplugging based on runnable thread average calculations
     - detection of browsing mode (long screen on activities with moderate run
       queue depth =< 45 )
     - limit max frequencies when in browsing mode to conserve battery for long
       duration tasks
    
    Browsing State Determination:
     1- Browsing state is enabled as power-on default. Browsing state is disabled
        when run queue depth is > 45 (very high instantaneous spikes)
    
    Browsing State Limit Max Freq Logic (sliding window algorithm):
     1- While Screen is ON > 3 min, and average load of the CPU is < 35%, then
        CPU Max frequencies will remain power-on default
     2- While Screen is On, if average CPU load > 35%, both CPUs max frequencies
        will be limited to a lower user configurable max value
     3- While Screen is On, After both CPUs' Max is limited to lowered max, when
        the average CPU load has dropped down to less than < 25%, then both CPUs
        Max freq will be restored to back to power-on default
    
    Runnable Thread Auto-hotplug (fast hotplug / slow unplug algorithm):
     1- When average number of running thread is < threshold, 2nd cpu will be
        unplugged.
     2- When > threshold, 2nd cpu will be hotplugged.
     3- The logic applies a fast hotplug and slow unplug algorithm so 2nd core
        will stay enabled longer to help deal with performance spikes
    
    changes from Ver 2.0 -> Ver 3.0:
      - remove GPU Busy dependency for browsing state detection (this will make
        the governor more portable to other platforms without specific GPU
        architecture knowledge)
    
    dependencies:
      -- Qualcomm's run queue average logic (SOC independent logic)
      -- TI's runnable thread average logic (SOC independent logic)
    
    forward ported to Linux 3.4 for use on HTC One
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    
    	arch/arm/mach-msm/board-m7.c
    	arch/arm/mach-msm/cpufreq.c
    	drivers/cpufreq/Kconfig
    	drivers/cpufreq/cpufreq.c
    
    Conflicts:
    
    	arch/arm/mach-msm/board-jf_spr.c
    
    Conflicts:
    	drivers/cpufreq/Kconfig
    	drivers/cpufreq/Makefile
    	include/linux/cpufreq.h

commit e4c390f51130cd51ee4a3b563b29081f19f5dfc6
Author: Alex Frid <afrid@nvidia.com>
Date:   Wed May 16 14:27:13 2012 -0700

    proc: enhance time-average nr_running stats
    
    Add time-average nr_running to loadavg printout
    
    Bug 958978
    
    Change-Id: I5c6904efb52a86f4964eb66c1576fc91f60f5b1d
    Signed-off-by: Alex Frid <afrid@nvidia.com>
    (cherry picked from commit 86f3642cc44a69d1e4798719bd9182cd6923f526)
    Reviewed-on: http://git-master/r/111636
    Reviewed-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Tested-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>

commit d27f6994a6077d4067897ed17f7e0e7c073df38e
Author: faux123 <reioux@gmail.com>
Date:   Thu Nov 22 07:47:41 2012 -0800

    arm/crypto: Add optimized AES and SHA1 routines
    
    Add assembler versions of AES and SHA1 for ARM platforms.  This has provided
    up to a 50% improvement in IPsec/TCP throughout for tunnels using AES128/SHA1.
    
    Platform   CPU SPeed    Endian   Before (bps)   After (bps)   Improvement
    
    IXP425      533 MHz      big     11217042        15566294        ~38%
    KS8695      166 MHz     little    3828549         5795373        ~51%
    
    Signed-off-by: David McCullough <ucdevel@gmail.com>
    Signed-off-by: faux123 <reioux@gmail.com>

commit 71005eefa3e5ebefa9e0180d9b6e5fc67f620840
Author: faux123 <reioux@gmail.com>
Date:   Mon Mar 19 17:22:43 2012 -0700

    Optimized ARM RWSEM algorithm
    
    RWSEM implementation for ARM using atomic functions.
    Heavily based on arch/sh/include/asm/rwsem.h
    
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit 288c48f38295922e7a6e9cb6ff9ad53b8c11ac37
Author: faux123 <reioux@gmail.com>
Date:   Thu Nov 22 07:28:42 2012 -0800

    lib/string: use glibc version
    
    the performance of memcpy and memmove of the general version is very
    inefficient, this patch improved them.
    
    Signed-off-by: Miao Xie <miaox*******>
    Signed-off-by: faux123 <reioux@gmail.com>

commit de8f52df8954bbb7d15480060343a3efbabf6225
Author: faux123 <reioux@gmail.com>
Date:   Thu Nov 22 07:25:02 2012 -0800

    lib/memcopy: use glibc version
    
    the kernel's memcpy and memmove is very inefficient. But the glibc version is
    quite fast, in some cases it is 10 times faster than the kernel version. So I
    introduce some memory copy macros and functions of the glibc to improve the
    kernel version's performance.
    
    The strategy of the memory functions is:
    1. Copy bytes until the destination pointer is aligned.
    2. Copy words in unrolled loops.  If the source and destination are not
       aligned in the same way, use word memory operations, but shift and merge
       two read words before writing.
    3. Copy the few remaining bytes.
    
    Signed-off-by: Miao Xie <miaox*******>
    Signed-off-by: faux123 <reioux@gmail.com>

commit 1f1c0e9b026e06ab94d259c72637a2adb58db091
Author: faux123 <reioux@gmail.com>
Date:   Thu Jan 10 14:18:34 2013 -0600

    scripts: remove localversion "+" from kernel version strings
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 9da3b3cf9f3f069a9670ad7cc70ccb4855e9d0a7
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 09:35:01 2013 -0700

    zram_drv: sync with kernel.org
    
    sync point @ 8f5f90a872c38b4e78f3cc95e8a25434b98e4db2
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 531960c33ced35f687f9dd78952f0a7c7a825268
Author: Davidlohr Bueso <davidlohr.bueso@hp.com>
Date:   Tue Jan 1 21:24:13 2013 -0800

    staging: zram: simplify num_devices paramater
    
    Simplify dealing with num_devices when initializing zram.
    Also cleanup some of the output messages.
    
    Signed-off-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e044b5f123f65b3e6c6b45ce093f66986ce46e4e
Author: Nitin Gupta <ngupta@vflare.org>
Date:   Wed Jan 2 08:53:41 2013 -0800

    staging: zram: fix invalid memory references during disk write
    
    Fixes a bug introduced by commit c8f2f0db1 ("zram: Fix handling
    of incompressible pages") which caused invalid memory references
    during disk write. Invalid references could occur in two cases:
     - Incoming data expands on compression: In this case, reference was
    made to kunmap()'ed bio page.
     - Partial (non PAGE_SIZE) write with incompressible data: In this
    case, reference was made to a kfree()'ed buffer.
    
    Fixes bug 50081:
    https://bugzilla.kernel.org/show_bug.cgi?id=50081
    
    Signed-off-by: Nitin Gupta <ngupta@vflare.org>
    Cc: stable <stable@vger.kernel.org>
    Reported-by: Mihail Kasadjikov <hamer.mk@gmail.com>
    Reported-by: Tomas M <tomas@slax.org>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 43ce2d0b99f23c072aab41decbbaeed19db0fdf1
Author: Masanari Iida <standby24x7@gmail.com>
Date:   Mon Jan 7 23:28:10 2013 +0900

    staging: Add angle bracket before and after the URL
    
    Add missing angle bracket before and after the URL.
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 56b2ec351a1922068ed92122ac1d084386d462e6
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Tue Oct 30 22:42:31 2012 +0300

    staging: zram: handle mem suffixes in disk size zram_sysfs parameter
    
    Use memparse() to allow mem suffixes in disksize sysfs number.
    Examples:
        echo 256K > /sys/block/zram0/disksize
        echo 512M > /sys/block/zram0/disksize
        echo 1G > /sys/block/zram0/disksize
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reviewed-by: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 10333a818175c8761bd47bf425d024a84f11a565
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 08:56:29 2013 -0700

    staging: zram: factor-out zram_decompress_page() function
    
    zram_bvec_read() shared decompress functionality with zram_read_before_write() function.
    Factor-out and make commonly used zram_decompress_page() function, which also simplified
    error handling in zram_bvec_read().
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reviewed-by: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    
    	drivers/staging/zram/zram_drv.c

commit b41dbb6c11eeaa1e0003f130c85d2ca2bd3eb34d
Author: Nitin Gupta <ngupta@vflare.org>
Date:   Wed Oct 10 17:42:18 2012 -0700

    staging: zram: Fix handling of incompressible pages
    
    Change 130f315a (staging: zram: remove special handle of uncompressed page)
    introduced a bug in the handling of incompressible pages which resulted in
    memory allocation failure for such pages.
    
    When a page expands on compression, say from 4K to 4K+30, we were trying to
    do zsmalloc(pool, 4K+30). However, the maximum size which zsmalloc can
    allocate is PAGE_SIZE (for obvious reasons), so such allocation requests
    always return failure (0).
    
    For a page that has compressed size larger than the original size (this may
    happen with already compressed or random data), there is no point storing
    the compressed version as that would take more space and would also require
    time for decompression when needed again. So, the fix is to store any page,
    whose compressed size exceeds a threshold (max_zpage_size), as-it-is i.e.
    without compression.  Memory required for storing this uncompressed page can
    then be requested from zsmalloc which supports PAGE_SIZE sized allocations.
    
    Lastly, the fix checks that we do not attempt to "decompress" the page which
    we stored in the uncompressed form -- we just memcpy() out such pages.
    
    Signed-off-by: Nitin Gupta <ngupta@vflare.org>
    Reported-by: viechweg@gmail.com
    Reported-by: paerley@gmail.com
    Reported-by: wu.tommy@gmail.com
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 542d227d42bfa0dbc0141b5c03f285650396c0cd
Author: Minchan Kim <minchan@kernel.org>
Date:   Wed Oct 10 08:49:52 2012 +0900

    staging: zram: correct obsolete comment on max_zpage_size
    
    Zram doesn't use xv_malloc any more so it doesn't have
    limitation about zobj_header.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4f64176be91b8c2fb3fbd26ccde634a1315c6d02
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 08:52:42 2013 -0700

    staging: zsmalloc: add mapping modes
    
    This patch improves mapping performance in zsmalloc by getting
    usage information from the user in the form of a "mapping mode"
    and using it to avoid unnecessary copying for objects that span
    pages.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    
    	drivers/staging/zram/zram_drv.c

commit f3b4f5d5d845822b71b501e16c220423d56bd631
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 08:50:00 2013 -0700

    staging: zram/zcache: swtich Kconfig dependency from X86 to ZSMALLOC
    
    This patch switches zcache and zram dependency to ZSMALLOC
    rather than X86.  There is no net change since ZSMALLOC
    depends on X86, however, this prevent further changes to
    these files as zsmalloc dependencies change.
    
    Signed-off-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    
    	drivers/staging/zram/Kconfig

commit 6b9d1ff50d236bc0c1e431c817db67aba1d15c9b
Author: Sam Hansen <solid.se7en@gmail.com>
Date:   Thu Jun 7 16:03:48 2012 -0700

    staging: zram: conventions, __aligned() attribute
    
    Using the __aligned() attribute in favor of __attribute__((aligned(size)))
    
    Signed-off-by: Sam Hansen <solid.se7en@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 61d1818984ff179601a211d894feb9eebee06a40
Author: Sam Hansen <solid.se7en@gmail.com>
Date:   Thu Jun 7 16:03:47 2012 -0700

    staging: zram: conventions pr_warning -> pr_warn()
    
    Porting zram to use the pr_warn() function instead of the deprecated
    pr_warning().
    
    Signed-off-by: Sam Hansen <solid.se7en@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a8e54b92af3e9926fb5cbb20af550aa3991dd1cd
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 08:40:38 2013 -0700

    staging: zram: remove special handle of uncompressed page
    
    xvmalloc can't handle PAGE_SIZE page so that zram have to
    handle it specially but zsmalloc can do it so let's remove
    unnecessary special handling code.
    
    Quote from Nitin
    "I think page vs handle distinction was added since xvmalloc could not
    handle full page allocation. Now that zsmalloc allows full page
    allocation, we can just use it for both cases. This would also allow
    removing the ZRAM_UNCOMPRESSED flag. The only downside will be slightly
    slower code path for full page allocation but this event is anyways
    supposed to be rare, so should be fine."
    
    1. This patch reduces code very much.
    
     drivers/staging/zram/zram_drv.c   |  104 +++++--------------------------------
     drivers/staging/zram/zram_drv.h   |   17 +-----
     drivers/staging/zram/zram_sysfs.c |    6 +--
     3 files changed, 15 insertions(+), 112 deletions(-)
    
    2. change pages_expand with bad_compress so it can count
       bad compression(above 75%) ratio.
    
    3. remove zobj_header which is for back-reference for defragmentation
       because firstly, it's not used at the moment and zsmalloc can't handle
       bigger size than PAGE_SIZE so zram can't do it any more without redesign.
    
    Cc: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    
    	drivers/staging/zram/zram_drv.c
    modified for SGS 4
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 0848296a1bacd682cf15c969be13e1157b0ce917
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 08:34:23 2013 -0700

    staging: zram: fix random data read
    
    fd1a30de makes a bug that it uses (struct page *) as zsmalloc's handle
    although it's a uncompressed page so that it can access random page,
    return random data or even crashed by get_first_page in zs_map_object.
    
    Cc: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    
    	drivers/staging/zram/zram_drv.c
    Modified for SGS4 Kernel
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit ba99ae99de6a8b863c9885602da36d94e31f9735
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 08:26:30 2013 -0700

    zram_drv: fix merge error from c234434835b1f4bad9bdbae6710044cba387c9e5
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit fa42c40aeb6cca812df79ab3a471cfa440638660
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 08:20:01 2013 -0700

    staging: zsmalloc: zsmalloc: use unsigned long instead of void *
    
    We should use unsigned long as handle instead of void * to avoid any
    confusion. Without this, users may just treat zs_malloc return value as
    a pointer and try to deference it.
    
    This patch passed compile test(zram, zcache and ramster) and zram is
    tested on qemu.
    
    changelog
      * from v2
    	- remove hval pointed out by Nitin
    	- based on next-20120607
      * from v1
    	- change zcache's zv_create return value
    	- baesd on next-20120604
    
    Cc: Dan Magenheimer <dan.magenheimer@oracle.com>
    Acked-by: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    
    	drivers/staging/zsmalloc/zsmalloc-main.c
    	drivers/staging/zsmalloc/zsmalloc.h

commit f6bd1165f60d68d137cdac4eeab46d61a9e4944a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 2 19:35:14 2014 +0100

    Fixing previous commits part 2!

commit ecc7794712484ae3ecb76624379b9a5e0127e581
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 2 18:53:48 2014 +0100

    Fixing previous commits!

commit 2c679e09386beca442e8958ce28744ed2671a191
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 24 14:08:51 2014 +0100

    Linux 3.4.82

commit cb59f3038e6e36f93ec5778a46c067531674481b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 21 21:41:42 2014 +0100

    Linux 3.4.81
    
    Conflicts:
    	mm/internal.h

commit 85a46a7160f3a3f2476795d74bb9d2a43c4f6530
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 14 20:44:16 2014 +0100

    Linux 3.4.80
    
    Conflicts:
    	kernel/sched/core.c
    	kernel/sched/sched.h

commit 0a3a452bd3bca88e8c982ae7bbd1e59e6102746c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 9 12:52:12 2014 +0100

    Linux 3.4.79

commit a0e6aa9f949e141eac370a559348fabbfad2442a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 30 11:16:02 2014 +0100

    Linux 3.4.78

commit a1438ea2868ba42d3bc9b1d826feebcb98aa4067
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:06:13 2014 +0100

    Linux 3.4.77

commit 10b64516e0c8e4a9bca8f65ad741d036f10a1e25
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:05:56 2014 +0100

    Linux 3.4.76
    
    Conflicts:
    	drivers/gpio/gpio-msm-v2.c

commit 77c0991af15013d681f4a76c0a5575004caf4bf3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:05:31 2014 +0100

    Linux 3.4.75

commit 9a19a7a8bde78d29d9a721c51fc935f7baaa7649
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:05:15 2014 +0100

    Linux 3.4.74
    
    Conflicts:
    	drivers/net/ethernet/smsc/smc91x.h

commit 6841b92db1bbca46864759d52caba83eee126338
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:04:57 2014 +0100

    Linux 3.4.73
    
    Conflicts:
    	drivers/md/dm-crypt.c
    	drivers/mmc/card/block.c
    	net/ipv4/ping.c

commit 3af047ec753091641b75df9a1239f9978ee59d52
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:04:35 2014 +0100

    Linux 3.4.72

commit a6dc055172655f1fbf8a9004557b0797f9e4651d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:04:16 2014 +0100

    Linux 3.4.71

commit 48f09e460d5a511b672e3163d240d8eb0dce8a47
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:03:58 2014 +0100

    Linux 3.4.70

commit 7ebd578dbef64378e28b27d146ef0b12f140af83
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:03:40 2014 +0100

    Linux 3.4.69

commit 7a44a73cf36d38e4c134a21109d0569638c8b27e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:03:24 2014 +0100

    Linux 3.4.68

commit ebfbc03accd3aa131423bfb68e665dccd24dee32
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:03:07 2014 +0100

    Linux 3.4.67

commit b673ad614272f1de027eba39980effcb7b735c9f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:00:24 2014 +0100

    Linux 3.4.66
    
    Conflicts:
    	net/bluetooth/hci_event.c

commit 08e6585d1a916e2d48e73c83fc05537e24498dbc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 21:00:09 2014 +0100

    Linux 3.4.65

commit 6260c7db7434d4b06484e309966155eee99a80c4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:59:53 2014 +0100

    Linux 3.4.64

commit 83d665d41a93831810839c74800e7e79a0b4eca2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:59:32 2014 +0100

    Linux 3.4.63
    
    Conflicts:
    	drivers/usb/host/xhci-plat.c
    	drivers/usb/host/xhci.h

commit 9beba0314c041c95f891c9f2b36f82a44b62ae4b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:59:16 2014 +0100

    Linux 3.4.62

commit dc6ad4fa59a06a9667e832bace6d2ce85f36a991
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:58:56 2014 +0100

    Linux 3.4.61

commit 3690c372c5b07101e80f92fed32015921ce11a28
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:58:37 2014 +0100

    Linux 3.4.60
    
    Conflicts:
    	kernel/workqueue.c

commit 11cec3830ba79b34f4c97dd60e07225a01ae4fa7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:58:07 2014 +0100

    Linux 3.4.59
    
    Conflicts:
    	arch/arm/kernel/perf_event.c

commit 5992b422abe18976a7ca8d81721f469a233bb747
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:57:48 2014 +0100

    Linux 3.4.58

commit 3e9a97b1415c82304d7e621732d6445ea551eb38
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:57:29 2014 +0100

    Linux 3.4.57

commit d2c20a3f3eda86e4a07a33b025a34cce587c082f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:57:13 2014 +0100

    Linux 3.4.56
    
    Conflicts:
    	drivers/virtio/virtio_ring.c
    	include/linux/virtio.h

commit ffad16553abf831e7ce910e21b2b8b4350eb57b1
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:56:54 2014 +0100

    Linux 3.4.55

commit bf37d63db59d0e9387420bb40c0d3699f4c9dc35
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:56:34 2014 +0100

    Linux 3.4.54

commit 8bf706b29dc165e49165f0b45617f2e59c21a008
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:56:16 2014 +0100

    Linux 3.4.53
    
    Conflicts:
    	crypto/algapi.c

commit d213190fa430c0f39c6bc105b573a8e9e651eeb3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:55:53 2014 +0100

    Linux 3.4.52
    
    Conflicts:
    	net/bluetooth/l2cap_core.c

commit c3105f1d72ded2b0040fbbc2c1338a11729b9b56
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:55:35 2014 +0100

    Linux 3.4.51

commit c9d0c31f7def84f13072899c5a1b33bf9ffa0c31
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:55:10 2014 +0100

    Linux 3.4.50
    
    Conflicts:
    	include/net/bluetooth/hci_core.h
    	include/net/bluetooth/mgmt.h
    	net/bluetooth/hci_core.c
    	net/bluetooth/mgmt.c

commit e1bc35b2fbbd721aa39201e9273a478e90b8e553
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:54:53 2014 +0100

    Linux 3.4.49

commit 88f044583ae29ed2e65b2d67131f743e245f7662
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:54:35 2014 +0100

    Linux 3.4.48

commit be1acf26af2a05c31ee876c1b3d2f7c06d955409
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:54:15 2014 +0100

    Linux 3.4.47

commit 55421855ce9c623150db1674d390bf7faffc0a29
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:53:51 2014 +0100

    Linux 3.4.46
    
    Conflicts:
    	kernel/timer.c

commit e7896a7ac957626f415c47fc96cfa8c6ba50f757
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:53:31 2014 +0100

    Linux 3.4.45

commit 93c60881d63ba6cdf6d790aa104a6d9bbc94c65f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:53:11 2014 +0100

    Linux 3.4.44
    
    Conflicts:
    	fs/jbd2/commit.c

commit 21ad671b3da954ee96e59bece1359f5176274d90
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:52:52 2014 +0100

    Linux 3.4.43

commit 1586e4031c8731695b29ab0f7ad2a6331aea4f9e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:52:33 2014 +0100

    Linux 3.4.42
    
    Conflicts:
    	arch/arm/kernel/perf_event.c
    	kernel/hrtimer.c

commit 3f54f8a1596ba220f60a834abdf3d1a09ea49d89
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:52:10 2014 +0100

    Linux 3.4.41
    
    Conflicts:
    	include/linux/kref.h

commit fd51b535043ca17cecd9d748b1baf30176fc7aa4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:51:39 2014 +0100

    Linux 3.4.40

commit 5953b7ffb0fa11004030994eb980187d7042a6b0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:51:19 2014 +0100

    Linux 3.4.39
    
    Conflicts:
    	drivers/bluetooth/ath3k.c
    	drivers/bluetooth/btusb.c

commit 2d5092b3407620d17e5e5bef83180f56a8c32fe5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:50:59 2014 +0100

    Linux 3.4.38

commit f0e5cab64dee6f620b7d8015b006c7dfbd272b11
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:50:39 2014 +0100

    Linux 3.4.37

commit 984a93f6efe9d03c03891c171df44dcf71ede673
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:50:18 2014 +0100

    Linux 3.4.36

commit 007bcc3b71ee0fd282685a9525db8776b6b35e1a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:49:31 2014 +0100

    Linux 3.4.35
    
    Conflicts:
    	kernel/cgroup.c

commit 955c4f68a48a6c82266929bf1604bfba1e1b7ddd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:48:14 2014 +0100

    Linux 3.4.34
    
    Conflicts:
    	net/ipv4/ping.c

commit 910c7474796fbb630d9472b5e7cc4c4208c2752c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:47:50 2014 +0100

    Linxu 3.4.33

commit 28f2bb39bdb136462d7050e8d336606115d5c6ea
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:47:30 2014 +0100

    Linux 3.4.32

commit 7e857763ffb3fba110becdf10fa1b23fbacd828a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:47:09 2014 +0100

    Linux 3.4.31

commit 29767183d9603df6b33a457fb2e0704c5d502ba2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:43:00 2014 +0100

    Linux 3.4.30

commit 8a00f50033aade7d3659788acf1aa9c0152df6b8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:42:41 2014 +0100

    Linux 3.4.29
    
    Conflicts:
    	arch/arm/mm/dma-mapping.c

commit 7c21508b21c6627f3d8a3ad6eeb85023efa94883
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:42:24 2014 +0100

    Linux 3.4.28

commit 17e0861abd0e856a6ad0b2514d19c1de969efdbc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:41:45 2014 +0100

    Linux 3.4.27

commit 21f16eb23eaecc8980cacab443e5ae96c9451767
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:41:21 2014 +0100

    Linux 3.4.26
    
    Conflicts:
    	drivers/gpu/drm/radeon/radeon_mode.h
    	drivers/usb/core/message.c

commit 42e205258db40e1e071e612abc8a6b032f7742da
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:40:55 2014 +0100

    Linux 3.4.25
    
    Conflicts:
    	drivers/bluetooth/ath3k.c
    	drivers/bluetooth/btusb.c
    	net/bluetooth/rfcomm/sock.c

commit 24b20388bc59941bd735e362c64b0a9151d230d4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:40:25 2014 +0100

    Linux 3.4.24

commit 819ba50ffb99f85f79aa1dc2c718e65312181028
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:40:02 2014 +0100

    Linux 3.4.23

commit 8e2218fd4880c6cf2d9557740daec7eabe7999d8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:39:41 2014 +0100

    Linux 3.4.22

commit c402b12348cdc88073adefc00b2f453a83354b5f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:39:17 2014 +0100

    Linux 3.4.21
    
    Conflicts:
    	fs/ubifs/dir.c
    	net/bluetooth/l2cap_core.c

commit 45c12249322b9ad914aabfe65122b5de7121f2f0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:37:31 2014 +0100

    Linux 3.4.20
    
    Conflicts:
    	fs/ecryptfs/main.c

commit 572a2812939c7e5bf066f116d8dd1a89af66bae3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:37:03 2014 +0100

    Linux 3.4.19

commit 08a48c25086261df0bc590c4930b7cf732a0bd1f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:36:38 2014 +0100

    Linux 3.4.18

commit 531fd2e1a866c7a3a605847ca1d16fb0a559c951
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:36:11 2014 +0100

    Linux 3.4.17
    
    Conflicts:
    	net/bluetooth/smp.c

commit 2bfb417bce7952566e17422025c2996796c93205
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:35:40 2014 +0100

    Linux 3.4.16

commit 6bd81d4cb7b859230b22e34254a0f196df78e172
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:35:17 2014 +0100

    Linux 3.4.15
    
    Conflicts:
    	arch/arm/Kconfig
    	fs/ecryptfs/file.c
    	fs/ecryptfs/inode.c
    	fs/ecryptfs/mmap.c

commit 962e32cbf8c7e32754cbf959be3548ff9bc8b7f4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:34:39 2014 +0100

    Linux 3.4.14

commit 635a45fa9dbf0eaf02e9d80ae9022fcbc2cffa83
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:34:07 2014 +0100

    Linux 3.4.13
    
    Conflicts:
    	drivers/usb/host/xhci.h

commit 5ea4f8b6f29b2c1ce195b8c8383a6db637d1477e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:33:10 2014 +0100

    Linux 3.4.12
    
    Conflicts:
    	arch/arm/include/asm/mutex.h
    	arch/arm/kernel/traps.c
    	drivers/bluetooth/btusb.c
    	drivers/mmc/card/block.c
    	drivers/usb/host/xhci.h
    	net/bluetooth/hci_conn.c
    	net/bluetooth/l2cap_core.c
    	net/bluetooth/mgmt.c
    	net/bluetooth/smp.c

commit 07dfb998ab00c96ccedf472e3996a4e364fe83ec
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:32:01 2014 +0100

    Linux 3.4.11
    
    Conflicts:
    	arch/arm/mm/tlb-v7.S
    	arch/arm/vfp/vfpmodule.c
    	net/bluetooth/hci_event.c

commit e17091d5b744214c376f2ad7cc0807bf56d89d81
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:31:26 2014 +0100

    Linux 3.4.10
    
    Conflicts:
    	drivers/usb/host/xhci.h

commit 6c888d7998bf6623f10f94e059f8809c82429bf0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:30:43 2014 +0100

    Linux 3.4.9
    
    Conflicts:
    	arch/arm/mm/tlb-v7.S
    	arch/arm/vfp/entry.S
    	drivers/usb/core/hub.c

commit 5ea68d193c9c608dd17ee2c75569fd50b2c29fa3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:30:19 2014 +0100

    Linux 3.4.8
    
    Conflicts:
    	drivers/base/power/main.c
    	drivers/mmc/host/sdhci.c
    	drivers/net/tun.c
    	drivers/staging/zsmalloc/zsmalloc-main.c
    	kernel/power/suspend.c

commit ea644e7754f32a321d866f570f9722e1e0ca9042
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 2 15:41:34 2014 +0100

    Kernel test version!

commit ac8bd4b64f68704255ef4de7a1779334f6062f47
Author: Lee Susman <lsusman@codeaurora.org>
Date:   Sun May 5 17:31:17 2013 +0300

    mm: pass readahead info down to the i/o scheduler
    
    Some i/o schedulers (i.e. row-iosched, cfq-iosched) deploy an idling
    algorithm in order to be better synced with the readahead algorithm.
    Idling is a prediction algorithm for incoming read requests.
    
    In this patch we mark pages which are part of a readahead window, by
    setting a newly introduced flag. With this flag, the i/o scheduler can
    identify a request which is associated with a readahead page. This
    enables the i/o scheduler's idling mechanism to be en-sync with the
    readahead mechanism and, in turn, can increase read throughput.
    
    Change-Id: I0654f23315b6d19d71bcc9cc029c6b281a44b196
    Signed-off-by: Lee Susman <lsusman@codeaurora.org>

commit e52367a32bd315e087905991164b1822ec640d74
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sun Jul 7 17:33:05 2013 +0300

    block: Add URGENT request notification support to CFQ scheduler
    
    When the scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. This is done
    in order to reduce the latency of an urgent request. For example:
    long WRITE may be stopped to handle an urgent READ.
    
    Change-Id: I3072b8a1423870fed9c04c28d93caaf9557a7b89
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit c6fb591c994ce5c0569b3e146af38f712e520df4
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu May 16 14:36:58 2013 +0300

    block: Remove "requeuing urgent req" error messages
    
    It is possible for URGENT request to be requeued/reinserted if it was
    fetched during the creation of a packed list. This end case is rare and is
    not handled at the moment.
    This patch changes the messages notifying of the above to debug level
    (instead of error) in order to clear the dmesg log.
    
    Change-Id: Ie8bc067e61559a6f702077b95c5dbcc426404232
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 66888f0a886c745e9f629b15bb583640366cfabf
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Wed May 1 14:35:20 2013 +0300

    block: urgent: Fix dispatching of URGENT mechanism
    
    There are cases when blk_peek_request is called not from blk_fetch_request
    thus the URGENT request may be started but the flag q->dispatched_urgent is
    not updated.
    
    Change-Id: I4fb588823f1b2949160cbd3907f4729767932e12
    CRs-fixed: 471736
    CRs-fixed: 473036
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit bdc49321fa11fbf2e1a05bb1ad53cea94718b5bd
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Sep 20 10:46:10 2012 +0300

    block: Adding ROW scheduling algorithm
    
    This patch adds the implementation of a new scheduling algorithm - ROW.
    The policy of this algorithm is to prioritize READ requests over WRITE
    as much as possible without starving the WRITE requests.
    
    Change-Id: I4ed52ea21d43b0e7c0769b2599779a3d3869c519
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: ROW: Correct minimum values of ROW tunable parameters
    
    The ROW scheduling algorithm exposes several tunable parameters.
    This patch updates the minimum allowed values for those parameters.
    
    Change-Id: I5ec19d54b694e2e83ad5376bd99cc91f084967f5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: ROW: Fix forced dispatch
    
    This patch fixes forced dispatch in the ROW scheduling algorithm.
    When the dispatch function is called with the forced flag on, we
    can't delay the dispatch of the requests that are in scheduler queues.
    Thus, when dispatch is called with forced turned on, we need to cancel
    idling, or not to idle at all.
    
    Change-Id: I3aa0da33ad7b59c0731c696f1392b48525b52ddc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: Add support for reinsert a dispatched req
    
    Add support for reinserting a dispatched request back to the
    scheduler's internal data structures.
    This capability is used by the device driver when it chooses to
    interrupt the current request transmission and execute another (more
    urgent) pending request. For example: interrupting long write in order
    to handle pending read. The device driver re-inserts the
    remaining write request back to the scheduler, to be rescheduled
    for transmission later on.
    
    Add API for verifying whether the current scheduler
    supports reinserting requests mechanism. If reinsert mechanism isn't
    supported by the scheduler, this code path will never be activated.
    
    Change-Id: I5c982a66b651ebf544aae60063ac8a340d79e67f
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: Add API for urgent request handling
    
    This patch add support in block & elevator layers for handling
    urgent requests. The decision if a request is urgent or not is taken
    by the scheduler. Urgent request notification is passed to the underlying
    block device driver (eMMC for example). Block device driver may decide to
    interrupt the currently running low priority request to serve the new
    urgent request. By doing so READ latency is greatly reduced in read&write
    collision scenarios.
    
    Note that if the current scheduler doesn't implement the urgent request
    mechanism, this code path is never activated.
    
    Change-Id: I8aa74b9b45c0d3a2221bd4e82ea76eb4103e7cfa
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    row: Adding support for reinsert already dispatched req
    
    Add support for reinserting already dispatched request back to the
    schedulers internal data structures.
    The request will be reinserted back to the queue (head) it was
    dispatched from as if it was never dispatched.
    
    Change-Id: I70954df300774409c25b5821465fb3aa33d8feb5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block:row: fix idling mechanism in ROW
    
    This patch addresses the following issues found in the ROW idling
    mechanism:
    1. Fix the delay passed to queue_delayed_work (pass actual delay
       and not the time when to start the work)
    2. Change the idle time and the idling-trigger frequency to be
       HZ dependent (instead of using msec_to_jiffies())
    3. Destroy idle_workqueue() in queue_exit
    
    Change-Id: If86513ad6b4be44fb7a860f29bd2127197d8d5bf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    cfq-iosched: Fix null pointer dereference
    
    NULL pointer dereference can happen in cfq_choose_cfqg()
    when there are no cfq groups to select other than the
    current serving group. Prevent this by adding a NULL
    check before dereferencing.
    
    Unable to handle kernel NULL pointer dereference at virtual address
    [<c02502cc>] (cfq_dispatch_requests+0x368/0x8c0) from
    [<c0243f30>] (blk_peek_request+0x220/0x25c)
    [<c0243f30>] (blk_peek_request+0x220/0x25c) from
    [<c0243f74>] (blk_fetch_request+0x8/0x1c)
    [<c0243f74>] (blk_fetch_request+0x8/0x1c) from
    [<c041cedc>] (mmc_queue_thread+0x58/0x120)
    [<c041cedc>] (mmc_queue_thread+0x58/0x120) from
    [<c00ad310>] (kthread+0x84/0x90)
    [<c00ad310>] (kthread+0x84/0x90) from
    [<c000eeac>] (kernel_thread_exit+0x0/0x8)
    
    CRs-Fixed: 416466
    Change-Id: I1fab93a4334b53e1d7c5dcc8f93cff174bae0d5e
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>
    
    row: Add support for urgent request handling
    
    This patch adds support for handling urgent requests.
    ROW queue can be marked as "urgent" so if it was un-served in last
    dispatch cycle and a request was added to it - it will trigger
    issuing an urgent-request-notification to the block device driver.
    The block device driver may choose at stop the transmission of current
    ongoing request to handle the urgent one. Foe example: long WRITE may
    be stopped to handle an urgent READ. This decreases READ latency.
    
    Change-Id: I84954c13f5e3b1b5caeadc9fe1f9aa21208cb35e
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Add some debug information on ROW queues
    
    1. Add a counter for number of requests on queue.
    2. Add function to print queues status (number requests
       currently on queue and number of already dispatched requests
       in current dispatch cycle).
    
    Change-Id: I1e98b9ca33853e6e6a8ddc53240f6cd6981e6024
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Insert dispatch_quantum into struct row_queue
    
    There is really no point in keeping the dispatch quantum
    of a queue outside of it. By inserting it to the row_queue
    structure we spare extra level in accessing it.
    
    Change-Id: Ic77571818b643e71f9aafbb2ca93d0a92158b199
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: fix sysfs functions - idle_time conversion
    
    idle_time was updated to be stored in msec instead of jiffies.
    So there is no need to convert the value when reading from user or
    displaying the value to him.
    
    Change-Id: I58e074b204e90a90536d32199ac668112966e9cf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Aggregate row_queue parameters to one structure
    
    Each ROW queues has several parameters which default values are defined
    in separate arrays. This patch aggregates all default values into one
    array.
    The values in question are:
     - is idling enabled for the queue
     - queue quantum
     - can the queue notify on urgent request
    
    Change-Id: I3821b0a042542295069b340406a16b1000873ec6
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Dispatch requests according to their io-priority
    
    This patch implements "application-hints" which is a way the issuing
    application can notify the scheduler on the priority of its request.
    This is done by setting the io-priority of the request.
    This patch reuses an already existing mechanism of io-priorities developed
    for CFQ. Please refer to kernel/Documentation/block/ioprio.txt for
    usage example and explanations.
    
    Change-Id: I228ec8e52161b424242bb7bb133418dc8b73925a
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Idling mechanism re-factoring
    
    At the moment idling in ROW is implemented by delayed work that uses
    jiffies granularity which is not very accurate. This patch replaces
    current idling mechanism implementation with hrtime API, which gives
    nanosecond resolution (instead of jiffies).
    
    Change-Id: I86c7b1776d035e1d81571894b300228c8b8f2d92
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Don't notify URGENT if there are un-completed urgent req
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. If the current
    transmitted request is an urgent request - we don't want it to be
    stopped.
    Due to the above ROW scheduler won't notify of an urgent request if
    there are urgent requests in flight.
    
    Change-Id: I2fa186d911b908ec7611682b378b9cdc48637ac7
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Update initial values of ROW data structures
    
    This patch sets the initial values of internal ROW
    parameters.
    
    Change-Id: I38132062a7fcbe2e58b9cc757e55caac64d013dc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    [smuckle@codeaurora.org: ported from msm-3.7]
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>
    
    block: add REQ_URGENT to request flags
    
    This patch adds a new flag to be used in cmd_flags field of struct request
    for marking request as urgent.
    Urgent request is the one that should be given priority currently handled
    (regular) request by the device driver. The decision of a request urgency
    is taken by the scheduler.
    
    Change-Id: Ic20470987ef23410f1d0324f96f00578f7df8717
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Re-design urgent request notification mechanism
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. This is done
    in order to reduce the latency of an urgent request. For example:
    long WRITE may be stopped to handle an urgent READ.
    
    This patch updates the ROW URGENT notification policy to apply with the
    below:
    
    - Don't notify URGENT if there is an un-completed URGENT request in driver
    - After notifying that URGENT request is present, the next request
      dispatched is the URGENT one.
    - At every given moment only 1 request can be marked as URGENT.
      Independent of it's location (driver or scheduler)
    
    Other changes to URGENT policy:
    - Only READ queues are allowed to notify of an URGENT request pending.
    
    CR fix:
    If a pending urgent request (A) gets merged with another request (B)
    A is removed from scheduler queue but is not removed from
    rd->pending_urgent_rq.
    
    CRs-Fixed: 453712
    Change-Id: I321e8cf58e12a05b82edd2a03f52fcce7bc9a900
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: urgent request: remove unnecessary urgent marking
    
    An urgent request is marked by the scheduler in rq->cmd_flags with the
    REQ_URGENT flag. There is no need to add an additional marking by
    the block layer.
    
    Change-Id: I05d5e9539d2f6c1bfa80240b0671db197a5d3b3f
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Prevent starvation of regular priority by high priority
    
    At the moment all REGULAR and LOW priority requests are starved as long as
    there are HIGH priority requests to dispatch.
    This patch prevents the above starvation by setting a starvation limit the
    REGULAR\LOW priority requests can tolerate.
    
    Change-Id: Ibe24207982c2c55d75c0b0230f67e013d1106017
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Update sysfs functions
    
    All ROW (time related) configurable parameters are stored in ms so there
    is no need to convert from/to ms when reading/updating them via sysfs.
    
    Change-Id: Ib6a1de54140b5d25696743da944c076dd6fc02ae
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: urgent request: Update dispatch_urgent in case of requeue/reinsert
    
    The block layer implements a mechanism for verifying that the device
    driver won't be notified of an URGENT request if there is already an
    URGENT request in flight. This is due to the fact that interrupting an
    URGENT request isn't efficient.
    This patch fixes the above described mechanism in case the URGENT request
    was returned back to the block layer from some reason: by requeue or
    reinsert.
    
    CRs-fixed: 473376, 473036, 471736
    Change-Id: Ie8b8208230a302d4526068531616984825f1050d
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Fix starvation tolerance values
    
    The current starvation tolerance values increase the boot time
    since high priority SW requests are delayed by regular priority requests.
    In order to overcome this, increase the starvation tolerance values.
    
    Change-Id: I9947fca9927cbd39a1d41d4bd87069df679d3103
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    Signed-off-by: Maya Erez <merez@codeaurora.org>
    
    Conflicts:
    	block/Makefile

commit b12a93da9d4f649b31ef06089b5efe0a16a43809
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 2 12:55:48 2014 +0100

    Updated configuration!

commit 7c8160ebcd805849ee2d75f9a5a0264575e50a4e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:29:56 2014 +0100

    Linux 3.4.7

commit 78344bed766be157282aacb45f9f5e443a418507
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:29:31 2014 +0100

    Linux 3.4.6
    
    Conflicts:
    	include/linux/sched.h

commit 7f469511e177b13a2ffa03b1cd23e03f3edc935d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:28:58 2014 +0100

    Linux 3.4.5
    
    Conflicts:
    	arch/arm/mm/mmu.c
    	drivers/base/power/main.c
    	mm/madvise.c

commit af110aeb9e2e342b76ee2fc8ab5570bfa6aa7637
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:28:21 2014 +0100

    Linux 3.4.4

commit 7774b83dc09515a873a6c51a81b42acfb9f504ec
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:25:51 2014 +0100

    Linux 3.4.3

commit 3556ecb9fbf0b7d0f698b3ea9c0f57dd0933f5a2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:25:04 2014 +0100

    Linux 3.4.2
    
    Conflicts:
    	fs/ext4/namei.c

commit f7fcd5a619bf2fe5e23008b0dae729efa9969854
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 27 20:24:32 2014 +0100

    Linux 3.4.1
    
    Conflicts:
    	drivers/usb/host/xhci.h

commit 1a9aa1f9a74d0b6c7895d8824c5add0adfeca413
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 2 12:26:31 2014 +0100

    Added bfq, sio, row, vr, fifo, fiops, zen... I/O scheduler!

commit e1839c2d34d2311a56a412c8143b3a3fc580d36a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 2 12:18:24 2014 +0100

    Update mkbootimg cmdline option for TW-KK

commit 7dac97a92b81c4a42a61398d4013bc34d81a3953
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Mar 2 12:11:42 2014 +0100

    Fixed useless conf!

commit 813490b1e269a3ffd65d1397ca63106404a68635
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 1 14:59:38 2014 +0100

    Upgraded DVFS touch interface to ver. 2.0

commit b986c448343374aa1f1dc82e8efe3b9129db52ba
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Mar 1 00:16:49 2014 +0100

    DYNAMIC FSYNC, ASYNC, ROOTFS SLAVE, cpufreq little changes!

commit 5d81fc7227dd020bcb12b38334222a85fcf1df24
Author: dorimanx <yuri@bynet.co.il>
Date:   Tue Jan 14 17:01:59 2014 +0200

    Merged FRANDOM code.
    
    Code changes are from furnace kernel source!
    
    http://forum.xda-developers.com/showthread.php?t=2572992
    
    https://github.com/dorimanx/Dorimanx-LG-G2-D802-Kernel/tree/furnace-kernel
    
    Conflicts:
    	arch/arm/configs/dorimanx_defconfig
    
    Conflicts:
    	drivers/char/Makefile

commit b300ecb2d52c9bd4585db12e133e72c82343fc72
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 28 23:28:18 2014 +0100

    Removed many debug and tracing options!

commit 59d01aa6d264929440cfe7c9b757f589b886d68b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 28 17:16:41 2014 +0100

    Some config modifications!

commit 6c1bbfa7269f8c85764603e463a952113cb07a3e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 28 01:19:41 2014 +0100

    Merged with my-cm-11.0 branch!

commit 048229b2fce7ec11391e8c8a91b8657618ba22a0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Feb 27 21:47:09 2014 +0100

    Added tw-kk original kernel config!

commit 6135d07ee2bc7cd04757369eb5e1c85f13e2fec6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Feb 27 21:42:21 2014 +0100

    Building scripts and makefile!

commit 5b0522c82867e66d28689f4c3fb78a50f6fce1f4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Feb 27 11:04:29 2014 +0100

    GT-I9505_KK_Opensource_Update5 merged with android-common-3.4!

commit 001e717154875f3c01d6cea2ccd193f96caae2cd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 26 23:06:29 2014 +0100

    Implemented gps_status active | not active.

commit 4a74fd6a1404f112ba525e579011f3612c9c9af2
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Wed Feb 19 08:10:55 2014 -0700

    exfat: Upgrade to 1.2.6

commit 780ce87ac2f22a9d32f46e2e5ea18e90dbe0f759
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat Jun 1 22:34:55 2013 -0700

    panel_colors: Place code in proper files

commit 1f29eee0bbf3f049493a41f78b5616746922eb39
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 26 12:22:51 2014 +0100

    Updated kernel configuration!

commit 424dfe4888bd9f8e50fdf1560959a0843adba6f1
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 26 12:20:37 2014 +0100

    Revert "Kconfig: Add config option to support vmalloc savings"
    
    This reverts commit 52529e2c1c52b97903f99bddc63450fd989905e7.

commit 99d9729ee734b63089b9819c65ab5290c5905d0e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 26 12:20:17 2014 +0100

    Revert "Kconfig: Add menu choice option to reclaim virtual memory"
    
    This reverts commit adeedc2fa642e05a0074a2a8e18ac67503ac2bdc.

commit 75a6fa5632b290ab29538ce7f4e80985cefda3c4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 26 12:19:55 2014 +0100

    Revert "mm: Update is_vmalloc_addr to account for vmalloc savings"
    
    This reverts commit 7c9d8906bdd295ef5cf8b24eb469835fc6a0d3e9.

commit 8818786a4f8716468a2eb40c317fb038b5a27868
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 26 12:19:11 2014 +0100

    Fixed eventpoll header!

commit b34e30613c87ba77ea9ed0bd80c08e4ddb864a15
Author: dorimanx <yuri@bynet.co.il>
Date:   Tue Feb 18 12:57:51 2014 +0200

    witch epoll_pwait to COMPAT_SYSCALL_DEFINE
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    Conflicts:
    	fs/eventpoll.c

commit eeba88a83593bdb0faf21d4c4a5847144f735a5c
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 17 21:11:16 2014 +0200

    Fix arch/arm/vfp/vfpmodule.c init logic.

commit 452a1243f2b6b8d126cb03ee6096862b91875aa9
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Mon Jul 15 16:47:35 2013 -0700

    msm: Add support for multiple memory holes
    
    If a target supports multiple memory holes, the base address and
    size of the memory holes are represented as an array of tuples.
    The device tree is scanned in order to extract these values and
    remove the memory.
    
    Change-Id: Id253112a384b7f64be067636e5bffc69e3950fb5
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit d58463ff037a94de7f5370c847772549a1f17998
Author: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>
Date:   Mon Jun 3 16:29:56 2013 -0700

    mms: rng: Add bus bandwidth support to PRNG
    
    PRNG has to use pnoc clock, but pnoc is a bus clock & clients cannot
    directly call pnoc apis, so prng calls bus bandwidth api to get the
    clock cycles.
    
    Change-Id: Ib3cd075097b4b70f82ec6325535e6e39808c6408
    Signed-off-by: Hariprasad Dhalinarasimha <hnamgund@codeaurora.org>

commit 32816b0ddb85ce4be3a0173b8b8604108cce0950
Author: Vikram Mulukutla <markivx@codeaurora.org>
Date:   Tue May 14 11:26:37 2013 -0700

    firmware_class: Introduce the request_firmware_direct API
    
    On devices with low memory, using request_firmware on rather
    large firmware images results in a memory usage penalty that
    might be unaffordable. Introduce a new API that allows the
    firmware image to be directly loaded to a destination address
    without using any intermediate buffer.
    
    Change-Id: I3ac3c5036c14773125291eccf83d494a8703b4cd
    Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>

commit f2354d3e1908835ee8fda47ca27f975ed02f276c
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Sat Dec 15 15:42:19 2012 +0000

    netlink: validate addr_len on bind
    
    Otherwise an out of bounds read could happen.
    
    Change-Id: Idaaf38bb61002a086c88734ac1f5be96825de26a
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Git-commit: 4e4b53768f1ddce38b7f6edcad3a063020ef0024
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 47165292977e32dea99a898edf6882cb0930ea93
Author: dorimanx <yuri@bynet.co.il>
Date:   Sun Feb 16 23:19:53 2014 +0200

    Fix last ns_capable(inode_userns in fs/namei.c

commit ad5d0dbb858ccd9273478744c37345d76aed41ae
Author: Michael Kerrisk <mtk.manpages@gmail.com>
Date:   Tue Jul 17 21:37:27 2012 +0200

    PM: Rename CAP_EPOLLWAKEUP to CAP_BLOCK_SUSPEND
    
    As discussed in
    http://thread.gmane.org/gmane.linux.kernel/1249726/focus=1288990,
    the capability introduced in 4d7e30d98939a0340022ccd49325a3d70f7e0238
    to govern EPOLLWAKEUP seems misnamed: this capability is about governing
    the ability to suspend the system, not using a particular API flag
    (EPOLLWAKEUP). We should make the name of the capability more general
    to encourage reuse in related cases. (Whether or not this capability
    should also be used to govern the use of /sys/power/wake_lock is a
    question that needs to be separately resolved.)
    
    This patch renames the capability to CAP_BLOCK_SUSPEND. In order to ensure
    that the old capability name doesn't make it out into the wild, could you
    please apply and push up the tree to ensure that it is incorporated
    for the 3.5 release.
    
    Signed-off-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    
    Conflicts:
    	fs/eventpoll.c

commit 7fbf484c018f11266021bd1a986c5fea7f949499
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 26 18:31:00 2012 -0400

    HAVE_RESTORE_SIGMASK is defined on all architectures now
    
    Everyone either defines it in arch thread_info.h or has TIF_RESTORE_SIGMASK
    and picks default set_restore_sigmask() in linux/thread_info.h.  Kill the
    ifdefs, slap #error in linux/thread_info.h to catch breakage when new ones
    get merged.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    Conflicts:
    	fs/eventpoll.c

commit 6fe9853f412aa4361fd583e3cb39cc2952284264
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon May 21 21:42:32 2012 -0400

    new helper: sigsuspend()
    
    guts of saved_sigmask-based sigsuspend/rt_sigsuspend.  Takes
    kernel sigset_t *.
    
    Open-coded instances replaced with calling it.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 95ed64a169ce4c4d7d3ff97325cfce930d2c313c
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Nov 14 16:24:06 2011 -0800

    userns: Replace the hard to write inode_userns with inode_capable.
    
    This represents a change in strategy of how to handle user namespaces.
    Instead of tagging everything explicitly with a user namespace and bulking
    up all of the comparisons of uids and gids in the kernel,  all uids and gids
    in use will have a mapping to a flat kuid and kgid spaces respectively.  This
    allows much more of the existing logic to be preserved and in general
    allows for faster code.
    
    In this new and improved world we allow someone to utiliize capabilities
    over an inode if the inodes owner mapps into the capabilities holders user
    namespace and the user has capabilities in their user namespace.  Which
    is simple and efficient.
    
    Moving the fs uid comparisons to be comparisons in a flat kuid space
    follows in later patches, something that is only significant if you
    are using user namespaces.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

commit 4284db377a4807c272db4d3aa35d31f529b126dd
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 17 19:50:11 2013 +0100

    ARM: 7927/1: dcache: select DCACHE_WORD_ACCESS for big-endian CPUs
    
    With commit 11ec50caedb5 ("word-at-a-time: provide generic big-endian
    zero_bytemask implementation"), the asm-generic word-at-a-time code now
    provides a zero_bytemask implementation, allowing us to make use of
    DCACHE_WORD_ACCESS on big-endian CPUs, providing our
    load_unaligned_zeropad function is endianness-clean.
    
    This patch reworks the load_unaligned_zeropad fixup code to work for
    both big- and little-endian CPUs, then removes the !CPU_BIG_ENDIAN check
    when selecting DCACHE_WORD_ACCESS.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 2217804c166c525106d41887395d75f5b93ebd00
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jan 2 17:14:45 2014 +0000

    CRYPTO: Fix more AES build errors
    
    Building a multi-arch kernel results in:
    
    arch/arm/crypto/built-in.o: In function `aesbs_xts_decrypt':
    sha1_glue.c:(.text+0x15c8): undefined reference to `bsaes_xts_decrypt'
    arch/arm/crypto/built-in.o: In function `aesbs_xts_encrypt':
    sha1_glue.c:(.text+0x1664): undefined reference to `bsaes_xts_encrypt'
    arch/arm/crypto/built-in.o: In function `aesbs_ctr_encrypt':
    sha1_glue.c:(.text+0x184c): undefined reference to `bsaes_ctr32_encrypt_blocks'
    arch/arm/crypto/built-in.o: In function `aesbs_cbc_decrypt':
    sha1_glue.c:(.text+0x19b4): undefined reference to `bsaes_cbc_encrypt'
    
    This code is already runtime-conditional on NEON being supported, so
    there's no point compiling it out depending on the minimum build
    architecture.
    
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 88d5757e73ca59d301756505539701461f637746
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 7 15:43:53 2013 +0100

    ARM: add .gitignore entry for aesbs-core.S
    
    This avoids this file being incorrectly added to git.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit d0158d27f546a34d7163caf5633cda09d5213148
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Sep 16 18:31:38 2013 +0200

    ARM: add support for bit sliced AES using NEON instructions
    
    Bit sliced AES gives around 45% speedup on Cortex-A15 for encryption
    and around 25% for decryption. This implementation of the AES algorithm
    does not rely on any lookup tables so it is believed to be invulnerable
    to cache timing attacks.
    
    This algorithm processes up to 8 blocks in parallel in constant time. This
    means that it is not usable by chaining modes that are strictly sequential
    in nature, such as CBC encryption. CBC decryption, however, can benefit from
    this implementation and runs about 25% faster. The other chaining modes
    implemented in this module, XTS and CTR, can execute fully in parallel in
    both directions.
    
    The core code has been adopted from the OpenSSL project (in collaboration
    with the original author, on cc). For ease of maintenance, this version is
    identical to the upstream OpenSSL code, i.e., all modifications that were
    required to make it suitable for inclusion into the kernel have been made
    upstream. The original can be found here:
    
        http://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=6f6a6130
    
    Note to integrators:
    While this implementation is significantly faster than the existing table
    based ones (generic or ARM asm), especially in CTR mode, the effects on
    power efficiency are unclear as of yet. This code does fundamentally more
    work, by calculating values that the table based code obtains by a simple
    lookup; only by doing all of that work in a SIMD fashion, it manages to
    perform better.
    
    Cc: Andy Polyakov <appro@openssl.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

commit 7cd0424afd1faca0b22f5dfcdcf1a8dd1ed63fbc
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sun Sep 15 17:10:43 2013 +0200

    ARM: move AES typedefs and function prototypes to separate header
    
    Put the struct definitions for AES keys and the asm function prototypes in a
    separate header and export the asm functions from the module.
    This allows other drivers to use them directly.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

commit 6ea782374325cd18ea66dddfeba11ced3bf57bc6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Sep 20 09:57:37 2013 +0200

    ARM: pull in <asm/simd.h> from asm-generic
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

commit 31d0bc568fa03a84937aa8e8c4e1d80e1126d8be
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Sep 20 09:55:40 2013 +0200

    crypto: create generic version of ablk_helper
    
    Create a generic version of ablk_helper so it can be reused
    by other architectures.
    
    Acked-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 1e8e1b14344e93495cb55ffbf6f043946445953d
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sat Sep 21 11:23:50 2013 +0100

    ARM: 7837/3: fix Thumb-2 bug in AES assembler code
    
    Patch 638591c enabled building the AES assembler code in Thumb2 mode.
    However, this code used arithmetic involving PC rather than adr{l}
    instructions to generate PC-relative references to the lookup tables,
    and this needs to take into account the different PC offset when
    running in Thumb mode.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit bf86d09ff1e1e9ce85b328d25d2913e2366cbf1a
Author: Ard Biesheuvel <ard.biesheuvel@gmail.com>
Date:   Wed May 15 10:46:30 2013 +0100

    ARM: 7723/1: crypto: sha1-armv4-large.S: fix SP handling
    
    Make the SHA1 asm code ABI conformant by making sure all stack
    accesses occur above the stack pointer.
    
    Origin:
    http://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=1a9d60d2
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 25228c9a46e7ecae9f50a0ea1f064d177beb0670
Author: Dave Martin <dave.martin@linaro.org>
Date:   Thu Jan 10 12:20:15 2013 +0100

    ARM: 7626/1: arm/crypto: Make asm SHA-1 and AES code Thumb-2 compatible
    
    This patch fixes aes-armv4.S and sha1-armv4-large.S to work
    natively in Thumb.  This allows ARM/Thumb interworking workarounds
    to be removed.
    
    I also take the opportunity to convert some explicit assembler
    directives for exported functions to the standard
    ENTRY()/ENDPROC().
    
    For the code itself:
    
      * In sha1_block_data_order, use of TEQ with sp is deprecated in
        ARMv7 and not supported in Thumb.  For the branches back to
        .L_00_15 and .L_40_59, the TEQ is converted to a CMP, under the
        assumption that clobbering the C flag here will not cause
        incorrect behaviour.
    
        For the first branch back to .L_20_39_or_60_79 the C flag is
        important, so sp is moved temporarily into another register so
        that TEQ can be used for the comparison.
    
      * In the AES code, most forms of register-indexed addressing with
        shifts and rotates are not permitted for loads and stores in
        Thumb, so the address calculation is done using a separate
        instruction for the Thumb case.
    
    The resulting code is unlikely to be optimally scheduled, but it
    should not have a large impact given the overall size of the code.
    I haven't run any benchmarks.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Tested-by: David McCullough <ucdevel@gmail.com> (ARM only)
    Acked-by: David McCullough <ucdevel@gmail.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 4ca13f7c46295285fbb9a474479c45d77966380a
Author: JP Abgrall <jpa@google.com>
Date:   Mon Jan 27 14:29:48 2014 -0800

    tcp: add a sysctl to config the tcp_default_init_rwnd
    
    The default initial rwnd is hardcoded to 10.
    
    Now we allow it to be controlled via
      /proc/sys/net/ipv4/tcp_default_init_rwnd
    which limits the values from 3 to 100
    
    This is somewhat needed because ipv6 routes are
    autoconfigured by the kernel.
    
    See "An Argument for Increasing TCP's Initial Congestion Window"
    in https://developers.google.com/speed/articles/tcp_initcwnd_paper.pdf
    
    Change-Id: I386b2a9d62de0ebe05c1ebe1b4bd91b314af5c54
    Signed-off-by: JP Abgrall <jpa@google.com>

commit 7ba88be58e90298e3c82fd2686daa4d06280067d
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:47 2012 -0700

    mm: methods for teaching filesystems about PG_swapcache pages
    
    In order to teach filesystems to handle swap cache pages, three new page
    functions are introduced:
    
      pgoff_t page_file_index(struct page *);
      loff_t page_file_offset(struct page *);
      struct address_space *page_file_mapping(struct page *);
    
    page_file_index() - gives the offset of this page in the file in
    PAGE_CACHE_SIZE blocks.  Like page->index is for mapped pages, this
    function also gives the correct index for PG_swapcache pages.
    
    page_file_offset() - uses page_file_index(), so that it will give the
    expected result, even for PG_swapcache pages.
    
    page_file_mapping() - gives the mapping backing the actual page; that is
    for swap cache pages it will give swap_file->f_mapping.
    
    Change-Id: I13d18bb25be606760eac26cc842eb7c9fc9e4766
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f981c5950fa85916ba49bea5d9a7a5078f47e569
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [ohaugan@codeaurora.org: Resolved merge issues]
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    
    Conflicts:
    	include/linux/swap.h

commit 9a85bf6d4825a17247a6c090e62d283a788548fd
Author: dorimanx <yuri@bynet.co.il>
Date:   Fri Feb 14 03:42:11 2014 +0200

    ARM: VFP fix proc fs entry not loaded, FS still not online.
    
    Conflicts:
    	arch/arm/vfp/vfpmodule.c

commit a06b16bf8dc65d61f344806e3460d04ae567f24c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Sep 22 10:08:50 2013 +0000

    ARM: only allow kernel mode neon with AEABI
    
    This prevents the linker erroring with:
    
    arm-linux-ld: error: arch/arm/lib/xor-neon.o uses VFP instructions, whereas arch/arm/lib/built-in.o does not
    arm-linux-ld: failed to merge target specific data of file arch/arm/lib/xor-neon.o
    
    This is due to the non-neon files being marked as containing FPA data/
    instructions (even though they do not) being mixed with files which
    contain VFP, which is an incompatible floating point format.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 7c772ea28387a851c31355dd6e3be5929f03c4ba
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Sep 9 14:08:38 2013 +0000

    ARM: 7835/2: fix modular build of xor_blocks() with NEON enabled
    
    Commit 0195659 introduced a NEON accelerated version of the xor_blocks()
    function, but it needs the changes in this patch to allow it to be built
    as a module rather than statically into the kernel.
    
    This patch creates a separate module xor-neon.ko which exports the NEON
    inner xor_blocks() functions depended upon by the regular xor.ko if it
    is built with CONFIG_KERNEL_MODE_NEON=y
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit d3bc45539c404b9ef01468ae3c6b1bff17663359
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri May 17 16:51:23 2013 +0000

    ARM: crypto: add NEON accelerated XOR implementation
    
    Add a source file xor-neon.c (which is really just the reference
    C implementation passed through the GCC vectorizer) and hook it
    up to the XOR framework.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

commit cef61110c1356c7ac58acda0df34547785b8db84
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Jan 9 01:16:06 2014 -0800

    ARM: add support for kernel mode NEON
    
    In order to safely support the use of NEON instructions in
    kernel mode, some precautions need to be taken:
    - the userland context that may be present in the registers (even
      if the NEON/VFP is currently disabled) must be stored under the
      correct task (which may not be 'current' in the UP case),
    - to avoid having to keep track of additional vfpstates for the
      kernel side, disallow the use of NEON in interrupt context
      and run with preemption disabled,
    - after use, re-enable preemption and re-enable the lazy restore
      machinery by disabling the NEON/VFP unit.
    
    This patch adds the functions kernel_neon_begin() and
    kernel_neon_end() which take care of the above. It also adds
    the Kconfig symbol KERNEL_MODE_NEON to enable it.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b3eef636289daa78f51ae77fb2110a0e2296c9bd
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Jan 9 01:22:04 2014 -0800

    ARM: be strict about FP exceptions in kernel mode
    
    The support code in vfp_support_entry does not care whether the
    exception that caused it to be invoked occurred in kernel mode or
    in user mode. However, neither condition that could trigger this
    exception (lazy restore and VFP bounce to support code) is
    currently allowable in kernel mode.
    
    In either case, print a message describing the condition before
    letting the undefined instruction handler run its course and trigger
    an oops.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel at linaro.org>
    Acked-by: Nicolas Pitre <nico at linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 5b336145e46298d2424ee06d82367ad84cad7fe4
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Jan 9 14:31:09 2014 -0600

    ARM: move VFP init to an earlier boot stage
    
    In order to use the NEON unit in the kernel, we should
    initialize it a bit earlier in the boot process so NEON users
    that like to do a quick benchmark at load time (like the
    xor_blocks or RAID-6 code) find the NEON/VFP unit already
    enabled.
    
    Replaced late_initcall() with core_initcall().
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel at linaro.org>
    Acked-by: Nicolas Pitre <nico at linaro.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/vfp/vfpmodule.c

commit ba0afba2ea08e7b4d070c4ff4d9c7726208809f4
Author: Sasha Levitskiy <sanek@google.com>
Date:   Fri Jun 28 11:06:31 2013 -0700

    Input: Propagate hardware event timestamp to evdev.
    
    Convey hardware timestamp associated with the current event packet.
    The use of these event codes by hardware drivers is optional.
    Used to reduce jitter and improve velocity tracking in ABS_MT devices.
    
    Change-Id: I89f3a958944f6a2979964ee372e61bad448061b6
    Signed-off-by: Sasha Levitskiy <sanek@google.com>

commit 95c55f297f786b9bfef973695ff8e123ab625cb0
Author: Arve Hjønnevåg <arve@android.com>
Date:   Tue Aug 21 17:28:30 2012 -0700

    Input: Set users to 0 when an input device is disabled though sysfs
    
    Some drivers check dev->users suspend and resume. If the device
    was turned off with the sysfs hook, it should not turn back on
    in resume.
    
    cherry-picking from android-exynos-manta-3.4-jb-mr1.1
    commit id ebdfb336782deafee6667730c1ca1b3c1fbd551e
    
    Change-Id: Iff17fdac307ec9c27744ac886f9dc94aac665c80
    Signed-off-by: Arve Hjønnevåg <arve@android.com>

commit b3d87f28f373a5e701610be6e4a7261578e038b4
Author: Arve Hjønnevåg <arve@android.com>
Date:   Tue Aug 7 17:59:09 2012 -0700

    Input: Add enabled device attribute
    
    Allows closing the hardware device while the input device is open.
    
    cherry-picking from android-exynos-manta-3.4-jb-mr1.1
    commit id 8da9e5fe1e25de1cb9ab8d2d74f4d7fff8d7a220
    
    Change-Id: I91b6e24f14b3334d17bfe8e5f86d2b33c85e1ea1
    Signed-off-by: Arve Hjønnevåg <arve@android.com>

commit 468ba9dee9e25e92c517f9386f89d05a2f61593c
Author: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
Date:   Thu Dec 26 18:23:08 2013 -0800

    ARM: mm: update __v7_setup() to the new LoUIS cache maintenance API
    
    The ARMv7 processor setup function __v7_setup() cleans and invalidates the
    CPU cache before enabling MMU to start the CPU with a clean CPU local cache.
    
    But on ARMv7 architectures like Cortex-[A15/A8], this code will end
    up flushing the L2 caches(up to level of Coherency) which is undesirable
    and expensive. The setup functions are used in the CPU hotplug scenario too
    and hence flushing all cache levels should be avoided.
    
    This patch replaces the cache flushing call with the newly introduced
    v7 dcache LoUIS API where only cache levels up to LoUIS are cleaned and
    invalidated when a processors executes __v7_setup which is the expected
    behavior.
    
    For processors like A9 and A5 where the L2 cache is an outer one the
    behavior should be unchanged.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 922d0f64c01f0561a2edab28b0533ab6b21d1abf
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Dec 26 18:22:21 2013 -0800

    ARM: kernel: update __cpu_disable to use cache LoUIS maintenance API
    
    When a CPU is hotplugged out caches that reside in its power domain
    lose their contents and so must be cleaned to the next memory level.
    
    Currently, __cpu_disable calls flush_cache_all() that for new generation
    processor like A15/A7 ends up cleaning and invalidating all cache levels
    up to Level of Coherency, which includes the unified L2.
    
    This ends up being a waste of cycles since the L2 cache contents are not
    lost on power down.
    
    This patch updates __cpu_disable to use the new LoUIS API cache operations.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 78829bd78c4e37b97c338a0aaf7b4d43a1c8f54b
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Dec 26 18:21:32 2013 -0800

    ARM: kernel: update cpu_suspend code to use cache LoUIS operations
    
    In processors like A15/A7 L2 cache is unified and integrated within the
    processor cache hierarchy, so that it is not considered an outer cache
    anymore. For processors like A15/A7 flush_cache_all() ends up cleaning
    all cache levels up to Level of Coherency (LoC) that includes
    the L2 unified cache.
    
    When a single CPU is suspended (CPU idle) a complete L2 clean is not
    required, so generic cpu_suspend code must clean the data cache using the
    newly introduced cache LoUIS function.
    
    The context and stack pointer (context pointer) are cleaned to main memory
    using cache area functions that operate on MVA and guarantee that the data
    is written back to main memory (perform cache cleaning up to the Point of
    Coherency - PoC) so that the processor can fetch the context when the MMU
    is off in the cpu_resume code path.
    
    outer_cache management remains unchanged.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@xxxxxx>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 9432da1f0b844c9b6abd0375b9043848fd56b4db
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Dec 26 18:20:41 2013 -0800

    ARM: mm: rename jump labels in v7_flush_dcache_all function
    
    This patch renames jump labels in v7_flush_dcache_all in order to define
    a specific flush cache levels entry point.
    
    TODO: factor out the level flushing loop if considered worthwhile and
          define the input registers requirements.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@xxxxxxx>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 7c9d8906bdd295ef5cf8b24eb469835fc6a0d3e9
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu Aug 22 13:46:07 2013 -0700

    mm: Update is_vmalloc_addr to account for vmalloc savings
    
    is_vmalloc_addr current assumes that all vmalloc addresses
    exist between VMALLOC_START and VMALLOC_END. This may not be
    the case when interleaving vmalloc and lowmem. Update the
    is_vmalloc_addr to properly check for this.
    
    Change-Id: I5def3d6ae1a4de59ea36f095b8c73649a37b1f36
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit b3f8d449b0d2de357be51ac4363c378fd6b4effa
Author: Thierry Reding <thierry.reding@avionic-design.de>
Date:   Mon Jan 21 10:08:54 2013 +0000

    lib: devres: Introduce devm_ioremap_resource()
    
    The devm_request_and_ioremap() function is very useful and helps avoid a
    whole lot of boilerplate. However, one issue that keeps popping up is
    its lack of a specific error code to determine which of the steps that
    it performs failed. Furthermore, while the function gives an example and
    suggests what error code to return on failure, a wide variety of error
    codes are used throughout the tree.
    
    In an attempt to fix these problems, this patch adds a new function that
    drivers can transition to. The devm_ioremap_resource() returns a pointer
    to the remapped I/O memory on success or an ERR_PTR() encoded error code
    on failure. Callers can check for failure using IS_ERR() and determine
    its cause by extracting the error code using PTR_ERR().
    
    devm_request_and_ioremap() is implemented as a wrapper around the new
    API and return NULL on failure as before. This ensures that backwards
    compatibility is maintained until all users have been converted to the
    new API, at which point the old devm_request_and_ioremap() function
    should be removed.
    
    A semantic patch is included which can be used to convert from the old
    devm_request_and_ioremap() API to the new devm_ioremap_resource() API.
    Some non-trivial cases may require manual intervention, though.
    
    Change-Id: I458afe18768979122c12e95bffe979a31f02f607
    Signed-off-by: Thierry Reding <thierry.reding@avionic-design.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Git-commit: 75096579c3ac39ddc2f8b0d9a8924eba31f4d920
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Dolev Raviv <draviv@codeaurora.org>

commit 0f8984ed94b3c952587fbd0bbd3bfe417f7db9d7
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Fri Nov 15 14:11:08 2013 -0800

    msm: acpuclock-krait: Update 'unknown rate' debug prints to warnings
    
    While not fatal, boot performance will be suboptimal if the L2 or
    CPU frequencies at boot are not in the frequency tables. This is
    because acpuclock 'plays it safe' by applying the lowest possible
    clock rate in such situations to ensure clocks are not set higher
    than boot-up voltages can support.
    
    Change-Id: I07fcff70d742b2590fa996ea4ca7a1f1c560eee3
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 7d470f11d37541013c0871834bf4a3837373646f
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Nov 11 09:31:38 2013 -0800

    msm: acpuclock-krait: Fix HFPLL regulator voting in regulator_init()
    
    Simply incrementing l2_vreg_count is not enough to ensure the
    regulators necessary for the L2 HFPLL are enabled. Instead,
    enable_l2_regulators() must be called to place votes for the
    regulators.
    
    This problem will only result in incorrect behaviour (a missing
    vote for the HFPLL regulators) if the L2 rate detected at boot is
    not in the frequency table. In all other cases, the vote will be
    applied as part of the hfpll_enable call when init_clock_sources()
    is called for the L2.
    
    Fix the code so that the voting works correctly even if the initial
    L2 rate is not in the frequency table.
    
    Change-Id: I8e84cd81de17a0efcc91281848180cf34ef9bb57
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit c60b57035d0b66906d70313195708a4d299de15b
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Fri Sep 6 14:01:26 2013 -0700

    msm: acpuclock-krait: Program both high- and low-performance sources
    
    The latest LDO/BHS switching sequence employed in the 8974
    krait-regulator driver involves mandatory operations that cause
    the CPUs to switch to the low-performance clock source when the
    CPU is powered from the LDO, and to the high-performance clock
    source when powered from the BHS.
    
    This results in potential undervolting of the CPUs since the
    acpuclock driver does not program the low-performance source,
    and the clock source it selects be default is running at the
    same speed as DDR (which can be very fast).
    
    Fix this by programming both the high and low-performance clock
    sources to be the same at all times.
    
    CRs-Fixed: 538629
    Change-Id: Iaa39cca9165b892e52acd8ec3aac5d0784112623
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 82875fac767e34182c17f7d30b914987984c1aea
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Feb 5 19:19:19 2014 +0200

    Add missing mutex.h to early_random.c

commit 353b94ca760dd274b7e20b21b6083c393d35c259
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Aug 13 10:25:58 2013 -0700

    msm: scm: Add scm_call_noalloc and helper macros
    
    Currently, scm_call internally calls kmalloc() to allocate a command
    buffer structure. This prevents scm_call from working before kmalloc
    structures are initialized or during atomic contexts. Allow clients
    to pass in a pre-allocated buffer for scm_call to be used for internal
    structures.
    
    Change-Id: Id09500c8c696298228aefdcc2a61953e654677c9
    CRs-Fixed: 498392
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a414c9dc8f7bd369771bdf5453f7442275eabf36
Author: Zaheerulla Meer <zmeer@codeaurora.org>
Date:   Wed Jun 19 16:31:17 2013 +0530

    msm: smem: Update SMEM log dump_sym to indicate QMI modules
    
    SMEM Log dump_sym file indicates log information from QCCI/QCSI modules
    as ONCRPC and log information from IPC Router as ROUTER which is
    misleading.
    
    Update the code so that dump_sym indicates correct module names.
    
    Change-Id: I8e67ccce60780ec935cfc71d0854f156f3e922b8
    Signed-off-by: Zaheerulla Meer <zmeer@codeaurora.org>

commit 16b1e9fed4a604ee7ec6b9afff0af3825441263e
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Wed Jun 5 14:14:25 2013 -0700

    msm: Add null-pointer checks for domains
    
    Add check for null-pointer before passing domain pointer
    into map and unmap api to avoid null-pointer deferences.
    
    CRs-fixed: 493503
    Change-Id: I185deb9f2878ef62601eb4ca1d2b71f892ca0185
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit a2d4823a034d57ded1a66e34677dc7dad2569395
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Wed Aug 28 18:45:18 2013 -0700

    msm: cpufreq: Move cpu_frequency_* trace points from acpuclock to cpufreq
    
    acpuclock is not used on newer targets, so move the trace points
    in it up to the generic msm cpufreq driver. Also drop the use of
    the 'trace_cpu_frequency' entirely, since it's already present in
    generic cpufreq code.
    
    As a side effect of this change, these trace events will no longer
    track cpu frequency changes from the power-collapse path. Only
    frequency changes coming from cpufreq will be tracked.
    
    CRs-Fixed: 535512
    Change-Id: Ib69d92f5d3209a7e36be678997b79bcdac92f953
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/acpuclock.c
    	arch/arm/mach-msm/cpufreq.c

commit c7e924137aff9fc78cbc85a8c16a07275c7282dd
Author: Junjie Wu <junjiew@codeaurora.org>
Date:   Thu Jun 20 20:49:00 2013 -0700

    msm: acpuclock: Fix acpuclk_data null pointer dereference
    
    If acpuclk_data is null, dereferencing it in acpuclk_get_switch_time
    causes a kernel panic.  Add pointer check to fix that.
    
    Change-Id: I69b92671c8cf26cb45ec8f1193c147be2ebe4c16
    Signed-off-by: Junjie Wu <junjiew@codeaurora.org>

commit adeedc2fa642e05a0074a2a8e18ac67503ac2bdc
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Wed Aug 14 17:34:55 2013 -0700

    Kconfig: Add menu choice option to reclaim virtual memory
    
    Add menu choice options for selecting the method of reclaiming
    virtual memory, either by mapping around the largest hole or
    reclaiming virtual memory belonging to any subsystem expected to
    have a lifetime of the entire system. By default virtual memory
    is not reclaimed.
    
    Change-Id: I1b8a1492062bb9532700122878618989e5148647
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit 52529e2c1c52b97903f99bddc63450fd989905e7
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Wed Jul 31 14:34:17 2013 -0700

    Kconfig: Add config option to support vmalloc savings
    
    The virtual space belonging to any subsystem which is
    expected to have a lifetime of the entire system can be
    reclaimed. Adding config option to enable or disable
    this feature.
    
    Change-Id: I56faef4b71c948185fdfe336054a5951dfa8d011
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>
    
    Conflicts:
    	arch/arm/Kconfig

commit 34029c8902999fc98db697e96e327aa8efe065f7
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Aug 9 16:45:33 2013 -0700

    arm: Add ARCH_RANDOM Kconfig
    
    Some boards may support architecture generated random numbers. Add
    the appropriate Kconfig.
    
    Change-Id: I0077a982c9bb6d3a35435cc27fe40b7e0596ca84
    CRs-Fixed: 498392
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 4f8ddead752a742009e27d41a1d701a83fdc0508
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Aug 9 18:17:06 2013 -0700

    msm: implement ARCH_RANDOM
    
    Currently, the software random number generator is not intialized
    until relatively late in the boot process. Software that relies on
    random numbers early will not be reliable. Entropy sources are
    available early but not early enough for some use cases which means
    that moving the software random number generation earlier is not an
    option. As an alternate solution, implement the functions associated
    with CONFIG_ARCH_RANDOM. These functions currently make a call into
    trustzone to get reliable random numbers. The functions are disabled
    once the regular random number generator is sufficiently reliable.
    
    Change-Id: If40cfcb96b091fa26a28047c10a902287b26f6c1
    CRs-Fixed: 498392
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 9561777f6f1deec75b6db05fb80cb7517d38c85c
Author: David Keitel <dkeitel@codeaurora.org>
Date:   Tue Mar 26 18:50:03 2013 -0700

    sysctl: add cold_boot sysctl entry
    
    Add a cold_boot parameter which supplements the
    boot_reason sysctl entry with information about
    whether the system was booted from cold or warm state.
    
    /proc/sys/kernel/cold_boot entry is updated with 1 or 0 when
    system was booted from cold or warm boot state respecitively.
    
    CRs-Fixed: 461256
    Change-Id: I2bc5d80c8f26eb9e9dbb4b34960d991a51a224e4
    Signed-off-by: David Keitel <dkeitel@codeaurora.org>

commit 3bd067334b0d10e5fd9bda265d6e36e113a7df22
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 4 12:56:44 2012 +0100

    ARM: 7592/1: nommu: prevent generation of kernel unaligned memory accesses
    
    Recent ARMv7 toolchains assume that unaligned memory accesses will not
    fault and will instead be handled by the processor.
    
    For the nommu case (without an MPU), memory will be treated as
    strongly-ordered and therefore unaligned accesses may fault regardless
    of the SCTLR.A setting.
    
    This patch passes -mno-unaligned-access to GCC when compiling for nommu
    targets, preventing the generation of unaligned memory access in the
    kernel.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Jonathan Austin <jonathan.austin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit ab49bc0f68f3fb1d111cbacfac3bdc0a445751c3
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:27:00 2013 -0700

    ARM: 7493/1: use generic unaligned.h
    
    This moves ARM over to the asm-generic/unaligned.h header. This has the
    benefit of better code generated especially for ARMv7 on gcc 4.7+
    compilers.
    
    As Arnd Bergmann, points out: The asm-generic version uses the "struct"
    version for native-endian unaligned access and the "byteshift" version
    for the opposite endianess. The current ARM version however uses the
    "byteshift" implementation for both.
    
    Thanks to Nicolas Pitre for the excellent analysis:
    
    Test case:
    
    int foo (int *x) { return get_unaligned(x); }
    long long bar (long long *x) { return get_unaligned(x); }
    
    With the current ARM version:
    
    foo:
    	ldrb	r3, [r0, #2]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 2B], MEM[(const u8 *)x_1(D) + 2B]
    	ldrb	r1, [r0, #1]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 1B], MEM[(const u8 *)x_1(D) + 1B]
    	ldrb	r2, [r0, #0]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D)], MEM[(const u8 *)x_1(D)]
    	mov	r3, r3, asl #16	@ tmp154, MEM[(const u8 *)x_1(D) + 2B],
    	ldrb	r0, [r0, #3]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 3B], MEM[(const u8 *)x_1(D) + 3B]
    	orr	r3, r3, r1, asl #8	@, tmp155, tmp154, MEM[(const u8 *)x_1(D) + 1B],
    	orr	r3, r3, r2	@ tmp157, tmp155, MEM[(const u8 *)x_1(D)]
    	orr	r0, r3, r0, asl #24	@,, tmp157, MEM[(const u8 *)x_1(D) + 3B],
    	bx	lr	@
    
    bar:
    	stmfd	sp!, {r4, r5, r6, r7}	@,
    	mov	r2, #0	@ tmp184,
    	ldrb	r5, [r0, #6]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 6B], MEM[(const u8 *)x_1(D) + 6B]
    	ldrb	r4, [r0, #5]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 5B], MEM[(const u8 *)x_1(D) + 5B]
    	ldrb	ip, [r0, #2]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 2B], MEM[(const u8 *)x_1(D) + 2B]
    	ldrb	r1, [r0, #4]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 4B], MEM[(const u8 *)x_1(D) + 4B]
    	mov	r5, r5, asl #16	@ tmp175, MEM[(const u8 *)x_1(D) + 6B],
    	ldrb	r7, [r0, #1]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 1B], MEM[(const u8 *)x_1(D) + 1B]
    	orr	r5, r5, r4, asl #8	@, tmp176, tmp175, MEM[(const u8 *)x_1(D) + 5B],
    	ldrb	r6, [r0, #7]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 7B], MEM[(const u8 *)x_1(D) + 7B]
    	orr	r5, r5, r1	@ tmp178, tmp176, MEM[(const u8 *)x_1(D) + 4B]
    	ldrb	r4, [r0, #0]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D)], MEM[(const u8 *)x_1(D)]
    	mov	ip, ip, asl #16	@ tmp188, MEM[(const u8 *)x_1(D) + 2B],
    	ldrb	r1, [r0, #3]	@ zero_extendqisi2	@ MEM[(const u8 *)x_1(D) + 3B], MEM[(const u8 *)x_1(D) + 3B]
    	orr	ip, ip, r7, asl #8	@, tmp189, tmp188, MEM[(const u8 *)x_1(D) + 1B],
    	orr	r3, r5, r6, asl #24	@,, tmp178, MEM[(const u8 *)x_1(D) + 7B],
    	orr	ip, ip, r4	@ tmp191, tmp189, MEM[(const u8 *)x_1(D)]
    	orr	ip, ip, r1, asl #24	@, tmp194, tmp191, MEM[(const u8 *)x_1(D) + 3B],
    	mov	r1, r3	@,
    	orr	r0, r2, ip	@ tmp171, tmp184, tmp194
    	ldmfd	sp!, {r4, r5, r6, r7}
    	bx	lr
    
    In both cases the code is slightly suboptimal.  One may wonder why
    wasting r2 with the constant 0 in the second case for example.  And all
    the mov's could be folded in subsequent orr's, etc.
    
    Now with the asm-generic version:
    
    foo:
    	ldr	r0, [r0, #0]	@ unaligned	@,* x
    	bx	lr	@
    
    bar:
    	mov	r3, r0	@ x, x
    	ldr	r0, [r0, #0]	@ unaligned	@,* x
    	ldr	r1, [r3, #4]	@ unaligned	@,
    	bx	lr	@
    
    This is way better of course, but only because this was compiled for
    ARMv7. In this case the compiler knows that the hardware can do
    unaligned word access.  This isn't that obvious for foo(), but if we
    remove the get_unaligned() from bar as follows:
    
    long long bar (long long *x) {return *x; }
    
    then the resulting code is:
    
    bar:
    	ldmia	r0, {r0, r1}	@ x,,
    	bx	lr	@
    
    So this proves that the presumed aligned vs unaligned cases does have
    influence on the instructions the compiler may use and that the above
    unaligned code results are not just an accident.
    
    Still... this isn't fully conclusive without at least looking at the
    resulting assembly fron a pre ARMv6 compilation.  Let's see with an
    ARMv5 target:
    
    foo:
    	ldrb	r3, [r0, #0]	@ zero_extendqisi2	@ tmp139,* x
    	ldrb	r1, [r0, #1]	@ zero_extendqisi2	@ tmp140,
    	ldrb	r2, [r0, #2]	@ zero_extendqisi2	@ tmp143,
    	ldrb	r0, [r0, #3]	@ zero_extendqisi2	@ tmp146,
    	orr	r3, r3, r1, asl #8	@, tmp142, tmp139, tmp140,
    	orr	r3, r3, r2, asl #16	@, tmp145, tmp142, tmp143,
    	orr	r0, r3, r0, asl #24	@,, tmp145, tmp146,
    	bx	lr	@
    
    bar:
    	stmfd	sp!, {r4, r5, r6, r7}	@,
    	ldrb	r2, [r0, #0]	@ zero_extendqisi2	@ tmp139,* x
    	ldrb	r7, [r0, #1]	@ zero_extendqisi2	@ tmp140,
    	ldrb	r3, [r0, #4]	@ zero_extendqisi2	@ tmp149,
    	ldrb	r6, [r0, #5]	@ zero_extendqisi2	@ tmp150,
    	ldrb	r5, [r0, #2]	@ zero_extendqisi2	@ tmp143,
    	ldrb	r4, [r0, #6]	@ zero_extendqisi2	@ tmp153,
    	ldrb	r1, [r0, #7]	@ zero_extendqisi2	@ tmp156,
    	ldrb	ip, [r0, #3]	@ zero_extendqisi2	@ tmp146,
    	orr	r2, r2, r7, asl #8	@, tmp142, tmp139, tmp140,
    	orr	r3, r3, r6, asl #8	@, tmp152, tmp149, tmp150,
    	orr	r2, r2, r5, asl #16	@, tmp145, tmp142, tmp143,
    	orr	r3, r3, r4, asl #16	@, tmp155, tmp152, tmp153,
    	orr	r0, r2, ip, asl #24	@,, tmp145, tmp146,
    	orr	r1, r3, r1, asl #24	@,, tmp155, tmp156,
    	ldmfd	sp!, {r4, r5, r6, r7}
    	bx	lr
    
    Compared to the initial results, this is really nicely optimized and I
    couldn't do much better if I were to hand code it myself.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    modified for Mako from kernel.org reference
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/include/asm/unaligned.h
    
    Conflicts:
    
    	arch/arm/include/asm/unaligned.h

commit bbfc24299c4eddef4cea380bfb1cc8e46d3b146e
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 22:38:49 2014 +0200

    arch/arm/Kconfig: enable unaligned capability for ARM
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit ddbd1f32219522defb9b3ca8e9912b39ddb14eb1
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Oct 15 23:42:55 2012 +0200

    random: make it possible to enable debugging without rebuild
    
    The module parameter that turns debugging mode (which basically means
    printing a few extra lines during runtime) is in '#if 0' block. Forcing
    everyone who would like to see how entropy is behaving on his system to
    rebuild seems to be a little bit too harsh.
    
    If we were concerned about speed, we could potentially turn 'debug' into a
    static key, but I don't think it's necessary.
    
    Drop the '#if 0' block to allow using the 'debug' parameter without rebuilding.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit 4b82801417366648d530def79b61ee9624513940
Author: Jarod Wilson <jarod@redhat.com>
Date:   Tue Nov 6 10:42:42 2012 -0500

    random: prime last_data value per fips requirements
    
    The value stored in last_data must be primed for FIPS 140-2 purposes. Upon
    first use, either on system startup or after an RNDCLEARPOOL ioctl, we
    need to take an initial random sample, store it internally in last_data,
    then pass along the value after that to the requester, so that consistency
    checks aren't being run against stale and possibly known data.
    
    CC: Herbert Xu <herbert@gondor.apana.org.au>
    CC: "David S. Miller" <davem@davemloft.net>
    CC: Matt Mackall <mpm@selenic.com>
    CC: linux-crypto@vger.kernel.org
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: Jarod Wilson <jarod@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit 32d12d4f1bdbca3d9827f80c93751b7533e576a5
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 23:17:37 2014 +0200

    random: fix debug format strings MIME-Version: 1.0 Content-Type: text/plain; charset=UTF-8 Content-Transfer-Encoding: 8bit
    
    Fix the following warnings in formatting debug output:
    
    drivers/char/random.c: In function âer_secondary_poolâdrivers/char/random.c:827: warning: format ââxpects type âtâbut argument 7 has type âze_târivers/char/random.c: In function âcountâdrivers/char/random.c:859: warning: format ââxpects type âtâbut argument 5 has type âze_târivers/char/random.c:881: warning: format ââxpects type âtâbut argument 5 has type âze_târivers/char/random.c: In function ândom_readâdrivers/char/random.c:1141: warning: format ââxpects type âtâbut argument 5 has type âize_târivers/char/random.c:1145: warning: format ââxpects type âtâbut argument 5 has type âize_târivers/char/random.c:1145: warning: format ââxpects type âtâbut argument 6 has type âng unsigned intâby using '%zd' instead of '%d' to properly denote ssize_t/size_t conversion.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit efa6385cf01c8953ff95dd41e3ecaac542103b7f
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 23:12:20 2014 +0200

    random: use the arch-specific rng in xfer_secondary_pool
    
    commit e6d4947b12e8ad947add1032dd754803c6004824 upstream.
    
    If the CPU supports a hardware random number generator, use it in
    xfer_secondary_pool(), where it will significantly improve things and
    where we can afford it.
    
    Also, remove the use of the arch-specific rng in
    add_timer_randomness(), since the call is significantly slower than
    get_cycles(), and we're much better off using it in
    xfer_secondary_pool() anyway.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Patrick Tjin <pattjin@google.com>

commit 123756c02260d6a95af8c0ca82d90c6924d64ad4
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Fri Jul 6 14:03:18 2012 -0400

    random: fix up sparse warnings
    
    Add extern and static declarations to suppress sparse warnings
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit 1af88d80e3fb49b68e9a790095cfd77ac05b3eb9
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Sep 16 19:00:25 2013 -0700

    msm: acpuclock-krait: Work-around retention corner-case
    
    The CPU clock source MUXes cannot switch while a CPU is in
    retention, but callers of acpuclock APIs may not known that
    the CPU is in retention.
    
    Work around this problem be always performing the CPU MUX
    switch on the affected CPU using smp_call_function_single()
    in cases where cross-calling might otherwise be possible.
    The one exception to this is during hotplug code paths,
    where cross-calling is expected by design (since the target
    CPU is not online and cannot be in retention).
    
    Change-Id: I76b6de436e71da7979f30bdce841f414c26bfe44
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit c807cadf2a93db03b2a16ad716e6ccc76123f779
Author: Henrik Rydberg <rydberg@euromail.se>
Date:   Sat Sep 15 15:23:35 2012 +0200

    Input: Send events one packet at a time
    
    On heavy event loads, such as a multitouch driver, the irqsoff latency
    can be as high as 250 us.  By accumulating a frame worth of data
    before passing it on, the latency can be dramatically reduced.  As a
    side effect, the special EV_SYN handling can be removed, since the
    frame is now atomic.
    
    This patch adds the events() handler callback and uses it if it
    exists. The latency is improved by 50 us even without the callback.
    
    Change-Id: Iebd9b1868ae6300a922a45b6d104e7c2b38e4cf5
    Cc: Daniel Kurtz <djkurtz@chromium.org>
    Tested-by: Benjamin Tissoires <benjamin.tissoires@enac.fr>
    Tested-by: Ping Cheng <pingc@wacom.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Acked-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Henrik Rydberg <rydberg@euromail.se>
    
    Input: Improve the events-per-packet estimate
    
    The events-per-packet estimate has so far been used by MT devices
    only. This patch adjusts the packet buffer size to also accomodate the
    KEY and MSC events.  Keyboards normally send one or two keys at a
    time. MT devices normally send a number of button keys along with the
    MT information.  The buffer size chosen here covers those cases, and
    matches the default buffer size in evdev. Since the input estimate is
    now preferred, remove the special input-mt estimate.
    
    Reviewed-and-tested-by: Ping Cheng <pingc@wacom.com>
    Tested-by: Benjamin Tissoires <benjamin.tissoires@enac.fr>
    Acked-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Henrik Rydberg <rydberg@euromail.se>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 49b443d96534fd87a33cfeb6419623d0bdfd7758
Author: franciscofranco <franciscofranco.1990@gmail.com>
Date:   Wed Dec 5 11:56:19 2012 -0800

    Enable pipe flag.
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 32f71da723e7ef664db2f890d0ddd87802228f48
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Mon Feb 4 15:33:44 2013 -0600

    cfg80211: Fix memory leak
    
    When a driver requests a specific regulatory domain after cfg80211 already
    has one, a struct ieee80211_regdomain is leaked.
    
    Change-Id: Id28fc9861b9c911a97bd242439eabca097d76258
    Reported-by: Larry Finger <Larry.Finger@lwfinger.net>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Git-commit: b7566fc363e23f0efd3fa1e1460f9421cdc0d77e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [mattw@codeaurora.org: trivially backport to the msm-3.4 kernel]
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit f1c0ff47a706ac7af2b54cbdb93fda10f406581f
Author: Gagan Mac <gmac@codeaurora.org>
Date:   Sun Jul 21 00:41:07 2013 -0600

    msm: msm_bus: Add checks during bus client unregistration
    
    During unregistration clients should withdraw all pending
    requests. This was earlier done in update-request function.
    However, that caused acpuclock client requests to be
    incorrectly zeroed out.
    
    It seems more appropriate to let the client ensure that
    they correctly clean up their requests than to have bus
    driver forcefully remove non-zero requests. Hence, a
    warning has been added in the unregistration path.
    
    Change-Id: Icfd4f62f46fdd3219dc685007029be21b983c2d0
    CRs-Fixed: 510389
    Signed-off-by: Gagan Mac <gmac@codeaurora.org>

commit 7d54f65932b7212cb6dafeabd904f33e7a9ad8af
Author: Alok Chauhan <alokc@codeaurora.org>
Date:   Wed Sep 11 16:39:20 2013 +0530

    msm: msm_bus: send correct value to bw division function
    
    While calculating bw for existing client request
    sometimes it can go to negative if client unregisters
    or send request with lesser ab/ib values. Bus driver
    will divide bandwidth if interleaving enabled.
    
    Added a macro wrapper to handle negative bw values
    since the division utility function can't handle
    negative values
    
    CRs-Fixed: 521940
    Change-Id: Ic3a216b8efb535593fe02ad3228d71808d2e0f98
    Signed-off-by: Alok Chauhan <alokc@codeaurora.org>

commit 769552d938b7d12da459ea917b55308da230a13e
Author: Alok Chauhan <alokc@codeaurora.org>
Date:   Wed Sep 18 18:00:20 2013 +0530

    msm: msm_bus: Fix the type error causing bandwidth overflow
    
    On legacy chipsets, long int was being used to store
    return value after calculating interleaved bw. However,
    NoCs support 64-bit integers ab/ib values. problme occurs
    if client request for higher bw and if difference of ab
    value exceeds the range of 32 bit integer, the Value
    overflows and turns negative, which leads to wrong bw calculation.
    
    This patch fixes this integer overflow by correcting argument
    type to store bw.
    
    CRs-Fixed: 537213
    Change-Id: I8c6c79ba245a988c2c54ccaca3f3eaf5cb857ce5
    Signed-off-by: Alok Chauhan <alokc@codeaurora.org>

commit b0b77932ca9bc42fde0d5ead2073f730772630ea
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 25 21:34:39 2014 +0100

    Updated fs eventpoll from dorimanx sources!

commit f680d7222b258a8999c4dfd4038a1bd212f2621a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 25 21:15:49 2014 +0100

    exfat: AIO Optimization compatibility fixup

commit 4a7df3b3e3b70d5d74fd026ccf208d5f0ffa90a2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 25 20:26:02 2014 +0100

    New kernel released!

commit f7f2cb8e44135aa220d3c081bb4554558aa6b486
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 22:46:57 2014 +0200

    hwrng: add randomness to system from rng sources
    
    When bringing a new RNG source online, it seems like it would make sense
    to use some of its bytes to make the system entropy pool more random,
    as done with all sorts of other devices that contain per-device or
    per-boot differences.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 9cb68ecae0f0d62a08239c50fee587e2d57ddb8a
Author: Shaohua Li <shli@fusionio.com>
Date:   Fri Nov 9 08:44:27 2012 +0100

    block: recursive merge requests
    
    In a workload, thread 1 accesses a, a+2, ..., thread 2 accesses a+1, a+3,....
    When the requests are flushed to queue, a and a+1 are merged to (a, a+1), a+2
    and a+3 too to (a+2, a+3), but (a, a+1) and (a+2, a+3) aren't merged.
    
    If we do recursive merge for such interleave access, some workloads throughput
    get improvement. A recent worload I'm checking on is swap, below change
    boostes the throughput around 5% ~ 10%.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 9b428e442aaca73e674ae1af6e6f328e06eac4ce
Author: Eric Paris <eparis@redhat.com>
Date:   Fri Jul 6 14:13:29 2012 -0400

    SELinux: include definition of new capabilities
    
    The kernel has added CAP_WAKE_ALARM and CAP_EPOLLWAKEUP.  We need to
    define these in SELinux so they can be mediated by policy.
    
    Change-Id: I8a3e0db15ec5f4eb05d455a57e8446a8c2b484c2
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Signed-off-by: James Morris <james.l.morris@oracle.com>
    [sds: rename epollwakeup to block_suspend to match upstream merge]
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>

commit f776189e8be6dbba288634285150783cfd7ceb2f
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 14 18:19:59 2013 +0400

    block: account iowait time when waiting for completion of IO request
    
    Using wait_for_completion() for waiting for a IO request to be executed
    results in wrong iowait time accounting. For example, a system having
    the only task doing write() and fdatasync() on a block device can be
    reported being idle instead of iowaiting as it should because
    blkdev_issue_flush() calls wait_for_completion() which in turn calls
    schedule() that does not increment the iowait proc counter and thus does
    not turn on iowait time accounting.
    
    The patch makes block layer use wait_for_completion_io() instead of
    wait_for_completion() where appropriate to account iowait time
    correctly.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit bea917d516fcefdbfdb3abaf0fd2d33ba1e42dac
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Oct 27 03:18:39 2013 -0500

    include/scsi/scsi.h: remove obsolete function
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	include/scsi/scsi.h

commit f5aaa30ef747602cea92cf0d44ed035c813e60e5
Author: Shawn Bohrer <sbohrer@rgmadvisors.com>
Date:   Sat Oct 5 12:20:29 2013 -0500

    sched/rt: Remove redundant nr_cpus_allowed test
    
    Date	Fri, 4 Oct 2013 14:24:53 -0500
    
    From: Shawn Bohrer <sbohrer@rgmadvisors.com>
    
    In 76854c7e8f3f4172fef091e78d88b3b751463ac6 "sched: Use
    rt.nr_cpus_allowed to recover select_task_rq() cycles" an optimization
    was added to select_task_rq_rt() that immediately returns when
    p->nr_cpus_allowed == 1 at the beginning of the function.  This makes
    the latter p->nr_cpus_allowed > 1 check redundant and can be removed.
    Signed-off-by: Shawn Bohrer <sbohrer@rgmadvisors.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 52e517a76c4ee50f1de4be5932a2a5757af1db16
Author: Tingting Yang <tingting@codeaurora.org>
Date:   Tue Aug 6 11:12:52 2013 +0800

    msm: move printk out of spin lock low_water_lock
    
    cpu3 stuck in printk more time in spin lock low_water_lock cause cpu0
    get spin lock fail and system crashed.
    
    CRs-Fixed: 521570
    Change-Id: I75356a4b4171ae2888ce6cce792f569b5ca8cdcf
    Signed-off-by: Tingting Yang <tingting@codeaurora.org>

commit 1c1af96c308385632ec08d184d1935f66c6298b5
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Sep 2 14:29:47 2013 -0500

    drivers/tty/smux_private.h: SLAB/SLOB compatibility fixup
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 62f8a2b379307be1a781c84597189d0522a170dc
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:24:39 2013 -0500

    sched: document the difference between nr_running and h_nr_running
    
    Date    Sun, 18 Aug 2013 16:25:22 +0800
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 7171df61b402d644be5b625cdfa600062404054e
Author: Lei Wen <leiwen@marvell.com>
Date:   Sun Sep 1 18:00:30 2013 -0500

    sched: change active_load_balance_cpu_stop to use h_nr_running
    
    Date	Sun, 18 Aug 2013 16:25:21 +0800
    
    We should only avoid do the active load balance when there is no
    cfs type task. If just use rq->nr_running, it is possible for the
    source cpu has multiple rt task, while zero cfs task, so that it
    would confuse the active load balance function that try to move,
    but find no task it could move.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit c82d48bae27b612340cd7d2a9035a382562d193f
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:20:33 2013 -0500

    sched: change find_busiest_queue to h_nr_running
    
    Date    Sun, 18 Aug 2013 16:25:20 +0800
    
    Since find_busiest_queue try to avoid do load balance for runqueue
    which has only one cfs task and its load is above the imbalance
    value calculated, we should use h_nr_running of cfs instead of
    nr_running of rq.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit bf4b03713621f38ab7ab0ffda089063f0c3a8bfd
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:16:42 2013 -0500

    sched: change update_sg_lb_stats to h_nr_running
    
    Date    Sun, 18 Aug 2013 16:25:19 +0800
    
    Since update_sg_lb_stats is used to calculate sched_group load
    difference of cfs type task, it should use h_nr_running instead of
    nr_running of rq.
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    bacported to Linux 3.4 by faux123
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 41960cd365eaef65bc0911b4e21f2b1c61d27dc5
Author: Lei Wen <leiwen@marvell.com>
Date:   Sun Sep 1 17:48:08 2013 -0500

    sched: change pick_next_task_fair to h_nr_running
    
    Date	Sun, 18 Aug 2013 16:25:18 +0800
    
    Since pick_next_task_fair only want to ensure there is some task in the
    run queue to be picked up, it should use the h_nr_running instead of
    nr_running, since nr_running cannot present all tasks if group existed.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit fcf032302b62b7e27d920c295cec286b085dbf27
Author: Lei Wen <leiwen@marvell.com>
Date:   Sun Sep 1 17:44:00 2013 -0500

    sched: change cpu_avg_load_per_task using h_nr_running
    
    Date	Sun, 18 Aug 2013 16:25:16 +0800
    
    Since cpu_avg_load_per_task is used only by cfs scheduler, its meaning
    should present the average cfs type task load in the current run queue.
    Thus we change it to h_nr_running for well presenting its meaning.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 5fe0b3dedd3f1268151b27529f280f3776dc9185
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 18:29:35 2013 -0500

    kernel/sched/fair.c: fix merge derp
    
     from patch sched: change load balance number to h_nr_running of run queue
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 00cc909397b105e6d43f4d0542afce88a7b12a22
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:14:16 2013 -0500

    sched: change load balance number to h_nr_running of run queue
    
    Date    Sun, 18 Aug 2013 16:25:15 +0800
    
    Since rq->nr_running would include both migration and rt task, it is not
    reasonable to seek to move nr_running number of task in the load_balance
    function, since it only apply to cfs type.
    
    Change it to cfs's h_nr_running, which could well present the task
    number in current cfs queue.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    backported to Linux 3.4 by faux123
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 90bf8bee2e92e31467c5c19689c795a4793f71fa
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:10:10 2013 -0500

    sched: Periodically decay max cost of idle balance
    
    This RFC patch builds on patch 2 and periodically decays that max value to
    do idle balancing per sched domain.
    
    Though we want to decay it fairly consistently, we may not want to lower it by
    too much each time, especially since avg_idle is capped based on that value.
    So I thought that decaying the value every second and lowering it by half a
    percent each time appeared to be fairly reasonable.
    
    This change would allow us to remove the limit we set on each domain's max cost
    to idle balance. Also, since the max can be reduced now, we now have to
    update rq->max_idle balance_cost more frequently. So after every idle balance,
    we loop through the sched domain to find the max sd's newidle load balance cost
    for any one domain. Then we will set rq->max_idle_balance_cost to that value.
    
    Since we are now decaying the max cost to do idle balancing, that max cost can
    also become not high enough. One possible explanation for why is that
    besides the time spent on each newidle load balance, there are other costs
    associated with attempting idle balancing. Idle balance also releases and
    reacquires a spin lock. That cost is not counted when we keep track of each
    domain's cost to do newidle load balance. Also, acquiring the rq locks can
    potentially prevent other CPUs from running something useful. And after
    migrating tasks, it might potentially have to pay the costs of cache misses and
    refreshing tasks' cache.
    
    Because of that, this patch also compares avg_idle with max cost to do idle
    balancing + sched_migration_cost. While using the max cost helps reduce
    overestimating the average idle, the sched_migration_cost can help account
    for those additional costs of idle balancing.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    [peterz: rewrote the logic, but kept the spirit]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    backported to Linux 3.4
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 2e084a46af5ab97eb17c2d21a2dc24e0dd3f6506
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:05:40 2013 -0500

    sched: Consider max cost of idle balance per sched domain
    
    Date    Thu, 29 Aug 2013 13:05:35 -0700
    
    In this patch, we keep track of the max cost we spend doing idle load balancing
    for each sched domain. If the avg time the CPU remains idle is less then the
    time we have already spent on idle balancing + the max cost of idle balancing
    in the sched domain, then we don't continue to attempt the balance. We also
    keep a per rq variable, max_idle_balance_cost, which keeps track of the max
    time spent on newidle load balances throughout all its domains. Additionally,
    we swap sched_migration_cost used in idle_balance for rq->max_idle_balance_cost.
    
    By using the max, we avoid overrunning the average. This further reduces the chance
    we attempt balancing when the CPU is not idle for longer than the cost to balance.
    
    I also limited the max cost of each domain to 5*sysctl_sched_migration_cost as
    a way to prevent the max from becoming too inflated.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    
    backported for Linux 3.4
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 67fd0ff945e8389a751b52042508f8f5d07d4e36
Author: Jason Low <jason.low2@hp.com>
Date:   Sat Aug 31 01:47:10 2013 -0500

    sched: Reduce overestimating rq->avg_idle
    
    Date	Thu, 29 Aug 2013 13:05:34 -0700
    
    When updating avg_idle, if the delta exceeds some max value, then avg_idle
    gets set to the max, regardless of what the previous avg was. This can cause
    avg_idle to often be overestimated.
    
    This patch modifies the way we update avg_idle by always updating it with the
    function call to update_avg() first. Then, if avg_idle exceeds the max, we set
    it to the max.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 88a3a5e79b137a3ba977b63aec230fac51057478
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Jun 17 12:49:48 2013 -0500

    sched: scale cpu load for judgment of group imbalance
    
    Date	Mon, 17 Jun 2013 21:00:24 +0800
    
    We cannot compare two load directly from two cpus, since the cpu power
    over two cpu may vary largely.
    
    Suppose we meet such two kind of cpus.
    CPU A:
    	No real time work, and there are 3 task, with rq->load.weight
    being 512.
    CPU B:
    	Has real time work, and it take 3/4 of the cpu power, which
    makes CFS only take 1/4, that is 1024/4=256 cpu power. And over its CFS
    runqueue, there is only one task with weight as 128.
    
    Since both cpu's CFS task take for half of the CFS's cpu power, it
    should be considered as balanced in such case.
    
    But original judgment like:
            if ((max_cpu_load - min_cpu_load) >= avg_load_per_task &&
                (max_nr_running - min_nr_running) > 1)
    It makes (512-128)>=((512+128)/4), and lead to imbalance conclusion...
    Make the load as scaled, to avoid such case.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    
    modified for Mako kernel from LKML reference
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 6698cd5d88404ce3a2111662689786c89a80253b
Author: Lei Wen <leiwen@marvell.com>
Date:   Mon Jun 17 12:45:40 2013 -0500

    sched: scale the busy and this queue's per-task load before compare
    
    Date	Mon, 17 Jun 2013 21:00:23 +0800
    
    Since for max_load and this_load, they are the value that already be
    scaled. It is not reasonble to get a minimum value between the scaled
    and non-scaled value, like below example.
    	min(sds->busiest_load_per_task, sds->max_load);
    
    Also add comment over in what condition, there would be cpu power gain
    in move the load.
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 2153a9723b1545d05930c6875228df0c105b27c8
Author: Lei Wen <leiwen@marvell.com>
Date:   Mon Jun 17 12:44:18 2013 -0500

    sched: reduce calculation effort in fix_small_imbalance
    
    Date	Mon, 17 Jun 2013 21:00:22 +0800
    
    Actually all below item could be repalced by scaled_busy_load_per_task
    	(sds->busiest_load_per_task * SCHED_POWER_SCALE)
    		/sds->busiest->sgp->power;
    
    Signed-off-by: Lei Wen <leiwen@marvell.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 019906038e8bc6399346e5bcb952055e94080d2c
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Jun 17 12:42:02 2013 -0500

    sched: remove WARN_ON(!sd) from init_sched_groups_power()
    
    Date	Tue, 11 Jun 2013 16:32:45 +0530
    
    sd can't be NULL in init_sched_groups_power() and so checking it for NULL isn't
    useful. In case it is required, then also we need to rearrange the code a bit as
    we already accessed invalid pointer sd to get sg: sg = sd->groups.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ed5075cd41865df6e074e6b29bdc6a8e5fb6109e
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Jun 17 12:41:16 2013 -0500

    sched: don't call get_group() for covered cpus
    
    Date	Tue, 11 Jun 2013 16:32:44 +0530
    
    In build_sched_groups() we don't need to call get_group() for cpus which are
    already covered in previous iterations. So, call get_group() after checking if
    cpu is covered or not.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit db4ad886ac748ec9f34079c02c762618e6eed608
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Jun 17 12:39:50 2013 -0500

    sched: Use cached value of span instead of calling sched_domain_span()
    
    Date	Tue, 11 Jun 2013 16:32:43 +0530
    
    In the beginning of build_sched_groups() we called sched_domain_span() and
    cached its return value in span. Few statements later we are calling it again to
    get the same pointer.
    
    Lets use the cached value instead as it hasn't changed in between.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 1f7c67adb4c0fcf95bab2a76edf0770ae8347713
Author: JP Abgrall <jpa@google.com>
Date:   Mon Apr 29 16:07:00 2013 -0700

    ARM: fault: assume no context when IRQs are disabled during data abort.
    
    Bail out early if IRQs are disabled in do_page_fault or else
      [14415.157266] BUG: sleeping function called from invalid context at arch/arm/mm/fault.c:301
    
    Russell King's idea from
      http://comments.gmane.org/gmane.linux.ports.arm.omap/59256
    
    Signed-off-by: JP Abgrall <jpa@google.com>

commit 3a37e5985ae0c0b29aed0a2cbf0e4ace20f41dd6
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jun 6 18:31:22 2013 -0500

    sched: Remove unused params of build_sched_domain()
    
    Date	Tue, 4 Jun 2013 16:50:19 +0530
    
    build_sched_domain() never uses parameter struct s_data *d and so passing it is
    useless.
    
    Remove it.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 44ea52381d15c121c7119349264bf65b925b4615
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jun 6 18:29:01 2013 -0500

    sched: Optimize build_sched_domains() for saving first SD node for a cpu
    
    We are saving first scheduling domain for a cpu in build_sched_domains() by
    iterating over the nested sd->child list. We don't actually need to do it this
    way.
    
    tl will be equal to sched_domain_topology for the first iteration and so we can
    set *per_cpu_ptr(d.sd, i) based on that.  So, save pointer to first SD while
    running the iteration loop over tl's.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 2e3b25d659878ddebae22ec3229ad515be9bff3c
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu Jun 6 18:26:42 2013 -0500

    sched: Optimize build_sched_domains() for saving first SD node for a cpu
    
    Date	Tue, 4 Jun 2013 16:50:18 +0530
    
    We are saving first scheduling domain for a cpu in build_sched_domains() by
    iterating over the nested sd->child list. We don't actually need to do it this
    way.
    
    *per_cpu_ptr(d.sd, i) is guaranteed to be NULL in the beginning as we have
    called __visit_domain_allocation_hell() which does a memset to zero for struct
    s_data.
    
    So, save pointer to first SD while running the iteration loop over tl's.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b7df9f9727c9353fbf067be1235371bd045acd9c
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 22:40:25 2014 +0200

    ARM: mutex: use generic atomic_dec-based implementation for ARMv6+
    
    Commit a76d7bd96d65 ("ARM: 7467/1: mutex: use generic xchg-based
    implementation for ARMv6+") removed the barrier-less, ARM-specific
    mutex implementation in favour of the generic xchg-based code.
    
    Since then, a bug was uncovered in the xchg code when running on SMP
    platforms, due to interactions between the locking paths and the
    MUTEX_SPIN_ON_OWNER code. This was fixed in 0bce9c46bf3b ("mutex: place
    lock in contended state after fastpath_lock failure"), however, the
    atomic_dec-based mutex algorithm is now marginally more efficient for
    ARM (~0.5% improvement in hackbench scores on dual A15).
    
    This patch moves ARMv6+ platforms to the atomic_dec-based mutex code.
    
    Change-Id: I8f64e98ccb61cc1cb9cb68ee15e55d8a792792f5
    Cc: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-on: http://git-master/r/130941
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Bharat Nihalani <bnihalani@nvidia.com>
    Tested-by: Bharat Nihalani <bnihalani@nvidia.com>
    
    Conflicts:
    	arch/arm/include/asm/mutex.h

commit c4e296ee0aee5e31eb4536ad9da0a15249051921
Author: Namjae Jeon <linkinjeon@gmail.com>
Date:   Tue Dec 11 16:00:21 2012 -0800

    writeback: remove nr_pages_dirtied arg from balance_dirty_pages_ratelimited_nr()
    
    There is no reason to pass the nr_pages_dirtied argument, because
    nr_pages_dirtied value from the caller is unused in
    balance_dirty_pages_ratelimited_nr().
    
    Signed-off-by: Namjae Jeon <linkinjeon@gmail.com>
    Signed-off-by: Vivek Trivedi <vtrivedi018@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit bb74cc89b0bc979500450554e2a1c37e2adc2df8
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Fri Sep 28 20:27:49 2012 +0800

    CPU hotplug, writeback: Don't call writeback_set_ratelimit() too often during hotplug
    
    The CPU hotplug callback related to writeback calls writeback_set_ratelimit()
    during every state change in the hotplug sequence. This is unnecessary
    since num_online_cpus() changes only once during the entire hotplug operation.
    
    So invoke the function only once per hotplug, thereby avoiding the
    unnecessary repetition of those costly calculations.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 612ddf8c22157e7a25770a44c0942164a0449ce4
Author: Peter Korsgaard <peter.korsgaard@barco.com>
Date:   Thu May 3 12:58:49 2012 +0200

    f_fs: ffs_func_free: cleanup requests allocated by autoconfig
    
    functionfs was leaking request objects created by autoconfig.
    
    Bug: 8659094
    
    Change-Id: I641326cb5cb26e0a2ffa082cd2be2c21c66c38e5
    Signed-off-by: Peter Korsgaard <peter.korsgaard@barco.com>
    Signed-off-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Benoit Goby <benoit@android.com>

commit e24bc775be7673c19aaa4e6cbf8da4d7a0aea075
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu Apr 4 13:42:51 2013 -0500

    aio: convert the ioctx list to radix tree
    
    Date	Wed, 3 Apr 2013 16:20:48 +0300
    
    When using a large number of threads performing AIO operations the
    IOCTX list may get a significant number of entries which will cause
    significant overhead. For example, when running this fio script:
    
    rw=randrw; size=256k ;directory=/mnt/fio; ioengine=libaio; iodepth=1
    blocksize=1024; numjobs=512; thread; loops=100
    
    on an EXT2 filesystem mounted on top of a ramdisk we can observe up to
    30% CPU time spent by lookup_ioctx:
    
     32.51%  [guest.kernel]  [g] lookup_ioctx
      9.19%  [guest.kernel]  [g] __lock_acquire.isra.28
      4.40%  [guest.kernel]  [g] lock_release
      4.19%  [guest.kernel]  [g] sched_clock_local
      3.86%  [guest.kernel]  [g] local_clock
      3.68%  [guest.kernel]  [g] native_sched_clock
      3.08%  [guest.kernel]  [g] sched_clock_cpu
      2.64%  [guest.kernel]  [g] lock_release_holdtime.part.11
      2.60%  [guest.kernel]  [g] memcpy
      2.33%  [guest.kernel]  [g] lock_acquired
      2.25%  [guest.kernel]  [g] lock_acquire
      1.84%  [guest.kernel]  [g] do_io_submit
    
    This patchs converts the ioctx list to a radix tree. For a performance
    comparison the above FIO script was run on a 2 sockets 8 core
    machine. This are the results for the original list based
    implementation and for the radix tree based implementation:
    
    cores         1         2         4         8         16        32
    list        111025 ms  62219 ms  34193 ms  22998 ms  19335 ms  15956 ms
    radix        75400 ms  42668 ms  23923 ms  17206 ms  15820 ms  13295 ms
    % of radix
    relative      68%       69%       70%       75%       82%       83%
    to list
    
    To consider the impact of the patch on the typical case of having
    only one ctx per process the following FIO script was run:
    
    rw=randrw; size=100m ;directory=/mnt/fio; ioengine=libaio; iodepth=1
    blocksize=1024; numjobs=1; thread; loops=100
    
    on the same system and the results are the following:
    
    list        65241 ms
    radix       65402 ms
    % of radix
    relative    100.25%
    to list
    
    Cc: Andi Kleen <ak@linux.intel.com>
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    modified for Mako hybrid from LKML
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit a1fbd0b2b15502ed718b1a2fac51587672d8a40f
Author: faux123 <reioux@gmail.com>
Date:   Mon Feb 4 23:27:21 2013 -0800

    tmpfs: add support for read_iter and write_iter
    
    Convert tmpfs do_shmem_file_read() to shmem_file_read_iter().
    Make file_read_iter_actor() global so tmpfs can use it too: delete
    file_read_actor(), which was made global in 2.4.4 for use by tmpfs.
    Replace tmpfs generic_file_aio_write() by generic_file_write_iter().
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit f5632443ec426d4dc965d4748505f40adb5a614a
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 19:47:42 2013 -0800

    iov_iter: move into its own file (bug fix)
    
    I found the problem. iov_iter_shorten() wasn't setting i->count to the new
    value.
    
    This fixes it. I'll fix the patchset tomorrow.
    
    Thanks,
    Shaggy

commit 460f3f8d280d29e1f2ece9b717444f53563e43eb
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 18:00:37 2013 -0800

    ecrpytfs: Convert aio_read/write ops to read/write_iter
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Tyler Hicks <tyhicks@canonical.com>
    Cc: Dustin Kirkland <dustin.kirkland@gazzang.com>
    Cc: ecryptfs@vger.kernel.org

commit 7fb2a88da795353e6ffd157f6777698b634eb3af
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:58:19 2013 -0800

    ext4: add support for read_iter and write_iter
    
    use the generic_file_read_iter(), create ext4_file_write_iter() based on
    ext4_file_write(), and make ext4_file_write() a wrapper around
    ext4_file_write_iter().
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: linux-ext4@vger.kernel.org
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 6efcccebac442e690893804b542b8ba68759b1a1
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 17:51:05 2013 -0800

    fs: add read_iter and write_iter to several file systems
    
    These are the simple ones.
    
    File systems that use generic_file_aio_read() and generic_file_aio_write()
    can trivially support generic_file_read_iter() and generic_file_write_iter().
    
    This patch adds those file_operations for 9p, adfs, affs, bfs, exofs, ext2,
    ext3, fat, f2fs, hfs, hfsplus, hostfs, hpfs, jfs, jffs2, logfs, minix, nilfs2,
    omfs, ramfs, reiserfs, romfs, sysv, and ufs.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Acked-by: Boaz Harrosh <bharrosh@panasas.com>
    Cc: Zach Brown <zab@zabbo.net>
    Cc: v9fs-developer@lists.sourceforge.net
    Cc: Tigran A. Aivazian <tigran@aivazian.fsnet.co.uk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: linux-ext4@vger.kernel.org
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Benny Halevy <bhalevy@tonian.com>
    Cc: osd-dev@open-osd.org
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Mikulas Patocka <mikulas@artax.karlin.mff.cuni.cz>
    Cc: jfs-discussion@lists.sourceforge.net
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: linux-mtd@lists.infradead.org
    Cc: Joern Engel <joern@logfs.org>
    Cc: Prasad Joshi <prasadjoshi.linux@gmail.com>
    Cc: logfs@logfs.org
    Cc: linux-nilfs@vger.kernel.org
    Cc: Bob Copeland <me@bobcopeland.com>
    Cc: linux-karma-devel@lists.sourceforge.net
    Cc: reiserfs-devel@vger.kernel.org
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Evgeniy Dushistov <dushistov@mail.ru>

commit dfd125f368ca082e78f2835479e0a9a950109c38
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 17:44:20 2013 -0800

    fs: use read_iter and write_iter rather than aio_read and aio_write
    
    File systems implementing read_iter & write_iter should not be required
    to keep aio_read and aio_write as well. The vfs should always call
    read/write_iter if they exist. This will make it easier to remove the
    aio_read/write operation in the future.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-fsdevel@vger.kernel.org

commit 90a2c23f0d85d1e2a9e91b09bc484434945fe1fb
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:41:00 2013 -0800

    fs: create file_readable() and file_writable() functions
    
    Create functions to simplify if file_ops contain either a read
    or aio_read op, or likewise write or aio_write. We will be adding
    read_iter and write_iter and don't need to be complicating the code
    in multiple places.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 6be9c9ffda0f56018add3a03d413a4e5a86dc0c4
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:35:59 2013 -0800

    loop: use aio to perform io on the underlying file
    
    This uses the new kernel aio interface to process loopback IO by
    submitting concurrent direct aio.  Previously loop's IO was serialized
    by synchronous processing in a thread.
    
    The aio operations specify the memory for the IO with the bio_vec arrays
    directly instead of mappings of the pages.
    
    The use of aio operations is enabled when the backing file supports the
    read_iter, write_iter and direct_IO methods.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit f9dd559e425db008907a2f06b3d099b0761b170b
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:22:49 2013 -0800

    bio: add bvec_length(), like iov_length()
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit a857d9d18200f68df6d6e7c9903827ff708949a6
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:10:09 2013 -0800

    aio: add aio support for iov_iter arguments
    
    This adds iocb cmds which specify that memory is held in iov_iter
    structures.  This lets kernel callers specify memory that can be
    expressed in an iov_iter, which includes pages in bio_vec arrays.
    
    Only kernel callers can provide an iov_iter so it doesn't make a lot of
    sense to expose the IOCB_CMD values for this as part of the user space
    ABI.
    
    But kernel callers should also be able to perform the usual aio
    operations which suggests using the the existing operation namespace and
    support code.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit f697d03d7b86e22294748094a753643d5e76e7ce
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 17:06:57 2013 -0800

    aio: add aio_kernel_() interface
    
    This adds an interface that lets kernel callers submit aio iocbs without
    going through the user space syscalls.  This lets kernel callers avoid
    the management limits and overhead of the context.  It will also let us
    integrate aio operations with other kernel apis that the user space
    interface doesn't have access to.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 185867f0080c39796608b234e31a38ca37adf2f6
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 16:56:45 2013 -0800

    fs: pull iov_iter use higher up the stack
    
    Right now only callers of generic_perform_write() pack their iovec
    arguments into an iov_iter structure.  All the callers higher up in the
    stack work on raw iovec arguments.
    
    This patch introduces the use of the iov_iter abstraction higher up the
    stack.  Private generic path functions are changed to operation on
    iov_iter instead of on raw iovecs.  Exported interfaces that take iovecs
    immediately pack their arguments into an iov_iter and call into the
    shared functions.
    
    File operation struct functions are added with iov_iter as an argument
    so that callers to the generic file system functions can specify
    abstract memory rather than iovec arrays only.
    
    Almost all of this patch only transforms arguments and shouldn't change
    functionality.  The buffered read path is the exception.  We add a
    read_actor function which uses the iov_iter helper functions instead of
    operating on each individual iovec element.  This may improve
    performance as the iov_iter helper can copy multiple iovec elements from
    one mapped page cache page.
    
    As always, the direct IO path is special.  Sadly, it may still be
    cleanest to have it work on the underlying memory structures directly
    instead of working through the iov_iter abstraction.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 688655d997bb4a9ea4feed0334e614b743947543
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 16:48:01 2013 -0800

    dio: add bio_vec support to __blockdev_direct_IO()
    
    The trick here is to initialize the dio state so that do_direct_IO()
    consumes the pages we provide and never tries to map user pages.  This
    is done by making sure that final_block_in_request covers the page that
    we set in the dio.  do_direct_IO() will return before running out of
    pages.
    
    The caller is responsible for dirtying these pages, if needed.  We add
    an option to the dio struct that makes sure we only dirty pages when
    we're operating on iovecs of user addresses.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako kernel from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 1bcc0beaa23945429223f480d2446e5dd7310b3d
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 22:39:42 2014 +0200

    dio: Convert direct_IO to use iov_iter
    
    Change the direct_IO aop to take an iov_iter argument rather than an iovec.
    This will get passed down through most filesystems so that only the
    __blockdev_direct_IO helper need be aware of whether user or kernel memory
    is being passed to the function.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    heavily backported for Mako from LKML reference
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 68d2baa82fb1a3eb354ffb01e7f9f787e38efeff
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 23:44:30 2014 +0200

    Add missing include/linux/memcopy.h

commit 4e2ed0e43054cccf4e6fefa44078684c201ecfa0
Author: Alex Frid <afrid@nvidia.com>
Date:   Wed May 16 14:27:13 2012 -0700

    proc: enhance time-average nr_running stats
    
    Add time-average nr_running to loadavg printout
    
    Bug 958978
    
    Change-Id: I5c6904efb52a86f4964eb66c1576fc91f60f5b1d
    Signed-off-by: Alex Frid <afrid@nvidia.com>
    (cherry picked from commit 86f3642cc44a69d1e4798719bd9182cd6923f526)
    Reviewed-on: http://git-master/r/111636
    Reviewed-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Tested-by: Sai Gurrappadi <sgurrappadi@nvidia.com>
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>

commit 66145c879d2c0292534f45fc0f1a4089356ebca6
Author: faux123 <reioux@gmail.com>
Date:   Thu Nov 22 07:28:42 2012 -0800

    lib/string: use glibc version
    
    the performance of memcpy and memmove of the general version is very
    inefficient, this patch improved them.
    
    Signed-off-by: Miao Xie <miaox*******>
    Signed-off-by: faux123 <reioux@gmail.com>

commit c8dca11476b4f45cd632ef0f94944fafa991ed71
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:10 2013 -0600

    iov_iter: let callers extract iovecs and bio_vecs
    
    direct IO treats memory from user iovecs and memory from arrays of
    kernel pages very differently.  User memory is pinned and worked with in
    batches while kernel pages are always pinned and don't require
    additional processing.
    
    Rather than try and provide an abstraction that includes these
    different behaviours we let direct IO extract the memory structs and
    hand them to the existing code.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit 9231bfa8334b59d6e75152a34d12730af90fe4d3
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:09 2013 -0600

    iov_iter: add a shorten call
    
    The generic direct write path wants to shorten its memory vector.  It
    does this when it finds that it has to perform a partial write due to
    LIMIT_FSIZE.  .direct_IO() always performs IO on all of the referenced
    memory because it doesn't have an argument to specify the length of the
    IO.
    
    We add an iov_iter operation for this so that the generic path can ask
    to shorten the memory vector without having to know what kind it is.
    We're happy to shorten the kernel copy of the iovec array, but we refuse
    to shorten the bio_vec array and return an error in this case.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit eaf08aa3795376d3f368daf05e9024e79451a8a3
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:08 2013 -0600

    iov_iter: add bvec support
    
    This adds a set of iov_iter_ops calls which work with memory which is
    specified by an array of bio_vec structs instead of an array of iovec
    structs.
    
    The big difference is that the pages referenced by the bio_vec elements
    are pinned.  They don't need to be faulted in and we can always use
    kmap_atomic() to map them one at a time.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit 203128de6574580dfa2e2810ba9e3e293c2aa2a5
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 15:32:38 2013 -0800

    iov_iter: hide iovec details behind ops function pointers
    
    This moves the current iov_iter functions behind an ops struct of
    function pointers.  The current iov_iter functions all work with memory
    which is specified by iovec arrays of user space pointers.
    
    This patch is part of a series that lets us specify memory with bio_vec
    arrays of page pointers.  By moving to an iov_iter operation struct we
    can add that support in later patches in this series by adding another
    set of function pointers.
    
    I only came to this after having initialy tried to teach the current
    iov_iter functions about bio_vecs by introducing conditional branches
    that dealt with bio_vecs in all the functions.  It wasn't pretty.  This
    approach seems to be the lesser evil.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>
    modified for Mako kernel from LKML
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit bc14b79c669f2da1d006188a5a40ba11ce8ccf0c
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Mon Jan 28 11:23:07 2013 -0600

    fuse: convert fuse to use iov_iter_copy_[to|from]_user
    
    A future patch hides the internals of struct iov_iter, so fuse should
    be using the supported interface.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Acked-by: Miklos Szeredi <mszeredi@suse.cz>
    Cc: fuse-devel@lists.sourceforge.net

commit a418bc8928b5407a84decc82c0e2ad33e9d9971f
Author: Zach Brown <zab@zabbo.net>
Date:   Mon Jan 28 11:23:06 2013 -0600

    iov_iter: add copy_to_user support
    
    This adds iov_iter wrappers around copy_to_user() to match the existing
    wrappers around copy_from_user().
    
    This will be used by the generic file system buffered read path.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Zach Brown <zab@zabbo.net>

commit 14d7d58c97bba0801c6a314f2c58d0980d383453
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Sat Feb 2 15:04:23 2013 -0800

    iov_iter: iov_iter_copy_from_user() should use non-atomic copy
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>

commit 6e0a31d695475b7075d745f5c789d04e6d96a8d0
Author: Zach Brown <zab@zabbo.net>
Date:   Sat Feb 2 15:03:12 2013 -0800

    iov_iter: move into its own file
    
    This moves the iov_iter functions in to their own file.  We're going to
    be working on them in upcoming patches.  They become sufficiently large,
    and remain self-contained, to justify seperating them from the rest of
    the huge mm/filemap.c.
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Acked-by: Jeff Moyer <jmoyer@redhat.com>
    Cc: Zach Brown <zab@zabbo.net>

commit a9b67f35f5784be8b61f2e8ceb34f1aed77701b6
Author: Archana Sathyakumar <asathyak@codeaurora.org>
Date:   Wed May 1 15:26:36 2013 -0600

    cpufreq: Resolve CPUFREQ_NOTIFY issue
    
    When the cpufreq_stat receives the frequency policy notification,
    it checks if the frequency table and stats table are present for the cpu.
    If both are present, it returns with -EBUSY error to the notification.
    Due to this, none of the other registered modules receive this
    notification.
    
    If the stat table and sysfs nodes are already created, return 0 instead
    of returning EBUSY error.
    
    Change-Id: I1fd4a46e6a65edfc6253c4271288f10912ae2ad7
    Signed-off-by: Archana Sathyakumar <asathyak@codeaurora.org>

commit b71af7ec3dc81ec968853abbd931b4fa1b93767e
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Tue Jun 18 17:05:19 2013 -0500

    SELinux: Reduce overhead of mls_level_isvalid() function call
    
    Date	Mon, 10 Jun 2013 13:55:08 -0400
    
    v4->v5:
      - Fix scripts/checkpatch.pl warning.
    
    v3->v4:
      - Merge the 2 separate while loops in ebitmap_contains() into
        a single one.
    
    v2->v3:
      - Remove unused local variables i, node from mls_level_isvalid().
    
    v1->v2:
     - Move the new ebitmap comparison logic from mls_level_isvalid()
       into the ebitmap_contains() helper function.
     - Rerun perf and performance tests on the latest v3.10-rc4 kernel.
    
    While running the high_systime workload of the AIM7 benchmark on
    a 2-socket 12-core Westmere x86-64 machine running 3.10-rc4 kernel
    (with HT on), it was found that a pretty sizable amount of time was
    spent in the SELinux code. Below was the perf trace of the "perf
    record -a -s" of a test run at 1500 users:
    
      5.04%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      1.96%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      1.95%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    The ebitmap_get_bit() was the hottest function in the perf-report
    output.  Both the ebitmap_get_bit() and find_next_bit() functions
    were, in fact, called by mls_level_isvalid(). As a result, the
    mls_level_isvalid() call consumed 8.95% of the total CPU time of
    all the 24 virtual CPUs which is quite a lot. The majority of the
    mls_level_isvalid() function invocations come from the socket creation
    system call.
    
    Looking at the mls_level_isvalid() function, it is checking to see
    if all the bits set in one of the ebitmap structure are also set in
    another one as well as the highest set bit is no bigger than the one
    specified by the given policydb data structure. It is doing it in
    a bit-by-bit manner. So if the ebitmap structure has many bits set,
    the iteration loop will be done many times.
    
    The current code can be rewritten to use a similar algorithm as the
    ebitmap_contains() function with an additional check for the
    highest set bit. The ebitmap_contains() function was extended to
    cover an optional additional check for the highest set bit, and the
    mls_level_isvalid() function was modified to call ebitmap_contains().
    
    With that change, the perf trace showed that the used CPU time drop
    down to just 0.08% (ebitmap_contains + mls_level_isvalid) of the
    total which is about 100X less than before.
    
      0.07%            ls  [kernel.kallsyms]     [k] ebitmap_contains
      0.05%            ls  [kernel.kallsyms]     [k] ebitmap_get_bit
      0.01%            ls  [kernel.kallsyms]     [k] mls_level_isvalid
      0.01%            ls  [kernel.kallsyms]     [k] find_next_bit
    
    The remaining ebitmap_get_bit() and find_next_bit() functions calls
    are made by other kernel routines as the new mls_level_isvalid()
    function will not call them anymore.
    
    This patch also improves the high_systime AIM7 benchmark result,
    though the improvement is not as impressive as is suggested by the
    reduction in CPU time spent in the ebitmap functions. The table below
    shows the performance change on the 2-socket x86-64 system (with HT
    on) mentioned above.
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | high_systime |     +0.1%     |     +0.9%      |     +2.6%       |
    +--------------+---------------+----------------+-----------------+
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 3b9afdd4eeb1399285806db53fdcb4ebce0407d5
Author: Francisco Franco <franciscofranco.1990@gmail.com>
Date:   Wed Sep 25 03:03:01 2013 +0100

    audit: kiss goodbye you stupid piece of crap logging messages.
    
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit e893be5992f0a12b43e184afce36b5c127b54e75
Author: glewarne <g.lewarne@gmx.co.uk>
Date:   Fri Nov 8 18:36:31 2013 +0000

    increase max readahead

commit d0aa493892875e37e46b63961aa73f88e4301e0d
Author: darren.kang <darren.kang@lge.com>
Date:   Thu Aug 8 18:26:26 2013 -0700

    msm: qpnp-power-on: Initialize earlier
    
    Some drivers need to use qpnp-power-on functionality ealier. So initialize
    this earlier for that.
    
    Change-Id: Ia555a2876acf3d1cb4046d9fe2262567bdb4610a
    Signed-off-by: darren.kang <darren.kang@lge.com>
    Signed-off-by: Devin Kim <dojip.kim@lge.com>

commit 007e999dc2fec9d2e86ff46c93878c437f1afa0f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 24 23:49:30 2014 +0100

    Optimized config. Enabled modules for PPP Widget!

commit e464db67f7226485f11ed90b1c838879d17f037a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 24 16:00:55 2014 +0100

    Revert "ARM: 7587/1: implement optimized percpu variable access"
    
    This reverts commit 73bac847d460f3e793fb38a10dd85c4b65aea03f.

commit 862d47f90c6a68cb2a8f6950c3e33413cfca4380
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 24 15:59:23 2014 +0100

    Test config!

commit e4db58d58e2396a3e35e7c207b026c61e747fbda
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 24 15:34:43 2014 +0100

    Merge "jf: update defconfig" into cm-11.0

commit 9a5ef63d458fdfc15f257db806518b4f05b254dd
Author: Dave Daynard <nardholio@gmail.com>
Date:   Thu Feb 20 22:03:50 2014 -0500

    sec-battery: let's override the charging mode only for LPM
    
    There's no reason for this override in normal booting mode.
    
    Change-Id: Ie370c17deddfee3372265cc1fa2bc1dc0e317269

commit 0ca5f3fb5841efcfcc14e94da6f55dc0ecfed49f
Author: Dave Daynard <nardholio@gmail.com>
Date:   Fri Feb 21 21:04:25 2014 +0000

    Revert "Revert "sec-battery: Standardize the output of the "online" property""
    
    This reverts commit c71b8f76b6f1eee12167cdc59d052d47e9431f69.
    
    Change-Id: I0f78dba48959345343dc3440f2c1fa1c5d0e42f7

commit 13957e479679c5af46787417a919b602fee68833
Author: dcd <dcd1182@gmail.com>
Date:   Tue Feb 11 12:48:32 2014 -0600

    jf: update defconfig
    
    go back to gzip; we have space
    
    ext4 provides ext2/3 support
    
    remove useless shit
    
    Change-Id: Icad8b3152661dd24e1e2fe9c45fff875fb7a0a89
    
    Conflicts:
    	arch/arm/configs/cyanogen_jf_defconfig

commit 3449a74abdc7d448da8f1e5dfdb24cf3b16debe8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 24 15:17:31 2014 +0100

    New public kernel released!

commit fedc5502dae73fe6cda1e4ee2ba66710f01fc2ee
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 24 14:08:51 2014 +0100

    Linux 3.4.82

commit 46e668e7552fa4d7a7af425d363dd04e623c1fd9
Author: Konstantin Dorfman <kdorfman@codeaurora.org>
Date:   Sun Feb 2 12:49:54 2014 +0200

    mmc: queue: do not clean current request when urgent in progress
    
    As a result of following fix: "mmc: core: do not reinsert prepeared
    FUA and FLUSH requests in stop flow" REQ_FUA and REQ_FLUSH requests
    are not reinserted back into i/o scheduler, but instead started request
    execution.
    
    This change will prevent cleaning current request pointer for such
    requests.
    
    Change-Id: I25f8706954fb538be62182c87d5fb20354696b7a
    CRs-fixed: 600127
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    
    Conflicts:
    	drivers/mmc/core/core.c

commit e3be62c7328e218dea752556a8ab51ca14ed5bd2
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sun Jan 26 12:05:57 2014 +0200

    block: do not notify urgent request, when flush with data in flight
    
    MMC device driver implements URGENT request execution with priority
    (using stop flow), as a result currently running (and prepared) request
    may be reinserted back into I/O scheduler. This will break block layer
    logic of flushes (flush request should not be inserted into I/O scheduler).
    
    Block layer flush machinery keep q->flush_data_in_flight list updated with
    started but not completed flush requests with data (REQ_FUA).
    
    This change will not notify underling block device driver about pending
    urgent request during flushes in flight.
    
    Change-Id: I8b654925a3c989250fcb8f4f7c998795fb203923
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit c81cc6b792a3324abb4083f45cd65dbe79226a06
Author: Asutosh Das <asutoshd@codeaurora.org>
Date:   Fri Jan 24 11:36:07 2014 +0530

    mmc: block: check for NULL pointer before dereferencing
    
    mmc block data can be NULL. Hence, check for NULL before
    dereferencing md.
    
    CRs-Fixed: 562259
    Change-Id: I0182c216ec73347cdd2ea464f593839fffd242a9
    Signed-off-by: Asutosh Das <asutoshd@codeaurora.org>
    
    Conflicts:
    	drivers/mmc/card/block.c

commit 594101066760eb664619db1828cacd3ad5458500
Author: dorimanx <yuri@bynet.co.il>
Date:   Sun Feb 23 12:39:48 2014 +0200

    Fix warning in fs/Kconfig

commit 5002da8462ea881a0e281d0cca30360682d80581
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 23 17:19:08 2014 +0100

    New kernel test version!

commit 97c2a4487b41b5fcafd0b8dd07f5dd8e1d76cb94
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 23 14:24:02 2014 +0100

    Revert "softirq: Use hotplug thread infrastructure"
    
    This reverts commit d0422b29702421197ef64fa9caad597bcf8fb7b2.

commit af9a083683b8524fe97becce0316a45332f6def8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 23 14:23:42 2014 +0100

    Revert "rcu: Yield simpler"
    
    This reverts commit abf4f50b915c3daceef4602413a990dcae61d344.

commit f7260babaf3b4012236108e30b2b19fb7d9a50fb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 23 14:23:19 2014 +0100

    Revert "rcu: Use smp_hotplug_thread facility for RCUs per-CPU kthread"
    
    This reverts commit 84fb1b1afc42135c1ce1e3c91162509b5102ae21.

commit f7652452f4710ee20dd32bc409a8f8f97f9f4809
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 23 14:22:51 2014 +0100

    Revert "ARM: smp: Fix suspicious RCU originating from cpu_die()"
    
    This reverts commit dd530c9a513d90f07bfbff8fa6fa4bc1b873f7e8.

commit eef2cd76e25b6940c388678fafcc1dcff55a76c1
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 20:52:12 2013 -0500

    init/Kconfig: decouple DEBUG_KERNEL from EXPERT settings
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 98b352429a8ef6712efd4fcceaf76e2930600124
Author: Lance Poore <linuxsociety@gmail.com>
Date:   Sun Aug 12 15:25:06 2012 -0500

    SCHEDULER: Autogroup patch group by current user android UID instead of task ID

commit 07bf9ca9d68dbccc1a74c97daefa21d8c3496b41
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Apr 15 14:36:55 2013 -0500

    mutex: back out architecture specific check for negative mutex count
    
    Date	Mon, 15 Apr 2013 10:37:59 -0400
    
    If it is confirmed that all the supported architectures can allow a
    negative mutex count without incorrect behavior, we can then back
    out the architecture specific change and allow the mutex count to
    go to any negative number. That should further reduce contention for
    non-x86 architecture.
    
    If this is not the case, this patch should be dropped.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>

commit 03eef1e6cb32f16524f50bcf24ddb292f200f64b
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Apr 15 14:34:47 2013 -0500

    mutex: Queue mutex spinners with MCS lock to reduce cacheline contention
    
    Date	Mon, 15 Apr 2013 10:37:58 -0400
    
    The current mutex spinning code (with MUTEX_SPIN_ON_OWNER option turned
    on) allow multiple tasks to spin on a single mutex concurrently. A
    potential problem with the current approach is that when the mutex
    becomes available, all the spinning tasks will try to acquire the
    mutex more or less simultaneously. As a result, there will be a lot of
    cacheline bouncing especially on systems with a large number of CPUs.
    
    This patch tries to reduce this kind of contention by putting the
    mutex spinners into a queue so that only the first one in the queue
    will try to acquire the mutex. This will reduce contention and allow
    all the tasks to move forward faster.
    
    The queuing of mutex spinners is done using an MCS lock based
    implementation which will further reduce contention on the mutex
    cacheline than a similar ticket spinlock based implementation. This
    patch will add a new field into the mutex data structure for holding
    the MCS lock. This expands the mutex size by 8 bytes for 64-bit system
    and 4 bytes for 32-bit system. This overhead will be avoid if the
    MUTEX_SPIN_ON_OWNER option is turned off.
    
    The following table shows the jobs per minute (JPM) scalability data
    on an 8-node 80-core Westmere box with a 3.7.10 kernel. The numactl
    command is used to restrict the running of the fserver workloads to
    1/2/4/8 nodes with hyperthreading off.
    
    +-----------------+-----------+-----------+-------------+----------+
    |  Configuration  | Mean JPM  | Mean JPM  |  Mean JPM   | % Change |
    |		  | w/o patch | patch 1   | patches 1&2 |  1->1&2  |
    +-----------------+------------------------------------------------+
    |		  |              User Range 1100 - 2000		   |
    +-----------------+------------------------------------------------+
    | 8 nodes, HT off |  227972   |  227237   |   305043    |  +34.2%  |
    | 4 nodes, HT off |  393503   |  381558   |   394650    |   +3.4%  |
    | 2 nodes, HT off |  334957   |  325240   |   338853    |   +4.2%  |
    | 1 node , HT off |  198141   |  197972   |   198075    |   +0.1%  |
    +-----------------+------------------------------------------------+
    |		  |              User Range 200 - 1000		   |
    +-----------------+------------------------------------------------+
    | 8 nodes, HT off |  282325   |  312870   |   332185    |   +6.2%  |
    | 4 nodes, HT off |  390698   |  378279   |   393419    |   +4.0%  |
    | 2 nodes, HT off |  336986   |  326543   |   340260    |   +4.2%  |
    | 1 node , HT off |  197588   |  197622   |   197582    |    0.0%  |
    +-----------------+-----------+-----------+-------------+----------+
    At low user range 10-100, the JPM differences were within +/-1%. So
    they are not that interesting.
    
    The fserver workload uses mutex spinning extensively. With just
    the mutex change in the first patch, there is no noticeable change
    in performance.  Rather, there is a slight drop in performance. This
    mutex spinning patch more than recovers the lost performance and show
    a significant increase of +30% at high user load with the full 8 nodes.
    Similar improvements were also seen in a 3.8 kernel.
    
    The table below shows the %time spent by different kernel functions
    as reported by perf when running the fserver workload at 1500 users
    with all 8 nodes.
    
    +-----------------------+-----------+---------+-------------+
    |        Function       |  % time   | % time  |   % time    |
    |                       | w/o patch | patch 1 | patches 1&2 |
    +-----------------------+-----------+---------+-------------+
    | __read_lock_failed    |  34.96%   | 34.91%  |   29.14%    |
    | __write_lock_failed   |  10.14%   | 10.68%  |    7.51%    |
    | mutex_spin_on_owner   |   3.62%   |  3.42%  |    2.33%    |
    | mspin_lock            |    N/A    |   N/A   |    9.90%    |
    | __mutex_lock_slowpath |   1.46%   |  0.81%  |    0.14%    |
    | _raw_spin_lock        |   2.25%   |  2.50%  |    1.10%    |
    +-----------------------+-----------+---------+-------------+
    The fserver workload for an 8-node system is dominated by the
    contention in the read/write lock. Mutex contention also plays a
    role. With the first patch only, mutex contention is down (as shown by
    the __mutex_lock_slowpath figure) which help a little bit. We saw only
    a few percents improvement with that.
    
    By applying patch 2 as well, the single mutex_spin_on_owner figure is
    now split out into an additional mspin_lock figure. The time increases
    from 3.42% to 11.23%. It shows a great reduction in contention among
    the spinners leading to a 30% improvement. The time ratio 9.9/2.33=4.3
    indicates that there are on average 4+ spinners waiting in the spin_lock
    loop for each spinner in the mutex_spin_on_owner loop. Contention in
    other locking functions also go down by quite a lot.
    
    The table below shows the performance change of both patches 1 & 2 over
    patch 1 alone in other AIM7 workloads (at 8 nodes, hyperthreading off).
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | alltests     |      0.0%     |     -0.8%      |     +0.6%       |
    | five_sec     |     -0.3%     |     +0.8%      |     +0.8%       |
    | high_systime |     +0.4%     |     +2.4%      |     +2.1%       |
    | new_fserver  |     +0.1%     |    +14.1%      |    +34.2%       |
    | shared       |     -0.5%     |     -0.3%      |     -0.4%       |
    | short        |     -1.7%     |     -9.8%      |     -8.3%       |
    +--------------+---------------+----------------+-----------------+
    The short workload is the only one that shows a decline in performance
    probably due to the spinner locking and queuing overhead.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>

commit 9eda39c3903eb09cb9b39fc88a70957d146eef56
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Apr 15 14:33:17 2013 -0500

    mutex: Make more scalable by doing less atomic operations
    
    Date	Mon, 15 Apr 2013 10:37:57 -0400
    
    In the __mutex_lock_common() function, an initial entry into
    the lock slow path will cause two atomic_xchg instructions to be
    issued. Together with the atomic decrement in the fast path, a total
    of three atomic read-modify-write instructions will be issued in
    rapid succession. This can cause a lot of cache bouncing when many
    tasks are trying to acquire the mutex at the same time.
    
    This patch will reduce the number of atomic_xchg instructions used by
    checking the counter value first before issuing the instruction. The
    atomic_read() function is just a simple memory read. The atomic_xchg()
    function, on the other hand, can be up to 2 order of magnitude or even
    more in cost when compared with atomic_read(). By using atomic_read()
    to check the value first before calling atomic_xchg(), we can avoid a
    lot of unnecessary cache coherency traffic. The only downside with this
    change is that a task on the slow path will have a tiny bit
    less chance of getting the mutex when competing with another task
    in the fast path.
    
    The same is true for the atomic_cmpxchg() function in the
    mutex-spin-on-owner loop. So an atomic_read() is also performed before
    calling atomic_cmpxchg().
    
    The mutex locking and unlocking code for the x86 architecture can allow
    any negative number to be used in the mutex count to indicate that some
    tasks are waiting for the mutex. I am not so sure if that is the case
    for the other architectures. So the default is to avoid atomic_xchg()
    if the count has already been set to -1. For x86, the check is modified
    to include all negative numbers to cover a larger case.
    
    The following table shows the jobs per minutes (JPM) scalability data
    on an 8-node 80-core Westmere box with a 3.7.10 kernel. The numactl
    command is used to restrict the running of the high_systime workloads
    to 1/2/4/8 nodes with hyperthreading on and off.
    
    +-----------------+-----------+------------+----------+
    |  Configuration  | Mean JPM  |  Mean JPM  | % Change |
    |		  | w/o patch | with patch |	      |
    +-----------------+-----------------------------------+
    |		  |      User Range 1100 - 2000	      |
    +-----------------+-----------------------------------+
    | 8 nodes, HT on  |    36980   |   148590  | +301.8%  |
    | 8 nodes, HT off |    42799   |   145011  | +238.8%  |
    | 4 nodes, HT on  |    61318   |   118445  |  +51.1%  |
    | 4 nodes, HT off |   158481   |   158592  |   +0.1%  |
    | 2 nodes, HT on  |   180602   |   173967  |   -3.7%  |
    | 2 nodes, HT off |   198409   |   198073  |   -0.2%  |
    | 1 node , HT on  |   149042   |   147671  |   -0.9%  |
    | 1 node , HT off |   126036   |   126533  |   +0.4%  |
    +-----------------+-----------------------------------+
    |		  |       User Range 200 - 1000	      |
    +-----------------+-----------------------------------+
    | 8 nodes, HT on  |   41525    |   122349  | +194.6%  |
    | 8 nodes, HT off |   49866    |   124032  | +148.7%  |
    | 4 nodes, HT on  |   66409    |   106984  |  +61.1%  |
    | 4 nodes, HT off |  119880    |   130508  |   +8.9%  |
    | 2 nodes, HT on  |  138003    |   133948  |   -2.9%  |
    | 2 nodes, HT off |  132792    |   131997  |   -0.6%  |
    | 1 node , HT on  |  116593    |   115859  |   -0.6%  |
    | 1 node , HT off |  104499    |   104597  |   +0.1%  |
    +-----------------+------------+-----------+----------+
    At low user range 10-100, the JPM differences were within +/-1%. So
    they are not that interesting.
    
    AIM7 benchmark run has a pretty large run-to-run variance due to random
    nature of the subtests executed. So a difference of less than +-5%
    may not be really significant.
    
    This patch improves high_systime workload performance at 4 nodes
    and up by maintaining transaction rates without significant drop-off
    at high node count.  The patch has practically no impact on 1 and 2
    nodes system.
    
    The table below shows the percentage time (as reported by perf
    record -a -s -g) spent on the __mutex_lock_slowpath() function by
    the high_systime workload at 1500 users for 2/4/8-node configurations
    with hyperthreading off.
    
    +---------------+-----------------+------------------+---------+
    | Configuration | %Time w/o patch | %Time with patch | %Change |
    +---------------+-----------------+------------------+---------+
    |    8 nodes    |      65.34%     |      0.69%       |  -99%   |
    |    4 nodes    |       8.70%	  |      1.02%	     |  -88%   |
    |    2 nodes    |       0.41%     |      0.32%       |  -22%   |
    +---------------+-----------------+------------------+---------+
    It is obvious that the dramatic performance improvement at 8
    nodes was due to the drastic cut in the time spent within the
    __mutex_lock_slowpath() function.
    
    The table below show the improvements in other AIM7 workloads (at 8
    nodes, hyperthreading off).
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | alltests     |     +0.6%     |   +104.2%      |   +185.9%       |
    | five_sec     |     +1.9%     |     +0.9%      |     +0.9%       |
    | fserver      |     +1.4%     |     -7.7%      |     +5.1%       |
    | new_fserver  |     -0.5%     |     +3.2%      |     +3.1%       |
    | shared       |    +13.1%     |   +146.1%      |   +181.5%       |
    | short        |     +7.4%     |     +5.0%      |     +4.2%       |
    +--------------+---------------+----------------+-----------------+
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Reviewed-by: Davidlohr Bueso <davidlohr.bueso@hp.com>

commit 6df4e2fccc5086361d8defd3d4b273d94cd0025e
Author: Michael Bohan <mbohan@codeaurora.org>
Date:   Fri Apr 12 13:42:11 2013 -0500

    hrtimer: Prevent enqueue of hrtimer on dead CPU
    
    Date	Wed, 10 Apr 2013 14:07:48 -0700
    
    When switching the hrtimer cpu_base, we briefly allow for
    preemption to become enabled by unlocking the cpu_base lock.
    During this time, the CPU corresponding to the new cpu_base
    that was selected may in fact go offline. In this scenario, the
    hrtimer is enqueued to a CPU that's not online, and therefore
    it never fires.
    
    As an example, consider this example:
    
    CPU #0                          CPU #1
    ----                            ----
    ...                             hrtimer_start()
                                     lock_hrtimer_base()
                                     switch_hrtimer_base()
                                      cpu = hrtimer_get_target() -> 1
                                      spin_unlock(&cpu_base->lock)
                                    <migrate thread to CPU #0>
                                    <offline>
    spin_lock(&new_base->lock)
    this_cpu = 0
    cpu != this_cpu
    enqueue_hrtimer(cpu_base #1)
    To prevent this scenario, verify that the CPU corresponding to
    the new cpu_base is indeed online before selecting it in
    hrtimer_switch_base(). If it's not online, fallback to using the
    base of the current CPU.
    
    Signed-off-by: Michael Bohan <mbohan@codeaurora.org>

commit 43ab036604dc1e33b85effb49d46cf1b91d7c202
Author: Michael Bohan <mbohan@codeaurora.org>
Date:   Fri Apr 12 13:40:55 2013 -0500

    hrtimer: Consider preemption when migrating hrtimer cpu_bases
    
    Date	Wed, 10 Apr 2013 14:07:47 -0700
    
    When switching to a new cpu_base in switch_hrtimer_base(), we
    briefly enable preemption by unlocking the cpu_base lock in two
    places. During this interval it's possible for the running thread
    to be swapped to a different CPU.
    
    Consider the following example:
    
    CPU #0                                 CPU #1
    ----                                   ----
    hrtimer_start()                        ...
     lock_hrtimer_base()
     switch_hrtimer_base()
      this_cpu = 0;
      target_cpu_base = 0;
      raw_spin_unlock(&cpu_base->lock)
    <migrate to CPU 1>
    ...                                    this_cpu == 0
                                           cpu == this_cpu
                                           timer->base = CPU #0
                                           timer->base != LOCAL_CPU
    Since the cached this_cpu is no longer accurate, we'll skip the
    hrtimer_check_target() check. Once we eventually go to program
    the hardware, we'll decide not to do so since it knows the real
    CPU that we're running on is not the same as the chosen base. As
    a consequence, we may end up missing the hrtimer's deadline.
    
    Fix this by updating the local CPU number each time we retake a
    cpu_base lock in switch_hrtimer_base().
    
    Another possibility is to disable preemption across the whole of
    switch_hrtimer_base. This looks suboptimal since preemption
    would be disabled while waiting for lock(s).
    
    Signed-off-by: Michael Bohan <mbohan@codeaurora.org>

commit 4412f5d8220d64f814c98a3e3756c5e1267e6812
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:32:24 2013 -0700

    arch/arm/kernel/armksyms: fix merge derp
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    
    	arch/arm/kernel/armksyms.c

commit 1f072641568361daa46df33597b4c5338d166d17
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Jul 31 09:28:31 2012 +0400

    switch the protection of percpu_counter list to spinlock
    
    ... making percpu_counter_destroy() non-blocking
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit ad7ee38719952087a727f915531c056fde8f4d85
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 09:25:16 2013 -0800

    decompress_unlzo: fix compilation error
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit df68d4f402223e6914de9e9f1c50d0e69c8a4ea4
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 09:02:08 2013 -0800

    ARM: 7593/1: nommu: do not enable DCACHE_WORD_ACCESS when !CONFIG_MMU
    
    Commit b9a50f74905a ("ARM: 7450/1: dcache: select DCACHE_WORD_ACCESS for
    little-endian ARMv6+ CPUs") added support for word-at-time path
    comparisons, relying on the ability to perform unaligned loads with
    negligible performance impact in hardware.
    
    For nommu configurations without MPU support, this is unpredictable and
    so we should fall back to the byte-by-byte routines.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Jonathan Austin <jonathan.austin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    modified for Mako from kernel.org
    
    Signed-off-by: faux123 <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/Kconfig

commit 6b95ae121a716504b6e7e0769aa00057ca2fa39c
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Wed Aug 15 16:28:36 2012 +0100

    ARM: 7492/1: add strstr declaration for decompressors
    
    With the generic unaligned.h, more kernel headers get pulled in including
    dynamic_debug.h which needs strstr. As it is not really used, we only need
    a declaration here.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Tested-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit a2920ead9ab2cd7312fd68ae1809b9411444b860
Author: Will Deacon <will.deacon@arm.com>
Date:   Sat Feb 2 08:49:59 2013 -0800

    ARM: dcache: select DCACHE_WORD_ACCESS for little-endian ARMv6+ CPUs
    
    DCACHE_WORD_ACCESS uses the word-at-a-time API for optimised string
    comparisons in the vfs layer.
    
    This patch implements support for load_unaligned_zeropad for ARM CPUs
    with native support for unaligned memory accesses (v6+) when running
    little-endian.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    Conflicts:
    	arch/arm/Kconfig

commit 9c74d1b8fc4709188dc96448ff33349ac925d946
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Oct 20 22:52:36 2013 -0500

    ARM: use generic strnlen_user and strncpy_from_user functions
    
    This patch implements the word-at-a-time interface for ARM using the
    same algorithm as x86. We use the fls macro from ARMv5 onwards, where
    we have a clz instruction available which saves us a mov instruction
    when targetting Thumb-2. For older CPUs, we use the magic 0x0ff0001
    constant. Big-endian configurations make use of the implementation from
    asm-generic.
    
    With this implemented, we can replace our byte-at-a-time strnlen_user
    and strncpy_from_user functions with the optimised generic versions.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    modified for Mako from LKML reference
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	arch/arm/Kconfig

commit b15c858e477e81a8c1a8ba358f6656fe5957d3b4
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon May 28 12:59:56 2012 +1000

    lib: Fix generic strnlen_user for 32-bit big-endian machines
    
    The aligned_byte_mask() definition is wrong for 32-bit big-endian
    machines: the "7-(n)" part of the definition assumes a long is 8
    bytes.  This fixes it by using BITS_PER_LONG - 8 instead of 8*7.
    Tested on 32-bit and 64-bit PowerPC.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 63257db944f4f005f6dbbc07d2197510204c809b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 20 22:51:14 2013 -0500

    lib: add generic strnlen_user() function
    
    This adds a new generic optimized strnlen_user() function that uses the
    <asm/word-at-a-time.h> infrastructure to portably do efficient string
    handling.
    
    In many ways, strnlen is much simpler than strncpy, and in particular we
    can always pre-align the words we load from memory.  That means that all
    the worries about alignment etc are a non-issue, so this one can easily
    be used on any architecture.  You obviously do have to do the
    appropriate word-at-a-time.h macros.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 52b25c893b558f49f01ce4cad63775c2e614a2a9
Author: faux123 <reioux@gmail.com>
Date:   Thu May 9 15:24:32 2013 -0700

    word-at-a-time: make the interfaces truly generic
    
    This changes the interfaces in <asm/word-at-a-time.h> to be a bit more
    complicated, but a lot more generic.
    
    In particular, it allows us to really do the operations efficiently on
    both little-endian and big-endian machines, pretty much regardless of
    machine details.  For example, if you can rely on a fast population
    count instruction on your architecture, this will allow you to make your
    optimized <asm/word-at-a-time.h> file with that.
    
    NOTE! The "generic" version in include/asm-generic/word-at-a-time.h is
    not truly generic, it actually only works on big-endian.  Why? Because
    on little-endian the generic algorithms are wasteful, since you can
    inevitably do better. The x86 implementation is an example of that.
    
    (The only truly non-generic part of the asm-generic implementation is
    the "find_zero()" function, and you could make a little-endian version
    of it.  And if the Kbuild infrastructure allowed us to pick a particular
    header file, that would be lovely)
    
    The <asm/word-at-a-time.h> functions are as follows:
    
     - WORD_AT_A_TIME_CONSTANTS: specific constants that the algorithm
       uses.
    
     - has_zero(): take a word, and determine if it has a zero byte in it.
       It gets the word, the pointer to the constant pool, and a pointer to
       an intermediate "data" field it can set.
    
       This is the "quick-and-dirty" zero tester: it's what is run inside
       the hot loops.
    
     - "prep_zero_mask()": take the word, the data that has_zero() produced,
       and the constant pool, and generate an *exact* mask of which byte had
       the first zero.  This is run directly *outside* the loop, and allows
       the "has_zero()" function to answer the "is there a zero byte"
       question without necessarily getting exactly *which* byte is the
       first one to contain a zero.
    
       If you do multiple byte lookups concurrently (eg "hash_name()", which
       looks for both NUL and '/' bytes), after you've done the prep_zero_mask()
       phase, the result of those can be or'ed together to get the "either
       or" case.
    
     - The result from "prep_zero_mask()" can then be fed into "find_zero()"
       (to find the byte offset of the first byte that was zero) or into
       "zero_bytemask()" (to find the bytemask of the bytes preceding the
       zero byte).
    
       The existence of zero_bytemask() is optional, and is not necessary
       for the normal string routines.  But dentry name hashing needs it, so
       if you enable DENTRY_WORD_AT_A_TIME you need to expose it.
    
    This changes the generic strncpy_from_user() function and the dentry
    hashing functions to use these modified word-at-a-time interfaces.  This
    gets us back to the optimized state of the x86 strncpy that we lost in
    the previous commit when moving over to the generic version.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	fs/namei.c
    
    Conflicts:
    
    	fs/namei.c

commit fd40f664f90351c37914de00d8723e8bea94e634
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Oct 20 22:50:15 2013 -0500

    lib: Sparc's strncpy_from_user is generic enough, move under lib/
    
    To use this, an architecture simply needs to:
    
    1) Provide a user_addr_max() implementation via asm/uaccess.h
    
    2) Add "select GENERIC_STRNCPY_FROM_USER" to their arch Kcnfig
    
    3) Remove the existing strncpy_from_user() implementation and symbol
       exports their architecture had.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: David Howells <dhowells@redhat.com>
    adapted for Mako from kernel.org reference
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	lib/Makefile

commit 8729b35fe3070f043157a234f94bc96c25f4168e
Author: faux123 <reioux@gmail.com>
Date:   Sat Feb 2 08:35:08 2013 -0800

    kernel: Move REPEAT_BYTE definition into linux/kernel.h
    
    And make sure that everything using it explicitly includes
    that header file.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    modified for Mako kernel from kernel.org
    
    Signed-off-by: faux123 <reioux@gmail.com>

commit 92459a6b9032b85c265a25504a6a80390ab0d2c0
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Sep 2 14:22:25 2013 -0500

    Revert "mm: micro-optimise slab to avoid a function call"
    
    This reverts commit 381760eadc393bcb1bb328510ad75cf13431806d.

commit 0c6245f65268919f2d4f5ed8b759c8035c3aa8cf
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:46 2012 -0800

    sl[au]b: always get the cache from its page in kmem_cache_free()
    
    struct page already has this information.  If we start chaining caches,
    this information will always be more trustworthy than whatever is passed
    into the function.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    backported to Android Linux 3.4 by faux123
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	include/linux/memcontrol.h
    	mm/slab.h

commit 7084e65cb48cd2a851b26a2104791bd6995dfe74
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:31 2012 -0800

    slab: annotate on-slab caches nodelist locks
    
    We currently provide lockdep annotation for kmalloc caches, and also
    caches that have SLAB_DEBUG_OBJECTS enabled.  The reason for this is that
    we can quite frequently nest in the l3->list_lock lock, which is not
    something trivial to avoid.
    
    My proposal with this patch, is to extend this to caches whose slab
    management object lives within the slab as well ("on_slab").  The need for
    this arose in the context of testing kmemcg-slab patches.  With such
    patchset, we can have per-memcg kmalloc caches.  So the same path that led
    to nesting between kmalloc caches will could then lead to in-memcg
    nesting.  Because they are not annotated, lockdep will trigger.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 12577b5068bdd516a28f9ea64397efdc26d71eed
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Dec 11 16:01:05 2012 -0800

    slub, hotplug: ignore unrelated node's hot-adding and hot-removing
    
    SLUB only focuses on the nodes which have normal memory and it ignores the
    other node's hot-adding and hot-removing.
    
    Aka: if some memory of a node which has no onlined memory is online, but
    this new memory onlined is not normal memory (for example, highmem), we
    should not allocate kmem_cache_node for SLUB.
    
    And if the last normal memory is offlined, but the node still has memory,
    we should remove kmem_cache_node for that node.  (The current code delays
    it when all of the memory is offlined)
    
    So we only do something when marg->status_change_nid_normal > 0.
    marg->status_change_nid is not suitable here.
    
    The same problem doesn't exist in SLAB, because SLAB allocates kmem_list3
    for every node even the node don't have normal memory, SLAB tolerates
    kmem_list3 on alien nodes.  SLUB only focuses on the nodes which have
    normal memory, it don't tolerate alien kmem_cache_node.  The patch makes
    SLUB become self-compatible and avoids WARNs and BUGs in rare conditions.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Rob Landley <rob@landley.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Cc: Greg Kroah-Hartman <gregkh@suse.de>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e7b0684ab3e11f5743ef8115f2daa3360c9a17a2
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:16 2012 +0000

    mm/sl[aou]b: Common alignment code
    
    Extract the code to do object alignment from the allocators.
    Do the alignment calculations in slab_common so that the
    __kmem_cache_create functions of the allocators do not have
    to deal with alignment.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit da884df39fa1bc4773d8fa803d5ec47836a0dbb8
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Oct 5 16:55:20 2012 +0200

    mm/slob: use min_t() to compare ARCH_SLAB_MINALIGN
    
    The definition of ARCH_SLAB_MINALIGN is architecture dependent
    and can be either of type size_t or int. Comparing that value
    with ARCH_KMALLOC_MINALIGN can cause harmless warnings on
    platforms where they are different. Since both are always
    small positive integer numbers, using the size_t type to compare
    them is safe and gets rid of the warning.
    
    Without this patch, building ARM collie_defconfig results in:
    
    mm/slob.c: In function '__kmalloc_node':
    mm/slob.c:431:152: warning: comparison of distinct pointer types lacks a cast [enabled by default]
    mm/slob.c: In function 'kfree':
    mm/slob.c:484:153: warning: comparison of distinct pointer types lacks a cast [enabled by default]
    mm/slob.c: In function 'ksize':
    mm/slob.c:503:153: warning: comparison of distinct pointer types lacks a cast [enabled by default]
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    [ penberg@kernel.org: updates for master ]
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 1e7c995deed04e552bcb492c7d64bf5328f8f691
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Mon Oct 22 09:04:31 2012 -0300

    mm/slob: Use free_page instead of put_page for page-size kmalloc allocations
    
    When freeing objects, the slob allocator currently free empty pages
    calling __free_pages(). However, page-size kmallocs are disposed
    using put_page() instead.
    
    It makes no sense to call put_page() for kernel pages that are provided
    by the object allocator, so we shouldn't be doing this ourselves.
    
    This is based on:
    commit d9b7f22623b5fa9cc189581dcdfb2ac605933bf4
    Author: Glauber Costa <glommer@parallels.com>
    slub: use free_page instead of put_page for freeing kmalloc allocation
    
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 0944a84fc595905bb3b58f9c7570f89ce8f7eccf
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:09 2012 +0000

    slab: Use the new create_boot_cache function to simplify bootstrap
    
    Simplify setup and reduce code in kmem_cache_init(). This allows us to
    get rid of initarray_cache as well as the manual setup code for
    the kmem_cache and kmem_cache_node arrays during bootstrap.
    
    We introduce a new bootstrap state "PARTIAL" for slab that signals the
    creation of a kmem_cache boot cache.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 6a3879324af0c5db17c1ccec96eabd66b8d4c09d
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:07 2012 +0000

    slub: Use statically allocated kmem_cache boot structure for bootstrap
    
    Simplify bootstrap by statically allocated two kmem_cache structures. These are
    freed after bootup is complete. Allows us to no longer worry about calculations
    of sizes of kmem_cache structures during bootstrap.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ca0858f525e57f6185783ae3969b6b03c98125d3
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:07 2012 +0000

    mm, sl[au]b: create common functions for boot slab creation
    
    Use a special function to create kmalloc caches and use that function in
    SLAB and SLUB.
    
    Acked-by: Joonsoo Kim <js1304@gmail.com>
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 64cbb827d8ffa29f612255f647dcc040a596c74a
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:01 2012 +0000

    slab: Simplify bootstrap
    
    The nodelists field in kmem_cache is pointing to the first unused
    object in the array field when bootstrap is complete.
    
    A problem with the current approach is that the statically sized
    kmem_cache structure use on boot can only contain NR_CPUS entries.
    If the number of nodes plus the number of cpus is greater then we
    would overwrite memory following the kmem_cache_boot definition.
    
    Increase the size of the array field to ensure that also the node
    pointers fit into the array field.
    
    Once we do that we no longer need the kmem_cache_nodelists
    array and we can then also use that structure elsewhere.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 7c20c4a81f5a3b63adc210557e6a171be227e04a
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Tue Nov 6 17:10:10 2012 -0800

    mm: fix slab.c kernel-doc warnings
    
    Fix new kernel-doc warnings in mm/slab.c:
    
    Warning(mm/slab.c:2358): No description found for parameter 'cachep'
    Warning(mm/slab.c:2358): Excess function parameter 'name' description in '__kmem_cache_create'
    Warning(mm/slab.c:2358): Excess function parameter 'size' description in '__kmem_cache_create'
    Warning(mm/slab.c:2358): Excess function parameter 'align' description in '__kmem_cache_create'
    Warning(mm/slab.c:2358): Excess function parameter 'ctor' description in '__kmem_cache_create'
    
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Cc:	Christoph Lameter <cl@linux-foundation.org>
    Cc:	Pekka Enberg <penberg@kernel.org>
    Cc:	Matt Mackall <mpm@selenic.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c47d5a1afa2bd2658ef7b9b6e6565b2f21389aff
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:00 2012 +0000

    slub: Use correct cpu_slab on dead cpu
    
    Pass a kmem_cache_cpu pointer into unfreeze partials so that a different
    kmem_cache_cpu structure than the local one can be specified.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit b34e7ef7bae04bda23326dfd918eb3edd3fe1aaf
Author: Glauber Costa <glommer@parallels.com>
Date:   Wed Oct 17 15:36:51 2012 +0400

    slab: Ignore internal flags in cache creation
    
    Some flags are used internally by the allocators for management
    purposes. One example of that is the CFLGS_OFF_SLAB flag that slab uses
    to mark that the metadata for that cache is stored outside of the slab.
    
    No cache should ever pass those as a creation flags. We can just ignore
    this bit if it happens to be passed (such as when duplicating a cache in
    the kmem memcg patches).
    
    Because such flags can vary from allocator to allocator, we allow them
    to make their own decisions on that, defining SLAB_AVAILABLE_FLAGS with
    all flags that are valid at creation time.  Allocators that doesn't have
    any specific flag requirement should define that to mean all flags.
    
    Common code will mask out all flags not belonging to that set.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 3a611f1f15f2bc39596850006f1a6a76dc3c1b29
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Fri Oct 19 09:33:12 2012 -0300

    mm/sl[aou]b: Move common kmem_cache_size() to slab.h
    
    This function is identically defined in all three allocators
    and it's trivial to move it to slab.h
    
    Since now it's static, inline, header-defined function
    this patch also drops the EXPORT_SYMBOL tag.
    
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 04a41eb0739a5d992db6772c799a02ca2e5520f1
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Fri Oct 19 09:33:11 2012 -0300

    mm/slob: Use object_size field in kmem_cache_size()
    
    Fields object_size and size are not the same: the latter might include
    slab metadata. Return object_size field in kmem_cache_size().
    Also, improve trace accuracy by correctly tracing reported size.
    
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 2abff64a550553d3c1280453dbd9fef5228c591c
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Fri Oct 19 09:33:10 2012 -0300

    mm/slob: Drop usage of page->private for storing page-sized allocations
    
    This field was being used to store size allocation so it could be
    retrieved by ksize(). However, it is a bad practice to not mark a page
    as a slab page and then use fields for special purposes.
    There is no need to store the allocated size and
    ksize() can simply return PAGE_SIZE << compound_order(page).
    
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 934e19787d60e987e87740f3ea446260357e965b
Author: David Rientjes <rientjes@google.com>
Date:   Tue Sep 25 12:53:51 2012 -0700

    mm, slob: fix build breakage in __kmalloc_node_track_caller
    
    On Sat, 8 Sep 2012, Ezequiel Garcia wrote:
    
    > @@ -454,15 +455,35 @@ void *__kmalloc_node(size_t size, gfp_t gfp, int node)
    >  			gfp |= __GFP_COMP;
    >  		ret = slob_new_pages(gfp, order, node);
    >
    > -		trace_kmalloc_node(_RET_IP_, ret,
    > +		trace_kmalloc_node(caller, ret,
    >  				   size, PAGE_SIZE << order, gfp, node);
    >  	}
    >
    >  	kmemleak_alloc(ret, size, 1, gfp);
    >  	return ret;
    >  }
    > +
    > +void *__kmalloc_node(size_t size, gfp_t gfp, int node)
    > +{
    > +	return __do_kmalloc_node(size, gfp, node, _RET_IP_);
    > +}
    >  EXPORT_SYMBOL(__kmalloc_node);
    >
    > +#ifdef CONFIG_TRACING
    > +void *__kmalloc_track_caller(size_t size, gfp_t gfp, unsigned long caller)
    > +{
    > +	return __do_kmalloc_node(size, gfp, NUMA_NO_NODE, caller);
    > +}
    > +
    > +#ifdef CONFIG_NUMA
    > +void *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,
    > +					int node, unsigned long caller)
    > +{
    > +	return __do_kmalloc_node(size, gfp, node, caller);
    > +}
    > +#endif
    
    This breaks Pekka's slab/next tree with this:
    
    mm/slob.c: In function '__kmalloc_node_track_caller':
    mm/slob.c:488: error: 'gfp' undeclared (first use in this function)
    mm/slob.c:488: error: (Each undeclared identifier is reported only once
    mm/slob.c:488: error: for each function it appears in.)
    
    mm, slob: fix build breakage in __kmalloc_node_track_caller
    
    "mm, slob: Add support for kmalloc_track_caller()" breaks the build
    because gfp is undeclared.  Fix it.
    
    Acked-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 02047deee20451603191195a317d232c7b25e15c
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:53 2012 -0300

    mm, slob: Add support for kmalloc_track_caller()
    
    Currently slob falls back to regular kmalloc for this case.
    With this patch kmalloc_track_caller() is correctly implemented,
    thus tracing the specified caller.
    
    This is important to trace accurately allocations performed by
    krealloc, kstrdup, kmemdup, etc.
    
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 34c8e09b452779de67d71d2fbe84b31631862296
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:51 2012 -0300

    mm, slob: Use NUMA_NO_NODE instead of -1
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	mm/slob.c

commit 1087f7f52f6fc249b1258ab044c15de14143e978
Author: Glauber Costa <glommer@parallels.com>
Date:   Mon Oct 22 18:05:36 2012 +0400

    slub: Commonize slab_cache field in struct page
    
    Right now, slab and slub have fields in struct page to derive which
    cache a page belongs to, but they do it slightly differently.
    
    slab uses a field called slab_cache, that lives in the third double
    word. slub, uses a field called "slab", living outside of the
    doublewords area.
    
    Ideally, we could use the same field for this. Since slub heavily makes
    use of the doubleword region, there isn't really much room to move
    slub's slab_cache field around. Since slab does not have such strict
    placement restrictions, we can move it outside the doubleword area.
    
    The naming used by slab, "slab_cache", is less confusing, and it is
    preferred over slub's generic "slab".
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 255e117e29282840631ba2887fb0c4b4bcf0bed7
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Oct 19 18:20:27 2012 +0400

    sl[au]b: Process slabinfo_show in common code
    
    With all the infrastructure in place, we can now have slabinfo_show
    done from slab_common.c. A cache-specific function is called to grab
    information about the cache itself, since that is still heavily
    dependent on the implementation. But with the values produced by it, all
    the printing and handling is done from common code.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    CC: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit d4aba13bbb5741dded3568cad95b3269e64858b4
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Oct 19 18:20:26 2012 +0400

    mm/sl[au]b: Move print_slabinfo_header to slab_common.c
    
    The header format is highly similar between slab and slub. The main
    difference lays in the fact that slab may optionally have statistics
    added here in case of CONFIG_SLAB_DEBUG, while the slub will stick them
    somewhere else.
    
    By making sure that information conditionally lives inside a
    globally-visible CONFIG_DEBUG_SLAB switch, we can move the header
    printing to a common location.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5e81ea6166770999812ad1aa439ec10fa55ee91d
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Oct 19 18:20:25 2012 +0400

    mm/sl[au]b: Move slabinfo processing to slab_common.c
    
    This patch moves all the common machinery to slabinfo processing
    to slab_common.c. We can do better by noticing that the output is
    heavily common, and having the allocators to just provide finished
    information about this. But after this first step, this can be done
    easier.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5442dc74ee767ce9559c666f20cc289ebbc344c1
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Oct 8 09:26:01 2012 +0200

    mm, slab: release slab_mutex earlier in kmem_cache_destroy()
    
    Commit 1331e7a1bbe1 ("rcu: Remove _rcu_barrier() dependency on
    __stop_machine()") introduced slab_mutex -> cpu_hotplug.lock dependency
    through kmem_cache_destroy() -> rcu_barrier() -> _rcu_barrier() ->
    get_online_cpus().
    
    Lockdep thinks that this might actually result in ABBA deadlock,
    and reports it as below:
    
    === [ cut here ] ===
     ======================================================
     [ INFO: possible circular locking dependency detected ]
     3.6.0-rc5-00004-g0d8ee37 #143 Not tainted
     -------------------------------------------------------
     kworker/u:2/40 is trying to acquire lock:
      (rcu_sched_state.barrier_mutex){+.+...}, at: [<ffffffff810f2126>] _rcu_barrier+0x26/0x1e0
    
     but task is already holding lock:
      (slab_mutex){+.+.+.}, at: [<ffffffff81176e15>] kmem_cache_destroy+0x45/0xe0
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #2 (slab_mutex){+.+.+.}:
            [<ffffffff810ae1e2>] validate_chain+0x632/0x720
            [<ffffffff810ae5d9>] __lock_acquire+0x309/0x530
            [<ffffffff810ae921>] lock_acquire+0x121/0x190
            [<ffffffff8155d4cc>] __mutex_lock_common+0x5c/0x450
            [<ffffffff8155d9ee>] mutex_lock_nested+0x3e/0x50
            [<ffffffff81558cb5>] cpuup_callback+0x2f/0xbe
            [<ffffffff81564b83>] notifier_call_chain+0x93/0x140
            [<ffffffff81076f89>] __raw_notifier_call_chain+0x9/0x10
            [<ffffffff8155719d>] _cpu_up+0xba/0x14e
            [<ffffffff815572ed>] cpu_up+0xbc/0x117
            [<ffffffff81ae05e3>] smp_init+0x6b/0x9f
            [<ffffffff81ac47d6>] kernel_init+0x147/0x1dc
            [<ffffffff8156ab44>] kernel_thread_helper+0x4/0x10
    
     -> #1 (cpu_hotplug.lock){+.+.+.}:
            [<ffffffff810ae1e2>] validate_chain+0x632/0x720
            [<ffffffff810ae5d9>] __lock_acquire+0x309/0x530
            [<ffffffff810ae921>] lock_acquire+0x121/0x190
            [<ffffffff8155d4cc>] __mutex_lock_common+0x5c/0x450
            [<ffffffff8155d9ee>] mutex_lock_nested+0x3e/0x50
            [<ffffffff81049197>] get_online_cpus+0x37/0x50
            [<ffffffff810f21bb>] _rcu_barrier+0xbb/0x1e0
            [<ffffffff810f22f0>] rcu_barrier_sched+0x10/0x20
            [<ffffffff810f2309>] rcu_barrier+0x9/0x10
            [<ffffffff8118c129>] deactivate_locked_super+0x49/0x90
            [<ffffffff8118cc01>] deactivate_super+0x61/0x70
            [<ffffffff811aaaa7>] mntput_no_expire+0x127/0x180
            [<ffffffff811ab49e>] sys_umount+0x6e/0xd0
            [<ffffffff81569979>] system_call_fastpath+0x16/0x1b
    
     -> #0 (rcu_sched_state.barrier_mutex){+.+...}:
            [<ffffffff810adb4e>] check_prev_add+0x3de/0x440
            [<ffffffff810ae1e2>] validate_chain+0x632/0x720
            [<ffffffff810ae5d9>] __lock_acquire+0x309/0x530
            [<ffffffff810ae921>] lock_acquire+0x121/0x190
            [<ffffffff8155d4cc>] __mutex_lock_common+0x5c/0x450
            [<ffffffff8155d9ee>] mutex_lock_nested+0x3e/0x50
            [<ffffffff810f2126>] _rcu_barrier+0x26/0x1e0
            [<ffffffff810f22f0>] rcu_barrier_sched+0x10/0x20
            [<ffffffff810f2309>] rcu_barrier+0x9/0x10
            [<ffffffff81176ea1>] kmem_cache_destroy+0xd1/0xe0
            [<ffffffffa04c3154>] nf_conntrack_cleanup_net+0xe4/0x110 [nf_conntrack]
            [<ffffffffa04c31aa>] nf_conntrack_cleanup+0x2a/0x70 [nf_conntrack]
            [<ffffffffa04c42ce>] nf_conntrack_net_exit+0x5e/0x80 [nf_conntrack]
            [<ffffffff81454b79>] ops_exit_list+0x39/0x60
            [<ffffffff814551ab>] cleanup_net+0xfb/0x1b0
            [<ffffffff8106917b>] process_one_work+0x26b/0x4c0
            [<ffffffff81069f3e>] worker_thread+0x12e/0x320
            [<ffffffff8106f73e>] kthread+0x9e/0xb0
            [<ffffffff8156ab44>] kernel_thread_helper+0x4/0x10
    
     other info that might help us debug this:
    
     Chain exists of:
       rcu_sched_state.barrier_mutex --> cpu_hotplug.lock --> slab_mutex
    
      Possible unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(slab_mutex);
                                    lock(cpu_hotplug.lock);
                                    lock(slab_mutex);
       lock(rcu_sched_state.barrier_mutex);
    
      *** DEADLOCK ***
    === [ cut here ] ===
    
    This is actually a false positive. Lockdep has no way of knowing the fact
    that the ABBA can actually never happen, because of special semantics of
    cpu_hotplug.refcount and its handling in cpu_hotplug_begin(); the mutual
    exclusion there is not achieved through mutex, but through
    cpu_hotplug.refcount.
    
    The "neither cpu_up() nor cpu_down() will proceed past cpu_hotplug_begin()
    until everyone who called get_online_cpus() will call put_online_cpus()"
    semantics is totally invisible to lockdep.
    
    This patch therefore moves the unlock of slab_mutex so that rcu_barrier()
    is being called with it unlocked. It has two advantages:
    
    - it slightly reduces hold time of slab_mutex; as it's used to protect
      the cachep list, it's not necessary to hold it over kmem_cache_free()
      call any more
    - it silences the lockdep false positive warning, as it avoids lockdep ever
      learning about slab_mutex -> cpu_hotplug.lock dependency
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 63889c8e93af45f2261d9dd3fa6e0acbc9c10b3a
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Sun Sep 30 17:28:25 2012 +0900

    slab: Fix build failure in __kmem_cache_create()
    
    Fix build failure with CONFIG_DEBUG_SLAB=y && CONFIG_DEBUG_PAGEALLOC=y caused
    by commit 8a13a4cc "mm/sl[aou]b: Shrink __kmem_cache_create() parameter lists".
    
    mm/slab.c: In function '__kmem_cache_create':
    mm/slab.c:2474: error: 'align' undeclared (first use in this function)
    mm/slab.c:2474: error: (Each undeclared identifier is reported only once
    mm/slab.c:2474: error: for each function it appears in.)
    make[1]: *** [mm/slab.o] Error 1
    make: *** [mm] Error 2
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 56aabe0e7653a9a2b11eb7a83529c8d5ca81939e
Author: Pekka Enberg <penberg@kernel.org>
Date:   Sat Sep 29 10:00:59 2012 +0300

    Revert "mm/slab: Fix kmem_cache_alloc_node_trace() declaration"
    
    This reverts commit 1e5965bf1f018cc30a4659fa3f1a40146e4276f6. Ezequiel
    Garcia has a better fix.

commit 9ee45878105cffc174867c15cd09cbaa20b9c7b6
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Tue Sep 25 08:07:09 2012 -0300

    mm/slab: Fix kmem_cache_alloc_node_trace() declaration
    
    The bug was introduced in commit 4052147c0afa ("mm, slab: Match SLAB
    and SLUB kmem_cache_alloc_xxx_trace() prototype").
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 4e5a56155c3454300095016f25608069b7b40cbc
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Tue Sep 25 08:07:08 2012 -0300

    mm/slab: Fix typo _RET_IP -> _RET_IP_
    
    The bug was introduced by commit 7c0cb9c64f83 ("mm, slab: Replace
    'caller' type, void* -> unsigned long").
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 2264ea0ed8652ca61b9ab8b3bca518026d1b88ee
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:57 2012 -0300

    mm, slab: Rename __cache_alloc() -> slab_alloc()
    
    This patch does not fix anything and its only goal is to
    produce common code between SLAB and SLUB.
    
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ff6f6d377681015e089d1436a48de94d9731d004
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:56 2012 -0300

    mm, slab: Match SLAB and SLUB kmem_cache_alloc_xxx_trace() prototype
    
    This long (seemingly unnecessary) patch does not fix anything and
    its only goal is to produce common code between SLAB and SLUB.
    
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 4015e60d854a850069283b601474387321a51402
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:55 2012 -0300

    mm, slab: Replace 'caller' type, void* -> unsigned long
    
    This allows to use _RET_IP_ instead of builtin_address(0), thus
    achiveing implementation consistency in all three allocators.
    Though maybe a nitpick, the real goal behind this patch is
    to be able to obtain common code between SLAB and SLUB.
    
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 33b9864037bea43e24c76063f43c7b83bbec2399
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:52 2012 -0300

    mm, slab: Remove silly function slab_buffer_size()
    
    This function is seldom used, and can be simply replaced with cachep->size.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 735ea8822dc421cfa3b5b6431a0c909de697e222
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Thu Aug 16 00:02:40 2012 +0900

    slub: remove one code path and reduce lock contention in __slab_free()
    
    When we try to free object, there is some of case that we need
    to take a node lock. This is the necessary step for preventing a race.
    After taking a lock, then we try to cmpxchg_double_slab().
    But, there is a possible scenario that cmpxchg_double_slab() is failed
    with taking a lock. Following example explains it.
    
    CPU A               CPU B
    need lock
    ...                 need lock
    ...                 lock!!
    lock..but spin      free success
    spin...             unlock
    lock!!
    free fail
    
    In this case, retry with taking a lock is occured in CPU A.
    I think that in this case for CPU A,
    "release a lock first, and re-take a lock if necessary" is preferable way.
    
    There are two reasons for this.
    
    First, this makes __slab_free()'s logic somehow simple.
    With this patch, 'was_frozen = 1' is "always" handled without taking a lock.
    So we can remove one code path.
    
    Second, it may reduce lock contention.
    When we do retrying, status of slab is already changed,
    so we don't need a lock anymore in almost every case.
    "release a lock first, and re-take a lock if necessary" policy is
    helpful to this.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 143fa77587e3d69898c85fb6e2cddfa3eef831c8
Author: Fengguang Wu <fengguang.wu@intel.com>
Date:   Fri Sep 28 16:34:05 2012 +0800

    slub: init_kmem_cache_cpus() and put_cpu_partial() can be static
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5b9295f857ee4f87e676c8b11d3c268de89c14bb
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:58 2012 -0300

    mm, slub: Rename slab_alloc() -> slab_alloc_node() to match SLAB
    
    This patch does not fix anything, and its only goal is to enable us
    to obtain some common code between SLAB and SLUB.
    Neither behavior nor produced code is affected.
    
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e676801f9dc9c1de7401fda75d9b1ada291eb604
Author: Dave Jones <davej@redhat.com>
Date:   Tue Sep 18 15:54:12 2012 -0400

    mm, sl[au]b: Taint kernel when we detect a corrupted slab
    
    It doesn't seem worth adding a new taint flag for this, so just re-use
    the one from 'bad page'
    
    Acked-by: Christoph Lameter <cl@linux.com> # SLUB
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 35dd5ebbc65d3f14df7ad544230a469217fde050
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 11 19:49:38 2012 +0000

    slab: Only define slab_error for DEBUG
    
    On Tue, 11 Sep 2012, Stephen Rothwell wrote:
    > After merging the final tree, today's linux-next build (sparc64 defconfig)
    > produced this warning:
    >
    > mm/slab.c:808:13: warning: '__slab_error' defined but not used [-Wunused-function]
    >
    > Introduced by commit 945cf2b6199b ("mm/sl[aou]b: Extract a common
    > function for kmem_cache_destroy").  All uses of slab_error() are now
    > guarded by DEBUG.
    
    There is no use case left for slab builds without DEBUG.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 237543645b361d15cb06ef916d99d7f9272646b7
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Mon Sep 17 14:09:06 2012 -0700

    slab: fix starting index for finding another object
    
    In array cache, there is a object at index 0, check it.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d89aac95d2c1e0f2f963c0ff3e31e8f06aa2fdd8
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Sep 17 14:09:03 2012 -0700

    slab: do ClearSlabPfmemalloc() for all pages of slab
    
    Right now, we call ClearSlabPfmemalloc() for first page of slab when we
    clear SlabPfmemalloc flag.  This is fine for most swap-over-network use
    cases as it is expected that order-0 pages are in use.  Unfortunately it
    is possible that that __ac_put_obj() checks SlabPfmemalloc on a tail
    page and while this is harmless, it is sloppy.  This patch ensures that
    the head page is always used.
    
    This problem was originally identified by Joonsoo Kim.
    
    [js1304@gmail.com: Original implementation and problem identification]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8cb7295bd49aa8dad86e290832f68cab107bb5b5
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Mon Sep 17 14:09:09 2012 -0700

    slub: consider pfmemalloc_match() in get_partial_node()
    
    get_partial() is currently not checking pfmemalloc_match() meaning that
    it is possible for pfmemalloc pages to leak to non-pfmemalloc users.
    This is a problem in the following situation.  Assume that there is a
    request from normal allocation and there are no objects in the per-cpu
    cache and no node-partial slab.
    
    In this case, slab_alloc enters the slow path and new_slab_objects() is
    called which may return a PFMEMALLOC page.  As the current user is not
    allowed to access PFMEMALLOC page, deactivate_slab() is called
    ([5091b74a: mm: slub: optimise the SLUB fast path to avoid pfmemalloc
    checks]) and returns an object from PFMEMALLOC page.
    
    Next time, when we get another request from normal allocation,
    slab_alloc() enters the slow-path and calls new_slab_objects().  In
    new_slab_objects(), we call get_partial() and get a partial slab which
    was just deactivated but is a pfmemalloc page.  We extract one object
    from it and re-deactivate.
    
      "deactivate -> re-get in get_partial -> re-deactivate" occures repeatedly.
    
    As a result, access to PFMEMALLOC page is not properly restricted and it
    can cause a performance degradation due to frequent deactivation.
    deactivation frequently.
    
    This patch changes get_partial_node() to take pfmemalloc_match() into
    account and prevents the "deactivate -> re-get in get_partial()
    scenario.  Instead, new_slab() is called.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b6dce57417404fbbc1b1d44ece7170ed112943d8
Author: Christoph Lameter <cl@linux.com>
Date:   Sat Sep 8 18:27:10 2012 +0000

    slub: Zero initial memory segment for kmem_cache and kmem_cache_node
    
    Tony Luck reported the following problem on IA-64:
    
      Worked fine yesterday on next-20120905, crashes today. First sign of
      trouble was an unaligned access, then a NULL dereference. SL*B related
      bits of my config:
    
      CONFIG_SLUB_DEBUG=y
      # CONFIG_SLAB is not set
      CONFIG_SLUB=y
      CONFIG_SLABINFO=y
      # CONFIG_SLUB_DEBUG_ON is not set
      # CONFIG_SLUB_STATS is not set
    
      And he console log.
    
      PID hash table entries: 4096 (order: 1, 32768 bytes)
      Dentry cache hash table entries: 262144 (order: 7, 2097152 bytes)
      Inode-cache hash table entries: 131072 (order: 6, 1048576 bytes)
      Memory: 2047920k/2086064k available (13992k code, 38144k reserved,
      6012k data, 880k init)
      kernel unaligned access to 0xca2ffc55fb373e95, ip=0xa0000001001be550
      swapper[0]: error during unaligned kernel access
       -1 [1]
      Modules linked in:
    
      Pid: 0, CPU 0, comm:              swapper
      psr : 00001010084a2018 ifs : 800000000000060f ip  :
      [<a0000001001be550>]    Not tainted (3.6.0-rc4-zx1-smp-next-20120906)
      ip is at new_slab+0x90/0x680
      unat: 0000000000000000 pfs : 000000000000060f rsc : 0000000000000003
      rnat: 9666960159966a59 bsps: a0000001001441c0 pr  : 9666960159965a59
      ldrs: 0000000000000000 ccv : 0000000000000000 fpsr: 0009804c8a70433f
      csd : 0000000000000000 ssd : 0000000000000000
      b0  : a0000001001be500 b6  : a00000010112cb20 b7  : a0000001011660a0
      f6  : 0fff7f0f0f0f0e54f0000 f7  : 0ffe8c5c1000000000000
      f8  : 1000d8000000000000000 f9  : 100068800000000000000
      f10 : 10005f0f0f0f0e54f0000 f11 : 1003e0000000000000078
      r1  : a00000010155eef0 r2  : 0000000000000000 r3  : fffffffffffc1638
      r8  : e0000040600081b8 r9  : ca2ffc55fb373e95 r10 : 0000000000000000
      r11 : e000004040001646 r12 : a000000101287e20 r13 : a000000101280000
      r14 : 0000000000004000 r15 : 0000000000000078 r16 : ca2ffc55fb373e75
      r17 : e000004040040000 r18 : fffffffffffc1646 r19 : e000004040001646
      r20 : fffffffffffc15f8 r21 : 000000000000004d r22 : a00000010132fa68
      r23 : 00000000000000ed r24 : 0000000000000000 r25 : 0000000000000000
      r26 : 0000000000000001 r27 : a0000001012b8500 r28 : a00000010135f4a0
      r29 : 0000000000000000 r30 : 0000000000000000 r31 : 0000000000000001
      Unable to handle kernel NULL pointer dereference (address
      0000000000000018)
      swapper[0]: Oops 11003706212352 [2]
      Modules linked in:
    
      Pid: 0, CPU 0, comm:              swapper
      psr : 0000121008022018 ifs : 800000000000cc18 ip  :
      [<a0000001004dc8f1>]    Not tainted (3.6.0-rc4-zx1-smp-next-20120906)
      ip is at __copy_user+0x891/0x960
      unat: 0000000000000000 pfs : 0000000000000813 rsc : 0000000000000003
      rnat: 0000000000000000 bsps: 0000000000000000 pr  : 9666960159961765
      ldrs: 0000000000000000 ccv : 0000000000000000 fpsr: 0009804c0270033f
      csd : 0000000000000000 ssd : 0000000000000000
      b0  : a00000010004b550 b6  : a00000010004b740 b7  : a00000010000c750
      f6  : 000000000000000000000 f7  : 1003e9e3779b97f4a7c16
      f8  : 1003e0a00000010001550 f9  : 100068800000000000000
      f10 : 10005f0f0f0f0e54f0000 f11 : 1003e0000000000000078
      r1  : a00000010155eef0 r2  : a0000001012870b0 r3  : a0000001012870b8
      r8  : 0000000000000298 r9  : 0000000000000013 r10 : 0000000000000000
      r11 : 9666960159961a65 r12 : a000000101287010 r13 : a000000101280000
      r14 : a000000101287068 r15 : a000000101287080 r16 : 0000000000000298
      r17 : 0000000000000010 r18 : 0000000000000018 r19 : a000000101287310
      r20 : 0000000000000290 r21 : 0000000000000000 r22 : 0000000000000000
      r23 : a000000101386f58 r24 : 0000000000000000 r25 : 000000007fffffff
      r26 : a000000101287078 r27 : a0000001013c69b0 r28 : 0000000000000000
      r29 : 0000000000000014 r30 : 0000000000000000 r31 : 0000000000000813
    
    Sedat Dilek and Hugh Dickins reported similar problems as well.
    
    Earlier patches in the common set moved the zeroing of the kmem_cache
    structure into common code. See "Move allocation of kmem_cache into
    common code".
    
    The allocation for the two special structures is still done from SLUB
    specific code but no zeroing is done since the cache creation functions
    used to zero. This now needs to be updated so that the structures are
    zeroed during allocation in kmem_cache_init().  Otherwise random pointer
    values may be followed.
    
    Reported-by: Tony Luck <tony.luck@intel.com>
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Reported-by: Hugh Dickins <hughd@google.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ddd8b7df9f5788680e5c552592fa1444c783c18d
Author: Pekka Enberg <penberg@kernel.org>
Date:   Wed Sep 5 12:07:44 2012 +0300

    Revert "mm/sl[aou]b: Move sysfs_slab_add to common"
    
    This reverts commit 96d17b7be0a9849d381442030886211dbb2a7061 which
    caused the following errors at boot:
    
      [    1.114885] kobject (ffff88001a802578): tried to init an initialized object, something is seriously wrong.
      [    1.114885] Pid: 1, comm: swapper/0 Tainted: G        W    3.6.0-rc1+ #6
      [    1.114885] Call Trace:
      [    1.114885]  [<ffffffff81273f37>] kobject_init+0x87/0xa0
      [    1.115555]  [<ffffffff8127426a>] kobject_init_and_add+0x2a/0x90
      [    1.115555]  [<ffffffff8127c870>] ? sprintf+0x40/0x50
      [    1.115555]  [<ffffffff81124c60>] sysfs_slab_add+0x80/0x210
      [    1.115555]  [<ffffffff81100175>] kmem_cache_create+0xa5/0x250
      [    1.115555]  [<ffffffff81cf24cd>] ? md_init+0x144/0x144
      [    1.115555]  [<ffffffff81cf25b6>] local_init+0xa4/0x11b
      [    1.115555]  [<ffffffff81cf24e1>] dm_init+0x14/0x45
      [    1.115836]  [<ffffffff810001ba>] do_one_initcall+0x3a/0x160
      [    1.116834]  [<ffffffff81cc2c90>] kernel_init+0x133/0x1b7
      [    1.117835]  [<ffffffff81cc25c4>] ? do_early_param+0x86/0x86
      [    1.117835]  [<ffffffff8171aff4>] kernel_thread_helper+0x4/0x10
      [    1.118401]  [<ffffffff81cc2b5d>] ? start_kernel+0x33f/0x33f
      [    1.119832]  [<ffffffff8171aff0>] ? gs_change+0xb/0xb
      [    1.120325] ------------[ cut here ]------------
      [    1.120835] WARNING: at fs/sysfs/dir.c:536 sysfs_add_one+0xc1/0xf0()
      [    1.121437] sysfs: cannot create duplicate filename '/kernel/slab/:t-0000016'
      [    1.121831] Modules linked in:
      [    1.122138] Pid: 1, comm: swapper/0 Tainted: G        W    3.6.0-rc1+ #6
      [    1.122831] Call Trace:
      [    1.123074]  [<ffffffff81195ce1>] ? sysfs_add_one+0xc1/0xf0
      [    1.123833]  [<ffffffff8103adfa>] warn_slowpath_common+0x7a/0xb0
      [    1.124405]  [<ffffffff8103aed1>] warn_slowpath_fmt+0x41/0x50
      [    1.124832]  [<ffffffff81195ce1>] sysfs_add_one+0xc1/0xf0
      [    1.125337]  [<ffffffff81195eb3>] create_dir+0x73/0xd0
      [    1.125832]  [<ffffffff81196221>] sysfs_create_dir+0x81/0xe0
      [    1.126363]  [<ffffffff81273d3d>] kobject_add_internal+0x9d/0x210
      [    1.126832]  [<ffffffff812742a3>] kobject_init_and_add+0x63/0x90
      [    1.127406]  [<ffffffff81124c60>] sysfs_slab_add+0x80/0x210
      [    1.127832]  [<ffffffff81100175>] kmem_cache_create+0xa5/0x250
      [    1.128384]  [<ffffffff81cf24cd>] ? md_init+0x144/0x144
      [    1.128833]  [<ffffffff81cf25b6>] local_init+0xa4/0x11b
      [    1.129831]  [<ffffffff81cf24e1>] dm_init+0x14/0x45
      [    1.130305]  [<ffffffff810001ba>] do_one_initcall+0x3a/0x160
      [    1.130831]  [<ffffffff81cc2c90>] kernel_init+0x133/0x1b7
      [    1.131351]  [<ffffffff81cc25c4>] ? do_early_param+0x86/0x86
      [    1.131830]  [<ffffffff8171aff4>] kernel_thread_helper+0x4/0x10
      [    1.132392]  [<ffffffff81cc2b5d>] ? start_kernel+0x33f/0x33f
      [    1.132830]  [<ffffffff8171aff0>] ? gs_change+0xb/0xb
      [    1.133315] ---[ end trace 2703540871c8fab7 ]---
      [    1.133830] ------------[ cut here ]------------
      [    1.134274] WARNING: at lib/kobject.c:196 kobject_add_internal+0x1f5/0x210()
      [    1.134829] kobject_add_internal failed for :t-0000016 with -EEXIST, don't try to register things with the same name in the same directory.
      [    1.135829] Modules linked in:
      [    1.136135] Pid: 1, comm: swapper/0 Tainted: G        W    3.6.0-rc1+ #6
      [    1.136828] Call Trace:
      [    1.137071]  [<ffffffff81273e95>] ? kobject_add_internal+0x1f5/0x210
      [    1.137830]  [<ffffffff8103adfa>] warn_slowpath_common+0x7a/0xb0
      [    1.138402]  [<ffffffff8103aed1>] warn_slowpath_fmt+0x41/0x50
      [    1.138830]  [<ffffffff811955a3>] ? release_sysfs_dirent+0x73/0xf0
      [    1.139419]  [<ffffffff81273e95>] kobject_add_internal+0x1f5/0x210
      [    1.139830]  [<ffffffff812742a3>] kobject_init_and_add+0x63/0x90
      [    1.140429]  [<ffffffff81124c60>] sysfs_slab_add+0x80/0x210
      [    1.140830]  [<ffffffff81100175>] kmem_cache_create+0xa5/0x250
      [    1.141829]  [<ffffffff81cf24cd>] ? md_init+0x144/0x144
      [    1.142307]  [<ffffffff81cf25b6>] local_init+0xa4/0x11b
      [    1.142829]  [<ffffffff81cf24e1>] dm_init+0x14/0x45
      [    1.143307]  [<ffffffff810001ba>] do_one_initcall+0x3a/0x160
      [    1.143829]  [<ffffffff81cc2c90>] kernel_init+0x133/0x1b7
      [    1.144352]  [<ffffffff81cc25c4>] ? do_early_param+0x86/0x86
      [    1.144829]  [<ffffffff8171aff4>] kernel_thread_helper+0x4/0x10
      [    1.145405]  [<ffffffff81cc2b5d>] ? start_kernel+0x33f/0x33f
      [    1.145828]  [<ffffffff8171aff0>] ? gs_change+0xb/0xb
      [    1.146313] ---[ end trace 2703540871c8fab8 ]---
    
    Conflicts:
    
    	mm/slub.c
    
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit fb965659fafda962dc48c39ad4848578933607d6
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:38:33 2012 +0000

    mm/sl[aou]b: Move kmem_cache refcounting to common code
    
    Get rid of the refcount stuff in the allocators and do that part of
    kmem_cache management in the common code.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e1afee54dbbd51a38f891d04eda161ea79bfc0cb
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:18:33 2012 +0000

    mm/sl[aou]b: Shrink __kmem_cache_create() parameter lists
    
    Do the initial settings of the fields in common code. This will allow us
    to push more processing into common code later and improve readability.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e7fada411f0b8972337dd88dd661edcee6c2e6f6
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:20:34 2012 +0000

    mm/sl[aou]b: Move kmem_cache allocations into common code
    
    Shift the allocations to common code. That way the allocation and
    freeing of the kmem_cache structures is handled by common code.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit f603fd20a8384d4e08b300201a02b995c6f105f9
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:18:32 2012 +0000

    mm/sl[aou]b: Move sysfs_slab_add to common
    
    Simplify locking by moving the slab_add_sysfs after all locks have been
    dropped. Eases the upcoming move to provide sysfs support for all
    allocators.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 93ff50520872362db85889cf25625c5f1dab50a2
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:18:32 2012 +0000

    mm/sl[aou]b: Do slab aliasing call from common code
    
    The slab aliasing logic causes some strange contortions in slub. So add
    a call to deal with aliases to slab_common.c but disable it for other
    slab allocators by providng stubs that fail to create aliases.
    
    Full general support for aliases will require additional cleanup passes
    and more standardization of fields in kmem_cache.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 2520f722c6b59c5a79d03c6f6125606f74470cac
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:45:41 2013 -0500

    mm/sl[aou]b: Move duping of slab name to slab_common.c
    
    Duping of the slabname has to be done by each slab. Moving this code to
    slab_common avoids duplicate implementations.
    
    With this patch we have common string handling for all slab allocators.
    Strings passed to kmem_cache_create() are copied internally. Subsystems
    can create temporary strings to create slab caches.
    
    Slabs allocated in early states of bootstrap will never be freed (and
    those can never be freed since they are essential to slab allocator
    operations).  During bootstrap we therefore do not have to worry about
    duping names.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 9e4a790c8bcbf89f254642db684e87d5aa38db95
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:38:33 2012 +0000

    mm/sl[aou]b: Get rid of __kmem_cache_destroy
    
    What is done there can be done in __kmem_cache_shutdown.
    
    This affects RCU handling somewhat. On rcu free all slab allocators do
    not refer to other management structures than the kmem_cache structure.
    Therefore these other structures can be freed before the rcu deferred
    free to the page allocator occurs.
    
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit dc329b4852d45cfc535de2dca309347cd61c9eaf
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:18:32 2012 +0000

    mm/sl[aou]b: Move freeing of kmem_cache structure to common code
    
    The freeing action is basically the same in all slab allocators.
    Move to the common kmem_cache_destroy() function.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c359010e13ab51048c08d90b03ceac8064c6b701
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:18:33 2012 +0000

    mm/sl[aou]b: Extract a common function for kmem_cache_destroy
    
    kmem_cache_destroy does basically the same in all allocators.
    
    Extract common code which is easy since we already have common mutex
    handling.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c12e1b821151a7b177671f428b511dca89c7d0b5
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:20:33 2012 +0000

    mm/sl[aou]b: Use "kmem_cache" name for slab cache with kmem_cache struct
    
    Make all allocators use the "kmem_cache" slabname for the "kmem_cache"
    structure.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit b1b09dd887dcc9a917d369118c858993dfc818b4
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:43:07 2013 -0500

    mm/sl[aou]b: Move list_add() to slab_common.c
    
    Move the code to append the new kmem_cache to the list of slab caches to
    the kmem_cache_create code in the shared code.
    
    This is possible now since the acquisition of the mutex was moved into
    kmem_cache_create().
    
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 00318729c12a224b919f8940dc939016439c0f93
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 5 00:20:33 2012 +0000

    mm/slab_common: Improve error handling in kmem_cache_create
    
    Instead of using s == NULL use an errorcode. This allows much more
    detailed diagnostics as to what went wrong. As we add more functionality
    from the slab allocators to the common kmem_cache_create() function we will
    also add more error conditions.
    
    Print the error code during the panic as well as in a warning if the module
    can handle failure. The API for kmem_cache_create() currently does not allow
    the returning of an error code. Return NULL but log the cause of the problem
    in the syslog.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 23c3ea5354f49fabde9a0c60e34813467acb3b54
Author: Shuah Khan <shuah.khan@hp.com>
Date:   Thu Aug 16 00:09:46 2012 -0700

    mm/slab: restructure kmem_cache_create() debug checks
    
    kmem_cache_create() does cache integrity checks when CONFIG_DEBUG_VM is
    defined.  These checks interspersed with the regular code path has lead
    to compile time warnings when compiled without CONFIG_DEBUG_VM defined.
    Restructuring the code to move the integrity checks in to a new function
    would eliminate the current compile warning problem and also will allow
    for future changes to the debug only code to evolve without introducing
    new warnings in the regular path.
    
    This restructuring work is based on the discussion in the following
    thread:
    
    https://lkml.org/lkml/2012/7/13/424
    
    [akpm@linux-foundation.org: fix build, cleanup]
    Signed-off-by: Shuah Khan <shuah.khan@hp.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5a17acfd73010c7da7386188b58f2c1bd17a3023
Author: Pekka Enberg <penberg@kernel.org>
Date:   Thu Aug 16 10:12:18 2012 +0300

    Revert "mm/slab_common.c: cleanup"
    
    This reverts commit 455ce9eb1cfa083da0def023094190aeb133855a. Andrew
    sent a better version.
    
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 86df776679bb90715ae03b9705b49795f2f87584
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Aug 14 14:53:22 2012 -0700

    mm/slab_common.c: cleanup
    
    Eliminate an ifdef and a label by moving all the CONFIG_DEBUG_VM checking
    inside the locked region.
    
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 5a16f5ec0ca238977b6ef46335b555fea8750537
Author: Shuah Khan <shuah.khan@hp.com>
Date:   Fri Jul 13 17:12:05 2012 -0600

    mm: Fix build warning in kmem_cache_create()
    
    The label oops is used in CONFIG_DEBUG_VM ifdef block and is defined
    outside ifdef CONFIG_DEBUG_VM block. This results in the following
    build warning when built with CONFIG_DEBUG_VM disabled. Fix to move
    label oops definition to inside a CONFIG_DEBUG_VM block.
    
    mm/slab_common.c: In function ‘kmem_cache_create’:
    mm/slab_common.c:101:1: warning: label ‘oops’ defined but not used
    [-Wunused-label]
    
    Signed-off-by: Shuah Khan <shuah.khan@hp.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	mm/slab_common.c

commit bc366efe489d00a01d2079d591f2d538451945bb
Author: David Rientjes <rientjes@google.com>
Date:   Tue Aug 28 19:57:21 2012 -0700

    mm, slab: lock the correct nodelist after reenabling irqs
    
    cache_grow() can reenable irqs so the cpu (and node) can change, so ensure
    that we take list_lock on the correct nodelist.
    
    This fixes an issue with commit 072bb0aa5e06 ("mm: sl[au]b: add
    knowledge of PFMEMALLOC reserve pages") where list_lock for the wrong
    node was taken after growing the cache.
    
    Reported-and-tested-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1e4dac7e85b4adf388cd89b1bd7fc2fa10db0912
Author: David Rientjes <rientjes@google.com>
Date:   Thu Aug 16 12:25:31 2012 -0700

    mm, slab: remove page_get_cache
    
    page_get_cache() isn't called from anything, so remove it.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 9271d750210f91670360ac99164768978a0f1759
Author: Michel Lespinasse <walken@google.com>
Date:   Tue Aug 14 14:53:20 2012 -0700

    slab: do not call compound_head() in page_get_cache()
    
    page_get_cache() does not need to call compound_head(), as its unique
    caller virt_to_slab() already makes sure to return a head page.
    
    Additionally, removing the compound_head() call makes page_get_cache()
    consistent with page_get_slab().
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit a1a7bb048cea95f6f4eb3ead441e4f17ffda9ac0
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:30 2012 -0700

    mm: micro-optimise slab to avoid a function call
    
    Getting and putting objects in SLAB currently requires a function call but
    the bulk of the work is related to PFMEMALLOC reserves which are only
    consumed when network-backed storage is critical.  Use an inline function
    to determine if the function call is required.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6223c36ed00001a8b528f4cc05028bfbc41507c2
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Nov 2 19:00:22 2013 -0500

    mm: introduce __GFP_MEMALLOC to allow access to emergency reserves
    
    __GFP_MEMALLOC will allow the allocation to disregard the watermarks, much
    like PF_MEMALLOC.  It allows one to pass along the memalloc state in
    object related allocation flags as opposed to task related flags, such as
    sk->sk_allocation.  This removes the need for ALLOC_PFMEMALLOC as callers
    using __GFP_MEMALLOC can get the ALLOC_NO_WATERMARK flag which is now
    enough to identify allocations related to page reclaim.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    backported to Android Linux 3.4 by faux123
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	mm/internal.h

commit 01f2cc76c90ed56496fbc306e0fc6f975c22fdc5
Author: Paul Reioux <reioux@gmail.com>
Date:   Sun Sep 8 17:41:26 2013 -0500

    mm/slub: Use kmem_cache for the kmem_cache structure
    
    Do not use kmalloc() but kmem_cache_alloc() for the allocation
    of the kmem_cache structures in slub.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    backported for Linux 3.4
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 539b32bf8d3e6d9a6e48a6ac3f387b0793fc33e4
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 4 23:06:14 2012 +0000

    mm/slub: Add debugging to verify correct cache use on kmem_cache_free()
    
    Add additional debugging to check that the objects is actually from the cache
    the caller claims. Doing so currently trips up some other debugging code. It
    takes a lot to infer from that what was happening.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    [ penberg@kernel.org: Use pr_err() ]
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit ad8b26fd18f1e68d5919869b613228de48786cad
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Jun 23 03:22:38 2012 +0900

    slub: reduce failure of this_cpu_cmpxchg in put_cpu_partial() after unfreezing
    
    In current implementation, after unfreezing, we doesn't touch oldpage,
    so it remain 'NOT NULL'. When we call this_cpu_cmpxchg()
    with this old oldpage, this_cpu_cmpxchg() is mostly be failed.
    
    We can change value of oldpage to NULL after unfreezing,
    because unfreeze_partial() ensure that all the cpu partial slabs is removed
    from cpu partial list. In this time, we could expect that
    this_cpu_cmpxchg is mostly succeed.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit acc059cb90bb21961b6e316285f94f08b7dd8551
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 30 12:54:46 2012 -0500

    slub: Take node lock during object free checks
    
    Only applies to scenarios where debugging is on:
    
    Validation of slabs can currently occur while debugging
    information is updated from the fast paths of the allocator.
    This results in various races where we get false reports about
    slab metadata not being in order.
    
    This patch makes the fast paths take the node lock so that
    serialization with slab validation will occur. Causes additional
    slowdown in debug scenarios.
    
    Reported-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 088e96d21e97c11321b42ce5d56d74dc97ad23c1
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Aug 3 22:51:37 2012 +0400

    slub: use free_page instead of put_page for freeing kmalloc allocation
    
    When freeing objects, the slub allocator will most of the time free
    empty pages by calling __free_pages(). But high-order kmalloc will be
    diposed by means of put_page() instead. It makes no sense to call
    put_page() in kernel pages that are provided by the object allocators,
    so we shouldn't be doing this ourselves. Aside from the consistency
    change, we don't change the flow too much. put_page()'s would call its
    dtor function, which is __free_pages. We also already do all of the
    Compound page tests ourselves, and the Mlock test we lose don't really
    matter.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: David Rientjes <rientjes@google.com>
    CC: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 89365c6b11e5106e8bfe68ef90c7b8887d18f081
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Jul 31 16:44:00 2012 -0700

    mm: slub: optimise the SLUB fast path to avoid pfmemalloc checks
    
    This patch removes the check for pfmemalloc from the alloc hotpath and
    puts the logic after the election of a new per cpu slab.  For a pfmemalloc
    page we do not use the fast path but force the use of the slow path which
    is also used for the debug case.
    
    This has the side-effect of weakening pfmemalloc processing in the
    following way;
    
    1. A process that is allocating for network swap calls __slab_alloc.
       pfmemalloc_match is true so the freelist is loaded and c->freelist is
       now pointing to a pfmemalloc page.
    
    2. A process that is attempting normal allocations calls slab_alloc,
       finds the pfmemalloc page on the freelist and uses it because it did
       not check pfmemalloc_match()
    
    The patch allows non-pfmemalloc allocations to use pfmemalloc pages with
    the kmalloc slabs being the most vunerable caches on the grounds they
    are most likely to have a mix of pfmemalloc and !pfmemalloc requests. A
    later patch will still protect the system as processes will get throttled
    if the pfmemalloc reserves get depleted but performance will not degrade
    as smoothly.
    
    [mgorman@suse.de: Expanded changelog]
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c045093be26c285ad0044a12bb6575426764a5de
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Nov 2 18:52:34 2013 -0500

    mm: sl[au]b: add knowledge of PFMEMALLOC reserve pages
    
    When a user or administrator requires swap for their application, they
    create a swap partition and file, format it with mkswap and activate it
    with swapon.  Swap over the network is considered as an option in diskless
    systems.  The two likely scenarios are when blade servers are used as part
    of a cluster where the form factor or maintenance costs do not allow the
    use of disks and thin clients.
    
    The Linux Terminal Server Project recommends the use of the Network Block
    Device (NBD) for swap according to the manual at
    https://sourceforge.net/projects/ltsp/files/Docs-Admin-Guide/LTSPManual.pdf/download
    There is also documentation and tutorials on how to setup swap over NBD at
    places like https://help.ubuntu.com/community/UbuntuLTSP/EnableNBDSWAP The
    nbd-client also documents the use of NBD as swap.  Despite this, the fact
    is that a machine using NBD for swap can deadlock within minutes if swap
    is used intensively.  This patch series addresses the problem.
    The core issue is that network block devices do not use mempools like
    normal block devices do.  As the host cannot control where they receive
    packets from, they cannot reliably work out in advance how much memory
    they might need.  Some years ago, Peter Zijlstra developed a series of
    patches that supported swap over an NFS that at least one distribution is
    carrying within their kernels.  This patch series borrows very heavily
    from Peter's work to support swapping over NBD as a pre-requisite to
    supporting swap-over-NFS.  The bulk of the complexity is concerned with
    preserving memory that is allocated from the PFMEMALLOC reserves for use
    by the network layer which is needed for both NBD and NFS.
    
    Patch 1 adds knowledge of the PFMEMALLOC reserves to SLAB and SLUB to
            preserve access to pages allocated under low memory situations
            to callers that are freeing memory.
    
    Patch 2 optimises the SLUB fast path to avoid pfmemalloc checks
    
    Patch 3 introduces __GFP_MEMALLOC to allow access to the PFMEMALLOC
            reserves without setting PFMEMALLOC.
    
    Patch 4 opens the possibility for softirqs to use PFMEMALLOC reserves        for later use by network packet processing.
    
    Patch 5 only sets page->pfmemalloc when ALLOC_NO_WATERMARKS was required
    
    Patch 6 ignores memory policies when ALLOC_NO_WATERMARKS is set.
    
    Patches 7-12 allows network processing to use PFMEMALLOC reserves when
            the socket has been marked as being used by the VM to clean pages. If
            packets are received and stored in pages that were allocated under
            low-memory situations and are unrelated to the VM, the packets
            are dropped.
    
            Patch 11 reintroduces __skb_alloc_page which the networking
            folk may object to but is needed in some cases to propogate
            pfmemalloc from a newly allocated page to an skb. If there is a
            strong objection, this patch can be dropped with the impact being
            that swap-over-network will be slower in some cases but it should
            not fail.
    
    Patch 13 is a micro-optimisation to avoid a function call in the
            common case.
    Patch 14 tags NBD sockets as being SOCK_MEMALLOC so they can use
            PFMEMALLOC if necessary.
    
    Patch 16 adds a statistic to track how often processes get throttled
    
    Some basic performance testing was run using kernel builds, netperf on
    loopback for UDP and TCP, hackbench (pipes and sockets), iozone and
    sysbench.  Each of them were expected to use the sl*b allocators
    reasonably heavily but there did not appear to be significant performance
    variances.
    
    For testing swap-over-NBD, a machine was booted with 2G of RAM with a
    swapfile backed by NBD.  8*NUM_CPU processes were started that create
    anonymous memory mappings and read them linearly in a loop.  The total
    size of the mappings were 4*PHYSICAL_MEMORY to use swap heavily under
    memory pressure.
    
    Without the patches and using SLUB, the machine locks up within minutes
    and runs to completion with them applied.  With SLAB, the story is
    different as an unpatched kernel run to completion.  However, the patched
    kernel completed the test 45% faster.
    
    MICRO
                                             3.5.0-rc2 3.5.0-rc2
                                             vanilla     swapnbd
    Unrecognised test vmscan-anon-mmap-write
    MMTests Statistics: duration
    Sys Time Running Test (seconds)             197.80    173.07
    User+Sys Time Running Test (seconds)        206.96    182.03
    Total Elapsed Time (seconds)               3240.70   1762.09
    
    This patch: mm: sl[au]b: add knowledge of PFMEMALLOC reserve pages
    
    Allocations of pages below the min watermark run a risk of the machine
    hanging due to a lack of memory.  To prevent this, only callers who have
    PF_MEMALLOC or TIF_MEMDIE set and are not processing an interrupt are
    allowed to allocate with ALLOC_NO_WATERMARKS.  Once they are allocated to
    a slab though, nothing prevents other callers consuming free objects
    within those slabs.  This patch limits access to slab pages that were
    alloced from the PFMEMALLOC reserves.
    
    When this patch is applied, pages allocated from below the low watermark
    are returned with page->pfmemalloc set and it is up to the caller to
    determine how the page should be protected.  SLAB restricts access to any
    page with page->pfmemalloc set to callers which are known to able to
    access the PFMEMALLOC reserve.  If one is not available, an attempt is
    made to allocate a new page rather than use a reserve.  SLUB is a bit more
    relaxed in that it only records if the current per-CPU page was allocated
    from PFMEMALLOC reserve and uses another partial slab if the caller does
    not have the necessary GFP or process flags.  This was found to be
    sufficient in tests to avoid hangs due to SLUB generally maintaining
    smaller lists than SLAB.
    
    In low-memory conditions it does mean that !PFMEMALLOC allocators can fail
    a slab allocation even though free objects are available because they are
    being preserved for callers that are freeing pages.
    
    [a.p.zijlstra@chello.nl: Original implementation]
    [sebastian@breakpoint.cc: Correct order of page flag clearing]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    backported for Android Linux 3.4 by faux123
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	mm/internal.h

commit 0539c8057d0f88e708ccb5928340ee6b2d6851db
Author: David Rientjes <rientjes@google.com>
Date:   Mon Jul 9 14:00:38 2012 -0700

    mm, slub: ensure irqs are enabled for kmemcheck
    
    kmemcheck_alloc_shadow() requires irqs to be enabled, so wait to disable
    them until after its called for __GFP_WAIT allocations.
    
    This fixes a warning for such allocations:
    
    	WARNING: at kernel/lockdep.c:2739 lockdep_trace_alloc+0x14e/0x1c0()
    
    Acked-by: Fengguang Wu <fengguang.wu@intel.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit c55799e762d7195d8a672206870204d4bde919a0
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:35:51 2013 -0500

    mm, sl[aou]b: Move kmem_cache_create mutex handling to common code
    
    Move the mutex handling into the common kmem_cache_create()
    function.
    
    Then we can also move more checks out of SLAB's kmem_cache_create()
    into the common code.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit bb7936f325c5a2f7a67e1ed37f9c4e0f709f251b
Author: dorimanx <yuri@bynet.co.il>
Date:   Tue Feb 4 00:03:43 2014 +0200

    mm, sl[aou]b: Use a common mutex definition
    
    Use the mutex definition from SLAB and make it the common way to take a sleeping lock.
    
    This has the effect of using a mutex instead of a rw semaphore for SLUB.
    
    SLOB gains the use of a mutex for kmem_cache_create serialization.
    Not needed now but SLOB may acquire some more features later (like slabinfo
    / sysfs support) through the expansion of the common code that will
    need this.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit b5c5595ee5875c03569a4b1d9f7de9de817c2d96
Author: dorimanx <yuri@bynet.co.il>
Date:   Tue Feb 4 00:02:11 2014 +0200

    mm, sl[aou]b: Common definition for boot state of the slab allocators
    
    All allocators have some sort of support for the bootstrap status.
    
    Setup a common definition for the boot states and make all slab
    allocators use that definition.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 19e38c3ee6c2526e65e208668afcb7f52bb55f35
Author: Julia Lawall <Julia.Lawall@lip6.fr>
Date:   Sun Jul 8 13:37:40 2012 +0200

    slub: remove invalid reference to list iterator variable
    
    If list_for_each_entry, etc complete a traversal of the list, the iterator
    variable ends up pointing to an address at an offset from the list head,
    and not a meaningful structure.  Thus this value should not be used after
    the end of the iterator.  The patch replaces s->name by al->name, which is
    referenced nearby.
    
    This problem was found using Coccinelle (http://coccinelle.lip6.fr/).
    
    Signed-off-by: Julia Lawall <Julia.Lawall@lip6.fr>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 40f0d4ae9b974efd0a3a35f0af6ce557581f7487
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Jun 9 02:23:16 2012 +0900

    slub: refactoring unfreeze_partials()
    
    Current implementation of unfreeze_partials() is so complicated,
    but benefit from it is insignificant. In addition many code in
    do {} while loop have a bad influence to a fail rate of cmpxchg_double_slab.
    Under current implementation which test status of cpu partial slab
    and acquire list_lock in do {} while loop,
    we don't need to acquire a list_lock and gain a little benefit
    when front of the cpu partial slab is to be discarded, but this is a rare case.
    In case that add_partial is performed and cmpxchg_double_slab is failed,
    remove_partial should be called case by case.
    
    I think that these are disadvantages of current implementation,
    so I do refactoring unfreeze_partials().
    
    Minimizing code in do {} while loop introduce a reduced fail rate
    of cmpxchg_double_slab. Below is output of 'slabinfo -r kmalloc-256'
    when './perf stat -r 33 hackbench 50 process 4000 > /dev/null' is done.
    
    ** before **
    Cmpxchg_double Looping
    ------------------------
    Locked Cmpxchg Double redos   182685
    Unlocked Cmpxchg Double redos 0
    
    ** after **
    Cmpxchg_double Looping
    ------------------------
    Locked Cmpxchg Double redos   177995
    Unlocked Cmpxchg Double redos 1
    
    We can see cmpxchg_double_slab fail rate is improved slightly.
    
    Bolow is output of './perf stat -r 30 hackbench 50 process 4000 > /dev/null'.
    
    ** before **
     Performance counter stats for './hackbench 50 process 4000' (30 runs):
    
         108517.190463 task-clock                #    7.926 CPUs utilized            ( +-  0.24% )
             2,919,550 context-switches          #    0.027 M/sec                    ( +-  3.07% )
               100,774 CPU-migrations            #    0.929 K/sec                    ( +-  4.72% )
               124,201 page-faults               #    0.001 M/sec                    ( +-  0.15% )
       401,500,234,387 cycles                    #    3.700 GHz                      ( +-  0.24% )
       <not supported> stalled-cycles-frontend
       <not supported> stalled-cycles-backend
       250,576,913,354 instructions              #    0.62  insns per cycle          ( +-  0.13% )
        45,934,956,860 branches                  #  423.297 M/sec                    ( +-  0.14% )
           188,219,787 branch-misses             #    0.41% of all branches          ( +-  0.56% )
    
          13.691837307 seconds time elapsed                                          ( +-  0.24% )
    
    ** after **
     Performance counter stats for './hackbench 50 process 4000' (30 runs):
    
         107784.479767 task-clock                #    7.928 CPUs utilized            ( +-  0.22% )
             2,834,781 context-switches          #    0.026 M/sec                    ( +-  2.33% )
                93,083 CPU-migrations            #    0.864 K/sec                    ( +-  3.45% )
               123,967 page-faults               #    0.001 M/sec                    ( +-  0.15% )
       398,781,421,836 cycles                    #    3.700 GHz                      ( +-  0.22% )
       <not supported> stalled-cycles-frontend
       <not supported> stalled-cycles-backend
       250,189,160,419 instructions              #    0.63  insns per cycle          ( +-  0.09% )
        45,855,370,128 branches                  #  425.436 M/sec                    ( +-  0.10% )
           169,881,248 branch-misses             #    0.37% of all branches          ( +-  0.43% )
    
          13.596272341 seconds time elapsed                                          ( +-  0.22% )
    
    No regression is found, but rather we can see slightly better result.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit cc03e92af148df75796f6b9ea4d4a8351aa1210f
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Fri May 18 22:01:17 2012 +0900

    slub: use __cmpxchg_double_slab() at interrupt disabled place
    
    get_freelist(), unfreeze_partials() are only called with interrupt disabled,
    so __cmpxchg_double_slab() is suitable.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 9b2be5bd2301f1246416d8d5bdd2d2346812469c
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 23:59:39 2014 +0200

    slab: move FULL state transition to an initcall
    
    During kmem_cache_init_late(), we transition to the LATE state,
    and after some more work, to the FULL state, its last state
    
    This is quite different from slub, that will only transition to
    its last state (previously SYSFS), in a (late)initcall, after a lot
    more of the kernel is ready.
    
    This means that in slab, we have no way to taking actions dependent
    on the initialization of other pieces of the kernel that are supposed
    to start way after kmem_init_late(), such as cgroups initialization.
    
    To achieve more consistency in this behavior, that patch only
    transitions to the UP state in kmem_init_late. In my analysis,
    setup_cpu_cache() should be happy to test for >= UP, instead of
    == FULL. It also has passed some tests I've made.
    
    We then only mark FULL state after the reap timers are in place,
    meaning that no further setup is expected.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e0840c133dc8b1bf65839c8d450dd32c9784d5d1
Author: Feng Tang <feng.tang@intel.com>
Date:   Mon Jul 2 14:29:10 2012 +0800

    slab: Fix a typo in commit 8c138b "slab: Get rid of obj_size macro"
    
    Commit  8c138b only sits in Pekka's and linux-next tree now, which tries
    to replace obj_size(cachep) with cachep->object_size, but has a typo in
    kmem_cache_free() by using "size" instead of "object_size", which casues
    some regressions.
    
    Reported-and-tested-by: Fengguang Wu <wfg@linux.intel.com>
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Cc: Christoph Lameter <cl@linux.com>
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 09fe4bc776fea0531c5e0f7462483bb16dcde324
Author: Thierry Reding <thierry.reding@avionic-design.de>
Date:   Fri Jun 22 19:42:49 2012 +0200

    mm, slab: Build fix for recent kmem_cache changes
    
    Commit 3b0efdf ("mm, sl[aou]b: Extract common fields from struct
    kmem_cache") renamed the kmem_cache structure's "next" field to "list"
    but forgot to update one instance in leaks_show().
    
    Signed-off-by: Thierry Reding <thierry.reding@avionic-design.de>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 4aa8d3f5dbcbd9844da513a48c9e76a43ba9b79f
Author: Glauber Costa <glommer@parallels.com>
Date:   Thu Jun 14 16:17:21 2012 +0400

    slab: rename gfpflags to allocflags
    
    A consistent name with slub saves us an acessor function.
    In both caches, this field represents the same thing. We would
    like to use it from the mem_cgroup code.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 921b2c1b0d3b9e06cf26bc78774d99717e35237a
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sat Jun 9 02:40:03 2012 -0700

    slab/mempolicy: always use local policy from interrupt context
    
    slab_node() could access current->mempolicy from interrupt context.
    However there's a race condition during exit where the mempolicy
    is first freed and then the pointer zeroed.
    
    Using this from interrupts seems bogus anyways. The interrupt
    will interrupt a random process and therefore get a random
    mempolicy. Many times, this will be idle's, which noone can change.
    
    Just disable this here and always use local for slab
    from interrupts. I also cleaned up the callers of slab_node a bit
    which always passed the same argument.
    
    I believe the original mempolicy code did that in fact,
    so it's likely a regression.
    
    v2: send version with correct logic
    v3: simplify. fix typo.
    Reported-by: Arun Sharma <asharma@fb.com>
    Cc: penberg@kernel.org
    Cc: cl@linux.com
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    [tdmackey@twitter.com: Rework control flow based on feedback from
    cl@linux.com, fix logic, and cleanup current task_struct reference]
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: David Mackey <tdmackey@twitter.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 0e7581d0cbfc866e06c768c7e0321f6422199401
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:58 2012 -0500

    slab: Get rid of obj_size macro
    
    The size of the slab object is frequently needed. Since we now
    have a size field directly in the kmem_cache structure there is no
    need anymore of the obj_size macro/function.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit cb76e3860b5bf445a344954e5ee2cc29ee0cc788
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Jul 10 18:31:05 2012 -0500

    slob: Fix early boot kernel crash
    
    Commit fd3142a59af2012a7c5dc72ec97a4935ff1c5fc6 broke
    slob since a piece of a change for a later patch slipped into
    it.
    
    Fengguang Wu writes:
    
      The commit crashes the kernel w/o any dmesg output (the attached one is
      created by the script as a summary for that run). This is very
      reproducible in kvm for the attached config.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 31b4246a60e7abb89386ed6e065105ac6ebc20e6
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Sep 8 17:30:44 2013 -0500

    mm, sl[aou]b: Extract common code for kmem_cache_create()
    
    Kmem_cache_create() does a variety of sanity checks but those
    vary depending on the allocator. Use the strictest tests and put them into
    a slab_common file. Make the tests conditional on CONFIG_DEBUG_VM.
    
    This patch has the effect of adding sanity checks for SLUB and SLOB
    under CONFIG_DEBUG_VM and removes the checks in SLAB for !CONFIG_DEBUG_VM.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 7641297157d0d084d27431b8a806fd714ce601fc
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:57 2012 -0500

    mm, sl[aou]b: Extract common fields from struct kmem_cache
    
    Define a struct that describes common fields used in all slab allocators.
    A slab allocator either uses the common definition (like SLOB) or is
    required to provide members of kmem_cache with the definition given.
    
    After that it will be possible to share code that
    only operates on those fields of kmem_cache.
    
    The patch basically takes the slob definition of kmem cache and
    uses the field namees for the other allocators.
    
    It also standardizes the names used for basic object lengths in
    allocators:
    
    object_size	Struct size specified at kmem_cache_create. Basically
    		the payload expected to be used by the subsystem.
    
    size		The size of memory allocator for each object. This size
    		is larger than object_size and includes padding, alignment
    		and extra metadata for each object (f.e. for debugging
    		and rcu).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 00619542803cf112a2e877d88d6c0dc29083589c
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:54 2012 -0500

    slob: Remove various small accessors
    
    Those have become so simple that they are no longer needed.
    
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: David Rientjes <rientjes@google.com>
    signed-off-by: Christoph Lameter <cl@linux.com>
    
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 23f961e0bdd1285e2a4328084b8bda56a2e6532c
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:53 2012 -0500

    slob: No need to zero mapping since it is no longer in use
    
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit b2cd3733339763378f59d4a226be52671e770c32
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:56 2012 -0500

    slab: Remove some accessors
    
    Those are rather trivial now and its better to see inline what is
    really going on.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 3eb8371098e8d157c849c7252227f087ed889390
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:55 2012 -0500

    slab: Use page struct fields instead of casting
    
    Add fields to the page struct so that it is properly documented that
    slab overlays the lru fields.
    
    This cleans up some casts in slab.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 629a60192d9c74f202d46a2c338c0d909904ea0c
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:52 2012 -0500

    slob: Define page struct fields used in mm_types.h
    
    Define the fields used by slob in mm_types.h and use struct page instead
    of struct slob_page in slob. This cleans up numerous of typecasts in slob.c and
    makes readers aware of slob's use of page struct fields.
    
    [Also cleans up some bitrot in slob.c. The page struct field layout
    in slob.c is an old layout and does not match the one in mm_types.h]
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 520737e440d711f008cd5962b83fe84df05ef90b
Author: Christoph Lameter <cl@linux.com>
Date:   Sat Nov 2 18:49:05 2013 -0500

    slub: pass page to node_match() instead of kmem_cache_cpu structure
    
    Avoid passing the kmem_cache_cpu pointer to node_match. This makes the
    node_match function more generic and easier to understand.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 7568da71a3d86493df34ca48ed8c153c0b7367a5
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:58 2012 -0500

    slub: Use page variable instead of c->page.
    
    Store the value of c->page to avoid additional fetches
    from per cpu data.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 8f650e4f1ad4d6aac207ddfbd05b99b9673ea6f5
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:57 2012 -0500

    slub: Separate out kmem_cache_cpu processing from deactivate_slab
    
    Processing on fields of kmem_cache_cpu is cleaner if code working on fields
    of this struct is taken out of deactivate_slab().
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 6f8b461e750c0454a83cc9935f45d1eca2d79cb3
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:56 2012 -0500

    slub: Get rid of the node field
    
    The node field is always page_to_nid(c->page). So its rather easy to
    replace. Note that there maybe slightly more overhead in various hot paths
    due to the need to shift the bits from page->flags. However, that is mostly
    compensated for by a smaller footprint of the kmem_cache_cpu structure (this
    patch reduces that to 3 words per cache) which allows better caching.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 270a6db16f6a5c86390507ec809c8018f66c5252
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:55 2012 -0500

    slub: new_slab_objects() can also get objects from partial list
    
    Moving the attempt to get a slab page from the partial lists simplifies
    __slab_alloc which is rather complicated.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit e8478410c6f7bb4524638a2121e6ea8784083253
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:54 2012 -0500

    slub: Simplify control flow in __slab_alloc()
    
    Simplify control flow a bit avoiding nesting.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 7180246ad10fb464eca4dcb54f3b99c44bba9f0b
Author: dorimanx <yuri@bynet.co.il>
Date:   Mon Feb 3 23:57:06 2014 +0200

    slub: Acquire_slab() avoid loop
    
    Avoid the loop in acquire slab and simply fail if there is a conflict.
    
    This will cause the next page on the list to be considered.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 88a1fb7c5bece3be8cb650ff5a1c2b6739bcddbb
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:52 2012 -0500

    slub: Add frozen check in __slab_alloc
    
    Verify that objects returned from __slab_alloc come from slab pages
    in the correct state.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit eb9ceac7edb642588c2c88c5d284b4d291b01bd9
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:51 2012 -0500

    slub: Use freelist instead of "object" in __slab_alloc
    
    The variable "object" really refers to a list of objects that we
    are handling.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 9c770b464078e608e6c87fe26685595cde52d024
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Fri May 18 00:47:47 2012 +0900

    slub: use __SetPageSlab function to set PG_slab flag
    
    To set page-flag, using SetPageXXXX() and __SetPageXXXX() is more
    understandable and maintainable. So change it.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

commit 0a31325a44e8d81ba6982dd1d5028213d69c751d
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Jun 17 12:26:33 2013 -0500

    mm: Clear page active before releasing pages
    
    Active pages should not be freed to the page allocator as it triggers
    a bad page state warning. Fengguang Wu reported the following
    bug and bisected it to the patch "mm: remove lru parameter from
    __lru_cache_add and lru_cache_add_lru" which is currently in mmotm as
    mm-remove-lru-parameter-from-__lru_cache_add-and-lru_cache_add_lru.patch
    
    [   84.212960] BUG: Bad page state in process rm  pfn:0b0c9
    [   84.214682] page:ffff88000d646240 count:0 mapcount:0 mapping:          (null) index:0x0
    [   84.216883] page flags: 0x20000000004c(referenced|uptodate|active)
    [   84.218697] CPU: 1 PID: 283 Comm: rm Not tainted 3.10.0-rc4-04361-geeb9bfc #49
    [   84.220729]  ffff88000d646240 ffff88000d179bb8 ffffffff82562956 ffff88000d179bd8
    [   84.223242]  ffffffff811333f1 000020000000004c ffff88000d646240 ffff88000d179c28
    [   84.225387]  ffffffff811346a4 ffff880000270000 0000000000000000 0000000000000006
    [   84.227294] Call Trace:
    [   84.227867]  [<ffffffff82562956>] dump_stack+0x27/0x30
    [   84.229045]  [<ffffffff811333f1>] bad_page+0x130/0x158
    [   84.230261]  [<ffffffff811346a4>] free_pages_prepare+0x8b/0x1e3
    [   84.231765]  [<ffffffff8113542a>] free_hot_cold_page+0x28/0x1cf
    [   84.233171]  [<ffffffff82585830>] ? _raw_spin_unlock_irqrestore+0x6b/0xc6
    [   84.234822]  [<ffffffff81135b59>] free_hot_cold_page_list+0x30/0x5a
    [   84.236311]  [<ffffffff8113a4ed>] release_pages+0x251/0x267
    [   84.237653]  [<ffffffff8112a88d>] ? delete_from_page_cache+0x48/0x9e
    [   84.239142]  [<ffffffff8113ad93>] __pagevec_release+0x2b/0x3d
    [   84.240473]  [<ffffffff8113b45a>] truncate_inode_pages_range+0x1b0/0x7ce
    [   84.242032]  [<ffffffff810e76ab>] ? put_lock_stats.isra.20+0x1c/0x53
    [   84.243480]  [<ffffffff810e77f5>] ? lock_release_holdtime+0x113/0x11f
    [   84.244935]  [<ffffffff8113ba8c>] truncate_inode_pages+0x14/0x1d
    [   84.246337]  [<ffffffff8119b3ef>] evict+0x11f/0x232
    [   84.247501]  [<ffffffff8119c527>] iput+0x1a5/0x218
    [   84.248607]  [<ffffffff8118f015>] do_unlinkat+0x19b/0x25a
    [   84.249828]  [<ffffffff810ea993>] ? trace_hardirqs_on_caller+0x210/0x2ce
    [   84.251382]  [<ffffffff8144372e>] ? trace_hardirqs_on_thunk+0x3a/0x3f
    [   84.252879]  [<ffffffff8118f10d>] SyS_unlinkat+0x39/0x4c
    [   84.254174]  [<ffffffff825874d6>] system_call_fastpath+0x1a/0x1f
    [   84.255596] Disabling lock debugging due to kernel taint
    The problem was that a page marked for activation was released via
    pagevec. This patch clears the active bit before freeing in this case.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reported-and-Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 5b941aa8729410eec94e7426481f9cdbc24064e1
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 22 12:53:56 2014 +0100

    Updated fiops io-schedule. Thanks to Yuri!

commit 6302293f41586fa664a59b4fd2d1fd404a630fe4
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Fri Jun 7 17:23:09 2013 -0700

    msm: cpufreq: Ensure cpufreq change happens on corresponding CPU
    
    * This removes stuttering a few ppl were experiencing -imoseyon
    
    Checking the cpus_allowed mask of the current thread before changing the
    frequency doesn't guarantee that the rest of the execution will continue on
    the same CPU. The only way to guarantee this is to schedule a work on the
    specific CPU and also prevent hotplug  of that CPU (already done by
    existing code).
    
    Change-Id: I51a02fcc777a47d3c16f2d83c47e96f2c59f7ae6
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/cpufreq.c

commit 2a8f1dc6d6b9bc199da88c4dff7d1074387ac1b1
Author: Paul Reioux <reioux@gmail.com>
Date:   Thu May 23 11:48:57 2013 -0500

    arch/arm/mach-msm/cpufreq.c: reduce dmesg log spam
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit c925c0379cf54824dcb43c7686ba90245e004c1e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 22 12:41:42 2014 +0100

    Imported gpu conservative governor into msm2 drivers. Disabled postmortem dump features!

commit 850ffaee88f02b18b0986c9d44eebf22a4a98d29
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 14 18:19:58 2013 +0400

    sched: add wait_for_completion_io[_timeout]
    
    The only difference between wait_for_completion[_timeout]() and
    wait_for_completion_io[_timeout]() is that the latter calls
    io_schedule_timeout() instead of schedule_timeout() so that the caller
    is accounted as waiting for IO, not just sleeping.
    
    These functions can be used for correct iowait time accounting when the
    completion struct is actually used for waiting for IO (e.g. completion
    of a bio request in the block layer).
    
    Modified to compile with 3.4.0 on the Nexus 4 (Mako)
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 4f678695c0f4f54b97dc373770546e828a3e70f0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Sep 7 19:33:51 2013 -0500

    sched: Micro-optimize the smart wake-affine logic
    
    Smart wake-affine is using node-size as the factor currently, but the overhead
    of the mask operation is high.
    
    Thus, this patch introduce the 'sd_llc_size' percpu variable, which will record
    the highest cache-share domain size, and make it to be the new factor, in order
    to reduce the overhead and make it more reasonable.
    
    Tested-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Tested-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Link: http://lkml.kernel.org/r/51D5008E.6030102@linux.vnet.ibm.com
    [ Tidied up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	kernel/sched/core.c

commit 9070e48a2350782aab8580b62e52ee81e51674b6
Author: Michael Wang <wangyun@linux.vnet.ibm.com>
Date:   Sat Sep 7 19:32:00 2013 -0500

    sched: Implement smarter wake-affine logic
    
    The wake-affine scheduler feature is currently always trying to pull
    the wakee close to the waker. In theory this should be beneficial if
    the waker's CPU caches hot data for the wakee, and it's also beneficial
    in the extreme ping-pong high context switch rate case.
    
    Testing shows it can benefit hackbench up to 15%.
    
    However, the feature is somewhat blind, from which some workloads
    such as pgbench suffer. It's also time-consuming algorithmically.
    
    Testing shows it can damage pgbench up to 50% - far more than the
    benefit it brings in the best case.
    
    So wake-affine should be smarter and it should realize when to
    stop its thankless effort at trying to find a suitable CPU to wake on.
    
    This patch introduces 'wakee_flips', which will be increased each
    time the task flips (switches) its wakee target.
    
    So a high 'wakee_flips' value means the task has more than one
    wakee, and the bigger the number, the higher the wakeup frequency.
    
    Now when making the decision on whether to pull or not, pay attention to
    the wakee with a high 'wakee_flips', pulling such a task may benefit
    the wakee. Also imply that the waker will face cruel competition later,
    it could be very cruel or very fast depends on the story behind
    'wakee_flips', waker therefore suffers.
    
    Furthermore, if waker also has a high 'wakee_flips', that implies that
    multiple tasks rely on it, then waker's higher latency will damage all
    of them, so pulling wakee seems to be a bad deal.
    
    Thus, when 'waker->wakee_flips / wakee->wakee_flips' becomes
    higher and higher, the cost of pulling seems to be worse and worse.
    
    The patch therefore helps the wake-affine feature to stop its pulling
    work when:
    
    	wakee->wakee_flips > factor &&
    	waker->wakee_flips > (factor * wakee->wakee_flips)
    The 'factor' here is the number of CPUs in the current CPU's NUMA node,
    so a bigger node will lead to more pulling since the trial becomes more
    severe.
    
    After applying the patch, pgbench shows up to 40% improvements and no regressions.
    
    Tested with 12 cpu x86 server and tip 3.10.0-rc7.
    
    The percentages in the final column highlight the areas with the biggest wins,
    all other areas improved as well:
    
    	pgbench		    base	smart
    
    	| db_size | clients |  tps  |	|  tps  |
    	+---------+---------+-------+   +-------+
    	| 22 MB   |       1 | 10598 |   | 10796 |
    	| 22 MB   |       2 | 21257 |   | 21336 |
    	| 22 MB   |       4 | 41386 |   | 41622 |
    	| 22 MB   |       8 | 51253 |   | 57932 |
    	| 22 MB   |      12 | 48570 |   | 54000 |
    	| 22 MB   |      16 | 46748 |   | 55982 | +19.75%
    	| 22 MB   |      24 | 44346 |   | 55847 | +25.93%
    	| 22 MB   |      32 | 43460 |   | 54614 | +25.66%
    	| 7484 MB |       1 |  8951 |   |  9193 |
    	| 7484 MB |       2 | 19233 |   | 19240 |
    	| 7484 MB |       4 | 37239 |   | 37302 |
    	| 7484 MB |       8 | 46087 |   | 50018 |
    	| 7484 MB |      12 | 42054 |   | 48763 |
    	| 7484 MB |      16 | 40765 |   | 51633 | +26.66%
    	| 7484 MB |      24 | 37651 |   | 52377 | +39.11%
    	| 7484 MB |      32 | 37056 |   | 51108 | +37.92%
    	| 15 GB   |       1 |  8845 |   |  9104 |
    	| 15 GB   |       2 | 19094 |   | 19162 |
    	| 15 GB   |       4 | 36979 |   | 36983 |
    	| 15 GB   |       8 | 46087 |   | 49977 |
    	| 15 GB   |      12 | 41901 |   | 48591 |
    	| 15 GB   |      16 | 40147 |   | 50651 | +26.16%
    	| 15 GB   |      24 | 37250 |   | 52365 | +40.58%
    	| 15 GB   |      32 | 36470 |   | 50015 | +37.14%
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/51D50057.9000809@linux.vnet.ibm.com
    [ Improved the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 3341a30cfd3e84897ef2a7713fae00f909ad4845
Author: Davidlohr Bueso <davidlohr.bueso@hp.com>
Date:   Mon Apr 29 16:18:09 2013 -0700

    lib/int_sqrt.c: optimize square root algorithm
    
    Optimize the current version of the shift-and-subtract (hardware)
    algorithm, described by John von Newmann[1] and Guy L Steele.
    
    Iterating 1,000,000 times, perf shows for the current version:
    
     Performance counter stats for './sqrt-curr' (10 runs):
    
             27.170996 task-clock                #    0.979 CPUs utilized            ( +-  3.19% )
                     3 context-switches          #    0.103 K/sec                    ( +-  4.76% )
                     0 cpu-migrations            #    0.004 K/sec                    ( +-100.00% )
                   104 page-faults               #    0.004 M/sec                    ( +-  0.16% )
            64,921,199 cycles                    #    2.389 GHz                      ( +-  0.03% )
            28,967,789 stalled-cycles-frontend   #   44.62% frontend cycles idle     ( +-  0.18% )
       <not supported> stalled-cycles-backend
           104,502,623 instructions              #    1.61  insns per cycle
                                                 #    0.28  stalled cycles per insn  ( +-  0.00% )
            34,088,368 branches                  # 1254.587 M/sec                    ( +-  0.00% )
                 4,901 branch-misses             #    0.01% of all branches          ( +-  1.32% )
    
           0.027763015 seconds time elapsed                                          ( +-  3.22% )
    
    And for the new version:
    
    Performance counter stats for './sqrt-new' (10 runs):
    
              0.496869 task-clock                #    0.519 CPUs utilized            ( +-  2.38% )
                     0 context-switches          #    0.000 K/sec
                     0 cpu-migrations            #    0.403 K/sec                    ( +-100.00% )
                   104 page-faults               #    0.209 M/sec                    ( +-  0.15% )
               590,760 cycles                    #    1.189 GHz                      ( +-  2.35% )
               395,053 stalled-cycles-frontend   #   66.87% frontend cycles idle     ( +-  3.67% )
       <not supported> stalled-cycles-backend
               398,963 instructions              #    0.68  insns per cycle
                                                 #    0.99  stalled cycles per insn  ( +-  0.39% )
                70,228 branches                  #  141.341 M/sec                    ( +-  0.36% )
                 3,364 branch-misses             #    4.79% of all branches          ( +-  5.45% )
    
           0.000957440 seconds time elapsed                                          ( +-  2.42% )
    
    Furthermore, this saves space in instruction text:
    
       text    data     bss     dec     hex filename
        111       0       0     111      6f lib/int_sqrt-baseline.o
         89       0       0      89      59 lib/int_sqrt.o
    
    [1] http://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC
    
    Signed-off-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Reviewed-by: Jonathan Gonzalez <jgonzlez@linets.cl>
    Tested-by: Jonathan Gonzalez <jgonzlez@linets.cl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>

commit 00bbe3e16b7d6a6d9038d12309308f9a848302f2
Author: Joe Perches <joe@perches.com>
Date:   Mon Nov 11 17:22:41 2013 -0600

    jiffies conversions: Use compile time constants when possible
    
    Date	Fri, 04 Jan 2013 13:15:43 -0800
    
    Do the multiplications and divisions at compile time
    instead of runtime when the converted value is a constant.
    
    Make the calculation functions static __always_inline to jiffies.h.
    
    Add #defines with __builtin_constant_p to test and use the
    static inline or the runtime functions as appropriate.
    
    Prefix the old exported symbols/functions with __
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit bd4a0604c75c4eb11f97ed5de0e1a47aba02a62d
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Jan 15 15:10:46 2014 +0200

    When kswapd is awoken due to reclaim by a running task, set the priority of kswapd to that of the calling task thus making memory reclaim cpu activity affected by nice level.
    
    http://ck.kolivas.org/patches/3.0/3.9/3.9-ck1/patches/mm-kswapd_inherit_prio-1.patch

commit 57c5023f286c1af45ce580e2784cf94222d3875a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 22 12:02:15 2014 +0100

    kgsl: disable tracing and postmortem

commit 2f5b98bc13f82238e2fc9e777e3299ab17665775
Author: glewarne <g.lewarne@gmx.co.uk>
Date:   Tue Nov 26 16:15:33 2013 +0000

    kgsl: disable tracing and postmortem

commit f91d96b478495e9fd1957fa62fe99d7625204d8a
Author: glewarne <g.lewarne@gmx.co.uk>
Date:   Tue Nov 26 16:10:54 2013 +0000

    kgsl: z180 - disable tracing and postmortem

commit e59d962ac16aab0a07f89fbd48d6ea12168c93b3
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Sep 6 18:35:13 2012 +0530

    ARM: mm: implement LoUIS API for cache maintenance ops
    
    ARM v7 architecture introduced the concept of cache levels and related
    control registers. New processors like A7 and A15 embed an L2 unified cache
    controller that becomes part of the cache level hierarchy. Some operations in
    the kernel like cpu_suspend and __cpu_disable do not require a flush of the
    entire cache hierarchy to DRAM but just the cache levels belonging to the
    Level of Unification Inner Shareable (LoUIS), which in most of ARM v7 systems
    correspond to L1.
    
    The current cache flushing API used in cpu_suspend and __cpu_disable,
    flush_cache_all(), ends up flushing the whole cache hierarchy since for
    v7 it cleans and invalidates all cache levels up to Level of Coherency
    (LoC) which cripples system performance when used in hot paths like hotplug
    and cpuidle.
    
    Therefore a new kernel cache maintenance API must be added to cope with
    latest ARM system requirements.
    
    This patch adds flush_cache_louis() to the ARM kernel cache maintenance API.
    
    This function cleans and invalidates all data cache levels up to the
    Level of Unification Inner Shareable (LoUIS) and invalidates the instruction
    cache for processors that support it (> v7).
    
    This patch also creates an alias of the cache LoUIS function to flush_kern_all
    for all processor versions prior to v7, so that the current cache flushing
    behaviour is unchanged for those processors.
    
    v7 cache maintenance code implements a cache LoUIS function that cleans and
    invalidates the D-cache up to LoUIS and invalidates the I-cache, according
    to the new API.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    
    Change-Id: I561c3b187b5b87b25de07ea82a4ef73a42761e27
    Signed-off-by: Karthik Parsha <kparsha@codeaurora.org>
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>

commit 4866520ba5947f15cbd94aac23272b0607f1ab7e
Author: David McCullough <david_mccullough@mcafee.com>
Date:   Fri Sep 7 04:17:02 2012 +0800

    arm/crypto: Add optimized AES and SHA1 routines
    
    Add assembler versions of AES and SHA1 for ARM platforms.  This has provided
    up to a 50% improvement in IPsec/TCP throughout for tunnels using AES128/SHA1.
    
    Platform   CPU SPeed    Endian   Before (bps)   After (bps)   Improvement
    
    IXP425      533 MHz      big     11217042        15566294        ~38%
    KS8695      166 MHz     little    3828549         5795373        ~51%
    
    Signed-off-by: David McCullough <ucdevel@gmail.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 327b61e2474ed2a54dffddbb9dce931748a3b097
Author: dorimanx <yuri@bynet.co.il>
Date:   Fri Feb 14 02:12:20 2014 +0200

     net: wireless: bcmdhd: reduced the wakelock time of RX packet
    
    	Reduce time : 1secs -> 500ms
    	Problem : [Issue 11512235] 34% battery eaten overnight
    		  [Issue 11374623] OS + Wifi used 41% of battery,
    		  wlan_rx_wake
    Signed-off-by: Ecco Park <eccopark@broadcom.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	drivers/net/wireless/bcmdhd/Makefile
    	drivers/net/wireless/bcmdhd/src/dhd/sys/dhd.h

commit ccbe9ed35e8f0d4e87f106537b329f30dabc82f6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 22 11:34:44 2014 +0100

    switch do_fsync() to fget_light()
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk># Please enter the commit message for your changes. Lines starting.
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit ef50a9a056239630b93e62503dc93f35d93983cd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 21 21:41:42 2014 +0100

    Linux 3.4.81
    
    Conflicts:
    	mm/internal.h

commit 321b58928f1fef9371a740bdecfb507cd50d28b6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 21 01:52:34 2014 +0100

    Test Kernel!

commit af63a903a63511ae6cc87f05a7859476627611db
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Tue Oct 23 01:30:54 2012 +0200

    ACPI / processor: prevent cpu from becoming online
    
    Even if acpi_processor_handle_eject() offlines cpu, there is a chance
    to online the cpu after that. So the patch closes the window by using
    get/put_online_cpus().
    
    Why does the patch change _cpu_up() logic?
    
    The patch cares the race of hot-remove cpu and _cpu_up(). If the patch
    does not change it, there is the following race.
    
    hot-remove cpu                         |  _cpu_up()
    ------------------------------------- ------------------------------------
    call acpi_processor_handle_eject()     |
         call cpu_down()                   |
         call get_online_cpus()            |
                                           | call cpu_hotplug_begin() and stop here
         call arch_unregister_cpu()        |
         call acpi_unmap_lsapic()          |
         call put_online_cpus()            |
                                           | start and continue _cpu_up()
         return acpi_processor_remove()    |
    continue hot-remove the cpu            |
    
    So _cpu_up() can continue to itself. And hot-remove cpu can also continue
    itself. If the patch changes _cpu_up() logic, the race disappears as below:
    
    hot-remove cpu                         | _cpu_up()
    -----------------------------------------------------------------------
    call acpi_processor_handle_eject()     |
         call cpu_down()                   |
         call get_online_cpus()            |
                                           | call cpu_hotplug_begin() and stop here
         call arch_unregister_cpu()        |
         call acpi_unmap_lsapic()          |
              cpu's cpu_present is set     |
              to false by set_cpu_present()|
         call put_online_cpus()            |
                                           | start _cpu_up()
                                           | check cpu_present() and return -EINVAL
         return acpi_processor_remove()    |
    continue hot-remove the cpu            |
    
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 1feb421b9dda29c31a084b483f2339d41dd9bc0e
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue Nov 13 11:32:43 2012 -0800

    kernel/cpu.c: Add comment for priority in cpu_hotplug_pm_callback
    
    cpu_hotplug_pm_callback should have higher priority than
    bsp_pm_callback which depends on cpu_hotplug_pm_callback to disable cpu hotplug
    to avoid race during bsp online checking.
    
    This is to hightlight the priorities between the two callbacks in case people
    may overlook the order.
    
    Ideally the priorities should be defined in macro/enum instead of fixed values.
    To do that, a seperate patchset may be pushed which will touch serveral other
    generic files and is out of scope of this patchset.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352835171-3958-7-git-send-email-fenghua.yu@intel.com
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 8902e5e94b674d2242b758cc1b047cf30aaaeeeb
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Tue Jul 31 16:43:28 2012 -0700

    mm/hotplug: correctly setup fallback zonelists when creating new pgdat
    
    When hotadd_new_pgdat() is called to create new pgdat for a new node, a
    fallback zonelist should be created for the new node.  There's code to try
    to achieve that in hotadd_new_pgdat() as below:
    
    	/*
    	 * The node we allocated has no zone fallback lists. For avoiding
    	 * to access not-initialized zonelist, build here.
    	 */
    	mutex_lock(&zonelists_mutex);
    	build_all_zonelists(pgdat, NULL);
    	mutex_unlock(&zonelists_mutex);
    
    But it doesn't work as expected.  When hotadd_new_pgdat() is called, the
    new node is still in offline state because node_set_online(nid) hasn't
    been called yet.  And build_all_zonelists() only builds zonelists for
    online nodes as:
    
            for_each_online_node(nid) {
                    pg_data_t *pgdat = NODE_DATA(nid);
    
                    build_zonelists(pgdat);
                    build_zonelist_cache(pgdat);
            }
    
    Though we hope to create zonelist for the new pgdat, but it doesn't.  So
    add a new parameter "pgdat" the build_all_zonelists() to build pgdat for
    the new pgdat too.
    
    Signed-off-by: Jiang Liu <liuj97@gmail.com>
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Keping Chen <chenkeping@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b548dd899b76594049b7f2bdaa80a047e6245078
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Thu May 31 16:26:26 2012 -0700

    kernel/cpu.c: document clear_tasks_mm_cpumask()
    
    Add more comments on clear_tasks_mm_cpumask, plus adds a runtime check:
    the function is only suitable for offlined CPUs, and if called
    inappropriately, the kernel should scream aloud.
    
    [akpm@linux-foundation.org: tweak comment: s/walks up/walks/, use 80 cols]
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit a51617812bd301b868b552baff2ae8ae518e12c1
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Thu May 31 16:26:22 2012 -0700

    cpu: introduce clear_tasks_mm_cpumask() helper
    
    Many architectures clear tasks' mm_cpumask like this:
    
    	read_lock(&tasklist_lock);
    	for_each_process(p) {
    		if (p->mm)
    			cpumask_clear_cpu(cpu, mm_cpumask(p->mm));
    	}
    	read_unlock(&tasklist_lock);
    
    Depending on the context, the code above may have several problems,
    such as:
    
    1. Working with task->mm w/o getting mm or grabing the task lock is
       dangerous as ->mm might disappear (exit_mm() assigns NULL under
       task_lock(), so tasklist lock is not enough).
    
    2. Checking for process->mm is not enough because process' main
       thread may exit or detach its mm via use_mm(), but other threads
       may still have a valid mm.
    
    This patch implements a small helper function that does things
    correctly, i.e.:
    
    1. We take the task's lock while whe handle its mm (we can't use
       get_task_mm()/mmput() pair as mmput() might sleep);
    
    2. To catch exited main thread case, we use find_lock_task_mm(),
       which walks up all threads and returns an appropriate task
       (with task lock held).
    
    Also, Per Peter Zijlstra's idea, now we don't grab tasklist_lock in
    the new helper, instead we take the rcu read lock. We can do this
    because the function is called after the cpu is taken down and marked
    offline, so no new tasks will get this cpu set in their mm mask.
    
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 73bac847d460f3e793fb38a10dd85c4b65aea03f
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Thu Nov 29 20:39:54 2012 +0100

    ARM: 7587/1: implement optimized percpu variable access
    
    Use the previously unused TPIDRPRW register to store percpu offsets.
    TPIDRPRW is only accessible in PL1, so it can only be used in the kernel.
    
    This replaces 2 loads with a mrc instruction for each percpu variable
    access. With hackbench, the performance improvement is 1.4% on Cortex-A9
    (highbank). Taking an average of 30 runs of "hackbench -l 1000" yields:
    
    Before: 6.2191
    After: 6.1348
    
    Will Deacon reported similar delta on v6 with 11MPCore.
    
    The asm "memory clobber" are needed here to ensure the percpu offset
    gets reloaded. Testing by Will found that this would not happen in
    __schedule() which is a bit of a special case as preemption is disabled
    but the execution can move cores.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit dd530c9a513d90f07bfbff8fa6fa4bc1b873f7e8
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Jul 3 18:46:35 2012 -0700

    ARM: smp: Fix suspicious RCU originating from cpu_die()
    
    While running hotplug tests I ran into this RCU splat
    
    ===============================
    [ INFO: suspicious RCU usage. ]
    3.4.0 #3275 Tainted: G        W
    -------------------------------
    include/linux/rcupdate.h:729 rcu_read_lock() used illegally while idle!
    
    other info that might help us debug this:
    
    RCU used illegally from idle CPU!
    rcu_scheduler_active = 1, debug_locks = 0
    RCU used illegally from extended quiescent state!
    4 locks held by swapper/2/0:
     #0:  ((cpu_died).wait.lock){......}, at: [<c00ab128>] complete+0x1c/0x5c
     #1:  (&p->pi_lock){-.-.-.}, at: [<c00b275c>] try_to_wake_up+0x2c/0x388
     #2:  (&rq->lock){-.-.-.}, at: [<c00b2860>] try_to_wake_up+0x130/0x388
     #3:  (rcu_read_lock){.+.+..}, at: [<c00abe5c>] cpuacct_charge+0x28/0x1f4
    
    stack backtrace:
    [<c001521c>] (unwind_backtrace+0x0/0x12c) from [<c00abec8>] (cpuacct_charge+0x94/0x1f4)
    [<c00abec8>] (cpuacct_charge+0x94/0x1f4) from [<c00b395c>] (update_curr+0x24c/0x2c8)
    [<c00b395c>] (update_curr+0x24c/0x2c8) from [<c00b59c4>] (enqueue_task_fair+0x50/0x194)
    [<c00b59c4>] (enqueue_task_fair+0x50/0x194) from [<c00afea4>] (enqueue_task+0x30/0x34)
    [<c00afea4>] (enqueue_task+0x30/0x34) from [<c00b0908>] (ttwu_activate+0x14/0x38)
    [<c00b0908>] (ttwu_activate+0x14/0x38) from [<c00b28a8>] (try_to_wake_up+0x178/0x388)
    [<c00b28a8>] (try_to_wake_up+0x178/0x388) from [<c00a82a0>] (__wake_up_common+0x34/0x78)
    [<c00a82a0>] (__wake_up_common+0x34/0x78) from [<c00ab154>] (complete+0x48/0x5c)
    [<c00ab154>] (complete+0x48/0x5c) from [<c07db7cc>] (cpu_die+0x2c/0x58)
    [<c07db7cc>] (cpu_die+0x2c/0x58) from [<c000f954>] (cpu_idle+0x64/0xfc)
    [<c000f954>] (cpu_idle+0x64/0xfc) from [<80208160>] (0x80208160)
    
    When a cpu is marked offline during its idle thread it calls
    cpu_die() during an RCU idle period. cpu_die() calls complete()
    to notify the killing process that the cpu has died. complete()
    calls into the scheduler code and eventually grabs an RCU read
    lock in cpuacct_charge().
    
    Mark complete() as RCU_NONIDLE so that RCU pays attention to this
    CPU for the duration of the complete() function even though it's
    in idle.
    
    Change-Id: I548a278e595737390bbc2c97bddda06a0725ecbd
    Suggested-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 422dadd326526c7a09edf0be0d4fda784529cb4f
Author: Stephen Smalley <sds@tycho.nsa.gov>
Date:   Thu Jan 30 11:26:59 2014 -0500

    SELinux: Fix kernel BUG on empty security contexts.
    
    Setting an empty security context (length=0) on a file will
    lead to incorrectly dereferencing the type and other fields
    of the security context structure, yielding a kernel BUG.
    As a zero-length security context is never valid, just reject
    all such security contexts whether coming from userspace
    via setxattr or coming from the filesystem upon a getxattr
    request by SELinux.
    
    Setting a security context value (empty or otherwise) unknown to
    SELinux in the first place is only possible for a root process
    (CAP_MAC_ADMIN), and, if running SELinux in enforcing mode, only
    if the corresponding SELinux mac_admin permission is also granted
    to the domain by policy.  In Fedora policies, this is only allowed for
    specific domains such as livecd for setting down security contexts
    that are not defined in the build host policy.
    
    [On Android, this can only be set by root/CAP_MAC_ADMIN processes,
    and if running SELinux in enforcing mode, only if mac_admin permission
    is granted in policy.  In Android 4.4, this would only be allowed for
    root/CAP_MAC_ADMIN processes that are also in unconfined domains. In current
    AOSP master, mac_admin is not allowed for any domains except the recovery
    console which has a legitimate need for it.  The other potential vector
    is mounting a maliciously crafted filesystem for which SELinux fetches
    xattrs (e.g. an ext4 filesystem on a SDcard).  However, the end result is
    only a local denial-of-service (DOS) due to kernel BUG.  This fix is
    queued for 3.14.]
    
    Reproducer:
    su
    setenforce 0
    touch foo
    setfattr -n security.selinux foo
    
    Caveat:
    Relabeling or removing foo after doing the above may not be possible
    without booting with SELinux disabled.  Any subsequent access to foo
    after doing the above will also trigger the BUG.
    
    BUG output from Matthew Thode:
    [  473.893141] ------------[ cut here ]------------
    [  473.962110] kernel BUG at security/selinux/ss/services.c:654!
    [  473.995314] invalid opcode: 0000 [#6] SMP
    [  474.027196] Modules linked in:
    [  474.058118] CPU: 0 PID: 8138 Comm: ls Tainted: G      D   I
    3.13.0-grsec #1
    [  474.116637] Hardware name: Supermicro X8ST3/X8ST3, BIOS 2.0
    07/29/10
    [  474.149768] task: ffff8805f50cd010 ti: ffff8805f50cd488 task.ti:
    ffff8805f50cd488
    [  474.183707] RIP: 0010:[<ffffffff814681c7>]  [<ffffffff814681c7>]
    context_struct_compute_av+0xce/0x308
    [  474.219954] RSP: 0018:ffff8805c0ac3c38  EFLAGS: 00010246
    [  474.252253] RAX: 0000000000000000 RBX: ffff8805c0ac3d94 RCX:
    0000000000000100
    [  474.287018] RDX: ffff8805e8aac000 RSI: 00000000ffffffff RDI:
    ffff8805e8aaa000
    [  474.321199] RBP: ffff8805c0ac3cb8 R08: 0000000000000010 R09:
    0000000000000006
    [  474.357446] R10: 0000000000000000 R11: ffff8805c567a000 R12:
    0000000000000006
    [  474.419191] R13: ffff8805c2b74e88 R14: 00000000000001da R15:
    0000000000000000
    [  474.453816] FS:  00007f2e75220800(0000) GS:ffff88061fc00000(0000)
    knlGS:0000000000000000
    [  474.489254] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  474.522215] CR2: 00007f2e74716090 CR3: 00000005c085e000 CR4:
    00000000000207f0
    [  474.556058] Stack:
    [  474.584325]  ffff8805c0ac3c98 ffffffff811b549b ffff8805c0ac3c98
    ffff8805f1190a40
    [  474.618913]  ffff8805a6202f08 ffff8805c2b74e88 00068800d0464990
    ffff8805e8aac860
    [  474.653955]  ffff8805c0ac3cb8 000700068113833a ffff880606c75060
    ffff8805c0ac3d94
    [  474.690461] Call Trace:
    [  474.723779]  [<ffffffff811b549b>] ? lookup_fast+0x1cd/0x22a
    [  474.778049]  [<ffffffff81468824>] security_compute_av+0xf4/0x20b
    [  474.811398]  [<ffffffff8196f419>] avc_compute_av+0x2a/0x179
    [  474.843813]  [<ffffffff8145727b>] avc_has_perm+0x45/0xf4
    [  474.875694]  [<ffffffff81457d0e>] inode_has_perm+0x2a/0x31
    [  474.907370]  [<ffffffff81457e76>] selinux_inode_getattr+0x3c/0x3e
    [  474.938726]  [<ffffffff81455cf6>] security_inode_getattr+0x1b/0x22
    [  474.970036]  [<ffffffff811b057d>] vfs_getattr+0x19/0x2d
    [  475.000618]  [<ffffffff811b05e5>] vfs_fstatat+0x54/0x91
    [  475.030402]  [<ffffffff811b063b>] vfs_lstat+0x19/0x1b
    [  475.061097]  [<ffffffff811b077e>] SyS_newlstat+0x15/0x30
    [  475.094595]  [<ffffffff8113c5c1>] ? __audit_syscall_entry+0xa1/0xc3
    [  475.148405]  [<ffffffff8197791e>] system_call_fastpath+0x16/0x1b
    [  475.179201] Code: 00 48 85 c0 48 89 45 b8 75 02 0f 0b 48 8b 45 a0 48
    8b 3d 45 d0 b6 00 8b 40 08 89 c6 ff ce e8 d1 b0 06 00 48 85 c0 49 89 c7
    75 02 <0f> 0b 48 8b 45 b8 4c 8b 28 eb 1e 49 8d 7d 08 be 80 01 00 00 e8
    [  475.255884] RIP  [<ffffffff814681c7>]
    context_struct_compute_av+0xce/0x308
    [  475.296120]  RSP <ffff8805c0ac3c38>
    [  475.328734] ---[ end trace f076482e9d754adc ]---
    
    [sds:  commit message edited to note Android implications and
    to generate a unique Change-Id for gerrit]
    
    Change-Id: I4d5389f0cfa72b5f59dada45081fa47e03805413
    Reported-by:  Matthew Thode <mthode@mthode.org>
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paul Moore <pmoore@redhat.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 84fb1b1afc42135c1ce1e3c91162509b5102ae21
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 16 10:42:38 2012 +0000

    rcu: Use smp_hotplug_thread facility for RCUs per-CPU kthread
    
    Bring RCU into the new-age CPU-hotplug fold by modifying RCU's per-CPU
    kthread code to use the new smp_hotplug_thread facility.
    
    [ tglx: Adapted it to use callbacks and to the simplified rcu yield ]
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.673354828@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit abf4f50b915c3daceef4602413a990dcae61d344
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:35 2012 +0000

    rcu: Yield simpler
    
    The rcu_yield() code is amazing. It's there to avoid starvation of the
    system when lots of (boosting) work is to be done.
    
    Now looking at the code it's functionality is:
    
     Make the thread SCHED_OTHER and very nice, i.e. get it out of the way
     Arm a timer with 2 ticks
     schedule()
    
    Now if the system goes idle the rcu task returns, regains SCHED_FIFO
    and plugs on. If the systems stays busy the timer fires and wakes a
    per node kthread which in turn makes the per cpu thread SCHED_FIFO and
    brings it back on the cpu. For the boosting thread the "make it FIFO"
    bit is missing and it just runs some magic boost checks. Now this is a
    lot of code with extra threads and complexity.
    
    It's way simpler to let the tasks when they detect overload schedule
    away for 2 ticks and defer the normal wakeup as long as they are in
    yielded state and the cpu is not idle.
    
    That solves the same problem and the only difference is that when the
    cpu goes idle it's not guaranteed that the thread returns right away,
    but it won't be longer out than two ticks, so no harm is done. If
    that's an issue than it is way simpler just to wake the task from
    idle as RCU has callbacks there anyway.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120716103948.131256723@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit d0422b29702421197ef64fa9caad597bcf8fb7b2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:37 2012 +0000

    softirq: Use hotplug thread infrastructure
    
    [ paulmck: Call rcu_note_context_switch() with interrupts enabled. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.456416747@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 8736b160a2391d627686659b8583dc8be958f607
Author: Andrew Dodd <atd7@cornell.edu>
Date:   Sat Mar 30 16:00:53 2013 -0400

    kgsl: Implement conservative scaling policy
    
    This behaves similarly to the conservative cpufreq governor,
    which is similar to how most Mali GPU scaling algorithms behave.
    
    The "trustzone" policy that is default relies on an undocumented
    black box to make scaling decisions.
    
    How to enable:
    echo conservative > /sys/devices/platform/kgsl-3d0.0/kgsl/kgsl-3d0/pwrscale/policy
    
    Conservative attr group:
    /sys/devices/platform/kgsl-3d0.0/kgsl/kgsl-3d0/pwrscale/policy_config/conservative
    
    Polling interval (10-1000):
    /sys/devices/platform/kgsl-3d0.0/kgsl/kgsl-3d0/pwrscale/policy_config/conservative/polling_interval
    
    Enable stats printing (causes microlags on low polling intervals):
    echo 1 > /sys/devices/platform/kgsl-3d0.0/kgsl/kgsl-3d0/pwrscale/policy_config/conservative/print_stats
    
    NOTE:
    - perms are set to 666 for testing purposes
    
    Change-Id: I9d854a73981515fc9850104302804928b04c4687
    Signed-off-by: Andrew Dodd <atd7@cornell.edu>

commit 4b4bc0b5fa760680fd39b3fcb9c2cd2903239f24
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 19 14:06:56 2014 +0100

    Revert "sec-battery: Standardize the output of the "online" property"
    
    This reverts commit ea1c2f92a5d6929a6a54b53222251e24ef1ce4cb.

commit 6390a6a6c9de62e7a651a9f2afc3adb01679192f
Author: dorimanx <yuri@bynet.co.il>
Date:   Sun Feb 16 23:28:10 2014 +0200

    Update BFQ I/O from BFQ-v7r1 to BFQ-v7r2 for 3.4.x

commit c1035dded93b3571a855043278b619e9610a6c17
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 18 09:41:33 2014 +0100

    Optimzed sampling time inside alucard_hotplug!

commit 0317a7d597ba7a9b8cc1efb8c94709bd6a3cc542
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 17 18:12:53 2014 +0100

    Removed info!

commit b47630a7381dde6c572095c1b9d601746626073c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 16 23:22:51 2014 +0100

    New kernel test version!

commit 162a1200046781dec4de34ea67d60019b65e93c9
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 16 21:29:56 2014 +0100

    Disabled DYNAMIC FSYNC by default!

commit b899ac4b2641a25f38a65e18957ff9c6dd7ce312
Author: Paul Reioux <reioux@gmail.com>
Date:   Mon Nov 25 00:48:14 2013 -0600

    Asynchronous Fsync: initial extraction of Async Fsync from HTC
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>
    
    Conflicts:
    	fs/Kconfig

commit 91acb49dbe94b517ace8f268f9b36ecbbaa17c8e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 16 14:46:08 2014 +0100

    Optimized alucard governor sampling time!

commit 7ab6d720a5f0f2fd7c16f9cb5cd5e8cbcf735567
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 16 12:53:57 2014 +0100

    New test kernel!

commit c79466c9b304bd0fbc2b39bb6522b5c9c5a6eb27
Author: Paul Reioux <reioux@gmail.com>
Date:   Sat Jan 4 10:54:24 2014 -0800

    kernel/futex.c: Linux 3.4 compatibility fix up
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 0f3870428fa46d5a4271180d2bc04cbc7cfd2369
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sat Jan 4 10:48:26 2014 -0800

    futex: Avoid taking hb lock if nothing to wakeup
    
    Date	Thu, 2 Jan 2014 07:05:20 -0800
    
    From: Davidlohr Bueso <davidlohr@hp.com>
    
    In futex_wake() there is clearly no point in taking the hb->lock if we know
    beforehand that there are no tasks to be woken. While the hash bucket's plist
    head is a cheap way of knowing this, we cannot rely 100% on it as there is a
    racy window between the futex_wait call and when the task is actually added to
    the plist. To this end, we couple it with the spinlock check as tasks trying to
    enter the critical region are most likely potential waiters that will be added
    to the plist, thus preventing tasks sleeping forever if wakers don't acknowledge
    all possible waiters.
    
    Furthermore, the futex ordering guarantees are preserved, ensuring that waiters
    either observe the changed user space value before blocking or is woken by a
    concurrent waker. For wakers, this is done by relying on the barriers in
    get_futex_key_refs() -- for archs that do have implicit mb in atomic_inc() we
    explicitly add them through a new futex_get_mm function. For waiters we rely
    on the fact that spin_lock calls already update the head counter, so spinners
    are visible even if the lock hasn't been acquired yet.
    
    For more details please refer to the updated comments in the code and related
    discussion: https://lkml.org/lkml/2013/11/26/556
    
    Special thanks to tglx for careful review and feedback.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Darren Hart <dvhart@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: Tom Vaden <tom.vaden@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: Jason Low <jason.low2@hp.com>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 9c3328e69753cb928e8669b48fd6f144efdf5baf
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sat Jan 4 10:47:26 2014 -0800

    futex: Document ordering guarantees
    
    Date	Thu, 2 Jan 2014 07:05:19 -0800
    
    From: Thomas Gleixner <tglx@linutronix.de>
    
    That's essential, if you want to hack on futexes.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Darren Hart <dvhart@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: Tom Vaden <tom.vaden@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: Jason Low <jason.low2@hp.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 5fb04a780763958be02f3fa66fa8e64a5508e769
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sat Jan 4 10:42:17 2014 -0800

    futex: Larger hash table
    
    Date	Thu, 2 Jan 2014 07:05:18 -0800
    
    From: Davidlohr Bueso <davidlohr@hp.com>
    
    Currently, the futex global hash table suffers from it's fixed, smallish
    (for today's standards) size of 256 entries, as well as its lack of NUMA
    awareness. Large systems, using many futexes, can be prone to high amounts
    of collisions; where these futexes hash to the same bucket and lead to
    extra contention on the same hb->lock. Furthermore, cacheline bouncing is a
    reality when we have multiple hb->locks residing on the same cacheline and
    different futexes hash to adjacent buckets.
    This patch keeps the current static size of 16 entries for small systems,
    or otherwise, 256 * ncpus (or larger as we need to round the number to a
    power of 2). Note that this number of CPUs accounts for all CPUs that can
    ever be available in the system, taking into consideration things like
    hotpluging. While we do impose extra overhead at bootup by making the hash
    table larger, this is a one time thing, and does not shadow the benefits
    of this patch.
    
    Furthermore, as suggested by tglx, by cache aligning the hash buckets we can
    avoid access across cacheline boundaries and also avoid massive cache line
    bouncing if multiple cpus are hammering away at different hash buckets which
    happen to reside in the same cache line.
    
    Also, similar to other core kernel components (pid, dcache, tcp), by using
    alloc_large_system_hash() we benefit from its NUMA awareness and thus the
    table is distributed among the nodes instead of in a single one.
    
    For a custom microbenchmark that pounds on the uaddr hashing -- making the wait
    path fail at futex_wait_setup() returning -EWOULDBLOCK for large amounts of
    futexes, we can see the following benefits on a 80-core, 8-socket 1Tb server:
    
    +---------+--------------------+------------------------+-----------------------+-------------------------------+
    | threads | baseline (ops/sec) | aligned-only (ops/sec) | large table (ops/sec) | large table+aligned (ops/sec) |
    +---------+--------------------+------------------------+-----------------------+-------------------------------+
    |     512 |		 32426 | 50531  (+55.8%)	| 255274  (+687.2%)	| 292553  (+802.2%)		|
    |     256 |		 65360 | 99588  (+52.3%)	| 443563  (+578.6%)	| 508088  (+677.3%)		|
    |     128 |		125635 | 200075 (+59.2%)	| 742613  (+491.1%)	| 835452  (+564.9%)		|
    |      80 |		193559 | 323425 (+67.1%)	| 1028147 (+431.1%)	| 1130304 (+483.9%)		|
    |      64 |		247667 | 443740 (+79.1%)	| 997300  (+302.6%)	| 1145494 (+362.5%)		|
    |      32 |		628412 | 721401 (+14.7%)	| 965996  (+53.7%)	| 1122115 (+78.5%)		|
    +---------+--------------------+------------------------+-----------------------+-------------------------------+
    Cc: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Darren Hart <dvhart@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: Tom Vaden <tom.vaden@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Reviewed-by: Waiman Long <Waiman.Long@hp.com>
    Reviewed-and-tested-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit ff6fd980130b8a99ad371c7828d25bd7d50259b9
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sat Jan 4 10:40:57 2014 -0800

    futex: Misc cleanups
    
    Date	Thu, 2 Jan 2014 07:05:17 -0800
    
    From: Jason Low <jason.low2@hp.com>
    
    - Remove unnecessary head variables.
    - Delete unused parameter in queue_unlock().
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Darren Hart <dvhart@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: Tom Vaden <tom.vaden@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 80c3c07c920272c77556ba75a163a327b7ca5f2f
Author: Luis Cruz <ljc2491@gmail.com>
Date:   Wed Dec 18 08:49:10 2013 -0600

    regulator: core: Use the power efficient workqueue for delayed powerdown
    
    Adapted for 3.4 from lsk-v3.10
    
    There is no need to use a normal per-CPU workqueue for delayed power downs
    as they're not timing or performance critical and waking up a core for them
    would defeat some of the point.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Liam Girdwood <liam.r.girdwood@intel.com>
    (cherry picked from commit 070260f07c7daec311f2466eb9d1df475d5a46f8)
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit cd13d82104c002ac71894dda4cd98ac900d9da01
Author: Luis Cruz <ljc2491@gmail.com>
Date:   Wed Dec 18 08:51:56 2013 -0600

    ASoC: jack: Use power efficient workqueue
    
    Adapted for 3.4 from lsk-v3.10
    
    The accessory detect debounce work is not performance sensitive so let
    the scheduler run it wherever is most efficient rather than in a per CPU
    workqueue by using the system power efficient workqueue.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    (cherry picked from commit e6058aaadcd473e5827720dc143af56aabbeecc7)
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 059d28203f2fde155a166b5fa5740a0c4b556245
Author: Mark Brown <broonie@linaro.org>
Date:   Thu Jul 18 11:52:17 2013 +0100

    ASoC: pcm: Use the power efficient workqueue for delayed powerdown
    
    Adapted for 3.4 from lsk-v3.10
    
    There is no need to use a normal per-CPU workqueue for delayed power downs
    as they're not timing or performance critical and waking up a core for them
    would defeat some of the point.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    (cherry picked from commit d4e1a73acd4e894f8332f2093bceaef585cfab67)
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 150bb574eb2289286c0d243cd8fc9f6d087d2fd1
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:55 2013 +0530

    PHYLIB: queue work on system_power_efficient_wq
    
    Adapted for 3.4 from lsk-v3.10
    
    Phylib uses workqueues for multiple purposes. There is no real dependency of
    scheduling these on the cpu which scheduled them.
    
    On a idle system, it is observed that and idle cpu wakes up many times just to
    service this work. It would be better if we can schedule it on a cpu which the
    scheduler believes to be the most appropriate one.
    
    This patch replaces system_wq with system_power_efficient_wq for PHYLIB.
    
    Cc: David S. Miller <davem@davemloft.net>
    Cc: netdev@vger.kernel.org
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit bbb47bdeae756f04b896b55b51f230f3eb21f207)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit 091f70a3821ec0a344abc5d4775b13eb9867e93f
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:56 2013 +0530

    block: queue work on power efficient wq
    
    Adapted for 3.4 from lsk-v3.10
    
    Block layer uses workqueues for multiple purposes. There is no real dependency
    of scheduling these on the cpu which scheduled them.
    
    On a idle system, it is observed that and idle cpu wakes up many times just to
    service this work. It would be better if we can schedule it on a cpu which the
    scheduler believes to be the most appropriate one.
    
    This patch replaces normal workqueues with power efficient versions.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit 695588f9454bdbc7c1a2fbb8a6bfdcfba6183348)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit c7c3345835c262a2e185054e7cb4f12aff456b79
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:54 2013 +0530

    workqueue: Add system wide power_efficient workqueues
    
    Adapted for 3.4 from lsk-v3.10
    
    This patch adds system wide workqueues aligned towards power saving. This is
    done by allocating them with WQ_UNBOUND flag if 'wq_power_efficient' is set to
    'true'.
    
    tj: updated comments a bit.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit 0668106ca3865ba945e155097fb042bf66d364d3)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>

commit ae3ca1d1a694d5681184ce216c4625ced208e683
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Apr 8 16:45:40 2013 +0530

    workqueues: Introduce new flag WQ_POWER_EFFICIENT for power oriented workqueues
    
    Adapted for 3.4 from lsk-v3.10
    
    Workqueues can be performance or power-oriented. Currently, most workqueues are
    bound to the CPU they were created on. This gives good performance (due to cache
    effects) at the cost of potentially waking up otherwise idle cores (Idle from
    scheduler's perspective. Which may or may not be physically idle) just to
    process some work. To save power, we can allow the work to be rescheduled on a
    core that is already awake.
    
    Workqueues created with the WQ_UNBOUND flag will allow some power savings.
    However, we don't change the default behaviour of the system.  To enable
    power-saving behaviour, a new config option CONFIG_WQ_POWER_EFFICIENT needs to
    be turned on. This option can also be overridden by the
    workqueue.power_efficient boot parameter.
    
    tj: Updated config description and comments.  Renamed
        CONFIG_WQ_POWER_EFFICIENT to CONFIG_WQ_POWER_EFFICIENT_DEFAULT.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Amit Kucheria <amit.kucheria@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    (cherry picked from commit cee22a15052faa817e3ec8985a28154d3fabc7aa)
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Luis Cruz <ljc2491@gmail.com>
    Signed-off-by: poondog <markj338@gmail.com>

commit b0fa4aead681fb77476f0bd6a19dc658e6127d95
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 14 20:44:16 2014 +0100

    Linux 3.4.80

commit fa19f23a4e06ce6645b46efc0556c684d5c2a857
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Feb 14 20:45:24 2014 +0100

    Revert "jf kernel: brighter, fading LED"
    
    This reverts commit a331d2469db42e1e3d2b5a776e392494c7dee817.

commit ace7b330d6ad5aabb6b053eb1172d0227bb35c92
Author: dorimanx <yuri@bynet.co.il>
Date:   Sun Feb 9 23:03:38 2014 +0200

    block: bfq: update from v6r2 to v7r1 for 3.4.y branch

commit 6ac07e5be40b15184b60e311b64ed61e90aa0f56
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 12 17:32:47 2014 +0100

    New test kernel released!

commit cb92d51d2466b61d8814e4f4cb581320971cf7ec
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Feb 12 17:06:27 2014 +0100

    Merge "jf kernel: bcmdhd: dont filter ip6 in suspend" into cm-11.0

commit abeb1c8df3d8e49d0ed1ccd1b6349c251c8fd553
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 11 23:26:35 2014 +0100

    cpufreq: Optimize cpufreq_frequency_table_verify()
    cpufreq_frequency_table_verify() is rewritten here to make it more
    logical# Please enter the commit message for your changes. Lines
    starting and efficient.# with '#' will be ignored, and an empty message
    aborts the commit.
     - merge multiple lines for variable declarations together.# On branch
    my-cm-11.0-3.X
     - quit early if any frequency between min/max is found.# Your branch is
    ahead of 'origin/my-cm-11.0-3.X' by 2 commits.
     - don't call cpufreq_verify_within_limits() in case any valid freq is#
    (use "git push" to publish your local commits)
       found as it is of no use.#
     - rename the count variable as found and change its type to boolean.#
    Changes to be committed:
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org># Signed-off-by:
    Rafael J. Wysocki <rafael.j.wysocki@intel.com># modified:
    drivers/cpufreq/freq_table.c
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>#

commit 6c7956e46933170b46e1b4b812059eeda394e141
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 11 23:18:15 2014 +0100

    cpufreq: Fix policy getting stuck when user & kernel min/max don't ov…
    ¦erlap# Please enter the commit message for your changes. Lines
    starting
    Every __cpufreq_set_policy starts with checking the new policy min/max
    has# (use "git push" to publish your local commits) some overlap with
    the current policy min/max. This works out fine until we# end up with
    the policy min/max being set to a range that doesn't overlap# Changes to
    be committed: with the user policy min/max. Once we get into this
    situation, the check at# (use "git reset HEAD <file>..." to unstage) the
    start of __cpufreq_set_policy fails and prevents us from getting out of#
    this state.# modified: drivers/cpufreq/cpufreq.c
    This only happens when one of the CPUFREQ_ADJUST/CPUFREQ_INCOMPATIBLE
    notifiers called inside __cpufreq_set_policy pick a min/max outside the
    range of user policy min/max. The real intent of the check at the start
    of __cpufreq_set_policy is to make sure userspace can't set user policy
    min > user policy max. Since __cpufreq_set_policy always gets called
    only with current user policy min/max except when the actual user space
    policy min/max is changed, we can fix the issue by simply checking the
    new policy min/max against current user policy min/max. Change-Id:
    Iaac805825e64d7985c41fb9052bd96baacdf3d6f Signed-off-by: Saravana Kannan
    <skannan@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 8a0047fd40b238cea41996e5bd9c91c076d2a9db
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 11 23:15:42 2014 +0100

    CPU hotplug, debug: detect imbalance between get_online_cpus() and pu…
    ¦t_online_cpus()# Please enter the commit message for your changes.
    Lines starting
    The synchronization between CPU hotplug readers and writers is achieved#
    (use "git reset HEAD <file>..." to unstage) by means of refcounting,
    safeguarded by the cpu_hotplug.lock.#
    get_online_cpus() increments the refcount, whereas put_online_cpus()#
    decrements it.  If we ever hit an imbalance between the two, we end up
    compromising the guarantees of the hotplug synchronization i.e, for
    example, an extra call to put_online_cpus() can end up allowing a
    hotplug reader to execute concurrently with a hotplug writer. So, add a
    WARN_ON() in put_online_cpus() to detect such cases where the refcount
    can go negative, and also attempt to fix it up, so that we can continue
    to run. Change-Id: I144efeaa5899a2e8a3cddd21f010679cbaaa2459
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com> Cc: Jiri
    Kosina <jkosina@suse.cz> Cc: Thomas Gleixner <tglx@linutronix.de> Cc:
    Ingo Molnar <mingo@kernel.org> Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org> Signed-off-by:
    Linus Torvalds <torvalds@linux-foundation.org> Git-commit: 075663d
    Git-repo:
    git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Osvaldo Banuelos <osvaldob@codeaurora.org>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit d8a20e0eea9ec4e0a804d53bd65a8f14dfc4045e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 11 23:08:28 2014 +0100

    net: loopback: set default mtu to 64K loopback current mtu of 16436
    bytes allows no more than 3 MSS TCP segments per frame, or 48 Kbytes.
    Changing mtu to 64K allows TCP stack to build large frames and
    significantly reduces stack overhead.# Please enter the commit message
    for your changes. Lines starting
    Performance boost on bulk TCP transferts can be up to 30 %, partly# (use
    "git push" to publish your local commits) because we now have one ACK
    message for two 64KB segments, and a lower# probability of hitting
    /proc/sys/net/ipv4/tcp_reordering default limit.# Changes to be
    committed:
    Signed-off-by: Eric Dumazet <edumazet@google.com># Signed-off-by: David
    S. Miller <davem@davemloft.net># modified: drivers/net/loopback.c
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>#

commit 374306bd93e06402af6ce2be5d6bacd5cdd351ba
Author: Paul Walmsley <pwalmsley@nvidia.com>
Date:   Wed Mar 6 19:02:56 2013 -0800

    sched: reinitialize rq->next_balance when a CPU is hot-added
    
    Reinitialize rq->next_balance when a CPU is hot-added.  Otherwise,
    scheduler domain rebalancing may be skipped if rq->next_balance was
    set to a future time when the CPU was last active, and the
    newly-re-added CPU is in idle_balance().  As a result, the
    newly-re-added CPU will remain idle with no tasks scheduled until the
    softlockup watchdog runs - potentially 4 seconds later.  This can
    waste energy and reduce performance.
    
    This behavior can be observed in some SoC kernels, which use CPU
    hotplug to dynamically remove and add CPUs in response to load.  In
    one case that triggered this behavior,
    
    0. the system started with all cores enabled, running multi-threaded
       CPU-bound code;
    
    1. the system entered some single-threaded code;
    
    2. a CPU went idle and was hot-removed;
    
    3. the system started executing a multi-threaded CPU-bound task;
    
    4. the CPU from event 2 was re-added, to respond to the load.
    
    The time interval between events 2 and 4 was approximately 300
    milliseconds.
    
    Of course, ideally CPU hotplug would not be used in this manner,
    but this patch does appear to fix a real bug.
    
    Nvidia folks: this patch is submitted as at least a partial fix for
    bug 1243368 ("[sched] Load-balancing not happening correctly after
    cores brought online")
    
    Change-Id: Iabac21e110402bb581b7db40c42babc951d378d0
    Signed-off-by: Paul Walmsley <pwalmsley@nvidia.com>
    Cc: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-on: http://git-master/r/208927
    Reviewed-by: Automatic_Commit_Validation_User
    Reviewed-by: Peter Zu <pzu@nvidia.com>
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    GVS: Gerrit_Virtual_Submit
    Tested-by: Peter Zu <pzu@nvidia.com>
    Reviewed-by: Yu-Huan Hsu <yhsu@nvidia.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 0094ce86792f131adf90330e9a30e5c6087136af
Author: Neil Zhang <zhangwm@marvell.com>
Date:   Fri Dec 28 10:00:26 2012 +0000

    sched: remove redundant update_runtime notifier
    
    migration_call() will do all the things that update_runtime() does.
    So it seems update_runtime() is a redundant notifier, remove it.
    
    Furthermore, there is potential risk that the current code will catch
    BUG_ON at line 687 of rt.c when do cpu hotplug while there are realtime
    threads running because of enable runtime twice.
    
    Change-Id: I0fdad8d5a1cebb845d3f308b205dbd6517c3e4de
    Cc: bitbucket@online.de
    Signed-off-by: Neil Zhang <zhangwm@marvell.com>
    Reviewed-on: http://git-master/r/215596
    (cherry picked from commit 8f646de983f24361814d9a6ca679845fb2265807)
    Reviewed-on: http://git-master/r/223067
    Reviewed-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Tested-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Reviewed-by: Paul Walmsley <pwalmsley@nvidia.com>
    Reviewed-by: Automatic_Commit_Validation_User
    GVS: Gerrit_Virtual_Submit
    Reviewed-by: Diwakar Tundlam <dtundlam@nvidia.com>

commit 82f47f18d8be95ae73d7a5c15fa2654f5ee14380
Author: Jan Kara <jack@suse.cz>
Date:   Fri Jun 28 21:32:27 2013 +0200

    block: Reserve only one queue tag for sync IO if only 3 tags are available
    
    In case a device has three tags available we still reserve two of them
    for sync IO. That leaves only a single tag for async IO such as
    writeback from flusher thread which results in poor performance.
    
    Allow async IO to consume two tags in case queue has three tag availabe
    to get a decent async write performance.
    
    This patch improves streaming write performance on a machine with such disk
    from ~21 MB/s to ~52 MB/s. Also postmark throughput in presence of
    streaming writer improves from 8 to 12 transactions per second so sync
    IO doesn't seem to be harmed in presence of heavy async writer.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit c80129e1fce3968f26c8090262d3c09013bb6aed
Author: Fengguang Wu <fengguang.wu@intel.com>
Date:   Wed Sep 11 14:21:47 2013 -0700

    readahead: make context readahead more conservative
    
    This helps performance on moderately dense random reads on SSD.
    
    Transaction-Per-Second numbers provided by Taobao:
    
    		QPS	case
    		-------------------------------------------------------
    		7536	disable context readahead totally
    w/ patch:	7129	slower size rampup and start RA on the 3rd read
    		6717	slower size rampup
    w/o patch:	5581	unmodified context readahead
    
    Before, readahead will be started whenever reading page N+1 when it happen
    to read N recently.  After patch, we'll only start readahead when *three*
    random reads happen to access pages N, N+1, N+2.  The probability of this
    happening is extremely low for pure random reads, unless they are very
    dense, which actually deserves some readahead.
    
    Also start with a smaller readahead window.  The impact to interleaved
    sequential reads should be small, because for a long run stream, the the
    small readahead window rampup phase is negletable.
    
    The context readahead actually benefits clustered random reads on HDD
    whose seek cost is pretty high.  However as SSD is increasingly used for
    random read workloads it's better for the context readahead to concentrate
    on interleaved sequential reads.
    
    Another SSD rand read test from Miao
    
            # file size:        2GB
            # read IO amount: 625MB
            sysbench --test=fileio          \
                    --max-requests=10000    \
                    --num-threads=1         \
                    --file-num=1            \
                    --file-block-size=64K   \
                    --file-test-mode=rndrd  \
                    --file-fsync-freq=0     \
                    --file-fsync-end=off    run
    
    shows the performance of btrfs grows up from 69MB/s to 121MB/s, ext4 from
    104MB/s to 121MB/s.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Tested-by: Tao Ma <tm@tao.ma>
    Tested-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit 143d08689151f215385530b4145acf26af810797
Author: JP Abgrall <jpa@google.com>
Date:   Fri Dec 20 16:51:11 2013 -0800

    nf: xt_qtaguid: fix handling for cases where tunnels are used.
    
    * fix skb->dev vs par->in/out
    When there is some forwarding going on, it introduces extra state
    around devs associated with xt_action_param->in/out and sk_buff->dev.
    E.g.
       par->in and par->out are both set, or
       skb->dev and par->out are both set (and different)
    This would lead qtaguid to make the wrong assumption about the
    direction and update the wrong device stats.
    Now we rely more on par->in/out.
    
    * Fix handling when qtaguid is used as "owner"
    When qtaguid is used as an owner module, and sk_socket->file is
    not there (happens when tunnels are involved), it would
    incorrectly do a tag stats update.
    
    * Correct debug messages.
    
    Bug: 11687690
    Change-Id: I2b1ff8bd7131969ce9e25f8291d83a6280b3ba7f
    Signed-off-by: JP Abgrall <jpa@google.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>

commit a331d2469db42e1e3d2b5a776e392494c7dee817
Author: dcd <dcd1182@gmail.com>
Date:   Sat Jan 18 07:35:25 2014 -0600

    jf kernel: brighter, fading LED
    
    Change-Id: Idac135503a37daeb09823c00c1dffb7c4caaea74

commit 7307c39dafb480ef5f0cc9c549b99230bb46f1df
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Feb 6 19:14:02 2014 +0100

    Removed cpu PERF EVENTS!
    
    Conflicts:
    	arch/arm/configs/alucard_defconfig

commit 07c00550985c244992830d4540eb6068c7774467
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 11 22:15:12 2014 +0100

    New test kernel released!

commit df625b856d85f38b804579eacb032206cd2c41e8
Author: Ming Lei <ming.lei@canonical.com>
Date:   Wed Feb 27 17:05:19 2013 -0800

    block/partitions: optimize memory allocation in check_partition()
    
    Currently, sizeof(struct parsed_partitions) may be 64KB in 32bit arch, so
    it is easy to trigger page allocation failure by check_partition,
    especially in hotplug block device situation(such as, USB mass storage,
    MMC card, ...), and Felipe Balbi has observed the failure.
    
    This patch does below optimizations on the allocation of struct
    parsed_partitions to try to address the issue:
    
    - make parsed_partitions.parts as pointer so that the pointed memory can
      fit in 32KB buffer, then approximate 32KB memory can be saved
    
    - vmalloc the buffer pointed by parsed_partitions.parts because 32KB is
      still a bit big for kmalloc
    
    - given that many devices have the partition count limit, so only
      allocate disk_max_parts() partitions instead of 256 partitions always
    
    Change-Id: Iee8afc643ea8e7dd2d7c038c542a0377f71bd34e
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Reported-by: Felipe Balbi <balbi@ti.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 44fb70f3461b97a4005460b591733514417fae04
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Wed Jan 2 20:00:40 2013 -0600

    writeback: fix writeback cache thrashing
    
    From: Namjae Jeon <namjae.jeon@samsung.com>
    
    Consider Process A: huge I/O on sda
            doing heavy write operation - dirty memory becomes more
            than dirty_background_ratio
            on HDD - flusher thread flush-8:0
    
    Consider Process B: small I/O on sdb
            doing while [1]; read 1024K + rewrite 1024K + sleep 2sec
            on Flash device - flusher thread flush-8:16
    
    As Process A is a heavy dirtier, dirty memory becomes more
    than dirty_background_thresh. Due to this, below check becomes
    true(checking global_page_state in over_bground_thresh)
    for all bdi devices(even for very small dirtied bdi - sdb):
    
    In this case, even small cached data on 'sdb' is forced to flush
    and writeback cache thrashing happens.
    
    When we added debug prints inside above 'if' condition and ran
    above Process A(heavy dirtier on bdi with flush-8:0) and
    Process B(1024K frequent read/rewrite on bdi with flush-8:16)
    we got below prints:
    
    [Test setup: ARM dual core CPU, 512 MB RAM]
    
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56064 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56704 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 84720 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 94720 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   384 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   960 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =    64 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92160 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   256 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   768 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =    64 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   256 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   320 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =     0 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92032 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 91968 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   192 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =  1024 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =    64 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   192 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   576 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =     0 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 84352 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   192 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =   512 KB
    [over_bground_thresh]: wakeup flush-8:16 : BDI_RECLAIMABLE =     0 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92608 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE = 92544 KB
    
    As mentioned in above log, when global dirty memory > global background_thresh
    small cached data is also forced to flush by flush-8:16.
    If removing global background_thresh checking code, we can reduce cache
    thrashing of frequently used small data.
    And It will be great if we can reserve a portion of writeback cache using
    min_ratio.
    
    After applying patch:
    $ echo 5 > /sys/block/sdb/bdi/min_ratio
    $ cat /sys/block/sdb/bdi/min_ratio
    5
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56064 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  56704 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  84160 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  96960 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  94080 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93120 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93120 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  91520 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  89600 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93696 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  93696 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  72960 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  90624 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  90624 KB
    [over_bground_thresh]: wakeup flush-8:0 : BDI_RECLAIMABLE =  90688 KB
    
    As mentioned in the above logs, once cache is reserved for Process B,
    and patch is applied there is less writeback cache thrashing on sdb
    by frequent forced writeback by flush-8:16 in over_bground_thresh.
    
    After all, small cached data will be flushed by periodic writeback
    once every dirty_writeback_interval.
    
    Change-Id: I678b30a5a28ed99bed0e782e38926ba4626b86ee
    Suggested-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Vivek Trivedi <t.vivek@samsung.com>

commit d264d0576ba43692cdd89334a3c09246050a1b8f
Author: Jan Kara <jack@suse.cz>
Date:   Fri Jul 12 17:30:07 2013 +0200

    writeback: Fix occasional slow sync(1)
    
    In case when system contains no dirty pages, wakeup_flusher_threads()
    will submit WB_SYNC_NONE writeback for 0 pages so wb_writeback() exits
    immediately without doing anything. Thus sync(1) will write all the
    dirty inodes from a WB_SYNC_ALL writeback pass which is slow.
    
    Fix the problem by using get_nr_dirty_pages() in
    wakeup_flusher_threads() instead of calculating number of dirty pages
    manually. That function also takes number of dirty inodes into account.
    
    Change-Id: I458027ae08d9a5a93202a7b97ace1f8da7a18a07
    CC: stable@vger.kernel.org
    Reported-by: Paul Taysom <taysom@chromium.org>
    Signed-off-by: Jan Kara <jack@suse.cz>

commit 615c666ef51f1dbdd82591c6d3184e5a50c763f3
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Wed Jul 17 06:53:08 2013 +0000

    net: flow: Prevent bringing up new CPUs during per-CPU initialization
    
    In a rare race-condition, it's possible that a CPU will be brought
    online between when the for_each_online_cpu() loop executes to
    call flow_cache_cpu_prepare(), and when the hotplug notifier for
    calling this same function is registered. If this happens,
    flow_cache_cpu_prepare() will never be called for the new CPU,
    resulting in crashes due to uninitialized per-cpu data.
    
    Fix this by preventing CPUs from being added or removed during
    this small but sensitive window.
    
    Change-Id: Iafbbaa8a50e5c527392d130561874313720849d0
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit f14dd8b8c3f48894ae9eaf1e6479e360b931f123
Author: Felix Fietkau <nbd@openwrt.org>
Date:   Fri May 4 21:08:33 2012 -0700

    timer: optimize apply_slack()
    
    __fls(mask) is equivalent to find_last_bit(&mask, BITS_PER_LONG), but cheaper.
    find_last_bit was showing up high on the list when I was profiling for stalls
    on icache misses on a system with very small cache size (MIPS).
    
    Signed-off-by: Felix Fietkau <nbd@openwrt.org>
    Signed-off-by: edoko <r_data@naver.com>
    
    Change-Id: I8a5021a2fb2936c00ffd456663a76cb1b23e3100

commit 3e374ff717cc4f3a81be1e53d34e10e10deb97fe
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Jul 2 22:38:35 2013 +1000

    sync: don't block the flusher thread waiting on IO
    
    When sync does it's WB_SYNC_ALL writeback, it issues data Io and
    then immediately waits for IO completion. This is done in the
    context of the flusher thread, and hence completely ties up the
    flusher thread for the backing device until all the dirty inodes
    have been synced. On filesystems that are dirtying inodes constantly
    and quickly, this means the flusher thread can be tied up for
    minutes per sync call and hence badly affect system level write IO
    performance as the page cache cannot be cleaned quickly.
    
    We already have a wait loop for IO completion for sync(2), so cut
    this out of the flusher thread and delegate it to wait_sb_inodes().
    Hence we can do rapid IO submission, and then wait for it all to
    complete.
    
    Effect of sync on fsmark before the patch:
    
    FSUse%        Count         Size    Files/sec     App Overhead
    .....
         0       640000         4096      35154.6          1026984
         0       720000         4096      36740.3          1023844
         0       800000         4096      36184.6           916599
         0       880000         4096       1282.7          1054367
         0       960000         4096       3951.3           918773
         0      1040000         4096      40646.2           996448
         0      1120000         4096      43610.1           895647
         0      1200000         4096      40333.1           921048
    
    And a single sync pass took:
    
      real    0m52.407s
      user    0m0.000s
      sys     0m0.090s
    
    After the patch, there is no impact on fsmark results, and each
    individual sync(2) operation run concurrently with the same fsmark
    workload takes roughly 7s:
    
      real    0m6.930s
      user    0m0.000s
      sys     0m0.039s
    
    IOWs, sync is 7-8x faster on a busy filesystem and does not have an
    adverse impact on ongoing async data write operations.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Change-Id: I9e55d65f5ecb2305497711d4688f0647d9346035

commit e325eaed9c6a29fbf7f041b6faf2eba5810eb9f7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Feb 11 00:14:16 2014 +0100

    Enable LZ4 compression

commit d6b4c923736fd1818f78b4694b199b7a19b04a55
Author: Chanho Min <chanho.min@lge.com>
Date:   Mon Jul 8 16:01:51 2013 -0700

    crypto: add lz4 Cryptographic API
    
    Add support for lz4 and lz4hc compression algorithm using the lib/lz4/*
    codebase.
    
    [akpm@linux-foundation.org: fix warnings]
    Signed-off-by: Chanho Min <chanho.min@lge.com>
    Cc: "Darrick J. Wong" <djwong@us.ibm.com>
    Cc: Bob Pearson <rpearson@systemfabricworks.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Herbert Xu <herbert@gondor.hengli.com.au>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Cc: Kyungsik Lee <kyungsik.lee@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	crypto/Kconfig
    	crypto/Makefile
    
    Change-Id: Ibd99a50638b773bfcf7cae32bf9558f35d8745e7

commit c8cc5d62c83cb1eb857481814d34b780a162dcb9
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Mon Jul 8 16:01:48 2013 -0700

    arm: add support for LZ4-compressed kernel
    
    Integrates the LZ4 decompression code to the arm pre-boot code.
    
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Florian Fainelli <florian@openwrt.org>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    	arch/arm/Kconfig
    	arch/arm/boot/compressed/.gitignore
    	arch/arm/boot/compressed/Makefile
    	arch/x86/boot/compressed/Makefile
    
    Change-Id: Ie2847e4e624ccfb03424807cd33f154b63587de9
    
    Conflicts:
    	arch/arm/boot/compressed/.gitignore
    	arch/x86/boot/compressed/Makefile

commit c7ede520ec49e94776a2ca839fd5b919498eee32
Author: Richard Laager <rlaager@wiktel.com>
Date:   Thu Aug 22 16:35:47 2013 -0700

    lib/lz4: correct the LZ4 license
    
    The LZ4 code is listed as using the "BSD 2-Clause License".
    
    Signed-off-by: Richard Laager <rlaager@wiktel.com>
    Acked-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Cc: Chanho Min <chanho.min@lge.com>
    Cc: Richard Yao <ryao@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    [ The 2-clause BSD can be just converted into GPL, but that's rude and
      pointless, so don't do it   - Linus ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Change-Id: I67a4e9ba4ee648c4d5b19709389f05c0fa979d17

commit d436234bbc0dfccb34618d4b3e0a77428c08cbec
Author: Chanho Min <chanho.min@lge.com>
Date:   Mon Jul 8 16:01:49 2013 -0700

    lib: add lz4 compressor module
    
    This patchset is for supporting LZ4 compression and the crypto API using
    it.
    
    As shown below, the size of data is a little bit bigger but compressing
    speed is faster under the enabled unaligned memory access.  We can use
    lz4 de/compression through crypto API as well.  Also, It will be useful
    for another potential user of lz4 compression.
    
    lz4 Compression Benchmark:
    Compiler: ARM gcc 4.6.4
    ARMv7, 1 GHz based board
       Kernel: linux 3.4
       Uncompressed data Size: 101 MB
             Compressed Size  compression Speed
       LZO   72.1MB		  32.1MB/s, 33.0MB/s(UA)
       LZ4   75.1MB		  30.4MB/s, 35.9MB/s(UA)
       LZ4HC 59.8MB		   2.4MB/s,  2.5MB/s(UA)
    - UA: Unaligned memory Access support
    - Latest patch set for LZO applied
    
    This patch:
    
    Add support for LZ4 compression in the Linux Kernel.  LZ4 Compression APIs
    for kernel are based on LZ4 implementation by Yann Collet and were changed
    for kernel coding style.
    
    LZ4 homepage : http://fastcompression.blogspot.com/p/lz4.html
    LZ4 source repository : http://code.google.com/p/lz4/
    svn revision : r90
    
    Two APIs are added:
    
    lz4_compress() support basic lz4 compression whereas lz4hc_compress()
    support high compression or CPU performance get lower but compression
    ratio get higher.  Also, we require the pre-allocated working memory with
    the defined size and destination buffer must be allocated with the size of
    lz4_compressbound.
    
    [akpm@linux-foundation.org: make lz4_compresshcctx() static]
    Signed-off-by: Chanho Min <chanho.min@lge.com>
    Cc: "Darrick J. Wong" <djwong@us.ibm.com>
    Cc: Bob Pearson <rpearson@systemfabricworks.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Herbert Xu <herbert@gondor.hengli.com.au>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Cc: Kyungsik Lee <kyungsik.lee@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Change-Id: I345a92090ed860e65e2dc08d322cc863ce769130

commit 7d4d264597249073bf65f434bc35d6316e39a57e
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Mon Jul 8 16:01:46 2013 -0700

    lib: add support for LZ4-compressed kernel
    
    Add support for extracting LZ4-compressed kernel images, as well as
    LZ4-compressed ramdisk images in the kernel boot process.
    
    Change-Id: I3a58df029b711c50070fdb825ba5dbfaf08098cb
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Florian Fainelli <florian@openwrt.org>
    Cc: Yann Collet <yann.collet.73@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 7572ad099ae905235770a1c4066b0d4b98c8344a
Author: Kyungsik Lee <kyungsik.lee@lge.com>
Date:   Mon Jul 8 16:01:45 2013 -0700

    decompressor: add LZ4 decompressor module
    
    Add support for LZ4 decompression in the Linux Kernel.  LZ4 Decompression
    APIs for kernel are based on LZ4 implementation by Yann Collet.
    
    Benchmark Results(PATCH v3)
    Compiler: Linaro ARM gcc 4.6.2
    
    1. ARMv7, 1.5GHz based board
       Kernel: linux 3.4
       Uncompressed Kernel Size: 14MB
            Compressed Size  Decompression Speed
       LZO  6.7MB            20.1MB/s, 25.2MB/s(UA)
       LZ4  7.3MB            29.1MB/s, 45.6MB/s(UA)
    
    2. ARMv7, 1.7GHz based board
       Kernel: linux 3.7
       Uncompressed Kernel Size: 14MB
            Compressed Size  Decompression Speed
       LZO  6.0MB            34.1MB/s, 52.2MB/s(UA)
       LZ4  6.5MB            86.7MB/s
    - UA: Unaligned memory Access support
    - Latest patch set for LZO applied
    
    This patch set is for adding support for LZ4-compressed Kernel.  LZ4 is a
    very fast lossless compression algorithm and it also features an extremely
    fast decoder [1].
    
    But we have five of decompressors already and one question which does
    arise, however, is that of where do we stop adding new ones?  This issue
    had been discussed and came to the conclusion [2].
    
    Russell King said that we should have:
    
     - one decompressor which is the fastest
     - one decompressor for the highest compression ratio
     - one popular decompressor (eg conventional gzip)
    
    If we have a replacement one for one of these, then it should do exactly
    that: replace it.
    
    The benchmark shows that an 8% increase in image size vs a 66% increase
    in decompression speed compared to LZO(which has been known as the
    fastest decompressor in the Kernel).  Therefore the "fast but may not be
    small" compression title has clearly been taken by LZ4 [3].
    
    [1] http://code.google.com/p/lz4/
    [2] http://thread.gmane.org/gmane.linux.kbuild.devel/9157
    [3] http://thread.gmane.org/gmane.linux.kbuild.devel/9347
    
    LZ4 homepage: http://fastcompression.blogspot.com/p/lz4.html
    LZ4 source repository: http://code.google.com/p/lz4/
    
    Change-Id: I68ad4345d298b5b3e461dddd0acd5e595e914e1e
    Signed-off-by: Kyungsik Lee <kyungsik.lee@lge.com>
    Signed-off-by: Yann Collet <yann.collet.73@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Florian Fainelli <florian@openwrt.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 127747f3ac84f0670fb04149ebf0262185a66533
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 10 23:43:23 2014 +0100

    New test kernel released!

commit 46abf7ea1394f9f9c1ebabf336305f03a9230dd0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 10 22:36:14 2014 +0100

    Revert "Removed cpu PERF EVENTS!"
    
    This reverts commit 3c950a70642cd85a47dc53f20ee6ddbd5243e4f7.
    
    Conflicts:
    	arch/arm/configs/alucard_defconfig

commit 25826631cb29a0682a5ee118bb8d96cc045150cb
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 10 22:30:20 2014 +0100

    Reverted some tweaks commit!

commit 35d455086dec5fa632f9c5c6af9cb75f3447a8b7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Feb 10 20:38:17 2014 +0100

    Added building and cleaning scripts for GNU-EABI compiler ver. 4.8.3!

commit 706ee4c314b3a29ea8a344af1c31ab948aef9eee
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 9 12:54:55 2014 +0100

    Linux 3.4.79

commit 2f2f8c84b35174aef895ab0b56a1eea864fa6390
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 9 12:35:01 2014 +0100

    some tweaks!

commit c659766d82870cc45e7a2a94901223c3622087f7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 8 14:54:24 2014 +0100

    New kernel released!

commit 3c950a70642cd85a47dc53f20ee6ddbd5243e4f7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Feb 6 19:14:02 2014 +0100

    Removed cpu PERF EVENTS!

commit 52b196c4928adfab7622a976d9bb84bc26a1dfb4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Feb 2 16:59:10 2014 +0100

    Disabled WIFI Wakelocks V2 now for real. Thanks to Yuri.

commit 14d3ce959b606846574936ed79d2380a780d1481
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 1 14:45:49 2014 +0100

    Merging with linux-3.4.78!

commit 5d44ef43e46d12cd11947b8b1bd173948e7f6ad0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 1 12:48:10 2014 +0100

    Merged shmem and ashmem with linux-3.4.78

commit cc03e6e9d5aa956ee271b31144a2921669c4aff6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 1 02:46:49 2014 +0100

    Removed CONFIG_SEC_GPIO_DVS!

commit f38bc03c0771c3674fe3d9b8c85ea85ded870657
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 1 02:12:54 2014 +0100

    Removed Samsung rooting restriction feature!

commit b798e7109132f9fd93668d36d92c984370760da0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 1 01:57:23 2014 +0100

    New test kernel!

commit b9af1f2ddbab491bd4252dfa8ec7434118af51e5
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Feb 1 01:32:11 2014 +0100

    Merged with linux-3.4.78

commit ed4db1ff6ce3070afa7774a620cc9379f8f10827
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 30 23:24:48 2014 +0100

    Merging with Linux 3.4.78

commit b0f75c39904fb88ee54f210238c95e95d47a4d5b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 30 21:59:42 2014 +0100

    Merged other arch from linux 3.4.77!

commit 9c84bf80ff5951a410ff9b5256b6c8091544a61e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 30 11:16:02 2014 +0100

    Linux 3.4.78

commit fbed8b127e9804d0fd93864ecec9bdcfef4f6363
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 30 01:48:07 2014 +0100

    Cleaning code!

commit 0905d3716be2b4350d6b0ba90fc48e6fe4847a38
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 30 00:39:18 2014 +0100

    Fixed restart after removed samsung debugging features!

commit 4cfb337b5945c566fa907a0fbd3f6ca07ee859aa
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 30 00:00:21 2014 +0100

    Code style!

commit 7a1292f4dbb6b8a7ef02742434b7f2a8f7b83596
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 29 23:38:09 2014 +0100

    Cleaned some samsung debugging features configs!

commit 19255a4d563ad209b805346fa9381d7e9e730175
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 29 17:59:50 2014 +0100

    Code style!

commit 214a8af6d9d16fca317dc88e3bbbd9c2bc6d245a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 29 12:47:49 2014 +0100

    Removed SAMSUNG Debugging feature that they were disabled!

commit 4e84521a13f25323c578eb230c2e9ef964fd5895
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 29 00:55:08 2014 +0100

    Removed TIMA. We don't use Knox!

commit dcefb33cbc7785f0479bb0930486e4c83dee09ec
Author: Anirudh Ghayal <aghayal@codeaurora.org>
Date:   Wed Jul 3 14:06:30 2013 +0530

    power: pm8921-charger: Fix delay in USB wall-charger removal
    
    Avoid scheduling time optimization for the vin_min work. This
    prevents delay in USB removal detection for USB wall-charger.
    
    CRs-Fixed: 579548
    Change-Id: I578a0a70122e4d10ff932a41d6c6ac58e5ef121d
    Signed-off-by: Anirudh Ghayal <aghayal@codeaurora.org>
    Signed-off-by: Saket Saurabh <ssaurabh@codeaurora.org>

commit c9935e20f861a20c95db4fe39cb942ea8685e1da
Author: Anirudh Ghayal <aghayal@codeaurora.org>
Date:   Mon Aug 5 19:07:32 2013 +0530

    power: pm8921-charger: Add delay after turning on D0 (TCXO) clock
    
    On APQ8064, TCXO cannot be turned off before its warm-up period (4ms).
    This is not required on the platform which have a CXO. Add a warm-up
    delay after turning on D0 (which turns on the TCXO).
    
    Signed-off-by: Anirudh Ghayal <aghayal@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/board-8064-pmic.c
    	drivers/power/pm8921-charger.c
    	include/linux/mfd/pm8xxx/pm8921-charger.h
    
    Change-Id: Iaa91b8051d6d6ed7bd6394cc920411792b176224

commit a1ee9d0e6447beb89cc086d7546803b16993ac27
Author: Abhijeet Dharmapurikar <adharmap@codeaurora.org>
Date:   Tue Jul 16 19:25:35 2013 -0700

    power: pm8921-bms: fix calibration and UVLO issues
    
    The bms is not running a hkadc calibration before calculating the
    first SOC. This causes accuracy issues with the power on SOC.
    
    Also when the battery gets in the low voltage range, don't limit
    the corrections. We have seen UVLO where even though the BMS was
    calculating SOC quickly it was not lowering the SOC fast enough.
    Skip limiting ocv change when battery is near cutoff voltage.
    
    Signed-off-by: Abhijeet Dharmapurikar <adharmap@codeaurora.org>
    
    Conflicts:
    	drivers/power/pm8921-bms.c
    
    Change-Id: I746a9b003135d51d51c7d039d411689ca69571dc

commit 214e4053e6809af559d801eb98c5b4cf49561350
Author: Abhijeet Dharmapurikar <adharmap@codeaurora.org>
Date:   Tue Jul 16 20:11:29 2013 -0700

    power: pm8921-charger: vote D0 when running kickstart
    
    There is a small window when the PMIC could lock up while
    running the kickstart routine. If PMIC sleeps right while
    kickstart is run, it causes a short sleep which is known
    to cause PMIC locksup.
    
    To prevent these, specifically vote for D0 clock while
    running kickstart which guarantees that the PMIC will not
    sleep.
    
    Signed-off-by: Abhijeet Dharmapurikar <adharmap@codeaurora.org>
    
    Conflicts:
    	drivers/power/pm8921-charger.c
    
    Change-Id: I830749c19c79fd63238769ecf7472d03d67e7aa2

commit ea1c2f92a5d6929a6a54b53222251e24ef1ce4cb
Author: Ricardo Cerqueira <cyanogenmod@cerqueira.org>
Date:   Mon Oct 28 20:23:38 2013 -0400

    sec-battery: Standardize the output of the "online" property
    
    Return "0" when no power supply is present
    
    Change-Id: Ic873705c20468cb5e7898fa2ddff5828cd5928cf

commit 62d1b6e3c7b823850aabeb0ef83fb3d09d129cb4
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 28 14:49:07 2014 +0100

    New test kernel released!

commit df01cd6224d8cd7340d747f3a42207cb74ad6022
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 28 14:21:24 2014 +0100

    Revert "ext4: fix memory leak in xattr"
    
    This reverts commit 683f3bd8bac24e483b2c941a5195fb1dfb480ec3.

commit 99d0603bb590cdcf83416e4e29d57e772d99d73a
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Mon Jan 27 18:28:59 2014 -0600

    jf: defconfig: disable VMWare Mobile Virtualization Platform
    
    Change-Id: I96e75b4f2e904465fe42fb64317dab1cf8835bbf

commit 4230e0534a0a5eff82bf2ba5238d8de365de5aec
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Mon Jan 27 17:55:05 2014 -0600

    gitignore: add all gitignores from caf
    
    * please, stop annoying me
    
    Change-Id: Ie6e491619f2b9c73ba4e43358df435f128de38e8

commit 627694bfb14ad9cb09dff0d5beebb4b25a0a3607
Author: mrg666 <drgungor@hotmail.com>
Date:   Sun Dec 22 20:52:32 2013 -0500

    jbd2: optimize jbd2_journal_force_commit
    
    Current implementation of jbd2_journal_force_commit() is suboptimal because
    result in empty and useless commits. But callers just want to force and wait
    any unfinished commits. We already have jbd2_journal_force_commit_nested()
    which does exactly what we want, except we are guaranteed that we do not hold
    journal transaction open.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    
    Change-Id: I5c041a1898838e880714a913b5a915f105a8dfb9

commit c0874c46650a864d949a6fbc6ac7859e6cdd637b
Author: Andrey Sidorov <qrxd43@motorola.com>
Date:   Wed Sep 19 18:14:53 2012 +0000

    ext4: speed up truncate/unlink by not using bforget() unless needed
    
    Do not iterate over data blocks scanning for bh's to forget as they're
    never exist. This improves time taken by unlink / truncate syscall.
    Tested by continuously truncating file that is being written by dd.
    Another test is rm -rf of linux tree while tar unpacks it. With
    ordered data mode condition unlikely(!tbh) was always met in
    ext4_free_blocks. With journal data mode tbh was found only few times,
    so optimisation is also possible.
    
    Unlinking fallocated 60G file after doing sync && echo 3 >
    /proc/sys/vm/drop_caches && time rm --help
    
    X86 before (linux 3.6-rc4):
    real    0m2.710s
    user    0m0.000s
    sys     0m1.530s
    
    X86 after:
    real    0m0.644s
    user    0m0.003s
    sys     0m0.060s
    
    MIPS before (linux 2.6.37):
    real    0m 4.93s
    user    0m 0.00s
    sys     0m 4.61s
    
    MIPS after:
    real    0m 0.16s
    user    0m 0.00s
    sys     0m 0.06s
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrey Sidorov <qrxd43@motorola.com>
    Signed-off-by: franciscofranco <franciscofranco.1990@gmail.com>
    
    Change-Id: Ie78a945cb82b3892eaf88701f2dc3b7726104fb5

commit 69428a6c0655817bcbe238d55afb74ed7fceaa49
Author: Junxiao Bi <junxiao.bi@oracle.com>
Date:   Wed Sep 11 14:23:04 2013 -0700

    writeback: fix race that cause writeback hung
    
    There is a race between mark inode dirty and writeback thread, see the
    following scenario.  In this case, writeback thread will not run though
    there is dirty_io.
    
    __mark_inode_dirty()                                          bdi_writeback_workfn()
    	...                                                       	...
    	spin_lock(&inode->i_lock);
    	...
    	if (bdi_cap_writeback_dirty(bdi)) {
    	    <<< assume wb has dirty_io, so wakeup_bdi is false.
    	    <<< the following inode_dirty also have wakeup_bdi false.
    	    if (!wb_has_dirty_io(&bdi->wb))
    		    wakeup_bdi = true;
    	}
    	spin_unlock(&inode->i_lock);
    	                                                            <<< assume last dirty_io is removed here.
    	                                                            pages_written = wb_do_writeback(wb);
    	                                                            ...
    	                                                            <<< work_list empty and wb has no dirty_io,
    	                                                            <<< delayed_work will not be queued.
    	                                                            if (!list_empty(&bdi->work_list) ||
    	                                                                (wb_has_dirty_io(wb) && dirty_writeback_interval))
    	                                                                queue_delayed_work(bdi_wq, &wb->dwork,
    	                                                                    msecs_to_jiffies(dirty_writeback_interval * 10));
    	spin_lock(&bdi->wb.list_lock);
    	inode->dirtied_when = jiffies;
    	<<< new dirty_io is added.
    	list_move(&inode->i_wb_list, &bdi->wb.b_dirty);
    	spin_unlock(&bdi->wb.list_lock);
    
    	<<< though there is dirty_io, but wakeup_bdi is false,
    	<<< so writeback thread will not be waked up and
    	<<< the new dirty_io will not be flushed.
    	if (wakeup_bdi)
    	    bdi_wakeup_thread_delayed(bdi);
    
    Writeback will run until there is a new flush work queued.  This may cause
    a lot of dirty pages stay in memory for a long time.
    
    Signed-off-by: Junxiao Bi <junxiao.bi@oracle.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Francisco Franco <franciscofranco.1990@gmail.com>
    
    Change-Id: I973fcba5381881a003a035ffff48f64348660079

commit 683f3bd8bac24e483b2c941a5195fb1dfb480ec3
Author: Dave Jones <davej@redhat.com>
Date:   Fri Oct 11 00:05:35 2013 +0000

    ext4: fix memory leak in xattr
    
    If we take the 2nd retry path in ext4_expand_extra_isize_ea, we
    potentionally return from the function without having freed these
    allocations.  If we don't do the return, we over-write the previous
    allocation pointers, so we leak either way.
    
    Spotted with Coverity.
    
    [ Fixed by tytso to set is and bs to NULL after freeing these
      pointers, in case in the retry loop we later end up triggering an
      error causing a jump to cleanup, at which point we could have a double
      free bug. -- Ted ]
    
    Change-Id: I49b8ca41a6c6d44b563eb23306870258a3affd3b
    Signed-off-by: Dave Jones <davej@fedoraproject.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Cc: stable@vger.kernel.org
    Git-commit: 6e4ea8e33b2057b85d75175dd89b93f5e26de3bc
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Osvaldo Banuelos <osvaldob@codeaurora.org>
    
    Conflicts:
    	fs/ext4/xattr.c

commit 470ddfaddc7364a9c9ffb6fdce05f31f8d54211c
Author: Prakash Kamliya <pkamliya@codeaurora.org>
Date:   Thu Sep 26 17:59:19 2013 +0530

    sync: signal pt before sync_timeline object gets destroyed
    
    There is a race condition
    
    Assume we have *one* sync_fence object, with *one* sync_pt
    which belongs to *one* sync_timeline, given this condition,
    sync_timeline->kref will have two counts, one for sync_timeline
    (implicit) and another for sync_pt.
    
    Assume following is the situation on CPU
    
    Theead-1 : (Thread which calls sync_timeline_destroy())
      -> (some function calls)
       -> sync_timeline_destory()
        -> sync_timeline_signal() (CPU is inside this
    function after putting reference to sync_timeline)
    
    At this time Thread-2 comes and does following
    
    Thread-2 : (fclose on fence fd)
    > sync_fence_release() -> because of fclose() on fence object
     -> sync_fence_free()
      -> sync_pt_free()
       -> kref_put(&pt->parent->kref, sync_timeline_free);
        -> sync_timeline_free() (CPU is inside this because
    this time kref will be zero after _put)
    
    Thread-2 will free sync_timeline object before Thread-1
    has finished its work inside sync_timeline_signal.
    
    With this change we signals all sync_pt before putting
    reference to sync_timeline object.
    
    Signed-off-by: Prakash Kamliya <pkamliya@codeaurora.org>
    Signed-off-by: Rom Lemarchand <romlem@google.com>
    
    Change-Id: Ic680e4d0bbef1c46bcb7cfba693395645241d203

commit 95f971ced477ab43dbf0857ff9535f83672fccec
Author: Alistair Strachan <alistair.strachan@imgtec.com>
Date:   Wed Apr 10 16:35:14 2013 -0700

    sync: Fix a race condition between release_obj and print_obj
    
    Before this change, a timeline would only be removed from the timeline
    list *after* the sync driver had its release_obj() called. However, the
    driver's release_obj() may free resources needed by print_obj().
    
    Although the timeline list is locked when print_obj() is called, it is
    not locked when release_obj() is called. If one CPU was in print_obj()
    when another was in release_obj(), the print_obj() may make unsafe
    accesses.
    
    It is not actually necessary to hold the timeline list lock when calling
    release_obj() if the call is made after the timeline is unlinked from
    the list, since there is no possibility another thread could be in --
    or enter -- print_obj() for that timeline.
    
    This change moves the release_obj() call to after the timeline is
    unlinked, preventing the above race from occurring.
    
    Signed-off-by: Alistair Strachan <alistair.strachan@imgtec.com>
    
    Change-Id: I563347e163ac4ef19c2f5930c3ec10e61f5f5d44

commit 2f493d50b2e84ef214c2509521e249590746e5ef
Author: Alexander Martinz <eviscerationls@gmail.com>
Date:   Wed Dec 25 15:01:44 2013 +0100

    Revert "mm: panic on the first bad page table entry access"
    
    This reverts commit 215a2c3137b906afd66c93eb9fb3479a1f06e1c9.
    
    Change-Id: Ica69d573fe0e2fab19ddfc3979513bdedb69c717

commit 156b8991613cc948f9120fdcd10bd526573daaef
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 26 19:02:36 2014 +0100

    Config updated!

commit 5bae746cf88fc89be9469b8b1978089ec78051a8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 26 18:57:35 2014 +0100

    WIFI: Reduced Wakelocks and block IPv6 noice.

commit cdff0caedebc4733d8750a4c6ed4cb04cd6e96bc
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sun Jan 26 11:17:18 2014 -0600

    Revert "gcc-wrapper: force python version 2"
    
    This reverts commit de84503c386557710d64f627cf2d77970a1f6740.

commit 388160f721df0d47897d48c3b095126903149cc9
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Thu Dec 27 01:42:50 2012 -0500

    ext4: avoid hang when mounting non-journal filesystems with orphan list
    
    commit 0e9a9a1ad619e7e987815d20262d36a2f95717ca upstream.
    
    When trying to mount a file system which does not contain a journal,
    but which does have a orphan list containing an inode which needs to
    be truncated, the mount call with hang forever in
    ext4_orphan_cleanup() because ext4_orphan_del() will return
    immediately without removing the inode from the orphan list, leading
    to an uninterruptible loop in kernel code which will busy out one of
    the CPU's on the system.
    
    This can be trivially reproduced by trying to mount the file system
    found in tests/f_orphan_extents_inode/image.gz from the e2fsprogs
    source tree.  If a malicious user were to put this on a USB stick, and
    mount it on a Linux desktop which has automatic mounts enabled, this
    could be considered a potential denial of service attack.  (Not a big
    deal in practice, but professional paranoids worry about such things,
    and have even been known to allocate CVE numbers for such problems.)
    
    -js: This is a fix for CVE-2013-2015.
    
    Change-Id: Ice3149e6c8df1ab9adc8bcf237f17ef37fd69e1e
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Zheng Liu <wenqing.lz@taobao.com>
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Change-Id: I91e472a154f72c4017712e5284a96d7599dc8829

commit a159a6558a626ba343f050dde98032cc1a796bb3
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Jan 25 01:45:48 2014 +0100

    New kernel released!

commit 6a6f2cea4930bfc46026cfc646cbe753015d4461
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Jan 25 01:20:12 2014 +0100

    Revert "Add ENABLE_VMALLOC_SAVINGS to Kconfig and enable it"
    
    This reverts commit 58b6c1359b415087caa9b81076e24070b1e7e1a0.

commit 18593971a374e78f89a3b66640fd273863083fbf
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Nov 11 17:25:02 2013 -0600

    softirq: reduce latencies
    
    Date	Thu, 03 Jan 2013 23:49:40 -0800
    
    In various network workloads, __do_softirq() latencies can be up
    to 20 ms if HZ=1000, and 200 ms if HZ=100.
    
    This is because we iterate 10 times in the softirq dispatcher,
    and some actions can consume a lot of cycles.
    
    This patch changes the fallback to ksoftirqd condition to :
    
    - A time limit of 2 ms.
    - need_resched() being set on current task
    
    When one of this condition is met, we wakeup ksoftirqd for further
    softirq processing if we still have pending softirqs.
    
    Using need_resched() as the only condition can trigger RCU stalls,
    as we can keep BH disabled for too long.
    
    I ran several benchmarks and got no significant difference in
    throughput, but a very significant reduction of latencies (one order
    of magnitude) :
    
    In following bench, 200 antagonist "netperf -t TCP_RR" are started in
    background, using all available cpus.
    
    Then we start one "netperf -t TCP_RR", bound to the cpu handling the NIC
    IRQ (hard+soft)
    
    Before patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=550110.424
    MIN_LATENCY=146858
    MAX_LATENCY=997109
    P50_LATENCY=305000
    P90_LATENCY=550000
    P99_LATENCY=710000
    MEAN_LATENCY=376989.12
    STDDEV_LATENCY=184046.92
    After patch :
    
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=40545.492
    MIN_LATENCY=9834
    MAX_LATENCY=78366
    P50_LATENCY=33583
    P90_LATENCY=59000
    P99_LATENCY=69000
    MEAN_LATENCY=38364.67
    STDDEV_LATENCY=12865.26
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    
    Change-Id: I4da978c5775dd2823451838b11b3192713a88445
    Signed-off-by: Paul Reioux <reioux@gmail.com>

commit 58b6c1359b415087caa9b81076e24070b1e7e1a0
Author: Alexander Martinz <eviscerationls@gmail.com>
Date:   Sun Dec 22 17:37:51 2013 +0100

    Add ENABLE_VMALLOC_SAVINGS to Kconfig and enable it
    
    Change-Id: I207e82a1bb39288e0d1682624955820bf08c234c

commit 23544efb5982b8e9679719e0e03bbb1301fd0e2f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jan 24 23:57:26 2014 +0100

    New kernel test version!

commit f04d7cad92bc0c259e3b26de7997f1f8af6b0e0e
Author: ausdim <koronaios@gmail.com>
Date:   Mon Sep 9 20:23:52 2013 +0300

    Cifs 4.2.x multiuser fix
    
    Change-Id: I0635a7d00d5f3cdaa2da7db3edafe1e72a20770c
    
    Conflicts:
    
    	arch/arm/configs/cyanogen_jf_defconfig
    	fs/Kconfig

commit a78d5951a6eda68dfa1df157b8ff2f1beed30972
Author: faux123 <reioux@gmail.com>
Date:   Mon Mar 19 17:22:43 2012 -0700

    Optimized ARM RWSEM algorithm
    
    RWSEM implementation for ARM using atomic functions.
    Heavily based on arch/sh/include/asm/rwsem.h
    
    Change-Id: I0e836157618a9757b5fe8ff1c799524b2dfda83b
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit 89d39724015a859f22c8e0375277b21f95899bef
Author: Alexander Martinz <eviscerationls@gmail.com>
Date:   Mon Dec 23 21:11:05 2013 +0100

    mm: set swappiness to 0
    
    we have about 2gb ram, we dont need swapiness at all
    
    Change-Id: Icc8725e6fa0e269017ef652555993358db8fafec

commit 9917c0ac7be1bf310f52d88769a81ed5c072b881
Author: Alexander Martinz <eviscerationls@gmail.com>
Date:   Mon Dec 23 23:27:30 2013 +0100

    sched: set power savings to POWERSAVINGS_BALANCE_WAKEUP
    
    save some power by default
    
    Change-Id: Ica16419a0af38efade80c527f5a69bce046ccca9

commit d35c952ea115dc42e7dd1bb04270200decbf16ad
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jan 24 00:29:23 2014 +0100

    Code Style!

commit 159c59fb4cbc4993d057c8b52dfa325bf6204501
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jan 24 00:06:59 2014 +0100

    New kernel released!

commit e3dbe8e2fb996ecbebe49b4ca80c2c6eeb9e6db6
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 23 20:40:52 2014 +0100

    Fixed some files! Code style! removed some info!

commit 659b2ed42340d3fddfb88be65ff2fa5f5a47dc9f
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 23 00:28:20 2014 +0100

    Code style!

commit 71b5922c99a75192a73d31702174c30d6d1064bf
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 22 23:51:13 2014 +0100

    Removed spaces!

commit 113df66b8c376fe5943bded17fe42ab52aaedf98
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 21 00:51:07 2014 +0100

    Removed MMC_TEST!

commit 6ed7f33aa09b15278ad05ff3b530190e3f2ed007
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sat Jan 18 13:56:33 2014 -0600

    gcc-wrapper: force python version 2
    
    Change-Id: I4d0f831be48fc4ab99cf24acdea3e7acdcb4d836

commit 75404d4922f4f72b419479d2c4af04cd6d7835be
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 20 23:29:37 2014 +0100

    Removed MMC perf profiling, who cares? less cpu work. more batt. Thanks to Yuri :)

commit 7d6b3c3cb06357438e7c9a55ce51c5b50d2a5a8b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 19 23:17:28 2014 +0100

    New kernel released!

commit 79f141e9d731a604aba848e748a74b4884a8e515
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 19 20:34:17 2014 +0100

    Removed 216, 324 Mhz frequency steps

commit 65546cd50ff03a3dc0ebc9e0bcd0751ee67f249d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 19 18:53:06 2014 +0100

    Fixed file permissions!

commit f076fd5e7eeedba40c7522aa94270ae5c2d8c213
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 19 13:10:36 2014 +0100

    Fixed MMC Code!

commit 930b0754192138fc53db56d2defec278ba47f834
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Thu Nov 14 07:05:40 2013 -0700

    Add internal bool for snd_pcm struct

commit 1587ce74d7e06717fbc8396e1f207df8f69f5eea
Author: Florent Guichard <guichardflorent@gmail.com>
Date:   Fri Jan 17 08:26:44 2014 +0100

    fix typo

commit dbfe9b51c6bf2913295651a83ce2d5b4252d1356
Author: broodplank <guichardflorent@gmail.com>
Date:   Thu Jan 16 21:04:02 2014 +0100

    Update Linux Kernel to 3.4.77

commit 92bc300f459ceb56fe1ca641c31ed75325007300
Author: broodplank <guichardflorent@gmail.com>
Date:   Fri Jan 10 01:10:30 2014 +0100

    Update Linux kernel to 3.4.76

commit bc2b43e130965479986cf44fc74703f4db18ea6a
Author: Pranav Vashi <neobuddy89@gmail.com>
Date:   Fri Dec 20 16:35:48 2013 -0800

    Linux 3.4.75
    
    Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>

commit b8c24ca718eee50ab49e2b13a30291996a31df27
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Thu Dec 12 19:54:27 2013 +0100

    Squashed update of kernel from 3.4.73 to 3.4.74

commit 2d00202736f19704f9737e67159ada6b202df588
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Mon Dec 9 19:49:20 2013 +0100

    Squashed update of kernel from 3.4.72 to 3.4.73
    
    Conflicts:
    	drivers/mmc/card/block.c

commit 5a619708440e03ba82bf05cffd55c07eb4c0a5cb
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Thu Dec 5 00:48:38 2013 +0100

    Squashed update of kernel from 3.4.71 to 3.4.72

commit 95fe996c5aef35454ad6b9d49cd4b8d83a1e72ce
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Sat Nov 30 12:53:05 2013 +0100

    Squashed update of kernel from 3.4.70 to 3.4.71

commit ec752de5c43cffe117a731c4e25129eae4b63d38
Author: Florent Guichard <guichardflorent@gmail.com>
Date:   Fri Nov 22 10:19:39 2013 +0100

    Squashed update of kernel from 3.4.69 to 3.4.70

commit 5bdaf4e6c0f5f1002452c130ab484c17a55e821c
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Wed Nov 13 20:41:27 2013 +0100

    Squashed update of kernel from 3.4.68 to 3.4.69

commit 43e415a56837b46e36b54adbd5c6b9a2cc753a79
Author: Florent Guichard <guichardflorent@gmail.com>
Date:   Fri Nov 15 11:41:37 2013 +0100

    Merge fixes

commit c39391038f585091a75bfb0ded14539a44052021
Author: Florent Guichard <guichardflorent@gmail.com>
Date:   Fri Aug 23 08:23:08 2013 +0200

    merge fixes

commit f896746bdff02720ba9ca31e4fcd09c36a8822b4
Author: Arne Coucheron <arco68@gmail.com>
Date:   Wed Nov 6 09:28:21 2013 +0100

    Squashed update of kernel from 3.4.67 to 3.4.68

commit 851d603e61567ea42f0e3e18c75e24bfeb2db9b2
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Tue Oct 22 20:27:47 2013 +0200

    Squashed update of kernel from 3.4.66 to 3.4.67

commit 8f3c9c4e291b890079f2049485434ed46a0c5568
Author: ausdim <koronaios@gmail.com>
Date:   Mon Oct 14 12:15:43 2013 +0300

    Linux 3.4.66

commit 38af7674ec1d3462d4ad14f1f1a18038f5c52f64
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Sat Oct 5 16:26:32 2013 +0200

    Squashed update of kernel from 3.4.64 to 3.4.65

commit 30a73023bf1e372a60ef41a068700e89fd9ac9d1
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Wed Oct 2 19:57:49 2013 +0200

    Squashed update of kernel from 3.4.63 to 3.4.64

commit 2572a32ba7e495d7b052399c527a8c2341bdc8e9
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Fri Sep 27 20:24:59 2013 +0200

    Squashed update of kernel from 3.4.62 to 3.4.63
    
    Conflicts:
    
    	drivers/usb/host/xhci.h

commit 0e49f6cea75e8e18c5ae645d04139fa88268f0df
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Sun Sep 15 11:23:30 2013 +0200

    Squashed update of kernel from 3.4.61 to 3.4.62

commit 5db393db8be5b6790a94fad0f20c21b9d5652c9d
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Sun Sep 8 11:23:35 2013 +0200

    Squashed update of kernel from 3.4.60 to 3.4.61

commit 6247f2530ae5176ec0a33ca30aeb6758157d778c
Author: ausdim <koronaios@gmail.com>
Date:   Fri Aug 30 23:24:01 2013 +0300

    Linux 3.4.60
    
    Conflicts:
    
    	kernel/workqueue.c

commit 4b42792d58d33e131b704a3ac44deacad0680256
Author: ausdim <koronaios@gmail.com>
Date:   Tue Aug 20 20:46:23 2013 +0300

    Linux 3.4.59
    
    Conflicts:
    	kernel/futex.c

commit 74a68f30a0908c57e78f584803965afab017f9a4
Author: ausdim <koronaios@gmail.com>
Date:   Mon Aug 19 18:17:54 2013 +0300

    Linux 3.4.58

commit 683c921628c2387cde1adbf453ffdff6010f904d
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Wed Aug 14 17:02:56 2013 -0700

    Linux 3.4.57

commit d4882c8373c77028748de74a348b2c65cd9f8e93
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sun Aug 4 12:27:35 2013 -0700

    Linux 3.4.56

commit 3b64ed82b8351366005e5386125a83119a30ad26
Author: ausdim <koronaios@gmail.com>
Date:   Mon Jul 29 11:00:52 2013 +0300

    Linux 3.4.55

commit ef4bf6a8e7223fb82a065676f27a729a61f3d56f
Author: Cristoforo Cataldo <cristoforo.cataldo@gmail.com>
Date:   Mon Jul 22 20:33:54 2013 +0200

    Squashed update of kernel from 3.4.53 to 3.4.54
    
    Conflicts:
    	fs/ext4/mballoc.c

commit 7e6f842efe13f666cc9e433f0c3e0050ce96c065
Author: Florent Guichard <guichardflorent@gmail.com>
Date:   Sun Jul 14 12:12:31 2013 +0200

    Linux kernel version 3.4.53

commit e8bcbd9c6c664500c8e3832ce0856274f8357ac5
Author: ausdim <koronaios@gmail.com>
Date:   Sat Jul 6 16:43:54 2013 +0300

    Linux 3.4.52

commit 477c45a12d8d4ce032281c46b97d0779d3e02c1e
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Mon Jul 1 21:07:59 2013 -0700

    Linux 3.4.51

commit eac5bdf4f4d8f29af5312f1b390cce53891471ef
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat Jun 22 19:23:32 2013 -0700

    Linux 3.4.50

commit e288861ad5ce3f638909f59b527631418e2cb679
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Mon Jun 17 17:33:03 2013 -0700

    Linux 3.4.49

commit 8b7a5c631bcd6726efb67098b4baca1441965865
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat Jun 8 00:18:52 2013 -0700

    Linux 3.4.48

commit 9e066629a56267136d2b5ed13325d5a61bd2e4bd
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:56:13 2013 -0700

    Linux 3.4.47

commit 9ef67ecc8a9dc1d6257195433119ca95719ca806
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:55:51 2013 -0700

    Linux 3.4.46

commit b0ce01b9ed185459148bccc197ee34f93344939e
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:51:19 2013 -0700

    Linux 3.4.45

commit 8b621cbcf6cfec74408765781193f4887c26c16f
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:51:00 2013 -0700

    Linux 3.4.44

commit 8487ab7f9cf2e6e8b12bb01a2d85be6ce3b59b69
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:41:18 2013 -0700

    Linux 3.4.43

commit 3a0330b38052dd60dd60731ce72b98d49f9f72a8
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:40:54 2013 -0700

    Linux 3.4.42
    
    Conflicts:
    
    	kernel/sched/core.c

commit 778e097369b26e11911058b46b5e7747376ddb63
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:34:58 2013 -0700

    Linux 3.4.41

commit 5c4b7e3a424adec59c0977c2ec12c7d6e2cc7119
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:34:13 2013 -0700

    Linux 3.4.40

commit 8968ddec62ee9bbfb9123296c2e98edf478e1e98
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:29:53 2013 -0700

    Linux 3.4.39

commit c630c0846dea53b7fd16e7c32880aec3b9b967cc
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:29:31 2013 -0700

    Linux 3.4.38

commit d45a0455bd1ee760a5e5b9950fdcf3a85e6ed92a
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:28:16 2013 -0700

    Linux 3.4.37

commit 5e66823d695dc379ce06dbbebe3d7711ca55f18d
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:27:37 2013 -0700

    Linux 3.4.36

commit 7fce260891db4e90ae23df59200431479fc8db52
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:25:17 2013 -0700

    Fix up some mis-merges from Linux merge

commit 25a5da5daa7a8f7cc7274303f04e8b56af2b99c0
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:13:13 2013 -0700

    Linux 3.4.35

commit aed83b04cc50f81d96c528668962fe128ebc2af2
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:10:55 2013 -0700

    Linux 3.4.34
    
    Conflicts:
    	drivers/video/console/vgacon.c

commit ff44c0824a10b0af3343078e888f1039267f3054
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:07:46 2013 -0700

    Linux 3.4.33

commit 1534c3ec2b15a26a7096cd020565ec6658905875
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:04:05 2013 -0700

    Linux 3.4.32

commit 8604d818a798228c3311564b67b47b40f644f6c6
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:03:15 2013 -0700

    Linux 3.4.30-31

commit 9fe7c8e34708900f0994564c6cda6d10d381d145
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:02:23 2013 -0700

    Linux 3.4.29

commit 448cde027b122e217b9eb03dda933790a991d211
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:02:02 2013 -0700

    Linux 3.4.29
    
    Conflicts:
    
    	arch/arm/mm/dma-mapping.c
    
    Conflicts:
    	arch/arm/mm/dma-mapping.c

commit 9edff1ddb93fcdda51a6b32fe24053d598642175
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:01:42 2013 -0700

    Linux 3.4.28

commit 86110c7e8cdf7eaaac431ab141a4eee774a52ee7
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:01:19 2013 -0700

    Linux 3.4.27

commit 2a3886cdc30a9e4d0877633dd45e5219e7e31d71
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 08:00:55 2013 -0700

    Linux 3.4.26

commit 623d5e3cec27bc21f4ac4054e0cac116f9f3051d
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:57:33 2013 -0700

    Linux 3.4.25
    
    Conflicts:
    
    	include/linux/freezer.h

commit 4c9a855b44840bd0962e449711fc321b2b2c5310
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:53:14 2013 -0700

    Linux 3.4.24

commit b7a9425d8d08c95fa7cda52a2916af56d9c00373
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:52:57 2013 -0700

    Linux 3.4.23

commit 9790a16f4d49fb2b13aece49ae1a25bede52ec6a
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:52:35 2013 -0700

    Linux 3.4.22

commit 493676b827ec5fe732aaede517e993f060fb8707
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:50:43 2013 -0700

    Linux 3.4.21

commit 651191a0606a5f73018d2ad78c05122fa4e16dfc
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:47:25 2013 -0700

    Linux 3.4.20

commit c2d38b566431661104e3fb0a01f2396b948f6106
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:45:35 2013 -0700

    Linux 3.4.19

commit 1b0868b213650984a1fdacd8087e91146438381e
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:44:09 2013 -0700

    Linux 3.4.18

commit 6ae0fa91f0ed8d2c0a2213d107eb962e9796a278
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:41:27 2013 -0700

    Linux 3.4.17

commit 388d78e6446ee8712f9e0d0e51c7da02ee0192ec
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:38:16 2013 -0700

    Linux 3.4.16

commit 2a8c91f65c41f24e98b246b7e430bdfd7b8c7ff2
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:35:14 2013 -0700

    Linux 3.4.15

commit e680001a73a565b4ccd6e7ca179dd48e3f84b9e2
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:14:50 2013 -0700

    Linux 3.4.14

commit 9198eb9aa9b8281bd538fab9d7776be892da3128
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat May 25 07:11:01 2013 -0700

    Linux 3.4.13
    
    Conflicts:
    
    	drivers/interceptor/linux_versions.h

commit 8051c8a85d24a7f4606b245519db26db93735702
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 18:55:16 2013 -0700

    Linux 3.4.12
    
    Conflicts:
    
    	arch/arm/include/asm/mutex.h

commit c0ff10607e05c8a95dc70557ee0830778bcdd150
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 18:04:35 2013 -0700

    Linux 3.4.11

commit 1f8f95bfca5b1b93626e2750eb755691101a2691
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 17:52:05 2013 -0700

    Linux 3.4.10

commit 9eab7ad012cf9406f2e857d2a712bdb4bdafe103
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 17:40:44 2013 -0700

    Linux 3.4.9

commit ba6fe5b5d290129adbaba56e9efb0c602619dfd7
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 17:28:00 2013 -0700

    Linux 3.4.8
    
    Conflicts:
    	fs/ext4/extents.c
    	fs/ext4/super.c

commit 6c2742c1730c4f5ad303e1a106183e028d6b0f84
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 17:07:46 2013 -0700

    Linux 3.4.7
    
    Conflicts:
    
    	mm/vmscan.c

commit f24679a8ffffd53660df366b63e95dd5722ed49e
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 17:07:18 2013 -0700

    Linux 3.4.6

commit 52d8859b3f4d5d6101486a968b5ce045e7be1cc4
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 17:06:39 2013 -0700

    Linux 3.4.5

commit 0d7a67a8debdb546622f68ca6f98735c86a43e22
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 16:43:35 2013 -0700

    Linux 3.4.4

commit f415f58447364f8844014608cfea06d57be00902
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 16:42:57 2013 -0700

    Linux 3.4.3

commit 4fde9d83e97977b00b30214bc3557e5b9437c098
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 16:42:22 2013 -0700

    Linux 3.4.2
    
    Conflicts:
    	fs/ext4/mballoc.c
    	fs/ext4/namei.c

commit ea2dda6ec1b0a3d3569f95e25fcdc757e912a07b
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Fri May 24 16:36:24 2013 -0700

    Linux 3.4.1

commit 4afe4ca554432a53f357c592f53d61c83fec1b62
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 16 23:43:00 2014 +0100

    New kernel test!

commit f80c26fac91a36c3671c5ed2c657ec8b96238976
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 16 22:52:46 2014 +0100

    Removed accurate cpufreq parameter of alucard_hotplug!

commit 4d57ed74ae82576de8fd6b7e1e0182fd74fa607d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 16 22:41:56 2014 +0100

    Revert "Updated alucard, darkness, nightmare governors! use apu function to get current cpu freq!"
    
    This reverts commit 8ca20e850da2b0ee4da0de752c738b0be2533c36.

commit 53f61ff45f5306ab0a605ccb7b0796cc89185656
Author: Stepan Moskovchenko <stepanm@codeaurora.org>
Date:   Tue Oct 22 18:57:56 2013 -0700

    ARM: Use -mcpu=cortex-a15 when targeting MSM Krait CPUs
    
    Enable compiler optimizations specific to the Cortex-A15
    processor when targeting MSM Krait CPUs. This is necessary
    take advantage of the UDIV/SDIV instructions supported by
    these processors. To accomplish this, we need to remove the
    -march=armv7-a ISA restriction from the compiler options
    because 'cortex-a15' is a superset of 'armv7-a'.
    
    Change-Id: I6215aecc11fb4f77c971de7b84f68649ef234357
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 8ca20e850da2b0ee4da0de752c738b0be2533c36
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 16 01:20:51 2014 +0100

    Updated alucard, darkness, nightmare governors! use apu function to get current cpu freq!

commit 6fd7645b01b84891533abaa54db1b34fe603928a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 16 01:20:06 2014 +0100

    Updated configuration removing log options!

commit 0eb55688c3675b15732c7fc82947b40cf3003bfb
Author: dorimanx <yuri@bynet.co.il>
Date:   Tue Jan 14 17:01:59 2014 +0200

    Merged FRANDOM code.
    
    Code changes are from furnace kernel source!
    
    http://forum.xda-developers.com/showthread.php?t=2572992
    
    https://github.com/dorimanx/Dorimanx-LG-G2-D802-Kernel/tree/furnace-kernel
    
    Conflicts:
    	arch/arm/configs/dorimanx_defconfig

commit 045e179504308088b12e86988cbc1d1019fb0c4b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 15 23:08:54 2014 +0100

    Removed frandom from root kernel!

commit 54de039206cf6545186a7b02d49bac74b5e248f0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 15 20:53:13 2014 +0100

    Updated configuration removing tracing options and deleted useless file!

commit a04b8c890b7bef97b1b33a5f9ce9ae55aac522dd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 15 20:32:41 2014 +0100

    Support for google arm-eabi-4.8 gcc!

commit 610db9868ec56e2f7e2d23857416c27cb0f6b414
Author: dorimanx <yuri@bynet.co.il>
Date:   Wed Jan 15 14:34:51 2014 +0200

     binfmt_elf.c: use get_random_int() to fix entropy depleting
    
    Changes:
    --------
    v4->v3:
    - s/random_stack_user()/get_atrandom_bytes()/
    - Move this function to ahead of its use to avoid the predeclaration.
    
    v3->v2:
    - Tweak code comments of random_stack_user().
    - Remove redundant bits mask and shift upon the random variable.
    
    v2->v1:
    - Fix random copy to check up buffer length that are not 4-byte multiples.
    
    v3 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59597.html
    v2 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59418.html
    v1 can be found at:
    http://www.spinics.net/lists/linux-fsdevel/msg59128.html
    
    Thanks,
    -Jeff
    
    Entropy is quickly depleted under normal operations like ls(1), cat(1),
    etc...  between 2.6.30 to current mainline, for instance:
    
    $ cat /proc/sys/kernel/random/entropy_avail
    3428
    $ cat /proc/sys/kernel/random/entropy_avail
    2911
    $cat /proc/sys/kernel/random/entropy_avail
    2620
    
    We observed this problem has been occurring since 2.6.30 with
    fs/binfmt_elf.c: create_elf_tables()->get_random_bytes(), introduced by
    f06295b ("ELF: implement AT_RANDOM for glibc PRNG seeding").
    
    /*
     * Generate 16 random bytes for userspace PRNG seeding.
     */
    get_random_bytes(k_rand_bytes, sizeof(k_rand_bytes));
    
    The patch introduces a wrapper around get_random_int() which has lower
    overhead than calling get_random_bytes() directly.
    
    With this patch applied:
    $ cat /proc/sys/kernel/random/entropy_avail
    2731
    $ cat /proc/sys/kernel/random/entropy_avail
    2802
    $ cat /proc/sys/kernel/random/entropy_avail
    2878
    
    Analyzed by John Sobecki.
    
    Signed-off-by: Jie Liu <jeff.liu@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andreas Dilger <aedilger@gmail.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Arnd Bergmann <arnn@arndb.de>
    Cc: John Sobecki <john.sobecki@oracle.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Cc: Ted Ts'o <tytso@mit.edu>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Ulrich Drepper <drepper@redhat.com>

commit 37e9ce311544dc898cc78f5e6ac5e3d8588f1582
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 15 19:01:52 2014 +0100

    Removed many things that we don't use into kernel config!

commit e953738a94246a665141f2493d9050f5840dfccd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 15 18:58:25 2014 +0100

    Updated alucard, darkness, nightmare governors!

commit dc3402978ca719816068dd27a5dcf30ead39dde1
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Wed Jan 15 18:54:30 2014 +0100

    Revert "Merged pmic commit from CM11 kernel source code!"
    
    This reverts commit 6472c07eb0e100f68d0f1acd2f2d413d44314b81.

commit 49272a96364ad7b8e20e72423ff5960c232830ef
Author: abmantis <amfcalt@gmail.com>
Date:   Mon Jan 13 13:24:23 2014 -0600

    jf: bcmdhd: filter multicast during sleep
    
    Change-Id: I0963b7dab0df032228a17e051d879e8bb2635860

commit c94ccedfefd3bd352c5c9f45c9a699570aca6776
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 14 20:27:13 2014 +0100

    Modified governors! removed useless instructions!

commit ba1f4d368537c10cade8667800b0ef80538d89d7
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 14 20:23:49 2014 +0100

    Updated configuration! Removed test module and added KEXEC HARDBOOT!

commit 6472c07eb0e100f68d0f1acd2f2d413d44314b81
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 14 20:22:00 2014 +0100

    Merged pmic commit from CM11 kernel source code!

commit 4fcb00d477d04fdbdb48ee9b6fa433444106ca7d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 14 00:46:00 2014 +0100

    DISABLE AUTHENTEC_VPNCLIENT_INTERCEPTOR!

commit 74a06aaa95c167d4fe5c70175bbd7ba78b0f950d
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Mon Jan 13 07:45:19 2014 -0600

    defconfig: disable CONFIG_AUTHENTEC_VPNCLIENT_INTERCEPTOR
    
    * this is likely a secret nsa backdoor
    * plus we don't use it for the cm botnet
    
    Change-Id: I0834f0942b621e5907db9a984fa46ca8d2e36430

commit 1419f6f21dff4ca5fbaa044474b300cf9f590a99
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 14 00:03:22 2014 +0100

    Added STweaks into flashing kernel package!

commit f01e029dca8840b67a6e5675555b2534d440a8ed
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Jan 14 00:01:17 2014 +0100

    Changed cleaning script to avoid deleting STweaks.apk!

commit ae29a7ccafe0c2aaf40a44e974f841dbe1f7bdb0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 13 23:58:51 2014 +0100

    Removed governors cpus-boost and cyanogen cpuboost!

commit a543d782497e5f998cb9772359b74130a46a69f8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 12 20:43:44 2014 +0100

    New kernel released!

commit 68ffc8090399ea66ca582f8a7496cff6b8f5127b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 12 19:28:25 2014 +0100

    Fixed Alucard hotplug and moved to sys/kernel folder!

commit ce5577b481e0237ccec114ff1d55fef568adf97c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sun Jan 12 00:43:03 2014 +0100

    Enabled CCACHE to speedup building + Enable PANIC_ON_DATA_CORRUPTION. Thanks to Yuri :)

commit e299ca81cea9565dfaead5eec017c85fa526df69
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Jan 11 17:31:59 2014 +0100

    Fixed previous commit!

commit 68324dd92d41cead1a631d1d8f98aa57d691dfab
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Jan 11 16:28:53 2014 +0100

    Improved DVFS touchfreq interface forcing to get previous freq limit previously stored if atomic_get function fail!

commit 4935a52660e8337ea83ab8ae5e41a8ca7ed4cbf0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Jan 11 02:59:40 2014 +0100

    Removed some useless debug! (Thanks to Dorimanx)

commit 1f31e762b56bb97fc9769df8b6db062287570c19
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jan 10 22:31:51 2014 +0100

    Fixed Phone drastically shutdown when antutu bechmark is running and phone is connected by wifi!

commit e723aaddcfc3f93949fdeb455a85e9093c672d5e
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jan 10 00:44:50 2014 +0100

    Merged again with MY GE KK!

commit cb7d81045853103afe1063df5a720df93b806cad
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jan 10 00:36:30 2014 +0100

    Merged with MY GE KK!

commit 8616c19b924bba1852ba4742744a16663f3f2a43
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Jan 10 00:21:55 2014 +0100

    New kernel released!

commit dcfe6eb278732c84b7885c89e9a1d135bb920bc2
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 9 20:19:52 2014 +0100

    Changed some dvfs touch interface parmater limits!

commit ddea086b4da6e955741ed10249ac41161fd5d775
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 9 00:07:33 2014 +0100

    New DVFS touch interface.. created by me :)
    
    Conflicts:
    	arch/arm/configs/alucard_defconfig

commit 47981fc942e7dc22e56a7a3b514aab7e99f1f688
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 6 19:13:52 2014 +0100

    Merged with CM11. Updated configuration. Removed spaces!

commit d7017de4387fafbf04bd95341b2290eaefaa2f5d
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 6 03:43:51 2014 +0100

    Fixed mach-msm board! My mistake :)

commit 29fee13ca339ae624fc913eb92aebe5dd2a67bc8
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Mon Jan 6 03:06:31 2014 +0100

    Merged with GE KK release!

commit 06388b95159dacb5b65b3895d4d871fccd3cd90b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 2 11:53:41 2014 +0100

    Disabled DVFS!

commit 22280135ad701fec617bd53fb85e5d53c8221b0b
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 2 02:52:28 2014 +0100

    Added dynamic fsync!

commit 1a6964c984e79eebb069a4440b1b5d7f219274a0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Jan 2 02:37:24 2014 +0100

    Some fixes and updated configuration!

commit f7d3100fa53e871a35f15611b89bdd0185f5de70
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Dec 28 15:22:44 2013 +0100

    Modified cpufreq. We don't need to put cpus online for changing frequency limit and governors! :)
    
    Conflicts:
    	arch/arm/configs/alucard_defconfig

commit 6945790da378393a155894f8332da04d71b09c92
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Dec 28 12:10:18 2013 +0100

    Removed DVFS touch booster and disabled DVFS module!
    
    Conflicts:
    	arch/arm/configs/alucard_defconfig

commit e645249aa7e9f097f917edb8143e87c4b4b11abd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Dec 27 11:50:23 2013 +0100

    Improved scaling governor all cpus in cpufreq!
    
    Conflicts:
    	arch/arm/configs/alucard_defconfig

commit 2ef0c4ed3d76ceb99727aa1ade172deeb69a00f0
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 24 18:49:26 2013 +0100

    Imported simple gpu governor!

commit e09dbf26a34f98335cb779271527a946b51a2429
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 24 17:59:40 2013 +0100

    Modified cpufreq and governors!

commit 484c6aea990cdf5d576138a05b75cfbbfccfee73
Merge: e95d546 6af394a
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Dec 19 20:40:00 2013 +0100

    Merge remote-tracking branch 'origin/my-cm-11.0' into my-cm-11.0

commit e95d5467e00479e38e021a539470cd19f14c1e84
Merge: 43b3848 5a32c11
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Thu Dec 19 20:39:47 2013 +0100

    Merge remote-tracking branch 'cm-upstream/cm-11.0' into my-cm-11.0

commit 5a32c1123918eccbe57113cb199debc9122734ac
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Thu Dec 19 08:16:05 2013 -0600

    sensorhub: update to 4.4.2 drop (ML4)
    
    Change-Id: I6f620bcefe93426e706cd542aa019a8acbfc9609

commit 6af394a99d33c732dea3bce49c8681eaf6fe1be9
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 17 09:31:52 2013 +0100

    Merge branch 'my-cm-11.0', remote-tracking branch 'origin/my-cm-11.0' into my-cm-11.0

commit 43b3848cf547a13688053c5fab58831a5f3a7930
Merge: b3e580d d569adc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 17 02:31:21 2013 +0100

    Merge branch 'my-cm-11.0' of github.com:Alucard24/samsung-kernel-jfltexx into my-cm-11.0

commit b3e580d7534547d8a51c3c77620899faf07e405c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 17 02:30:21 2013 +0100

    Fixed cpufreq into mach-msm!

commit 95145ab3c527dac7e90bec9122eb76bf1c612d5f
Merge: db7cd98 9c1f1bc
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 17 02:26:38 2013 +0100

    Merge remote-tracking branch 'cm-upstream/cm-11.0' into my-cm-11.0

commit d569adc3736d1a979aa10672997c045e26f28a44
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Mon Dec 16 16:54:05 2013 +0100

    Updated conf!

commit 5855aa6b9af0b89576b63aa05483b2a13197e6da
Merge: f468c0b 9c1f1bc
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Mon Dec 16 14:19:05 2013 +0100

    Merge branch 'my-cm-11.0', remote-tracking branch 'cm-upstream/cm-11.0' into my-cm-11.0

commit f468c0b140c71f19896ac1864fd6482a7d57a95b
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Mon Dec 16 14:18:47 2013 +0100

    Merge branch 'my-cm-11.0', remote-tracking branch 'origin/my-cm-11.0' into my-cm-11.0

commit 9c1f1bcdd7ff53c31b46d0660e68d97e73fb2728
Author: Steve Kondik <shade@chemlab.org>
Date:   Mon Dec 16 00:59:50 2013 -0500

    video: msm: Sync camera to Samsung release ML4
    
    Change-Id: Ibd6e177c15120de2a9d6c2dbb44d45c9baa4aa13

commit 5f1966021b469f7c189e7628d8c748919b01f8e2
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Dec 15 23:56:54 2013 -0500

    gpu: msm2: Sync with upstream
    
    Change-Id: I6a9e8e44d993d9c9ee26f7462169ef1d20a6d113

commit a6013e8f2a62144d8b6251af1fce3560e794d20e
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Wed Jun 5 14:13:40 2013 -0700

    msm: Fix race condition in domain lookup
    
    During lookup of domains we take a pointer to the root node
    of the rb tree before taking the mutex to protect the rb tree.
    However, the rb root node might change if another process updates
    the rb tree after we take the pointer to the root node. This can
    cause us to not find the entry that we are looking for.
    Fix this by ensuring we have taken the mutex before getting the
    root node pointer.
    
    CRs-fixed: 493503
    Change-Id: I4702740e486fda0cd92df0401d8e4706598a899b
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit b793a73b5b65c4a3fb2e18081e824eddfc76ca29
Author: Sunid Wilson <sunidw@codeaurora.org>
Date:   Sat Apr 20 23:31:41 2013 -0700

    msm: gemini: Fix the error irq handling sequence
    
    Update the gemini IRQ handling sequence in ISR.
    
    Change-Id: Ia47e1600174462341027540247a79cba7ffd9c56
    Signed-off-by: Sunid Wilson <sunidw@codeaurora.org>
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>

commit f59d80b45aff8db63577299abf1b75e3631b6c3d
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Mon Dec 9 17:25:07 2013 -0800

    cpufreq: interactive: Remove trace event from idle_start handler
    
    Removed the trace_cpufreq_interactive_idle_start.
    Also fix a crash resulting from accessing NULL policy before taking
    the pcpu->enable_sem lock. The policy can be NULL if the core is
    hotplugged out before the enable_sem lock is taken.
    
    Change-Id: I7e2809cc016b3b383a44cdf3c697013e2d2b5417
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    (cherry picked from commit 3b7992b90216c74a9ae9ce0df90bc446687a2313)

commit 046a6f1158656820922f4a2650c5e7ac94422ae1
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Thu Nov 7 14:32:16 2013 -0800

    cpufreq: interactive: sync freq feature for interactive governor
    
    1) Add load info to cpufreq_interactive_cpuinfo
    2) If load on any other online cpu exceeds sync_freq_load_threshold,
       do not allow the frequency to drop below sync_freq
    
    Change-Id: I3617e10f87b85178914a18bcf04ac2a31a4f1ec1
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    (cherry picked from commit c26e25944543c61b34b1830938bb6f353a2580ac)

commit 2276baa2addffb5818e6251fa5a28434c40a86c8
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Nov 6 18:36:09 2013 -0800

    cpufreq: interactive: Add a sampling_down_factor for max frequencies
    
    Change min_sample_time to 100ms when running at max
    
    Change-Id: Ia7e35b0625bedf20e7ef3a1f52e5828ffbfed93e
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit a68423cfc9c2dbee8935cfd58670cb501289a67e
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Nov 27 17:08:55 2013 -0800

    cpufreq: interactive: Reset floor_validate_time if busy at max for 100ms
    
    When the interactive governor selects to run at max frequency it doesn't
    re-schedule the timer until it hits an idle. This change checks if the CPU
    has been continuously busy for last 100ms on hitting an idle start. If yes,
    then floor_validate_time is reset so that the CPU stays at max frequency
    for at least another 100 ms before stepping down.
    This is an important feature for detecting CPU intensive workloads which
    require high frequencies for achieving better performance.
    
    Change-Id: I7d48ffbc3d50a80af9be3bf94667ee3d0120b763
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit 08b421ca25f2aad243e6d9e41bc75adc09e7149c
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Thu Nov 21 12:42:09 2013 -0800

    cpufreq: interactive: Allow 1 ms error in above_hispeed_delay comparisons
    
    Allow for an error of 1 ms while taking into account
    above_hispeed_delay for a frequency
    
    Change-Id: I744e44387152e4efb5978df4f2b9533bf79d4582
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit f354fc0f7993874d83104a7f727b7340ba3e8551
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Dec 4 14:46:18 2013 -0800

    cpufreq: Disable cpu-boost by default
    
    Disable cpu-boost by default, so that it can be turned ON only by
    setting the module parameter boost_ms through command prompt when
    required.
    
    Change-Id: Ia9bc280892f65ed1d2e8051d1951e51922ad13db
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit eb04f71723e7c67d3a960ed12e6d29a362e4ec7e
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Tue Nov 26 18:20:57 2013 -0800

    cpufreq: Add Input Boost feature to the cpu-boost driver
    
    On incoming input events boost the frequency of all online cpus
    for at least input_boost_ms ms. This is accomplished by changing
    the policy->min of all the online cpus to input_boost_freq.
    
    Change-Id: Idb0ab75d68ae4ceff259cbbaaec1a9bb3bc871d3
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit fc6c23d9cbd041fceaf30711776d6add02f8eb63
Author: Swetha Chikkaboraiah <schikk@codeaurora.org>
Date:   Fri Nov 29 12:22:07 2013 +0530

    cpufreq: ondemand:kernel NULL pointer dereference at dbs_check_cpu
    
    Function dbs_check_cpu  calls __cpufreq_driver_getavg
    With  policy as IN/OUT parameter.
    There is possiblity of policy becoming NULL in
     __cpufreq_driver_getavg. Same policy is getting
    accessed in the for loop which leads to NULL pointer dereference.
    So NULL check is required for the policy
    
    CRs-fixed: 582925
    Change-Id: Ib8d8de8d66500430a5aa4c275937640e4f934aa8
    Signed-off-by: Swetha Chikkaboraiah <schikk@codeaurora.org>

commit d8a60831c174f4610294b1c1ddc0c5e24bee04a4
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Thu Nov 21 14:51:07 2013 -0800

    cpufreq: Add a sync limit to cpu-boost
    
    Perform frequency synchronization only when source CPU's frequency
    is less than sync_threshold, else sync to the sync_threshold.
    
    Change-Id: I544c414568d4e015b80ce5891dd215275bac95da
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit 77a03ccb4d4d3fb7b89a873304b435d5cf854faf
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Sat Jul 13 01:49:09 2013 -0700

    cpufreq: Add cpu-boost driver
    
    When certain bursty and important events take place, it might take a while
    for the current cpufreq governor to notice the new load and react to it.
    That would result in poor user experience. To alleviate this, the cpu-boost
    driver boosts the frequency of a CPU for a short duration to maintain good
    user experience while the governor catches up.
    
    Specifically, this commit deals with ensuring that when "important" tasks
    migrate from a fast CPU to a slow CPU, the frequency of the slow CPU is
    boosted to be at least as high as the fast CPU for a short duration.
    
    Since this driver enforces the boost by hooking into standard cpufreq
    ADJUST notifiers, it has several advantages:
    - More portable across kernel versions where the cpufreq internals might
      have been rewritten.
    - Governor agnostic and hence works with multiple governors like
      conservative, ondemand, interactive, etc.
    - Does not affect the sampling period/logic of existing governors.
    - Can have the boost period adjusted independent of governor sampling
      period.
    
    Change-Id: Ibd814a20043d0aba64ee7637a4a79b9ffa1b0991
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 7f0ef867eb8ef1c50056965094563a0e8b1e6855
Author: Srikanth Uyyala <suyyala@codeaurora.org>
Date:   Wed Jul 17 18:28:08 2013 +0530

    avtimer: kernel driver to expose avtimer to userspace
    
    This driver expose the API to get LPASS subsystem
    out of power collapsed state and exposes ioctl control to
    read avtimer tick.
    
    Change-Id: I4b50fbfca5741449a8e5f0d0514f5ca7bccec04d
    Signed-off-by: Srikanth Uyyala <suyyala@codeaurora.org>

commit bf95890014ed39d79b8b0e5e91cc9b78bc1a28ef
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Mon Dec 9 10:52:49 2013 +0530

    msm: camera: Added bounds check for index parameter
    
    Bound check the index param from user space to avoid
    any invalid memory access.
    
    CRs-Fixed: 583366
    
    Change-Id: I0f887bb8f1fa5a69a55e23dbb522b3bb694ad27f
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit fb7e5653bd743a16e9d1e04b7cdb91ba7e573c1c
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Thu Oct 10 11:00:50 2013 +0530

    msm:camera: Copy data from user to kernel space before access
    
    This would avoid any invalid access of data.
    
    CRs-fixed: 550608
    
    Change-Id: I30993a9bc731227e72f3be34874505f01357f04d
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit 28fb59c8c1679a31773782fc6587416731f37ab7
Author: Katta Santhisindhu <kattas@codeaurora.org>
Date:   Wed Oct 9 15:07:40 2013 +0530

    msm-camera : Fix for Unbounded copy in msm_camera_v4l2_private_control
    
    Fix for the Unbounded copy in msm_camera_v4l2_private_control function
    
    CRs-Fixed: 545962
    
    Change-Id: I36b3cb3193610a090f3fc87273bc71317e7029bd
    Signed-off-by: Katta Santhisindhu <kattas@codeaurora.org>

commit b9a81685931e08824689b67653e71fbff364b293
Author: Pawan Kumar <pavaku@codeaurora.org>
Date:   Thu Mar 21 19:55:49 2013 +0530

    msm_fb: Fix color format mapping between V4L2 and MDP
    
    Add V4L2_PIX_FMT_UYVY, V4L2_PIX_FMT_YUYV, V4L2_PIX_FMT_YVU420
    format mapping in video driver. Add support  for
    MDP_CBYCRY_H2V1 format for mdp4.
    
    Change-Id: I4d5f1ae4ea71aa57c168f7143ff15f79e27cc1d0
    Signed-off-by: Pawan Kumar <pavaku@codeaurora.org>

commit c20f832ba0eb9c0d34ecfd13098327961c041a08
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Thu Oct 10 12:11:01 2013 +0530

    msm:camera: Bounds and validity check for params
    
    Check the range and validity of parameters before accessing.
    
    CRs-fixed: 550607, 554434, 554436
    
    Change-Id: I2d6aec4f9cb9385789c0df6a2c4abefe9e87539f
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit 49ff418a69190f4fd07555e7ac8498738d8d3835
Author: Vivek Singh <vising@codeaurora.org>
Date:   Thu Oct 10 13:02:37 2013 -0700

    msm:camera: Initialize the structure before to pass userspace.
    
    Initialize the structure by memset so that value will pass to
    proper to usersapce.
    
    CRs-fixed: 554845
    Change-Id: I4adf4bf1bb6e45e16f5fd2e26a74232e6ee7f8bd
    Signed-off-by:Vivek Singh<vising@codeaurora.org>

commit 6748304b9d49a7cb54274cd621c0a8d6ed91d8ac
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Wed Oct 9 16:27:05 2013 +0530

    msm:camera: Initialize a structure before usage
    
    Initialize a structure using memset such that it is
    populated properly before returning to userspace.
    
    CRs-fixed: 550567
    Change-Id: I2b3c78cd2b309de13acf2811fcc16b8bc04fa052
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit 71d912542a2c972e25934955c22a3237934b954e
Author: Ethan Chen <intervigil@gmail.com>
Date:   Mon Sep 30 13:12:07 2013 -0700

    msm: camera: ifdef AVTimer APIs

commit 4ccf4d28ab30bdb5dfbc704184b2d1bf8010cadb
Author: Katta Santhisindhu <kattas@codeaurora.org>
Date:   Mon Sep 16 13:32:06 2013 +0530

    msm-camera: Bound check num_cid from userspace in csid driver
    
    Upper and lower bound checks are enforced for num_cid
    which is passed from userspace with lower as 1 and
    max of 16.
    
    Change-Id: I69a405995d2f53c3a0b0a72b50460bebfdf38442
    CRs-Fixed: 511358
    Signed-off-by: Katta Santhisindhu <kattas@codeaurora.org>

commit 632fc7bd17327d7851d231189984f0d5fa471b36
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Wed Jul 31 10:28:38 2013 +0530

    msm: camera: Enable AVTimer related APIs
    
    Uncomment the code related to using AVTimer APIs to retrieve
    TimeStamp information from QDSP.
    
    CRs-fixed: 498762
    Change-Id: I727f0d4219da2e9ccd5e88d18d1041a82abbb4cc
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit 758289879b9daf3cd2a69efeef5e6125180177ff
Author: Manoj Rao <manojraj@codeaurora.org>
Date:   Tue May 7 21:56:28 2013 -0700

    msm: camera: prevent invalid access through mmap
    
    Disable access to invalid addresses through mmap.
    Add appropriate range checks to ensure valid accesses
    through framebuffer mmap. This prevents illegal
    access into memory.
    
    Change-Id: Ie053d9d8efe951b88f28b020dd9c2356e54723a7
    CRs-Fixed: 475794
    Signed-off-by: Manoj Rao <manojraj@codeaurora.org>

commit 70ead1f2b59a59f0df71f6794558facb0f7db4c3
Author: Katta Santhisindhu <kattas@codeaurora.org>
Date:   Fri Jul 26 14:41:22 2013 +0530

    msm-camera: Added the  NULL check in gemini driver.
    
    Adding NULL check for ionhandle and y_off in gemini driver files.
    
    CRs-Fixed: 504159
    
    Change-Id: I6d20298183bec0c3c2c16088a936ac6f7d307da4
    Signed-off-by: Katta Santhisindhu <kattas@codeaurora.org>

commit 0f3829d1470d85ef3948ba32da7b5a983f5655b2
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Thu Jul 25 10:47:30 2013 +0530

    msm: camera: Guard the release of resources using mutex
    
    The critical section of the code related to release needs to be
    protected against race conditions.
    
    Change-Id: I2165d37168882c345e2f845df08b74eb49b3cb23
    CRs-fixed: 518302
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit 5b85588b470a281e7c17910c12c2806d5271b64b
Author: manish gotefode <mgote@codeaurora.org>
Date:   Fri Jun 28 16:06:08 2013 +0530

    msm: camera: Fix for kernel panic
    
    check if buffer instance handle is zero while
    getting pcam instance based on instance handle
    in msm_mctl_reserve_free_buf function
    
    Change-Id: Id3861c4f29fb4f7fd62ad6de6597e65c7c4f656b
    Signed-off-by: manish gotefode <mgote@codeaurora.org>

commit 120a7f08dedfd504473d6b70040eda17580b79dd
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Mon Jul 8 17:17:14 2013 +0530

    msm: camera: Check for open camera instances before release
    
    Release the subdevs and QOS variables only if there are active
    camera instances
    
    Change-Id: If8f6eb80cf7bd21e36de988ad6c4df7441d655ae
    CRs-fixed: 490249
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit 5585a73d7bd2c4e97c2a4ceb8d06bf38573680fb
Author: Monika Alekhya <malekh@codeaurora.org>
Date:   Fri Jun 28 18:23:40 2013 +0530

     msm:camera: Fix overflow issue in ioctl_hw_cmds function
    
        'len' is of type signed int 32bit,but the assigned value
        may exceed maximum unsigned int32 range.Add overflow check
        and graceful exit if 'm'exceeds UINT32_MAX value.
    
    Change-Id: I38f0d10a0cb44d08d0054f91044fc891c246ebd1
    CRs-Fixed: 493314
    Signed-off-by: Monika Alekhya <malekh@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit fb9011fb8e88b6f67ab94ce44d03f751dcc7b4c0
Author: Lakshmi Narayana Kalavala <lkalaval@codeaurora.org>
Date:   Wed Jun 12 10:36:53 2013 -0700

    msm:camera: Increase the v4l2 event queue length
    
    Observed missing of v4l2 events to userspace media controller,
    this can happen if media controller is blocked in post processing
    the frames. Hence increasing the queue length gives enough time
    to accomodate all the events without being overwritten with
    newer events.
    
    Change-Id: Ic3b5fd29df8c6384e83312b9c9ffd788bdf60951
    Signed-off-by: Lakshmi Narayana Kalavala <lkalaval@codeaurora.org>

commit 29d21182482e3ee20266b4e1bc31c6413e59b7a4
Author: Kiran Kumar H N <hurlisal@codeaurora.org>
Date:   Wed Jun 12 10:15:12 2013 +0530

    msm: gemini: Fix the size argument of hw_region_dump function.
    
    The size argument for the msm_gemini_hw_region_dump function is
    signed which is not correct, since the function operates on
    it as if it is unsigned. Change this to unsigned parameter.
    
    Change-Id: Icea70ad20b010651047b911e7b6ae57889e610e7
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>

commit 1a7bea267fd02ce803e1c0c99c2220b94d27df95
Author: Sunid Wilson <sunidw@codeaurora.org>
Date:   Mon Apr 22 17:58:53 2013 -0700

    msm: gemini: Reset the core incase of overflow error
    
    Reset the gemini core for the overflow usecases.
    
    Change-Id: I64b4283806d67615b2963e0d6a42f04a40d6a892
    Signed-off-by: Sunid Wilson <sunidw@codeaurora.org>

commit e3f3ebd2fcc63b47fda7c4730e2076d3cfd6eb6c
Author: Aditya Jonnalagadda <ajonnala@codeaurora.org>
Date:   Tue Apr 23 10:26:29 2013 +0530

    msm: camera: Handle incorrect parameter properly
    
    By this change, when incorrect size is passed, we
    return back rather than proceeding forward.
    
    CRs-Fixed: 477936
    
    Change-Id: I8f3ea3c7680a468e36ed0cdb026e95cc29e3f4be
    Signed-off-by: Aditya Jonnalagadda <ajonnala@codeaurora.org>

commit 025eada4d1d559a15e37f1f4a5957100b03350b4
Author: Monika Alekhya <malekh@codeaurora.org>
Date:   Tue Jun 11 19:32:27 2013 +0530

    msm:camera: Fix signedness issue in hw_exec_cmds
    
      In hw_exec_cmds()second argument m_cmds should be
      of type unsigned interger
    
    Change-Id: Idad2eb1a59481f3fe9f90221ff2061e8dae57013
    CRs-Fixed: 493314
    Signed-off-by: Monika Alekhya <malekh@codeaurora.org>

commit 022eaf905026807c979cf15ba8a3fe7da36aaa5a
Author: Kiran Kumar H N <hurlisal@codeaurora.org>
Date:   Sat Jun 8 18:53:00 2013 +0530

    msm: camera: Return from DQBUF ioctl in case of error.
    
    Incase the vb2_dqbuf call returns error, we are continuing
    ahead and trying to access the fields of the vb2 buffer which
    may not be initialized yet. Avoid this by checking the
    return value of vb2_dqbuf and return immediately in case of
    errors.
    
    Change-Id: I33036fe9b3bd7ce2b28e9cd6e4cd5f293425be95
    CRs-Fixed: 495635
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>

commit 029cfa6ce00fa010578a29a650b78a470519f7e3
Author: Krupal Divvela <kdivvela@codeaurora.org>
Date:   Thu Apr 18 10:30:31 2013 +0530

    msm: camera: ABCC feature porting in kernel space
    
    Reference - https://review-android.quicinc.com/#/c/141518/
    CRs-fixed: 502029
    
    Change-Id: I40ee7349f26cc9d040a9b36cf8cf30020991c381
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit 226488d2491e18a8b283a4e013f4de2479a9882c
Author: Sai Kumar Sanagavarapu <ssanagav@codeaurora.org>
Date:   Mon May 13 16:36:01 2013 +0530

    msm: camera: Enable CAMIF SOF in case of non dual camera usecase.
    
    When Dual camera usecase is enabled, the ISPIF PIX SOF is enabled
    by default to synchronize with the RDI interfaces.
    But in non dual camera usecases, use CAMIF SOF IRQ to increment
    the frame IDs being embedded inside the output frames.
    Sometimes the SOF IRQs of frame 'n' is clubbed along with the
    Output IRQs of the frame 'n-1'. Thus although the output IRQs
    are supposed for frame 'n-1' it results in a frame ID of 'n' which
    will cause incorrect behaviour downstream. Correct this behaviour
    by checking the SOF and Output status bits in the IRQ Mask and
    adjust the frame ID accordingly.
    
    CRs-Fixed: 481721
    Change-Id: If7a9db2337febf10e89d44c58b9d980d874c69b6
    Signed-off-by: Sai Kumar Sanagavarapu <ssanagav@codeaurora.org>
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>

commit 0a8f6a12e8558d45b1877d625b88f6aaceeff00a
Author: Sai Kumar Sanagavarapu <ssanagav@codeaurora.org>
Date:   Mon Jun 17 16:52:41 2013 +0530

    msm: camera: Enable frame based AXI WM for RDI0
    
    Frame based AXI WM configuration for RDI0 path
    Also populate sensor type in the device info structure.
    
    Change-Id: Ib6d65cd8cb1e4655c4f67903a8f329962512115e
    Signed-off-by: Sai Kumar Sanagavarapu <ssanagav@codeaurora.org>
    
    Conflicts:
    	drivers/media/video/msm/vfe/msm_vfe32.c

commit 6c16b25b8fd6aef8dac5184f93f30b5c344023c1
Author: Aditya Jonnalagadda <ajonnala@codeaurora.org>
Date:   Fri Jun 21 13:01:48 2013 +0530

    msm: camera: Null check added at iommu_attach_device
    
    In case domain has not been initialized before, added a null
    check for it inorder to avoid a potential kernel panic.
    
    CRs-Fixed: 502383
    Change-Id: I43a173e96415bdaea0968ce6ca37e6c1ee75d666
    Signed-off-by: Aditya Jonnalagadda <ajonnala@codeaurora.org>

commit ac6fa2d3e2fc6b1c1ccbf253edb2e40206861e58
Author: Aditya Jonnalagadda <ajonnala@codeaurora.org>
Date:   Fri Jun 21 11:06:11 2013 +0530

    msm: camera: Handle ioctls issued on vfe subdevice properly
    
    In case vfebase is not set by the time ioctl is issued on the
    vfe subdevice, it might lead to kernel panic if the arguments
    of the ioctl are not set properly. Now handling these ioctls
    before checking for vfebase will remove the possibility of a
    kernel panic.
    
    CRs-Fixed: 502383
    Change-Id: Ib36db26c58acbaef14f1052aed71b7c3701e93a3
    Signed-off-by: Aditya Jonnalagadda <ajonnala@codeaurora.org>

commit 4551f63c76472ce2c0d131c1011733af63f52eb5
Author: Roja Rani Yarubandi <rojay@codeaurora.org>
Date:   Tue Jun 11 17:29:43 2013 +0530

    msm: camera: Add mechanism to stop overflow recovery and handle IOMMU pagefault
    
    CRs-Fixed: 491200
    
    Change-Id: I1e498d87853c4dee8b1821595aad1958cb5341d3
    Signed-off-by: Roja Rani Yarubandi <rojay@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit 7ea3dba30f6945cf3bd8c8202a8456876fec550c
Author: Kevin Chan <ktchan@codeaurora.org>
Date:   Fri Apr 26 08:38:56 2013 -0700

    msm: camera: Move halt logic in case of overflow to ISR.
    
    Move the halt logic which includes AXI HALT and CAMIF Disable
    to ISR to start the recovery mechanism sooner.
    Add some protection in case of liveshot.
    
    Change-Id: I28ec462474524c0064cd7ecd4f77801cd494e514
    Signed-off-by: Kevin Chan <ktchan@codeaurora.org>
    Signed-off-by: Praveen Ac <praveenac@codeaurora.org>
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>

commit 56e91d572a717f131db2725a1d93de0ace566c6a
Author: Kevin Chan <ktchan@codeaurora.org>
Date:   Thu Apr 25 01:33:44 2013 -0700

    msm: camera: Add recovery routine for live snapshot.
    
    During live snapshot, if bus overflow happen, recovery routine will
    kick in and reset the hardware. At this point, the live snapshot
    might not be completed and application will continue to wait for
    the snapshot to complete. This change adds the logic to restart
    the snapshot process after recovery is completed.
    
    Change-Id: I9ba0a27ce53eb008a4af8d65a3f37a7cded03559
    Signed-off-by: Kevin Chan <ktchan@codeaurora.org>
    Signed-off-by: Praveen Ac <praveenac@codeaurora.org>
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>

commit cee00f794b44bcc92a6f6029907a256186b0f959
Author: Kevin Chan <ktchan@codeaurora.org>
Date:   Fri Apr 19 22:29:55 2013 -0700

    msm: camera: Recovery through camif stop and overall reset.
    
    Add recovery mechanism in case of bus overflow. As part of the
    recovery, stop the CAMIF, reset the VFE and reload the AXI
    write masters and then restore the IRQ settings to VFE.
    
    Change-Id: Iba6576777f05135cc13c1ee0c9db764d35c7282a
    Signed-off-by: Kevin Chan <ktchan@codeaurora.org>
    Signed-off-by: Praveen Ac <praveenac@codeaurora.org>
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>

commit 8bb3670fd70544a3151708bdae815ac3034ed76c
Author: Alok Kediya <kediya@codeaurora.org>
Date:   Fri May 31 08:50:51 2013 +0530

    msm: camera: Enable use of AVTimer, Rotation and Crop for VT
    
    The changes expose parameters for the application to set
    the use of AVTimer for TimeStamp information and rotation
    angle for video stream along with crop for VT use case.
    
    CRs-fixed: 498762
    Change-Id: Icf883f53513ab084f0970e84bbccd8bb9ec64737
    Signed-off-by: Alok Kediya <kediya@codeaurora.org>

commit ee0be818ec7c70b15c6d81493990272617a0ec88
Author: Lakshmi Narayana Kalavala <lkalaval@codeaurora.org>
Date:   Tue Apr 2 15:00:27 2013 -0700

    msm: camera: fix the out of order of frames issue
    
    observed that if the frame processing in the userspace
    is delayed, the incoming frames from the hardware are being sent
    to user before the diverted frame returns.so dropping the
    the delayed postprocessing frame to prevent out of order
    
    Change-Id: I43b0f9ea7a7683a419ce142b7d4fdf9e13d11d95
    Signed-off-by: Lakshmi Narayana Kalavala <lkalaval@codeaurora.org>

commit 34e06bd6032093e412576a903e13b4eb0199f7ac
Author: satyavaraprasad yerramsetti <satyav@codeaurora.org>
Date:   Fri May 24 12:14:32 2013 +0530

    msm: camera: Handle locks properly
    
    Update camera drivers to unlock mutex while returning
    the function in caseof errors.
    
    Change-Id: Iaac7ae3a0337445ee072c10c69080c24ed6e512c
    Signed-off-by: satyavaraprasad yerramsetti <satyav@codeaurora.org>

commit e81c074231ec33e2e24a43b9210922a4ae9d71ba
Author: Sai Kumar Sanagavarapu <ssanagav@codeaurora.org>
Date:   Wed Jun 12 11:54:01 2013 +0530

    msm: camera: Send VFE_REG_UPDATE_CMD while enabling liveshot.
    
    Sometimes, especially with YUV sensors, snapshot frame is
    not received in time when liveshot is triggered.
    
    CRs-Fixed: 501466
    Change-Id: Ib92929ebe82eadb92492d899b2077832fab02842
    
    Signed-off-by: Sai Kumar Sanagavarapu <ssanagav@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit ca666045f69c3f01dd62cffafc6f8bb69663aca4
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:47 2012 -0700

    mm: methods for teaching filesystems about PG_swapcache pages
    
    In order to teach filesystems to handle swap cache pages, three new page
    functions are introduced:
    
      pgoff_t page_file_index(struct page *);
      loff_t page_file_offset(struct page *);
      struct address_space *page_file_mapping(struct page *);
    
    page_file_index() - gives the offset of this page in the file in
    PAGE_CACHE_SIZE blocks.  Like page->index is for mapped pages, this
    function also gives the correct index for PG_swapcache pages.
    
    page_file_offset() - uses page_file_index(), so that it will give the
    expected result, even for PG_swapcache pages.
    
    page_file_mapping() - gives the mapping backing the actual page; that is
    for swap cache pages it will give swap_file->f_mapping.
    
    Change-Id: I13d18bb25be606760eac26cc842eb7c9fc9e4766
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f981c5950fa85916ba49bea5d9a7a5078f47e569
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [ohaugan@codeaurora.org: Resolved merge issues]
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 56c06d851699fb9302dd2871350b1a850ab5f507
Author: Subbaraman Narayanamurthy <subbaram@codeaurora.org>
Date:   Fri Sep 20 15:23:56 2013 -0700

    debug-pagealloc: Panic on pagealloc corruption
    
    Currently, we just print the pagealloc corruption warnings and
    proceed. Sometimes, we are getting multiple errors printed down
    the line. It will be good to get the device state as early as
    possible when we get the first pagealloc error.
    
    Change-Id: I79155ac8a039b30a3a98d5dd1384d3923082712f
    Signed-off-by: Subbaraman Narayanamurthy <subbaram@codeaurora.org>

commit 1cd081e2fa251c0761cf47153981cd432da70272
Author: Pushkar Joshi <pushkarj@codeaurora.org>
Date:   Wed Sep 11 11:23:26 2013 -0700

    mm: panic on the first bad page table entry access
    
    Sometimes having a number of bad page table entries precipitates in
    a crash much later. Because of this, we do not have any context for
    the point at which the first bad pte entry was encountered. Hence,
    panic on first such instance to help gather context for debug.
    
    Change-Id: Idddf2b977214eb1463d08e16630e98264b9af487
    Signed-off-by: Pushkar Joshi <pushkarj@codeaurora.org>

commit 618cc98da7609b6e396ba7e144b65352bf159bc0
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jan 23 21:45:48 2013 +0000

    slub: tid must be retrieved from the percpu area of the current processor
    
    As Steven Rostedt has pointer out: rescheduling could occur on a
    different processor after the determination of the per cpu pointer and
    before the tid is retrieved. This could result in allocation from the
    wrong node in slab_alloc().
    
    The effect is much more severe in slab_free() where we could free to the
    freelist of the wrong page.
    
    The window for something like that occurring is pretty small but it is
    possible.
    
    Change-Id: I9e658e97ed2096f658d6e67739764a86151f13e3
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Git-commit: 7cccd80b4397699902aced1ad3d692d384aaab77
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit db7de7c3b9caa81fe42c15406a007ca1a7d54469
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed Jun 5 10:13:11 2013 +0200

    sched: Fix clear NOHZ_BALANCE_KICK
    
    I have faced a sequence where the Idle Load Balance was sometime not
    triggered for a while on my platform, in the following scenario:
    
     CPU 0 and CPU 1 are running tasks and CPU 2 is idle
    
     CPU 1 kicks the Idle Load Balance
     CPU 1 selects CPU 2 as the new Idle Load Balancer
     CPU 2 sets NOHZ_BALANCE_KICK for CPU 2
     CPU 2 sends a reschedule IPI to CPU 2
    
     While CPU 3 wakes up, CPU 0 or CPU 1 migrates a waking up task A on CPU 2
    
     CPU 2 finally wakes up, runs task A and discards the Idle Load Balance
           task A quickly goes back to sleep (before a tick occurs on CPU 2)
     CPU 2 goes back to idle with NOHZ_BALANCE_KICK set
    
    Whenever CPU 2 will be selected as the ILB, no reschedule IPI will be sent
    because NOHZ_BALANCE_KICK is already set and no Idle Load Balance will be
    performed.
    
    We must wait for the sched softirq to be raised on CPU 2 thanks to another
    part the kernel to come back to clear NOHZ_BALANCE_KICK.
    
    The proposed solution clears NOHZ_BALANCE_KICK in schedule_ipi if
    we can't raise the sched_softirq for the Idle Load Balance.
    
    Change since V1:
    
    - move the clear of NOHZ_BALANCE_KICK in got_nohz_idle_kick if the ILB
      can't run on this CPU (as suggested by Peter)
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1370419991-13870-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Git-commit: 873b4c65b519fd769940eb281f77848227d4e5c1
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [smuckle@codeaurora.org: minor merge resolution for 3.4 in scheduler_ipi()]
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>
    Change-Id: I3548612057cccc2ecc29429c129c44183083831f

commit 4cb7603ea35d588ac95d8bb237b4ce9dc616cc4d
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Tue Nov 19 14:16:53 2013 -0800

    tracing/sched: add load balancer tracepoint
    
    When doing performance analysis it can be useful to see exactly
    what is going on with the load balancer - when it runs and why
    exactly it may not be redistributing load.
    
    This additional tracepoint will show the idle context of the
    load balance operation (idle, not idle, newly idle), various
    values from the load balancing operation, the final result,
    and the new balance interval.
    
    Change-Id: I9e5c97ae3878bea44e60d189ff3cec2275f2c75e
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit e79fa2819e835100371bd13b81a23500a4132680
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Oct 9 22:23:23 2013 -0400

    tracing: Fix potential out-of-bounds in trace_get_user()
    
    commit 057db8488b53d5e4faa0cedb2f39d4ae75dfbdbb upstream.
    
    Andrey reported the following report:
    
    ERROR: AddressSanitizer: heap-buffer-overflow on address ffff8800359c99f3
    ffff8800359c99f3 is located 0 bytes to the right of 243-byte region [ffff8800359c9900, ffff8800359c99f3)
    Accessed by thread T13003:
      #0 ffffffff810dd2da (asan_report_error+0x32a/0x440)
      #1 ffffffff810dc6b0 (asan_check_region+0x30/0x40)
      #2 ffffffff810dd4d3 (__tsan_write1+0x13/0x20)
      #3 ffffffff811cd19e (ftrace_regex_release+0x1be/0x260)
      #4 ffffffff812a1065 (__fput+0x155/0x360)
      #5 ffffffff812a12de (____fput+0x1e/0x30)
      #6 ffffffff8111708d (task_work_run+0x10d/0x140)
      #7 ffffffff810ea043 (do_exit+0x433/0x11f0)
      #8 ffffffff810eaee4 (do_group_exit+0x84/0x130)
      #9 ffffffff810eafb1 (SyS_exit_group+0x21/0x30)
      #10 ffffffff81928782 (system_call_fastpath+0x16/0x1b)
    
    Allocated by thread T5167:
      #0 ffffffff810dc778 (asan_slab_alloc+0x48/0xc0)
      #1 ffffffff8128337c (__kmalloc+0xbc/0x500)
      #2 ffffffff811d9d54 (trace_parser_get_init+0x34/0x90)
      #3 ffffffff811cd7b3 (ftrace_regex_open+0x83/0x2e0)
      #4 ffffffff811cda7d (ftrace_filter_open+0x2d/0x40)
      #5 ffffffff8129b4ff (do_dentry_open+0x32f/0x430)
      #6 ffffffff8129b668 (finish_open+0x68/0xa0)
      #7 ffffffff812b66ac (do_last+0xb8c/0x1710)
      #8 ffffffff812b7350 (path_openat+0x120/0xb50)
      #9 ffffffff812b8884 (do_filp_open+0x54/0xb0)
      #10 ffffffff8129d36c (do_sys_open+0x1ac/0x2c0)
      #11 ffffffff8129d4b7 (SyS_open+0x37/0x50)
      #12 ffffffff81928782 (system_call_fastpath+0x16/0x1b)
    
    Shadow bytes around the buggy address:
      ffff8800359c9700: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
      ffff8800359c9780: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa
      ffff8800359c9800: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
      ffff8800359c9880: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
      ffff8800359c9900: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    =>ffff8800359c9980: 00 00 00 00 00 00 00 00 00 00 00 00 00 00[03]fb
      ffff8800359c9a00: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
      ffff8800359c9a80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
      ffff8800359c9b00: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 00 00
      ffff8800359c9b80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
      ffff8800359c9c00: 00 00 00 00 00 00 00 00 fa fa fa fa fa fa fa fa
    Shadow byte legend (one shadow byte represents 8 application bytes):
      Addressable:           00
      Partially addressable: 01 02 03 04 05 06 07
      Heap redzone:          fa
      Heap kmalloc redzone:  fb
      Freed heap region:     fd
      Shadow gap:            fe
    
    The out-of-bounds access happens on 'parser->buffer[parser->idx] = 0;'
    
    Although the crash happened in ftrace_regex_open() the real bug
    occurred in trace_get_user() where there's an incrementation to
    parser->idx without a check against the size. The way it is triggered
    is if userspace sends in 128 characters (EVENT_BUF_SIZE + 1), the loop
    that reads the last character stores it and then breaks out because
    there is no more characters. Then the last character is read to determine
    what to do next, and the index is incremented without checking size.
    
    Then the caller of trace_get_user() usually nulls out the last character
    with a zero, but since the index is equal to the size, it writes a nul
    character after the allocated space, which can corrupt memory.
    
    Luckily, only root user has write access to this file.
    
    Link: http://lkml.kernel.org/r/20131009222323.04fd1a0d@gandalf.local.home
    
    Reported-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 981614a2e6df356268c812b643a3d3e36b538a8b
Author: Colin Cross <ccross@android.com>
Date:   Wed Jun 26 17:26:01 2013 -0700

    mm: add a field to store names for private anonymous memory
    
    Userspace processes often have multiple allocators that each do
    anonymous mmaps to get memory.  When examining memory usage of
    individual processes or systems as a whole, it is useful to be
    able to break down the various heaps that were allocated by
    each layer and examine their size, RSS, and physical memory
    usage.
    
    This patch adds a user pointer to the shared union in
    vm_area_struct that points to a null terminated string inside
    the user process containing a name for the vma.  vmas that
    point to the same address will be merged, but vmas that
    point to equivalent strings at different addresses will
    not be merged.
    
    Userspace can set the name for a region of memory by calling
    prctl(PR_SET_VMA, PR_SET_VMA_ANON_NAME, start, len, (unsigned long)name);
    Setting the name to NULL clears it.
    
    The names of named anonymous vmas are shown in /proc/pid/maps
    as [anon:<name>] and in /proc/pid/smaps in a new "Name" field
    that is only present for named vmas.  If the userspace pointer
    is no longer valid all or part of the name will be replaced
    with "<fault>".
    
    The idea to store a userspace pointer to reduce the complexity
    within mm (at the expense of the complexity of reading
    /proc/pid/mem) came from Dave Hansen.  This results in no
    runtime overhead in the mm subsystem other than comparing
    the anon_name pointers when considering vma merging.  The pointer
    is stored in a union with fieds that are only used on file-backed
    mappings, so it does not increase memory usage.
    
    Change-Id: Ie2ffc0967d4ffe7ee4c70781313c7b00cf7e3092
    Signed-off-by: Colin Cross <ccross@android.com>

commit 1c31c9bceca12853025bebf7b05e79aecfe347a6
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Sep 23 15:19:49 2013 -0700

    smp: Relax irqs_disable() warning in smp_call_function_single()
    
    The change that long-ago added the interrupts-disabled warning in
    smp_call_function_single() cited the following deadlock as the reason:
    	CPU A				CPU B
    	Disable interrupts
    					smp_call_function()
    					Take call_lock
    					Send IPIs
    					Wait for all cpus to acknowledge IPI
    					CPU A has not responded, spin waiting
    					for cpu A to respond, holding call_lock
    	smp_call_function()
    	Spin waiting for call_lock
    	Deadlock			Deadlock
    
    Such a deadlock is not possible however, in the event that
    smp_call_function_single() is called for the currently-executing CPU.
    While not a common usecase, relaxing the warning may simplify some
    driver implementations.
    
    The 'acpuclock-krait' driver in the MSM ARM sub-architecture provides
    one example, where smp_call_function_single() is sometimes called from
    a cpuidle path with interrupts disabled, but only ever from the target
    CPU. Other callers of that same function may cross-call, but only do so
    when interrupts are enabled.
    
    Change-Id: I647b3b46fe6aa4dfb06f7750396986b80b92d700
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 730753cac88c4f84ad170204662b4c6654ceeae0
Author: Srinivasarao P <spathi@codeaurora.org>
Date:   Thu Dec 5 15:29:35 2013 +0530

    Revert "[ARM] msm: iommu_domains: replace vmalloc with kmalloc"
    
    When overmapping is used, system can request higher order page allocaitons.
    this can lead to page allocation failure if kmalloc is used
    
    CRs-Fixed: 571753
    
    This reverts commit 191a48ee7bda08bf88a61b48fd0433627c5918dc.
    
    Change-Id: I4d44bf88d84d96d315f89a1304a2d25b75be79d8
    Signed-off-by: Srinivasarao P <spathi@codeaurora.org>

commit 08cd490b9939daad38a4d8bb40f5ea28059dca67
Author: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>
Date:   Tue Oct 22 11:58:08 2013 +0530

    msm: msm_fb: Fix Regressions because of overmap changes.
    
    Previous MDP overmap change is causing issues with HDMI because of
    assumptions about overmap. This patch fixes all such issues.
    
    Change-Id: Ifd2f8a0596d9df08538464bdcdd5946cf84db4d9
    Signed-off-by: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>

commit 975265e7a628dd88acc214b8d8dd1e5f49b69d04
Author: Justin Philip <jphili@codeaurora.org>
Date:   Mon Dec 9 21:42:25 2013 +0530

    msm_fb: display: update video tile framesize register
    
    The register should not be delayed in the case of
    Witeback and command mode pannel
    
    Change-Id: Ia90ccc1bb376ed829bedee131ba600194d471521
    Signed-off-by: Justin Philip <jphili@codeaurora.org>

commit e6a5eb9d3f7b7fa72b4e62fed18e51e7e704ad90
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Thu Oct 10 00:46:38 2013 +0530

    msm_fb: display: Program TILE video frame size in dma ISR
    
    Since, video tile frame size register is single buffered, its
    value takes effect as soon as it is programmed. Single buffered
    registers like this should be configured very close to the vsync
    to prevent issues like flicker or artifacts.
    Program this register in dma ISR to prevent flickers and IOMMU
    page faults.
    
    Change-Id: Ifa770e4b4f29d6e4070e51388043422cc60e5872
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit 9262bd12712cce4fc03ad9281c398caeb00d3317
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Nov 22 15:35:05 2013 +0530

    msm: vidc: Set input buffer size dynamically
    
    This change supports overriding of default input
    buffer size using a set parameter call from the
    client.
    
    Change-Id: I5225ec53ba16b68aeedd4c30897bd291e81b4b09
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 7f257585c73acb244dacc9cec506a2a6bca257b6
Author: Praveen Chavan <pchavan@codeaurora.org>
Date:   Wed Sep 25 19:07:42 2013 -0700

    msm: vidc: estimate min_dpb for smooth streaming mode
    
    In case of smoothstreaming:
    [1] get dpb requirement for the firmware
    [2] get count in resource resource tracker
    Use higher of the above
    
    b/10192531
    
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>
    Signed-off-by: Praveen Chavan <pchavan@codeaurora.org>

commit c6e08e74d5bdedc87262e9fd1807b35667bcf8b8
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Tue Oct 8 16:25:39 2013 -0400

    msm: mdp: Revert changes for delaying commit for tiled video
    
    This reverts commits 3ec662e7079cad581e1b6f13a386059b40fa237a and.
    bb3d5cf6e59c86dc619332fad568430c737e7a45.
    These commits cause corruption with dynamic resolution changes
    
    Bug: 10192531
    Change-Id: I2dcce2fdc358bb0b87885e2ae3f853b4e728b498
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit 30f82c5be3b58aef212133d47b1345db2266e462
Author: Neti, Ravi Kumar <ravineti@codeaurora.org>
Date:   Sat Oct 5 01:01:57 2013 -0700

    msm: fb: Fix security bugs in msmfb_notify_update & msm_fb_ioctl
    
    msmfb_notify_update: Correct signature of msmfb_notify_update from
    the ioctl declaration.
    
    msm_fb_ioctl: Add appropriate null check for mfd data structure
    before accessing it's members. This prevents null pointer accesses
    in the driver.
    
    CRs-Fixed: 526286
    Change-Id: I731018057214fb0fa58c7617934100d2425617f8
    Signed-off-by: Neti, Ravi Kumar <ravineti@codeaurora.org>

commit d585966b5520946504ca3ad5c90a3fd3ab0c2a03
Author: raghavendra ambadas <rambad@codeaurora.org>
Date:   Tue Oct 29 11:26:24 2013 +0530

    msm: msm_fb: Do not use ION_IOMMU_UNMAP_DELAYED flag.
    
    Since iommu buffers will be unmapped after two newer commits
    had passed, there is no need to request iommu driver to further
    delay unmapping an iommu buffer.
    
    Change-Id: I44099054114403fa7ac6a5e3c71a43d6b932f4a3
    Signed-off-by: raghavendra ambadas <rambad@codeaurora.org>

commit 1d36adfa90eadea2e40d8a89d8b77c2e4396e6fa
Author: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>
Date:   Fri Oct 18 21:20:46 2013 +0530

    msm: msm_fb: Map RGB layers to the max possible size.
    
    IOMMU Page Faults are observed on RGB layers during stability
    because MDP is trying to fetch out of mapped area of the buffer.
    This patch avoids the issue by overmapping the RGB layers to the
    max possible RGB buffer size.
    
    Change-Id: Idc7ac9acad70e351acac5dba2ae43b8731e84d35
    Signed-off-by: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>

commit f5d146d317d731fa8c1db391a9175a4801015734
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Wed Sep 25 15:44:27 2013 +0530

    msm: vidc: Use min_dpb from resource tracker in smooth streaming mode
    
    Get min_dpb from resource tracker always if smooth streaming
    mode is enabled to handle number of output buffers properly.
    
    Change-Id: Ic46f76061f3895fda44e1d8e94198f898e5edf9b
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit db7cd9819b0bbdb018e3ceb3b8203da0d4eef49c
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Dec 14 20:24:45 2013 +0100

    New kernel version!

commit 1b84094bd62b0f094eadca4948e79b467c9fd6dd
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Sat Dec 14 14:30:54 2013 +0100

    Modified cpus_boost parameters for my governors. It is possible to use avg cpu load for frequency scaling!

commit 89622f701f599a2a65ac1ca40a4e7975f95a2283
Author: Alucard24 <dmbaoh2@gmail.com>
Date:   Fri Dec 13 23:12:02 2013 +0100

    Added new parameter for my governors!

commit 01b1099b564434eb90bb9708e54270e208636146
Merge: ab0c1b1 a28611c
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Wed Dec 11 20:41:50 2013 +0100

    Merge branch 'my-cm-11.0', remote-tracking branch 'cm-upstream/cm-11.0' into my-cm-11.0

commit ab0c1b15285fb39d1bd048500a55d298e8f95b9a
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 10 21:29:30 2013 +0100

    Fixed config!

commit cdc28dc25b5732f7beb5778640aa0cfb562d97d5
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 10 21:11:21 2013 +0100

    Fixed some files!

commit dce4c87ec1be1aed0eb3829f1a7923ed32a3f10e
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 10 21:03:53 2013 +0100

    Added exfat as module!

commit 2870386d64413b2c66f1b6c060a69a3cdfc08759
Author: ausdim <koronaios@gmail.com>
Date:   Tue Aug 27 18:41:26 2013 +0300

    Add a simple GPU governor for Adreno xxx GPU series (Thanks to faux)
    
    Conflicts:
    	arch/arm/configs/jf_defconfig
    	drivers/gpu/msm/Kconfig
    	drivers/gpu/msm/kgsl_pwrscale_trustzone.c
    
    Conflicts:
    
    	drivers/gpu/msm/kgsl_pwrscale_trustzone.c

commit 8ddf46d4cec9eaf6d8eee5073899c4c9f607a558
Author: ausdim <koronaios@gmail.com>
Date:   Tue Jul 23 11:50:05 2013 +0300

    Add fast charge
    
    Conflicts:
    	arch/arm/configs/jf_defconfig
    	arch/arm/mach-msm/Makefile

commit f078b1e32d5be275eb52dec7b5814882576e44a7
Author: ktoonsez <ktoonsez@gmail.com>
Date:   Sat Sep 14 15:10:39 2013 -0700

    sched: bfq: Update to version 3.4-v6-r2

commit 7a58f0b6da7efc9e2b107fbf6ebb1638bad5689c
Author: ausdim <koronaios@gmail.com>
Date:   Tue Jul 23 03:20:36 2013 +0300

    Enable and added some ioshedulers (thanks to ktoonsez)
    
    Conflicts:
    	arch/arm/configs/jf_defconfig
    	block/Makefile
    	block/elevator.c
    	block/row-iosched.c
    	include/linux/elevator.h

commit f923b6cf25d3fd16dcd220585735c4c43d7323c7
Author: alucard24 <dmbaoh2@gmail.com>
Date:   Tue Dec 10 20:49:56 2013 +0100

    Imported commits from my 4.4 sourcecode!

commit a28611c46005ff1401d0f21749d51e4d21ea1d11
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Tue Dec 10 00:32:49 2013 -0600

    bcmdhd: update to 1.88.51 from gpe 4.4
    
    Change-Id: Ide40296c73e000798e174c9043a4cb8ec6a82639

commit 34b44d5cf2ef7a3442d69712d42bfd547297abeb
Author: Christopher R. Palmer <crpalmer@gmail.com>
Date:   Thu Nov 28 19:35:29 2013 -0500

    msm: We are using the msm2 gpu driver, not msm
    
    Fix references that are incorrectly pointing at drivers/gpu/msm
    instead of msm2.
    
    The incorrect includes cause linaro gcc 4.8 with -O3 to produce
    an unbootable kernel on the DLX.
    
    While it doesn't seem to be affecting our normal builds, not
    fixing it will likely bite us in the future in some other
    mysterious way.
    
    Change-Id: I3c51062ca6a66ddb9782365d2c51c5e485556c1b

commit 7027bba6d45050f37e66f56becf63def78a1ce93
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sun Dec 1 13:40:17 2013 -0600

    fix derps
    
    Change-Id: I327cecbd3159621b7655f51033f14f87589c03ed

commit 350d46e8a0a84673bb05dfa0254afc8b62d8640c
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed May 30 14:18:41 2012 -0600

    driver core: Add iommu_group tracking to struct device
    
    IOMMU groups allow IOMMU drivers to represent DMA visibility
    and isolation of devices.  Multiple devices may be grouped
    together for the purposes of DMA.  Placing a pointer on
    struct device enable easy access for things like streaming
    DMA programming and drivers like VFIO.
    
    Change-Id: I0a92a94aea0ac699676914a033477243a70dd0b0
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit fd0decceb1b50e6a752edcb4a38e7063e10a0d1c
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Apr 9 10:57:54 2013 -0700

    msm: scm: Add scm_call_atomic3
    
    Some drivers need to pass three arguments atomically. Add support
    for it.
    
    Change-Id: I0077d26eeab0e6a3828f994fd4aabc4d73d48b31
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 9fb405661a6edd6be91ed41a6a3f33eb7df9f8da
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri Mar 8 09:03:42 2013 -0800

    ARM: Add support for 64 bit register reads/writes
    
    Add macros to read and write to 64 bit registers.
    
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    
    Conflicts:
    	arch/arm/include/asm/io.h
    
    Change-Id: Ied2b38e6a3803377cd271fe7b13f6cf0fc586e70

commit 72266679641ed1a99e12f7aeef3748816b8591d5
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri Apr 26 13:47:11 2013 -0700

    msm: Fix mem leak when using per-process pages
    
    When GPU is using per process page tables the code creates
    a new page table (domain) for every process. There is also
    some associated structures allocated to keep track of each
    domain. However, when cleaning up the memory used for a page
    table only the domain is freed and not the associated structures.
    
    Add a function to allow a client to properly unregister a domain
    which will free all the memory associated with a domain.
    Call the unregister function from kgsl driver code.
    
    CRs-fixed: 480801
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/iommu_domains.c
    
    Change-Id: I78365cac8afb900450b512527f0cb71da5e9ddb2

commit 73c8716e574473837cbedc74b10ee83fc2f51d66
Author: Ohad Ben-Cohen <ohad@wizery.com>
Date:   Mon May 21 20:20:05 2012 +0300

    iommu/core: pass a user-provided token to fault handlers
    
    Sometimes a single IOMMU user may have to deal with several
    different IOMMU devices (e.g. remoteproc).
    
    When an IOMMU fault happens, such users have to regain their
    context in order to deal with the fault.
    
    Users can't use the private fields of neither the iommu_domain nor
    the IOMMU device, because those are already used by the IOMMU core
    and low level driver (respectively).
    
    This patch just simply allows users to pass a private token (most
    notably their own context pointer) to iommu_set_fault_handler(),
    and then makes sure it is provided back to the users whenever
    an IOMMU fault happens.
    
    The patch also adopts remoteproc to the new fault handling
    interface, but the real functionality using this (recovery of
    remote processors) will only be added later in a subsequent patch
    set.
    
    Cc: Fernando Guzman Lugo <fernando.lugo@ti.com>
    Signed-off-by: Ohad Ben-Cohen <ohad@wizery.com>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    [ohaugan@codeaurora.org: Resolved compilation and merge issues]
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    
    Conflicts:
    	drivers/video/msm/mdss/mdss_mdp.c
    
    Change-Id: I610966aeb936e0d026c6ef35a56453337e1b3c76

commit e9d771eb241c6d36ef36dce9ceb493b9eba4d945
Author: Lucille Sylvester <lsylvest@codeaurora.org>
Date:   Fri Jan 18 17:09:02 2013 -0700

    msm: kgsl: Add intermediate power levels
    
    Make use of the two independent power domains to
    add intermediate levels.  Attempt to just increase
    the bus frequency before also raising the GPU
    frequency.
    
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>
    
    Conflicts:
    	arch/arm/boot/dts/msm8974-gpu.dtsi
    	arch/arm/boot/dts/msm8974-v2.dtsi
    	drivers/gpu/msm/kgsl_pwrctrl.c
    	drivers/gpu/msm/kgsl_pwrctrl.h
    
    Change-Id: Ibe85d1c02941430c5da83a75a09d5bc0ec5f8ec0

commit b90cfb017e30c36bb2dcd02dedb358637a533dc1
Author: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
Date:   Sun Oct 28 20:54:17 2012 -0600

    msm: kgsl: Add separate GPU shader memory mapping
    
    Add separate kernel memory mapping for GPU shader memory.
    Previously, both the register and the shader memory were mapped
    as one entity, with the mapping length being equal to combined
    size of register memory and shader memory. Now, we separate the
    shader memory mapping to help in cases of GPU devices where the
    shader and register memory may not be adjacent. This helps to
    dump the shader memory in the postmortem snapshot. By having a
    separate mapping for shader memory, the snapshot dump routine
    simply reads in the shader memory range specified and dumps.
    
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
    
    Conflicts:
    	drivers/gpu/msm/adreno_a3xx_snapshot.c
    
    Change-Id: Ia51cdbc6a682abe6505bcc53c997ef9c9192f9e6

commit 9a18d8fd18dcd74c72d908186fc8bf3c77943391
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Tue Aug 6 16:42:15 2013 -0600

    kref: Implement kref_get_unless_zero v3
    
    This function is intended to simplify locking around refcounting for
    objects that can be looked up from a lookup structure, and which are
    removed from that lookup structure in the object destructor.
    Operations on such objects require at least a read lock around
    lookup + kref_get, and a write lock around kref_put + remove from lookup
    structure. Furthermore, RCU implementations become extremely tricky.
    With a lookup followed by a kref_get_unless_zero *with return value check*
    locking in the kref_put path can be deferred to the actual removal from
    the lookup structure and RCU lookups become trivial.
    
    v2: Formatting fixes.
    v3: Invert the return value.
    
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Git-commit: 4b20db3de8dab005b07c74161cb041db8c5ff3a7
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [shrenujb@codeaurora.org: resolve trivial merge conflicts]
    Signed-off-by: Shrenuj Bansal <shrenujb@codeaurora.org>
    
    Conflicts:
    	include/linux/kref.h
    
    Change-Id: Ia41a533f964804264a8e6e31ed0a25f383dd11e7

commit 948ae82369f7bbbd57ae6f3b9f9e250d571b7e5d
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Nov 26 15:20:51 2013 -0800

    gpu: msm: Add new Adreno driver
    
    * Temporary place for this.
    
    Change-Id: I83b5d75fbd201c352d011ed43f21ebe3576e058c

commit eaf50b4fce36a8f129f59f6f46dcff5405b351fe
Author: Pachika, Vikas Reddy <vpachi@codeaurora.org>
Date:   Fri Nov 1 21:06:37 2013 +0530

    msm: vidc: Validate userspace buffer count
    
    Makesure the number of buffers count is less than
    the maximum limit to avoid structure overflow errors.
    
    Change-Id: Icf3850de36325637ae43ac95f1c8f0f63e201d31
    CRs-fixed: 563694
    Signed-off-by: Pachika, Vikas Reddy <vpachi@codeaurora.org>

commit 48ce983de6c4aeeab45b8c1bd029914ad30e0718
Author: Pachika, Vikas Reddy <vpachi@codeaurora.org>
Date:   Tue Nov 5 12:48:36 2013 +0530

    msm: vidc: Validate userspace buffer count before using it
    
    Validate the number of buffers count variable before
    using it to avoid structure overflow error.
    
    Change-Id: I61582c93e0f26ec6842e437134fb8a42bdbc36ff
    CRs-fixed: 563654
    Signed-off-by: Pachika, Vikas Reddy <vpachi@codeaurora.org>

commit 8c68bbd750339490863ae1b139d5cde90a6779bb
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Mon Oct 21 17:37:11 2013 +0530

    msm: vidc: Check validity of userspace address
    
    Before writing to a userspace address, verification
    of the validity of user space address is required.
    
    Change-Id: I9141e44a6c11aaf3f4d57c08bb0dd26a7b214f34
    CRs-fixed: 556356
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 327ae230a5b0bb14a56cc9d5ccb0bdcd5926a0f6
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Sep 20 17:02:53 2013 +0530

    msm: vidc: Validate input data before processing
    
    User controlled data needs to be validated before
    processing, otherwise it may corrupt existing data
    and other programming variables which may further
    lead to crash in kernel space.
    This change validates user specified buffer's data,
    such as offset and data length, before performing
    any operations in kernel.
    
    Change-Id: Idb2a5c4c270cdbb55adfc0c22b9296b7dd359de9
    CRs-fixed: 547695
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 510976890ed1e28e8af1e582fe7229129ce0ece4
Author: Pushkaraj Patil <ppatil@codeaurora.org>
Date:   Thu Sep 12 11:00:30 2013 +0530

    msm: vidc: return error in case of init failure
    
    Return error if error comes in video driver
    initialization.
    
    Change-Id: I54c516808587fb7d94c3d517f8c72c6c9aa9d91d
    CRs-fixed: 542535
    Signed-off-by: Pushkaraj Patil <ppatil@codeaurora.org>

commit f67c1c98b7bb5d90de87d727c751bd02ae167772
Author: srikarri <sridur@codeaurora.org>
Date:   Thu Sep 5 15:48:49 2013 +0530

    msm: vidc: free the recon buffers in cleanup function
    
    - The recon buffers were not released when video recording
      was killed abruptly leading to memory leak.
    
    - Fix provided to release the recon buffers when media
      server killed abruptly.
    
    Change-Id: I48ac1a592719446b7e4bf87bbd707d09351d1e61
    Signed-off-by: srikarri <sridur@codeaurora.org>

commit 068be17002f3d54be9c9dec3816b9f006d491a79
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Oct 18 15:17:33 2013 +0530

    msm: vidc: Initialize kernel space stack variables
    
    This change initializes kernel space stack variables
    that are passed between kernel space and user space
    using ioctls.
    Non initialization of these variables may lead to
    leakage of memory values from the kernel stack to
    user space.
    
    Change-Id: Icb195470545ee48b55671ac09798610178e833e1
    CRs-fixed: 556771,563420
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 9af1f4872267e6653ad70c1764f9087ecdcff078
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Wed Oct 23 13:31:19 2013 -0500

    bcmdhd: update to latest samsung source drop (1.61.74)
    
    Change-Id: Ibedc4e05b4d7b5ecc1967782744c28d8b78eec4b

commit 5dc7548c44a7573751985a65967e2f6e2b4c90be
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Nov 5 01:22:26 2013 -0800

    Re-enable earlysuspend for MDP driver
    
     * This has some pretty horrific side effects, like causing
       the modem to crash when resuming while playing audio over
       USB. Seriously, I do not get a break. Ever.
    
    Revert "msm: mdp: Disable early suspend locally"
    
    This reverts commit dbb8932f450b32afe69442a53358041023f71b31.
    
    Revert "msm: mdp: Disable early suspend only for the MDP driver"
    
    This reverts commit 07f284de46bdbd26a56a35a82f1f6110096b6238.
    
    Change-Id: If032f1ed0cc10431ed877f4ce793176f1484977d

commit f50470afca8d808b5ce1666c7c75c7f6a4d0e551
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Nov 3 00:18:40 2013 -0700

    jf: Update defconfig
    
     * Enable all the stuff required for 4.3.
    
    Change-Id: I9558de06c1cecf4e5335927da77b6a630d1cbe08

commit 904faaad4a1133f9624a58cd8fff16c92bf1ec83
Author: Steve Kondik <shade@chemlab.org>
Date:   Thu Oct 24 16:36:32 2013 -0700

    jf: Set "magic" MDP bandwidth values
    
    Change-Id: I5f7674c6aaade015edbf706df2e30b945fa898da

commit 2cfe072f4ff1e25af4ae9dedd0684fd1aced9324
Author: Ashwin Chaugule <ashwinc@codeaurora.org>
Date:   Thu Sep 6 17:49:31 2012 -0400

    Perf: Restore correct CPU's PMU counters after power collpase
    
    Since the L1CC PMU's are per CPU, the variable to detect if a CPU
    came out of powercollapse also needs to be a per CPU variable. This
    ensures that we reset and restore the correct CPU's PMU counters.
    
    Change-Id: I02273df2eff9f6d88d68f46a7752c107b290a8ef
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit dbb47e5de2aa7d71a0c2c6d10bf3bb7dc0901b07
Author: Ashwin Chaugule <ashwinc@codeaurora.org>
Date:   Thu Dec 6 09:56:15 2012 -0500

    Perf: Assign proper routine to free IRQ
    
    If the platform descriptor does not provide
    its own free_irq function, default to the fallback
    version.
    
    Change-Id: I185186bf68b79e2dbfba77b0de66eb4daaf32fb8
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit f9b00d375a7656a7cb39946f817f6c9f81ab5cc9
Author: Ashwin Chaugule <ashwinc@codeaurora.org>
Date:   Tue Nov 27 14:49:58 2012 -0500

    msm: Perf: Differentiate between L1 and L2 PMU IRQs
    
    The L2 perf code shares some resource reservation functions
    with the L1 perf code. In these functions, use defaults
    only if they're not defined for the L1 and L2.
    
    Change-Id: I5a390aa0085694466800db78b51837556aa12cdd
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit b71eebb0a97455c0e9c7daea2341ad077cc6da3a
Author: Colin Cross <ccross@android.com>
Date:   Wed Sep 26 14:21:22 2012 -0700

    timekeeping: fix 32-bit overflow in get_monotonic_boottime
    
    get_monotonic_boottime adds three nanonsecond values stored
    in longs, followed by an s64.  If the long values are all
    close to 1e9 the first three additions can overflow and
    become negative when added to the s64.  Cast the first
    value to s64 so that all additions are 64 bit.
    
    Change-Id: Ic996d8b6fbef0b72f2d027b0d8ef5259b5c1a540
    Signed-off-by: Colin Cross <ccross@android.com>

commit 4a95bba3b939536832ae3ee71200d7b49d7f0f51
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Nov 3 02:54:34 2013 -0800

    arm: unwind: Remove logspam while in debug mode
    
    Change-Id: I87501ca4609a883a99cddd2836c5d036e2cacd06

commit 255d88fce4c7aefcb986c0d0fc782acad89d92cc
Author: Anil kumar mamidala <amami@codeaurora.org>
Date:   Fri Oct 25 14:04:34 2013 +0530

    msm:pm: Fix for race condition of starting cpu1 when cpu0 in PC.
    
    There is a window where core sends ipi to secondary core and
    wait for it to come online for 1 second. This would allow
    core0 to go power collapse in a rare condition where core1
    started booting and doesn't become online yet.
    
    Fix this by not allowing power collapse on core0
    if hot plug operation is in progress.
    
    CRs-fixed: 545714.
    Change-Id: I1ca503a2f09cd9a65c2fdcd41eb54466a1e486c5
    Signed-off-by: Anil kumar mamidala <amami@codeaurora.org>

commit 9453c64b7fe2e159df2d6cc7ce1ca238dbb2d6fb
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Mon May 28 14:16:57 2012 -0400

    ext4: Update from upstream
    
    ext4: fix potential NULL dereference in ext4_free_inodes_counts()
    
    commit bb3d132a24cd8bf5e7773b2d9f9baa58b07a7dae upstream.
    
    The ext4_get_group_desc() function returns NULL on error, and
    ext4_free_inodes_count() function dereferences it without checking.
    There is a check on the next line, but it's too late.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: force ro mount if ext4_setup_super() fails
    
    commit 7e84b6216467b84cd332c8e567bf5aa113fd2f38 upstream.
    
    If ext4_setup_super() fails i.e. due to a too-high revision,
    the error is logged in dmesg but the fs is not mounted RO as
    indicated.
    
    Tested by:
    
    [164919.759248] EXT4-fs (sdb6): revision level too high, forcing read-only mode
    /dev/sdb6 /mnt/test2 ext4 rw,seclabel,relatime,data=ordered 0 0
    
    Reviewed-by: Andreas Dilger <adilger@whamcloud.com>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix potential integer overflow in alloc_flex_gd()
    
    commit 967ac8af4475ce45474800709b12137aa7634c77 upstream.
    
    In alloc_flex_gd(), when flexbg_size is large, kmalloc size would
    overflow and flex_gd->groups would point to a buffer smaller than
    expected, causing OOB accesses when it is used.
    
    Note that in ext4_resize_fs(), flexbg_size is calculated using
    sbi->s_log_groups_per_flex, which is read from the disk and only bounded
    to [1, 31]. The patch returns NULL for too large flexbg_size.
    
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Haogang Chen <haogangchen@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: disallow hard-linked directory in ext4_lookup
    
    commit 7e936b737211e6b54e34b71a827e56b872e958d8 upstream.
    
    A hard-linked directory to its parent can cause the VFS to deadlock,
    and is a sign of a corrupted file system.  So detect this case in
    ext4_lookup(), before the rmdir() lockup scenario can take place.
    
    Change-Id: I0e6af09e4131d07448c73fc10e7bf3ed4234ff37
    Signed-off-by: Andreas Dilger <adilger@dilger.ca>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: add missing save_error_info() to ext4_error()
    
    commit f3fc0210c0fc91900766c995f089c39170e68305 upstream.
    
    The ext4_error() function is missing a call to save_error_info().
    Since this is the function which marks the file system as containing
    an error, this oversight (which was introduced in 2.6.36) is quite
    significant, and should be backported to older stable kernels with
    high urgency.
    
    Reported-by: Ken Sumrall <ksumrall@google.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: ksumrall@google.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't trash state flags in EXT4_IOC_SETFLAGS
    
    commit 79906964a187c405db72a3abc60eb9b50d804fbc upstream.
    
    In commit 353eb83c we removed i_state_flags with 64-bit longs, But
    when handling the EXT4_IOC_SETFLAGS ioctl, we replace i_flags
    directly, which trashes the state flags which are stored in the high
    32-bits of i_flags on 64-bit platforms.  So use the the
    ext4_{set,clear}_inode_flags() functions which use atomic bit
    manipulation functions instead.
    
    Reported-by: Tao Ma <boyu.mt@taobao.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: add ext4_mb_unload_buddy in the error path
    
    commit 02b7831019ea4e7994968c84b5826fa8b248ffc8 upstream.
    
    ext4_free_blocks fails to pair an ext4_mb_load_buddy with a matching
    ext4_mb_unload_buddy when it fails a memory allocation.
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: remove mb_groups before tearing down the buddy_cache
    
    commit 95599968d19db175829fb580baa6b68939b320fb upstream.
    
    We can't have references held on pages in the s_buddy_cache while we are
    trying to truncate its pages and put the inode.  All the pages must be
    gone before we reach clear_inode.  This can only be gauranteed if we
    can prevent new users from grabbing references to s_buddy_cache's pages.
    
    The original bug can be reproduced and the bug fix can be verified by:
    
    while true; do mount -t ext4 /dev/ram0 /export/hda3/ram0; \
    	umount /export/hda3/ram0; done &
    
    while true; do cat /proc/fs/ext4/ram0/mb_groups; done
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't set i_flags in EXT4_IOC_SETFLAGS
    
    commit b22b1f178f6799278d3178d894f37facb2085765 upstream.
    
    Commit 7990696 uses the ext4_{set,clear}_inode_flags() functions to
    change the i_flags automatically but fails to remove the error setting
    of i_flags.  So we still have the problem of trashing state flags.
    Fix this by removing the assignment.
    
    Signed-off-by: Tao Ma <boyu.mt@taobao.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix the free blocks calculation for ext3 file systems w/ uninit_bg
    
    commit b0dd6b70f0fda17ae9762fbb72d98e40a4f66556 upstream.
    
    Ext3 filesystems that are converted to use as many ext4 file system
    features as possible will enable uninit_bg to speed up e2fsck times.
    These file systems will have a native ext3 layout of inode tables and
    block allocation bitmaps (as opposed to ext4's flex_bg layout).
    Unfortunately, in these cases, when first allocating a block in an
    uninitialized block group, ext4 would incorrectly calculate the number
    of free blocks in that block group, and then errorneously report that
    the file system was corrupt:
    
    EXT4-fs error (device vdd): ext4_mb_generate_buddy:741: group 30, 32254 clusters in bitmap, 32258 in gd
    
    This problem can be reproduced via:
    
        mke2fs -q -t ext4 -O ^flex_bg /dev/vdd 5g
        mount -t ext4 /dev/vdd /mnt
        fallocate -l 4600m /mnt/test
    
    The problem was caused by a bone headed mistake in the check to see if a
    particular metadata block was part of the block group.
    
    Many thanks to Kees Cook for finding and bisecting the buggy commit
    which introduced this bug (commit fd034a84e1, present since v3.2).
    
    Reported-by: Sander Eikelenboom <linux@eikelenboom.it>
    Reported-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Tested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix duplicated mnt_drop_write call in EXT4_IOC_MOVE_EXT
    
    commit 331ae4962b975246944ea039697a8f1cadce42bb upstream.
    
    Caused, AFAICS, by mismerge in commit ff9cb1c4eead ("Merge branch
    'for_linus' into for_linus_merged")
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: pass a char * to ext4_count_free() instead of a buffer_head ptr
    
    commit f6fb99cadcd44660c68e13f6eab28333653621e6 upstream.
    
    Make it possible for ext4_count_free to operate on buffers and not
    just data in buffer_heads.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix overhead calculation used by ext4_statfs()
    
    commit 952fc18ef9ec707ebdc16c0786ec360295e5ff15 upstream.
    
    Commit f975d6bcc7a introduced bug which caused ext4_statfs() to
    miscalculate the number of file system overhead blocks.  This causes
    the f_blocks field in the statfs structure to be larger than it should
    be.  This would in turn cause the "df" output to show the number of
    data blocks in the file system and the number of data blocks used to
    be larger than they should be.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix hole punch failure when depth is greater than 0
    
    commit 968dee77220768a5f52cf8b21d0bdb73486febef upstream.
    
    Whether to continue removing extents or not is decided by the return
    value of function ext4_ext_more_to_rm() which checks 2 conditions:
    a) if there are no more indexes to process.
    b) if the number of entries are decreased in the header of "depth -1".
    
    In case of hole punch, if the last block to be removed is not part of
    the last extent index than this index will not be deleted, hence the
    number of valid entries in the extent header of "depth - 1" will
    remain as it is and ext4_ext_more_to_rm will return 0 although the
    required blocks are not yet removed.
    
    This patch fixes the above mentioned problem as instead of removing
    the extents from the end of file, it starts removing the blocks from
    the particular extent from which removing blocks is actually required
    and continue backward until done.
    
    Signed-off-by: Ashish Sangwan <ashish.sangwan2@gmail.com>
    Signed-off-by: Namjae Jeon <linkinjeon@gmail.com>
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't let i_reserved_meta_blocks go negative
    
    commit 97795d2a5b8d3c8dc4365d4bd3404191840453ba upstream.
    
    If we hit a condition where we have allocated metadata blocks that
    were not appropriately reserved, we risk underflow of
    ei->i_reserved_meta_blocks.  In turn, this can throw
    sbi->s_dirtyclusters_counter significantly out of whack and undermine
    the nondelalloc fallback logic in ext4_nonda_switch().  Warn if this
    occurs and set i_allocated_meta_blocks to avoid this problem.
    
    This condition is reproduced by xfstests 270 against ext2 with
    delalloc enabled:
    
    Mar 28 08:58:02 localhost kernel: [  171.526344] EXT4-fs (loop1): delayed block allocation failed for inode 14 at logical offset 64486 with max blocks 64 with error -28
    Mar 28 08:58:02 localhost kernel: [  171.526346] EXT4-fs (loop1): This should not happen!! Data will be lost
    
    270 ultimately fails with an inconsistent filesystem and requires an
    fsck to repair.  The cause of the error is an underflow in
    ext4_da_update_reserve_space() due to an unreserved meta block
    allocation.
    
    Signed-off-by: Brian Foster <bfoster@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: undo ext4_calc_metadata_amount if we fail to claim space
    
    commit 03179fe92318e7934c180d96f12eff2cb36ef7b6 upstream.
    
    The function ext4_calc_metadata_amount() has side effects, although
    it's not obvious from its function name.  So if we fail to claim
    space, regardless of whether we retry to claim the space again, or
    return an error, we need to undo these side effects.
    
    Otherwise we can end up incorrectly calculating the number of metadata
    blocks needed for the operation, which was responsible for an xfstests
    failure for test #271 when using an ext2 file system with delalloc
    enabled.
    
    Reported-by: Brian Foster <bfoster@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: make sure the journal sb is written in ext4_clear_journal_err()
    
    commit d796c52ef0b71a988364f6109aeb63d79c5b116b upstream.
    
    After we transfer set the EXT4_ERROR_FS bit in the file system
    superblock, it's not enough to call jbd2_journal_clear_err() to clear
    the error indication from journal superblock --- we need to call
    jbd2_journal_update_sb_errno() as well.  Otherwise, when the root file
    system is mounted read-only, the journal is replayed, and the error
    indicator is transferred to the superblock --- but the s_errno field
    in the jbd2 superblock is left set (since although we cleared it in
    memory, we never flushed it out to disk).
    
    This can end up confusing e2fsck.  We should make e2fsck more robust
    in this case, but the kernel shouldn't be leaving things in this
    confused state, either.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: avoid kmemcheck complaint from reading uninitialized memory
    
    commit 7e731bc9a12339f344cddf82166b82633d99dd86 upstream.
    
    Commit 03179fe923 introduced a kmemcheck complaint in
    ext4_da_get_block_prep() because we save and restore
    ei->i_da_metadata_calc_last_lblock even though it is left
    uninitialized in the case where i_da_metadata_calc_len is zero.
    
    This doesn't hurt anything, but silencing the kmemcheck complaint
    makes it easier for people to find real bugs.
    
    Addresses https://bugzilla.kernel.org/show_bug.cgi?id=45631
    (which is marked as a regression).
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix long mount times on very big file systems
    
    commit 0548bbb85337e532ca2ed697c3e9b227ff2ed4b4 upstream.
    
    Commit 8aeb00ff85a: "ext4: fix overhead calculation used by
    ext4_statfs()" introduced a O(n**2) calculation which makes very large
    file systems take forever to mount.  Fix this with an optimization for
    non-bigalloc file systems.  (For bigalloc file systems the overhead
    needs to be set in the the superblock.)
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix kernel BUG on large-scale rm -rf commands
    
    commit 89a4e48f8479f8145eca9698f39fe188c982212f upstream.
    
    Commit 968dee7722: "ext4: fix hole punch failure when depth is greater
    than 0" introduced a regression in v3.5.1/v3.6-rc1 which caused kernel
    crashes when users ran run "rm -rf" on large directory hierarchy on
    ext4 filesystems on RAID devices:
    
        BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
    
        Process rm (pid: 18229, threadinfo ffff8801276bc000, task ffff880123631710)
        Call Trace:
         [<ffffffff81236483>] ? __ext4_handle_dirty_metadata+0x83/0x110
         [<ffffffff812353d3>] ext4_ext_truncate+0x193/0x1d0
         [<ffffffff8120a8cf>] ? ext4_mark_inode_dirty+0x7f/0x1f0
         [<ffffffff81207e05>] ext4_truncate+0xf5/0x100
         [<ffffffff8120cd51>] ext4_evict_inode+0x461/0x490
         [<ffffffff811a1312>] evict+0xa2/0x1a0
         [<ffffffff811a1513>] iput+0x103/0x1f0
         [<ffffffff81196d84>] do_unlinkat+0x154/0x1c0
         [<ffffffff8118cc3a>] ? sys_newfstatat+0x2a/0x40
         [<ffffffff81197b0b>] sys_unlinkat+0x1b/0x50
         [<ffffffff816135e9>] system_call_fastpath+0x16/0x1b
        Code: 8b 4d 20 0f b7 41 02 48 8d 04 40 48 8d 04 81 49 89 45 18 0f b7 49 02 48 83 c1 01 49 89 4d 00 e9 ae f8 ff ff 0f 1f 00 49 8b 45 28 <48> 8b 40 28 49 89 45 20 e9 85 f8 ff ff 0f 1f 80 00 00 00
    
        RIP  [<ffffffff81233164>] ext4_ext_remove_space+0xa34/0xdf0
    
    This could be reproduced as follows:
    
    The problem in commit 968dee7722 was that caused the variable 'i' to
    be left uninitialized if the truncate required more space than was
    available in the journal.  This resulted in the function
    ext4_ext_truncate_extend_restart() returning -EAGAIN, which caused
    ext4_ext_remove_space() to restart the truncate operation after
    starting a new jbd2 handle.
    
    Reported-by: Maciej Żenczykowski <maze@google.com>
    Reported-by: Marti Raudsepp <marti@juffo.org>
    Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: ignore last group w/o enough space when resizing instead of BUG'ing
    
    commit 03c1c29053f678234dbd51bf3d65f3b7529021de upstream.
    
    If the last group does not have enough space for group tables, ignore
    it instead of calling BUG_ON().
    
    Reported-by: Daniel Drake <dsd@laptop.org>
    Signed-off-by: Yongqiang Yang <xiaoqiangnk@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't copy non-existent gdt blocks when resizing
    
    commit 6df935ad2fced9033ab52078825fcaf6365f34b7 upstream.
    
    The resize code was copying blocks at the beginning of each block
    group in order to copy the superblock and block group descriptor table
    (gdt) blocks.  This was, unfortunately, being done even for block
    groups that did not have super blocks or gdt blocks.  This is a
    complete waste of perfectly good I/O bandwidth, to skip writing those
    blocks for sparse bg's.
    
    Signed-off-by: Yongqiang Yang <xiaoqiangnk@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: avoid duplicate writes of the backup bg descriptor blocks
    
    commit 2ebd1704ded88a8ae29b5f3998b13959c715c4be upstream.
    
    The resize code was needlessly writing the backup block group
    descriptor blocks multiple times (once per block group) during an
    online resize.
    
    Signed-off-by: Yongqiang Yang <xiaoqiangnk@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix potential deadlock in ext4_nonda_switch()
    
    commit 00d4e7362ed01987183e9528295de3213031309c upstream.
    
    In ext4_nonda_switch(), if the file system is getting full we used to
    call writeback_inodes_sb_if_idle().  The problem is that we can be
    holding i_mutex already, and this causes a potential deadlock when
    writeback_inodes_sb_if_idle() when it tries to take s_umount.  (See
    lockdep output below).
    
    As it turns out we don't need need to hold s_umount; the fact that we
    are in the middle of the write(2) system call will keep the superblock
    pinned.  Unfortunately writeback_inodes_sb() checks to make sure
    s_umount is taken, and the VFS uses a different mechanism for making
    sure the file system doesn't get unmounted out from under us.  The
    simplest way of dealing with this is to just simply grab s_umount
    using a trylock, and skip kicking the writeback flusher thread in the
    very unlikely case that we can't take a read lock on s_umount without
    blocking.
    
    Also, we now check the cirteria for kicking the writeback thread
    before we decide to whether to fall back to non-delayed writeback, so
    if there are any outstanding delayed allocation writes, we try to get
    them resolved as soon as possible.
    
       [ INFO: possible circular locking dependency detected ]
       3.6.0-rc1-00042-gce894ca #367 Not tainted
       -------------------------------------------------------
       dd/8298 is trying to acquire lock:
        (&type->s_umount_key#18){++++..}, at: [<c02277d4>] writeback_inodes_sb_if_idle+0x28/0x46
    
       but task is already holding lock:
        (&sb->s_type->i_mutex_key#8){+.+...}, at: [<c01ddcce>] generic_file_aio_write+0x5f/0xd3
    
       which lock already depends on the new lock.
    
       2 locks held by dd/8298:
        #0:  (sb_writers#2){.+.+.+}, at: [<c01ddcc5>] generic_file_aio_write+0x56/0xd3
        #1:  (&sb->s_type->i_mutex_key#8){+.+...}, at: [<c01ddcce>] generic_file_aio_write+0x5f/0xd3
    
       stack backtrace:
       Pid: 8298, comm: dd Not tainted 3.6.0-rc1-00042-gce894ca #367
       Call Trace:
        [<c015b79c>] ? console_unlock+0x345/0x372
        [<c06d62a1>] print_circular_bug+0x190/0x19d
        [<c019906c>] __lock_acquire+0x86d/0xb6c
        [<c01999db>] ? mark_held_locks+0x5c/0x7b
        [<c0199724>] lock_acquire+0x66/0xb9
        [<c02277d4>] ? writeback_inodes_sb_if_idle+0x28/0x46
        [<c06db935>] down_read+0x28/0x58
        [<c02277d4>] ? writeback_inodes_sb_if_idle+0x28/0x46
        [<c02277d4>] writeback_inodes_sb_if_idle+0x28/0x46
        [<c026f3b2>] ext4_nonda_switch+0xe1/0xf4
        [<c0271ece>] ext4_da_write_begin+0x27/0x193
        [<c01dcdb0>] generic_file_buffered_write+0xc8/0x1bb
        [<c01ddc47>] __generic_file_aio_write+0x1dd/0x205
        [<c01ddce7>] generic_file_aio_write+0x78/0xd3
        [<c026d336>] ext4_file_write+0x480/0x4a6
        [<c0198c1d>] ? __lock_acquire+0x41e/0xb6c
        [<c0180944>] ? sched_clock_cpu+0x11a/0x13e
        [<c01967e9>] ? trace_hardirqs_off+0xb/0xd
        [<c018099f>] ? local_clock+0x37/0x4e
        [<c0209f2c>] do_sync_write+0x67/0x9d
        [<c0209ec5>] ? wait_on_retry_sync_kiocb+0x44/0x44
        [<c020a7b9>] vfs_write+0x7b/0xe6
        [<c020a9a6>] sys_write+0x3b/0x64
        [<c06dd4bd>] syscall_call+0x7/0xb
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix crash when accessing /proc/mounts concurrently
    
    commit 50df9fd55e4271e89a7adf3b1172083dd0ca199d upstream.
    
    The crash was caused by a variable being erronously declared static in
    token2str().
    
    In addition to /proc/mounts, the problem can also be easily replicated
    by accessing /proc/fs/ext4/<partition>/options in parallel:
    
    $ cat /proc/fs/ext4/<partition>/options > options.txt
    
    ... and then running the following command in two different terminals:
    
    $ while diff /proc/fs/ext4/<partition>/options options.txt; do true; done
    
    This is also the cause of the following a crash while running xfstests
    
    	https://bugs.launchpad.net/bugs/1053019
    	https://bugzilla.kernel.org/show_bug.cgi?id=47731
    
    Signed-off-by: Herton Ronaldo Krzesinski <herton.krzesinski@canonical.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Brad Figg <brad.figg@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: move_extent code cleanup
    
    commit 03bd8b9b896c8e940f282f540e6b4de90d666b7c upstream.
    
    - Remove usless checks, because it is too late to check that inode != NULL
      at the moment it was referenced several times.
    - Double lock routines looks very ugly and locking ordering relays on
      order of i_ino, but other kernel code rely on order of pointers.
      Let's make them simple and clean.
    - check that inodes belongs to the same SB as soon as possible.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: online defrag is not supported for journaled files
    
    commit f066055a3449f0e5b0ae4f3ceab4445bead47638 upstream.
    
    Proper block swap for inodes with full journaling enabled is
    truly non obvious task. In order to be on a safe side let's
    explicitly disable it for now.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: always set i_op in ext4_mknod()
    
    commit 6a08f447facb4f9e29fcc30fb68060bb5a0d21c2 upstream.
    
    ext4_special_inode_operations have their own ifdef CONFIG_EXT4_FS_XATTR
    to mask those methods. And ext4_iget also always sets it, so there is
    an inconsistency.
    
    Signed-off-by: Bernd Schubert <bernd.schubert@itwm.fraunhofer.de>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: race-condition protection for ext4_convert_unwritten_extents_endio
    
    commit dee1f973ca341c266229faa5a1a5bb268bed3531 upstream.
    
    We assumed that at the time we call ext4_convert_unwritten_extents_endio()
    extent in question is fully inside [map.m_lblk, map->m_len] because
    it was already split during submission.  But this may not be true due to
    a race between writeback vs fallocate.
    
    If extent in question is larger than requested we will split it again.
    Special precautions should being done if zeroout required because
    [map.m_lblk, map->m_len] already contains valid data.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: Avoid underflow in ext4_trim_fs()
    
    commit 5de35e8d5c02d271c20e18337e01bc20e6ef472e upstream.
    
    Currently if len argument in ext4_trim_fs() is smaller than one block,
    the 'end' variable underflow. Avoid that by returning EINVAL if len is
    smaller than file system block.
    
    Also remove useless unlikely().
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix unjournaled inode bitmap modification
    
    commit ffb5387e85d528fb6d0d924abfa3fbf0fc484071 upstream.
    
    commit 119c0d4460b001e44b41dcf73dc6ee794b98bd31 changed
    ext4_new_inode() such that the inode bitmap was being modified
    outside a transaction, which could lead to corruption, and was
    discovered when journal_checksum found a bad checksum in the
    journal during log replay.
    
    Nix ran into this when using the journal_async_commit mount
    option, which enables journal checksumming.  The ensuing
    journal replay failures due to the bad checksums led to
    filesystem corruption reported as the now infamous
    "Apparent serious progressive ext4 data corruption bug"
    
    [ Changed by tytso to only call ext4_journal_get_write_access() only
      when we're fairly certain that we're going to allocate the inode. ]
    
    I've tested this by mounting with journal_checksum and
    running fsstress then dropping power; I've also tested by
    hacking DM to create snapshots w/o first quiescing, which
    allows me to test journal replay repeatedly w/o actually
    power-cycling the box.  Without the patch I hit a journal
    checksum error every time.  With this fix it survives
    many iterations.
    
    Reported-by: Nix <nix@esperi.org.uk>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix memory leak in ext4_xattr_set_acl()'s error path
    
    commit 24ec19b0ae83a385ad9c55520716da671274b96c upstream.
    
    In ext4_xattr_set_acl(), if ext4_journal_start() returns an error,
    posix_acl_release() will not be called for 'acl' which may result in a
    memory leak.
    
    This patch fixes that.
    
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Eugene Shatokhin <eugene.shatokhin@rosalab.ru>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix possible use after free with metadata csum
    
    commit aeb1e5d69a5be592e86a926be73efb38c55af404 upstream.
    
    Commit fa77dcfafeaa introduces block bitmap checksum calculation into
    ext4_new_inode() in the case that block group was uninitialized.
    However we brelse() the bitmap buffer before we attempt to checksum it
    so we have no guarantee that the buffer is still there.
    
    Fix this by releasing the buffer after the possible checksum
    computation.
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Acked-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix extent tree corruption caused by hole punch
    
    commit c36575e663e302dbaa4d16b9c72d2c9a913a9aef upstream.
    
    When depth of extent tree is greater than 1, logical start value of
    interior node is not correctly updated in ext4_ext_rm_idx.
    
    Signed-off-by: Forrest Liu <forrestl@synology.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Ashish Sangwan <ashishsangwan2@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: check dioread_nolock on remount
    
    commit 261cb20cb2f0737a247aaf08dff7eb065e3e5b66 upstream.
    
    Currently we allow enabling dioread_nolock mount option on remount for
    filesystems where blocksize < PAGE_CACHE_SIZE.  This isn't really
    supported so fix the bug by moving the check for blocksize !=
    PAGE_CACHE_SIZE into parse_options(). Change the original PAGE_SIZE to
    PAGE_CACHE_SIZE along the way because that's what we are really
    interested in.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: do not try to write superblock on ro remount w/o journal
    
    commit d096ad0f79a782935d2e06ae8fb235e8c5397775 upstream.
    
    When a journal-less ext4 filesystem is mounted on a read-only block
    device (blockdev --setro will do), each remount (for other, unrelated,
    flags, like suid=>nosuid etc) results in a series of scary messages
    from kernel telling about I/O errors on the device.
    
    This is becauese of the following code ext4_remount():
    
           if (sbi->s_journal == NULL)
                    ext4_commit_super(sb, 1);
    
    at the end of remount procedure, which forces writing (flushing) of
    a superblock regardless whenever it is dirty or not, if the filesystem
    is readonly or not, and whenever the device itself is readonly or not.
    
    We only need call ext4_commit_super when the file system had been
    previously mounted read/write.
    
    Thanks to Eric Sandeen for help in diagnosing this issue.
    
    Signed-off-By: Michael Tokarev <mjt@tls.msk.ru>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: lock i_mutex when truncating orphan inodes
    
    commit 721e3eba21e43532e438652dd8f1fcdfce3187e7 upstream.
    
    Commit c278531d39 added a warning when ext4_flush_unwritten_io() is
    called without i_mutex being taken.  It had previously not been taken
    during orphan cleanup since races weren't possible at that point in
    the mount process, but as a result of this c278531d39, we will now see
    a kernel WARN_ON in this case.  Take the i_mutex in
    ext4_orphan_cleanup() to suppress this warning.
    
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Zheng Liu <wenqing.lz@taobao.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: init pagevec in ext4_da_block_invalidatepages
    
    commit 66bea92c69477a75a5d37b9bfed5773c92a3c4b4 upstream.
    
    ext4_da_block_invalidatepages is missing a pagevec_init(),
    which means that pvec->cold contains random garbage.
    
    This affects whether the page goes to the front or
    back of the LRU when ->cold makes it to
    free_hot_cold_page()
    
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Reviewed-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: CAI Qian <caiqian@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: check bh in ext4_read_block_bitmap()
    
    commit 15b49132fc972c63894592f218ea5a9a61b1a18f upstream.
    
    Validate the bh pointer before using it, since
    ext4_read_block_bitmap_nowait() might return NULL.
    
    I've seen this in fsfuzz testing.
    
     EXT4-fs error (device loop0): ext4_read_block_bitmap_nowait:385: comm touch: Cannot get buffer for block bitmap - block_group = 0, block_bitmap = 3925999616
     BUG: unable to handle kernel NULL pointer dereference at           (null)
     IP: [<ffffffff8121de25>] ext4_wait_block_bitmap+0x25/0xe0
     ...
     Call Trace:
      [<ffffffff8121e1e5>] ext4_read_block_bitmap+0x35/0x60
      [<ffffffff8125e9c6>] ext4_free_blocks+0x236/0xb80
      [<ffffffff811d0d36>] ? __getblk+0x36/0x70
      [<ffffffff811d0a5f>] ? __find_get_block+0x8f/0x210
      [<ffffffff81191ef3>] ? kmem_cache_free+0x33/0x140
      [<ffffffff812678e5>] ext4_xattr_release_block+0x1b5/0x1d0
      [<ffffffff812679be>] ext4_xattr_delete_inode+0xbe/0x100
      [<ffffffff81222a7c>] ext4_free_inode+0x7c/0x4d0
      [<ffffffff812277b8>] ? ext4_mark_inode_dirty+0x88/0x230
      [<ffffffff8122993c>] ext4_evict_inode+0x32c/0x490
      [<ffffffff811b8cd7>] evict+0xa7/0x1c0
      [<ffffffff811b8ed3>] iput_final+0xe3/0x170
      [<ffffffff811b8f9e>] iput+0x3e/0x50
      [<ffffffff812316fd>] ext4_add_nondir+0x4d/0x90
      [<ffffffff81231d0b>] ext4_create+0xeb/0x170
      [<ffffffff811aae9c>] vfs_create+0xac/0xd0
      [<ffffffff811ac845>] lookup_open+0x185/0x1c0
      [<ffffffff8129e3b9>] ? selinux_inode_permission+0xa9/0x170
      [<ffffffff811acb54>] do_last+0x2d4/0x7a0
      [<ffffffff811af743>] path_openat+0xb3/0x480
      [<ffffffff8116a8a1>] ? handle_mm_fault+0x251/0x3b0
      [<ffffffff811afc49>] do_filp_open+0x49/0xa0
      [<ffffffff811bbaad>] ? __alloc_fd+0xdd/0x150
      [<ffffffff8119da28>] do_sys_open+0x108/0x1f0
      [<ffffffff8119db51>] sys_open+0x21/0x30
      [<ffffffff81618959>] system_call_fastpath+0x16/0x1b
    
    Also fix comment for ext4_read_block_bitmap_nowait()
    
    Signed-off-by: Eryu Guan <guaneryu@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix race in ext4_mb_add_n_trim()
    
    commit f1167009711032b0d747ec89a632a626c901a1ad upstream.
    
    In ext4_mb_add_n_trim(), lg_prealloc_lock should be taken when
    changing the lg_prealloc_list.
    
    Signed-off-by: Niu Yawei <yawei.niu@intel.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix xattr block allocation/release with bigalloc
    
    commit 1231b3a1eb5740192aeebf5344dd6d6da000febf upstream.
    
    Currently when new xattr block is created or released we we would call
    dquot_free_block() or dquot_alloc_block() respectively, among the else
    decrementing or incrementing the number of blocks assigned to the
    inode by one block.
    
    This however does not work for bigalloc file system because we always
    allocate/free the whole cluster so we have to count with that in
    dquot_free_block() and dquot_alloc_block() as well.
    
    Use the clusters-to-blocks conversion EXT4_C2B() when passing number of
    blocks to the dquot_alloc/free functions to fix the problem.
    
    The problem has been revealed by xfstests #117 (and possibly others).
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix free clusters calculation in bigalloc filesystem
    
    commit 304e220f0879198b1f5309ad6f0be862b4009491 upstream.
    
    ext4_has_free_clusters() should tell us whether there is enough free
    clusters to allocate, however number of free clusters in the file system
    is converted to blocks using EXT4_C2B() which is not only wrong use of
    the macro (we should have used EXT4_NUM_B2C) but it's also completely
    wrong concept since everything else is in cluster units.
    
    Moreover when calculating number of root clusters we should be using
    macro EXT4_NUM_B2C() instead of EXT4_B2C() otherwise the result might be
    off by one. However r_blocks_count should always be a multiple of the
    cluster ratio so doing a plain bit shift should be enough here. We
    avoid using EXT4_B2C() because it's confusing.
    
    As a result of the first problem number of free clusters is much bigger
    than it should have been and ext4_has_free_clusters() would return 1 even
    if there is really not enough free clusters available.
    
    Fix this by removing the EXT4_C2B() conversion of free clusters and
    using bit shift when calculating number of root clusters. This bug
    affects number of xfstests tests covering file system ENOSPC situation
    handling. With this patch most of the ENOSPC problems with bigalloc file
    system disappear, especially the errors caused by delayed allocation not
    having enough space when the actual allocation is finally requested.
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix the wrong number of the allocated blocks in ext4_split_extent()
    
    commit 3a2256702e47f68f921dfad41b1764d05c572329 upstream.
    
    This commit fixes a wrong return value of the number of the allocated
    blocks in ext4_split_extent.  When the length of blocks we want to
    allocate is greater than the length of the current extent, we return a
    wrong number.  Let's see what happens in the following case when we
    call ext4_split_extent().
    
      map: [48, 72]
      ex:  [32, 64, u]
    
    'ex' will be split into two parts:
      ex1: [32, 47, u]
      ex2: [48, 64, w]
    
    'map->m_len' is returned from this function, and the value is 24.  But
    the real length is 16.  So it should be fixed.
    
    Meanwhile in this commit we use right length of the allocated blocks
    when get_reserved_cluster_alloc in ext4_ext_handle_uninitialized_extents
    is called.
    
    Signed-off-by: Zheng Liu <wenqing.lz@taobao.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix data=journal fast mount/umount hang
    
    commit 2b405bfa84063bfa35621d2d6879f52693c614b0 upstream.
    
    In data=journal mode, if we unmount the file system before a
    transaction has a chance to complete, when the journal inode is being
    evicted, we can end up calling into jbd2_log_wait_commit() for the
    last transaction, after the journalling machinery has been shut down.
    
    Arguably we should adjust ext4_should_journal_data() to return FALSE
    for the journal inode, but the only place it matters is
    ext4_evict_inode(), and so to save a bit of CPU time, and to make the
    patch much more obviously correct by inspection(tm), we'll fix it by
    explicitly not trying to waiting for a journal commit when we are
    evicting the journal inode, since it's guaranteed to never succeed in
    this case.
    
    This can be easily replicated via:
    
         mount -t ext4 -o data=journal /dev/vdb /vdb ; umount /vdb
    
    ------------[ cut here ]------------
    WARNING: at /usr/projects/linux/ext4/fs/jbd2/journal.c:542 __jbd2_log_start_commit+0xba/0xcd()
    Hardware name: Bochs
    JBD2: bad log_start_commit: 3005630206 3005630206 0 0
    Modules linked in:
    Pid: 2909, comm: umount Not tainted 3.8.0-rc3 #1020
    Call Trace:
     [<c015c0ef>] warn_slowpath_common+0x68/0x7d
     [<c02b7e7d>] ? __jbd2_log_start_commit+0xba/0xcd
     [<c015c177>] warn_slowpath_fmt+0x2b/0x2f
     [<c02b7e7d>] __jbd2_log_start_commit+0xba/0xcd
     [<c02b8075>] jbd2_log_start_commit+0x24/0x34
     [<c0279ed5>] ext4_evict_inode+0x71/0x2e3
     [<c021f0ec>] evict+0x94/0x135
     [<c021f9aa>] iput+0x10a/0x110
     [<c02b7836>] jbd2_journal_destroy+0x190/0x1ce
     [<c0175284>] ? bit_waitqueue+0x50/0x50
     [<c028d23f>] ext4_put_super+0x52/0x294
     [<c020efe3>] generic_shutdown_super+0x48/0xb4
     [<c020f071>] kill_block_super+0x22/0x60
     [<c020f3e0>] deactivate_locked_super+0x22/0x49
     [<c020f5d6>] deactivate_super+0x30/0x33
     [<c0222795>] mntput_no_expire+0x107/0x10c
     [<c02233a7>] sys_umount+0x2cf/0x2e0
     [<c02233ca>] sys_oldumount+0x12/0x14
     [<c08096b8>] syscall_call+0x7/0xb
    ---[ end trace 6a954cc790501c1f ]---
    jbd2_log_wait_commit: error: j_commit_request=-1289337090, tid=0
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: convert number of blocks to clusters properly
    
    commit 810da240f221d64bf90020f25941b05b378186fe upstream.
    
    We're using macro EXT4_B2C() to convert number of blocks to number of
    clusters for bigalloc file systems.  However, we should be using
    EXT4_NUM_B2C().
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: CAI Qian <caiqian@redhat.com>
    Signed-off-by: Lingzhu Xiang <lxiang@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: use atomic64_t for the per-flexbg free_clusters count
    
    commit 90ba983f6889e65a3b506b30dc606aa9d1d46cd2 upstream.
    
    A user who was using a 8TB+ file system and with a very large flexbg
    size (> 65536) could cause the atomic_t used in the struct flex_groups
    to overflow.  This was detected by PaX security patchset:
    
    http://forums.grsecurity.net/viewtopic.php?f=3&t=3289&p=12551#p12551
    
    This bug was introduced in commit 9f24e4208f7e, so it's been around
    since 2.6.30.  :-(
    
    Fix this by using an atomic64_t for struct orlav_stats's
    free_clusters.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Lingzhu Xiang <lxiang@redhat.com>
    Reviewed-by: CAI Qian <caiqian@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix journal callback list traversal
    
    commit 5d3ee20855e28169d711b394857ee608a5023094 upstream.
    
    It is incorrect to use list_for_each_entry_safe() for journal callback
    traversial because ->next may be removed by other task:
    ->ext4_mb_free_metadata()
      ->ext4_mb_free_metadata()
        ->ext4_journal_callback_del()
    
    This results in the following issue:
    
    WARNING: at lib/list_debug.c:62 __list_del_entry+0x1c0/0x250()
    Hardware name:
    list_del corruption. prev->next should be ffff88019a4ec198, but was 6b6b6b6b6b6b6b6b
    Modules linked in: cpufreq_ondemand acpi_cpufreq freq_table mperf coretemp kvm_intel kvm crc32c_intel ghash_clmulni_intel microcode sg xhci_hcd button sd_mod crc_t10dif aesni_intel ablk_helper cryptd lrw aes_x86_64 xts gf128mul ahci libahci pata_acpi ata_generic dm_mirror dm_region_hash dm_log dm_mod
    Pid: 16400, comm: jbd2/dm-1-8 Tainted: G        W    3.8.0-rc3+ #107
    Call Trace:
     [<ffffffff8106fb0d>] warn_slowpath_common+0xad/0xf0
     [<ffffffff8106fc06>] warn_slowpath_fmt+0x46/0x50
     [<ffffffff813637e9>] ? ext4_journal_commit_callback+0x99/0xc0
     [<ffffffff8148cae0>] __list_del_entry+0x1c0/0x250
     [<ffffffff813637bf>] ext4_journal_commit_callback+0x6f/0xc0
     [<ffffffff813ca336>] jbd2_journal_commit_transaction+0x23a6/0x2570
     [<ffffffff8108aa42>] ? try_to_del_timer_sync+0x82/0xa0
     [<ffffffff8108b491>] ? del_timer_sync+0x91/0x1e0
     [<ffffffff813d3ecf>] kjournald2+0x19f/0x6a0
     [<ffffffff810ad630>] ? wake_up_bit+0x40/0x40
     [<ffffffff813d3d30>] ? bit_spin_lock+0x80/0x80
     [<ffffffff810ac6be>] kthread+0x10e/0x120
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
     [<ffffffff818ff6ac>] ret_from_fork+0x7c/0xb0
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
    
    This patch fix the issue as follows:
    - ext4_journal_commit_callback() make list truly traversial safe
      simply by always starting from list_head
    - fix race between two ext4_journal_callback_del() and
      ext4_journal_callback_try_del()
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix online resizing for ext3-compat file systems
    
    commit c5c72d814cf0f650010337c73638b25e6d14d2d4 upstream.
    
    Commit fb0a387dcdc restricts block allocations for indirect-mapped
    files to block groups less than s_blockfile_groups.  However, the
    online resizing code wasn't setting s_blockfile_groups, so the newly
    added block groups were not available for non-extent mapped files.
    
    Reported-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix Kconfig documentation for CONFIG_EXT4_DEBUG
    
    commit 7f3e3c7cfcec148ccca9c0dd2dbfd7b00b7ac10f upstream.
    
    Fox the Kconfig documentation for CONFIG_EXT4_DEBUG to match the
    change made by commit a0b30c1229: ext4: use module parameters instead
    of debugfs for mballoc_debug
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: add check for inodes_count overflow in new resize ioctl
    
    commit 3f8a6411fbada1fa482276591e037f3b1adcf55b upstream.
    
    Addresses-Red-Hat-Bugzilla: #913245
    
    Reported-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Reviewed-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Lingzhu Xiang <lxiang@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: limit group search loop for non-extent files
    
    commit e6155736ad76b2070652745f9e54cdea3f0d8567 upstream.
    
    In the case where we are allocating for a non-extent file,
    we must limit the groups we allocate from to those below
    2^32 blocks, and ext4_mb_regular_allocator() attempts to
    do this initially by putting a cap on ngroups for the
    subsequent search loop.
    
    However, the initial target group comes in from the
    allocation context (ac), and it may already be beyond
    the artificially limited ngroups.  In this case,
    the limit
    
    	if (group == ngroups)
    		group = 0;
    
    at the top of the loop is never true, and the loop will
    run away.
    
    Catch this case inside the loop and reset the search to
    start at group 0.
    
    [sandeen@redhat.com: add commit msg & comments]
    
    Signed-off-by: Lachlan McIlroy <lmcilroy@redhat.com>
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext3,ext4: don't mess with dir_file->f_pos in htree_dirblock_to_tree()
    
    commit 64cb927371cd2ec43758d8a094a003d27bc3d0dc upstream.
    
    Both ext3 and ext4 htree_dirblock_to_tree() is just filling the
    in-core rbtree for use by call_filldir().  All updates of ->f_pos are
    done by the latter; bumping it here (on error) is obviously wrong - we
    might very well have it nowhere near the block we'd found an error in.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix data offset overflow in ext4_xattr_fiemap() on 32-bit archs
    
    commit a60697f411eb365fb09e639e6f183fe33d1eb796 upstream.
    
    On 32-bit architectures with 32-bit sector_t computation of data offset
    in ext4_xattr_fiemap() can overflow resulting in reporting bogus data
    location. Fix the problem by typing block number to proper type before
    shifting.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix overflow when counting used blocks on 32-bit architectures
    
    commit 8af8eecc1331dbf5e8c662022272cf667e213da5 upstream.
    
    The arithmetics adding delalloc blocks to the number of used blocks in
    ext4_getattr() can easily overflow on 32-bit archs as we first multiply
    number of blocks by blocksize and then divide back by 512. Make the
    arithmetics more clever and also use proper type (unsigned long long
    instead of unsigned long).
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: don't allow ext4_free_blocks() to fail due to ENOMEM
    
    commit e7676a704ee0a1ef71a6b23760b5a8f6896cb1a1 upstream.
    
    The filesystem should not be marked inconsistent if ext4_free_blocks()
    is not able to allocate memory.  Unfortunately some callers (most
    notably ext4_truncate) don't have a way to reflect an error back up to
    the VFS.  And even if we did, most userspace applications won't deal
    with most system calls returning ENOMEM anyway.
    
    Reported-by: Nagachandra P <nagachandra@gmail.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: make sure group number is bumped after a inode allocation race
    
    commit a34eb503742fd25155fd6cff6163daacead9fbc3 upstream.
    
    When we try to allocate an inode, and there is a race between two
    CPU's trying to grab the same inode, _and_ this inode is the last free
    inode in the block group, make sure the group number is bumped before
    we continue searching the rest of the block groups.  Otherwise, we end
    up searching the current block group twice, and we end up skipping
    searching the last block group.  So in the unlikely situation where
    almost all of the inodes are allocated, it's possible that we will
    return ENOSPC even though there might be free inodes in that last
    block group.
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: fix mount/remount error messages for incompatible mount options
    
    commit 6ae6514b33f941d3386da0dfbe2942766eab1577 upstream.
    
    Commit 5688978 ("ext4: improve handling of conflicting mount options")
    introduced incorrect messages shown while choosing wrong mount options.
    
    First of all, both cases of incorrect mount options,
    "data=journal,delalloc" and "data=journal,dioread_nolock" result in
    the same error message.
    
    Secondly, the problem above isn't solved for remount option: the
    mismatched parameter is simply ignored.  Moreover, ext4_msg states
    that remount with options "data=journal,delalloc" succeeded, which is
    not true.
    
    To fix it up, I added a simple check after parse_options() call to
    ensure that data=journal and delalloc/dioread_nolock parameters are
    not present at the same time.
    
    Signed-off-by: Piotr Sarna <p.sarna@partner.samsung.com>
    Acked-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    jbd2: Fix use after free after error in jbd2_journal_dirty_metadata()
    
    commit 91aa11fae1cf8c2fd67be0609692ea9741cdcc43 upstream.
    
    When jbd2_journal_dirty_metadata() returns error,
    __ext4_handle_dirty_metadata() stops the handle. However callers of this
    function do not count with that fact and still happily used now freed
    handle. This use after free can result in various issues but very likely
    we oops soon.
    
    The motivation of adding __ext4_journal_stop() into
    __ext4_handle_dirty_metadata() in commit 9ea7a0df seems to be only to
    improve error reporting. So replace __ext4_journal_stop() with
    ext4_journal_abort_handle() which was there before that commit and add
    WARN_ON_ONCE() to dump stack to provide useful information.
    
    Reported-by: Sage Weil <sage@inktank.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ext4: Remove unused variable
    
    Change-Id: I7ffd1345e7a6acdd9f443efa8296938b8cc7f1e7
    
    jbd2: don't write superblock when if its empty
    
    commit eeecef0af5ea4efd763c9554cf2bd80fc4a0efd3 upstream.
    
    This sequence:
    
    results in an IO error when unmounting the RO filesystem:
    
    [  318.020828] Buffer I/O error on device loop1, logical block 196608
    [  318.027024] lost page write due to I/O error on loop1
    [  318.032088] JBD2: Error -5 detected when updating journal superblock for loop1-8.
    
    This was a regression introduced by commit 24bcc89c7e7c: "jbd2: split
    updating of journal superblock and marking journal empty".
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    jbd2: fix use after free in jbd2_journal_dirty_metadata()
    
    commit ad56edad089b56300fd13bb9eeb7d0424d978239 upstream.
    
    jbd2_journal_dirty_metadata() didn't get a reference to journal_head it
    was working with. This is OK in most of the cases since the journal head
    should be attached to a transaction but in rare occasions when we are
    journalling data, __ext4_journalled_writepage() can race with
    jbd2_journal_invalidatepage() stripping buffers from a page and thus
    journal head can be freed under hands of jbd2_journal_dirty_metadata().
    
    Fix the problem by getting own journal head reference in
    jbd2_journal_dirty_metadata() (and also in jbd2_journal_set_triggers()
    which can possibly have the same issue).
    
    Reported-by: Zheng Liu <gnehzuil.liu@gmail.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    jbd2: fix race between jbd2_journal_remove_checkpoint and ->j_commit_callback
    
    commit 794446c6946513c684d448205fbd76fa35f38b72 upstream.
    
    The following race is possible:
    
    [kjournald2]                              other_task
    jbd2_journal_commit_transaction()
      j_state = T_FINISHED;
      spin_unlock(&journal->j_list_lock);
                                             ->jbd2_journal_remove_checkpoint()
    					   ->jbd2_journal_free_transaction();
    					     ->kmem_cache_free(transaction)
      ->j_commit_callback(journal, transaction);
        -> USE_AFTER_FREE
    
    WARNING: at lib/list_debug.c:62 __list_del_entry+0x1c0/0x250()
    Hardware name:
    list_del corruption. prev->next should be ffff88019a4ec198, but was 6b6b6b6b6b6b6b6b
    Modules linked in: cpufreq_ondemand acpi_cpufreq freq_table mperf coretemp kvm_intel kvm crc32c_intel ghash_clmulni_intel microcode sg xhci_hcd button sd_mod crc_t10dif aesni_intel ablk_helper cryptd lrw aes_x86_64 xts gf128mul ahci libahci pata_acpi ata_generic dm_mirror dm_region_hash dm_log dm_mod
    Pid: 16400, comm: jbd2/dm-1-8 Tainted: G        W    3.8.0-rc3+ #107
    Call Trace:
     [<ffffffff8106fb0d>] warn_slowpath_common+0xad/0xf0
     [<ffffffff8106fc06>] warn_slowpath_fmt+0x46/0x50
     [<ffffffff813637e9>] ? ext4_journal_commit_callback+0x99/0xc0
     [<ffffffff8148cae0>] __list_del_entry+0x1c0/0x250
     [<ffffffff813637bf>] ext4_journal_commit_callback+0x6f/0xc0
     [<ffffffff813ca336>] jbd2_journal_commit_transaction+0x23a6/0x2570
     [<ffffffff8108aa42>] ? try_to_del_timer_sync+0x82/0xa0
     [<ffffffff8108b491>] ? del_timer_sync+0x91/0x1e0
     [<ffffffff813d3ecf>] kjournald2+0x19f/0x6a0
     [<ffffffff810ad630>] ? wake_up_bit+0x40/0x40
     [<ffffffff813d3d30>] ? bit_spin_lock+0x80/0x80
     [<ffffffff810ac6be>] kthread+0x10e/0x120
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
     [<ffffffff818ff6ac>] ret_from_fork+0x7c/0xb0
     [<ffffffff810ac5b0>] ? __init_kthread_worker+0x70/0x70
    
    In order to demonstrace this issue one should mount ext4 with mount -o
    discard option on SSD disk.  This makes callback longer and race
    window becomes wider.
    
    In order to fix this we should mark transaction as finished only after
    callbacks have completed
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Conflicts:
    	fs/jbd2/commit.c
    
    jbd2: fix theoretical race in jbd2__journal_restart
    
    commit 39c04153fda8c32e85b51c96eb5511a326ad7609 upstream.
    
    Once we decrement transaction->t_updates, if this is the last handle
    holding the transaction from closing, and once we release the
    t_handle_lock spinlock, it's possible for the transaction to commit
    and be released.  In practice with normal kernels, this probably won't
    happen, since the commit happens in a separate kernel thread and it's
    unlikely this could all happen within the space of a few CPU cycles.
    
    On the other hand, with a real-time kernel, this could potentially
    happen, so save the tid found in transaction->t_tid before we release
    t_handle_lock.  It would require an insane configuration, such as one
    where the jbd2 thread was set to a very high real-time priority,
    perhaps because a high priority real-time thread is trying to read or
    write to a file system.  But some people who use real-time kernels
    have been known to do insane things, including controlling
    laser-wielding industrial robots.  :-)
    
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Change-Id: I1464ba68cf74daf1f7dc1be391c3b2469460bc97

commit 9e6f6317d55ea2804b430446cc8adb4ab24b6a67
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Apr 1 14:57:45 2013 -0700

    msm: clock-8960: Add 1.8MHz rate to 8064's gfx3d_clk plan
    
    This rate will be used by the 'footswitch-8x60' driver when
    controlling GFX3D power.
    
    Change-Id: Ib796114dc90b468f1c0aa014a9db699f3b854adf
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 02ce0384fd8fea3397dce254d8a9f4129530969d
Author: paris_yeh <paris_yeh@asus.com>
Date:   Mon Feb 4 11:38:10 2013 +0800

    msm: footswitch-8x60: Allow specification of per-fs reset delays
    
    Custom reset rates can be specified for clocks via fs_clk_data. If
    these rates are very low, then the default reset delay of 1us may
    not be sufficient. Address this by allowing custom reset delays to
    be specified on a per-footswitch bases.
    
    Change-Id: Id9b9377d79436a7fe5007a8f37216a456d66ce1a
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    
    msm: footswitch-8x60: Update 8064 footswitch_enable() sequence for GFX3D
    
    Update 8064's GFX3D enable sequence per hardware designer recommendations
    to improve system stability when responding to the inrush current event.
    
    Differences from the standard footswitch_enable() sequence include:
     - Decreasing the reset_rate from 27MHz to 1.8MHz, and increasing
       reset_delay_us from 1us to 10us to compensate for the slower clock.
     - Enable clocks for reset propagation only after the core has been
       powered on.
     - Forcefully limit the AFAB and EBI1 clocks to a low value (decided
       by the RPM) by means of a write to the RPM_CTL RPM resource.
     - Removal of the reset toggle after powering on the core, which (while
       harmless) is not applicable to 8064.
    
    Change-Id: I943842b56ff96b2e6077a419566d91ac184a6fda
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>
    
    msm: footswitch-8x60: Skip GFX memory power collapse on 8064.
    
    Update footswitch_disable to skip GFX memory power collapse on 8064 to avoid
    any voltage droop on vddmx during GFX3D footswitch toggling.
    
    As per hardware designer recommendations this would help to improve system
    stability when responding to the inrush current event.
    
    Change-Id: Ifa2f62629f0639d29f882d009dc5f4c10da46ca8
    Signed-off-by: Shashank Mittal <mittals@codeaurora.org>
    Signed-off-by: Ed Tam <etam@google.com>

commit cddcad9979c5a6ad85c9b8d42a372818a36e0fe6
Author: Joel King <joelking@codeaurora.org>
Date:   Wed Apr 17 08:40:50 2013 -0700

    msm: mdm2: Add delay between subsequent PS_HOLD for 8064 fusion3
    
    This feature was previously added for the 8960 sglte target,
    in commit 21274cf2219899ef9b46cf8b9fd8bf4ce52d057b, but it also
    applies to the 8064 sglte2 target. The original commit message
    follows.
    
    During SSR first a reset of external modem is issued
    and mdm_do_soft_power_on() toggles ap2mdm_soft_reset which
    in turn toogles the PS_HOLD. Then a part of SSR external
    modem is powered up and mdm_do_soft_power_on() again toggles
    the gpio. For PMIC register stabilization we need a 1sec delay
    between subsequent mdm_do_soft_power_on().
    By default the delay is 500msec, so adding another 500msec for
    stablization.
    
    Signed-off-by: Joel King <joelking@codeaurora.org>
    
    Conflicts:
    	arch/arm/mach-msm/board-8064.c
    
    Change-Id: I8ac22d94205cb309dd5ba4b1f3a20933632298db

commit b63bb3962e04ce87a5344f4f6c6044929c4f32d3
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Thu Oct 10 00:08:04 2013 -0500

    jf: clean up and consolidate defconfigs
    
    * regenerate commonized defconfig
    * clean up the mess
    
    Change-Id: I6895d70d3256d0d0337373919ed9f29658f30e86

commit 5812fb551bc32aca6a1ea8c4a4486f6a028b7aa3
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Tue Oct 15 08:03:21 2013 -0500

    jflteatt: conform to the rest of jf devices
    
    * fixes build issues with our consolidated defconfig
    
    Change-Id: I514385e7ca3f1cbd884f7fc15c4426575da29fb0

commit cd838940c4ac1d27d8d15bc5251eae735fff1552
Merge: 2c8f5cd 47a5a4a
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Tue Oct 22 04:39:28 2013 +0000

    Merge "Revert "mmc: core: handle flush requests timeout"" into cm-10.2

commit 47a5a4a98f4f4430ea89626f3b123b313f832c69
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Tue Oct 22 04:39:16 2013 +0000

    Revert "mmc: core: handle flush requests timeout"
    
    * somehow causes random reboots when on 3g only
    
    This reverts commit 2ec43f0da44d74e7adca62867fc2f8245a62e878.
    
    Change-Id: Iff7e8e8b76e8ca12428175c1e882c5570aa27480

commit 2c8f5cd8e4df8658f743d649d04668a923087302
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sat Oct 19 14:27:19 2013 -0500

    board-8064-storage: fix splash screen hang
    
    What an annoying bug...
    
    To reproduce:
    * Turn device off, unplugged
    * Plug in, allow offmode charging to do its thing
    * Once on the black screen, turn device on
    * It'll hang on the splash screen until you pull the battery
    
    Likely this will fix the random occurances of this too. That is yet to
    be verified though.
    
    Change-Id: Id3c419d93e7ae73c6b7d34e989483003cfece7f3

commit 2ec43f0da44d74e7adca62867fc2f8245a62e878
Author: Konstantin Dorfman <kdorfman@codeaurora.org>
Date:   Sun Jun 30 09:31:21 2013 +0300

    mmc: core: handle flush requests timeout
    
    According to the eMMC 4.5 spec "Flushing a large amount of cached data may
    take very unpredictably long time". Therefore the timeout for FLUSH should
    be increased to prevent timeouts.
    
    In case the timeout occurs HPI issued.
    
    CRs-Fixed: 500874
    Change-Id: Ib00d087d3fe2fa72f5eac096976d3f24b5e4966a
    Signed-off-by: Maya Erez <merez@codeaurora.org>
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>

commit 087567e44ebc1b5d6175bbc6272642ab5ff92587
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Tue Jun 25 13:13:24 2013 +0530

    mmc: core: expose HPI capability to SWITCH commands
    
    Some of the time consuming operations such as BKOPS, SANITIZE, CACHE
    flush/off use the SWITCH command (CMD6) but as these operations don't
    have card specification defined timeout for completion, we may see
    timeout errors if card doesn't complete the operation within the SW
    defined timeout. If SW defined timeout is hit, above operations are
    considered to be failed and no real recovery mechanism is implemented
    after timeout.
    
    Most of the above operations (BKOPS/SANITIZE/CACHE flush/off) can be
    interrupted by sending the HPI (High Priority Interrupt) command to card
    if they taking longer than expected. This change adds the base support
    which will these operations to be HPIed after timeout.
    
    Change-Id: Ibd9061525756aaae656b1ceeeaed62e04fb80cce
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>

commit 5b5aa297853b91a25ef4db13f9c0d3f33c6889fe
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Thu Jun 13 10:34:48 2013 +0530

    mmc: core: Fix MMC clock scaling in case of tuning failure
    
    When the clock scaling state is changed from MMC_LOAD_LOW to
    MMC_LOAD_HIGH, the clocks are first scaled up and then tuning
    is performed. But in case of tuning failure, the current code
    does nothing but still retain the previous clock scale stats
    (state and curr_freq within struct clk_scaling). Hence, correct
    it to scale down the clocks in case of tuning failure so that
    clock scaling stats reflect the correct status. This also helps
    proceed with data transfers at lower clock rate in such cases.
    
    Change-Id: I7e9379d1e3ddc863132af31019604c22a42f8d59
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>

commit 372e3e28fbcc10d484190903df800dbd379cde7b
Author: Maya Erez <merez@codeaurora.org>
Date:   Mon Jul 8 18:23:44 2013 +0300

    mmc: core: remove the polling for BKOPS completion
    
    The original intention of polling for BKOPS completion was to give
    the card enough time to perform the BKOPS before it is runtime suspended.
    But as the BKOPS completion polling was happening in a different
    context, it may race with card runtime/platform suspend which is quite
    difficult to fix. So instead of BKOPS polling, let the runtime suspend
    get deferred if the BKOPS is running on the card. Also if BKOPS is running
    when platform suspend is triggered, stop the BKOPS before suspending the
    card.
    
    Conflicts:
    	drivers/mmc/core/core.c
    	include/linux/mmc/card.h
    
    CRs-Fixed: 489523
    Change-Id: I21e524dc2da37c4985c210abfaca00a28049c651
    Signed-off-by: Maya Erez <merez@codeaurora.org>
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>

commit eecb06a03c28c5106af4d940cb31dbf5d924dbe3
Author: Tomasz Stanislawski <t.stanislaws@samsung.com>
Date:   Wed Jun 12 21:05:02 2013 +0000

    mm/page_alloc.c: fix watermark check in __zone_watermark_ok()
    
    The watermark check consists of two sub-checks.  The first one is:
    
    	if (free_pages <= min + lowmem_reserve)
    		return false;
    
    The check assures that there is minimal amount of RAM in the zone.  If
    CMA is used then the free_pages is reduced by the number of free pages
    in CMA prior to the over-mentioned check.
    
    	if (!(alloc_flags & ALLOC_CMA))
    		free_pages -= zone_page_state(z, NR_FREE_CMA_PAGES);
    
    This prevents the zone from being drained from pages available for
    non-movable allocations.
    
    The second check prevents the zone from getting too fragmented.
    
    	for (o = 0; o < order; o++) {
    		free_pages -= z->free_area[o].nr_free << o;
    		min >>= 1;
    		if (free_pages <= min)
    			return false;
    	}
    
    The field z->free_area[o].nr_free is equal to the number of free pages
    including free CMA pages.  Therefore the CMA pages are subtracted twice.
    This may cause a false positive fail of __zone_watermark_ok() if the CMA
    area gets strongly fragmented.  In such a case there are many 0-order
    free pages located in CMA.  Those pages are subtracted twice therefore
    they will quickly drain free_pages during the check against
    fragmentation.  The test fails even though there are many free non-cma
    pages in the zone.
    
    This patch fixes this issue by subtracting CMA pages only for a purpose of
    (free_pages <= min + lowmem_reserve) check.
    
    Laura said:
    
      We were observing allocation failures of higher order pages (order 5 =
      128K typically) under tight memory conditions resulting in driver
      failure.  The output from the page allocation failure showed plenty of
      free pages of the appropriate order/type/zone and mostly CMA pages in
      the lower orders.
    
      For full disclosure, we still observed some page allocation failures
      even after applying the patch but the number was drastically reduced and
      those failures were attributed to fragmentation/other system issues.
    
    Change-Id: Ic2c0c233993c41c630e24d71df5e12aa614588e5
    CRs-Fixed:549847
    Signed-off-by: Tomasz Stanislawski <t.stanislaws@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: <stable@vger.kernel.org>	[3.7+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 026b08147923142e925a7d0aaa39038055ae0156
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srinivasarao P <spathi@codeaurora.org>

commit de142806214fcc559d780fe012586a280ae3cdb2
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Fri Sep 6 16:08:25 2013 +0530

    msm: rotator: Add support to YCBYCR rotator format
    
    Add support for MDP_YCBYCR_H2V1 interleaved YUV format in rotator
    block.
    
    Change-Id: I4bb192aaab1e72f6e5687ae222a5f9ea2c254bd4
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit 361d5ab106fc7b1278058b83e927a8c05aa60cb4
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Fri Sep 6 16:07:59 2013 +0530

    msm_fb: display: Add support to YCBYCR MDP format
    
    Add support of MDP_YCBYCR_H2V1 interleaved YUV format to MDP
    for a-family targets.
    
    Change-Id: I5afb84a95693d1ced114152364782a10c4d56bc2
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit a646f5583a217ef7039c8a746f1610d0e3309626
Author: Patrick McHardy <kaber@trash.net>
Date:   Fri Apr 5 08:13:30 2013 +0000

    netfilter: nf_ct_sip: don't drop packets with offsets pointing outside the packet
    
    Some Cisco phones create huge messages that are spread over multiple packets.
    After calculating the offset of the SIP body, it is validated to be within
    the packet and the packet is dropped otherwise. This breaks operation of
    these phones. Since connection tracking is supposed to be passive, just let
    those packets pass unmodified and untracked.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    
    Conflicts:
    	net/netfilter/nf_conntrack_sip.c
    
    This commit appears in the 3.8 and 3.9 branches of the Linux kernel
    and according to feanor3 on xda-developers:
    
    The "Cisco Jabber" app lets you use your cell phone as a SIP
    endpoint with your work number on a Cisco phone system. The
    registration packets that Cisco uses are apparently larger
    than normal. With your kernel, the registration does not complete.
    With your kernel and line 1421 changed to NF_ACCEPT, the registration
    complete
    
    Change-Id: If0c4eff68fa10af43767ad49808394910cae4309

commit 912483fa94f5fb729864fa3073bf8981ba851149
Author: Arne Coucheron <arco68@gmail.com>
Date:   Fri Sep 27 16:53:10 2013 +0200

    defconfig: Fix selinux typos
    
    Change-Id: I9b3e5b9ac50449a6190b1876cde912d7174b7023

commit 5f59c80823249282a0a0b5124519880ca01dee41
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Thu Dec 15 15:44:45 2011 -0600

    AIO: Don't plug the I/O queue in do_io_submit()
    Asynchronous I/O latency to a solid-state disk greatly increased
    between the 2.6.32 and 3.0 kernels. By removing the plug from
    do_io_submit(), we observed a 34% improvement in the I/O latency.
    
    Unfortunately, at this level, we don't know if the request is to
    a rotating disk or not.
    
    Change-Id: I7101df956473ed9fd5dcff18e473dd93b688a5c1
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: linux-aio@kvack.org
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>

commit 3255197d1af7f2d655d9c336817ccb93d2f48d6a
Author: Arun Bharadwaj <abharadw@codeaurora.org>
Date:   Wed Jul 3 10:35:02 2013 -0700

    tracing/sched: Add trace events to track cpu hotplug.
    
    Add ftrace event trace_sched_cpu_hotplug to track cpu
    hot-add and hot-remove events.
    
    This is useful in a variety of power, performance and
    debug analysis scenarios.
    
    Change-Id: I5d202c7a229ffacc3aafb7cf9afee0b0ee7b0931
    Signed-off-by: Arun Bharadwaj <abharadw@codeaurora.org>

commit 65dc7d28165199d7eea21cfcdee3a85180aede82
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Mon Aug 5 18:03:22 2013 -0700

    watchdog: Fix warning caused by use of smp_processor_id()
    
    watchdog driver creates per-cpu kernel threads that are bound to
    separate cpus. Use of smp_processor_id() by such threads in
    __touch_watchdog() is perfectly fine, as they are generally pinned to
    the cpu on which they are running.
    
    During cpu offline event, however, affinity for the watchdog thread
    bound to dying cpu is broken before it is killed. There is a small
    time window between affinity broken and thread dying in which thread
    can run on a cpu with its affinity not set to that cpu exclusively.
    This will trigger a warning from use of smp_processor_id(), which is
    harmless in this case as it will provide the required heartbeat on cpu
    where it is running and moreover that thread will be shortly killed.
    
    Mute the harmless warning by use of raw_smp_processor_id() in
    __touch_watchdog(). This seems less intrusive fix than killing threads
    in CPU_DOWN_PREPARE event handler.
    
    Change-Id: I7fa22ff529aeea0ec3d5610cfec87aea92cf95a0
    CRs-Fixed: 517188
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit a4e496ab8eea1c5e09a5f49a63aa35ac34fd5eb2
Author: Peter Boonstoppel <pboonstoppel@nvidia.com>
Date:   Thu Aug 9 15:34:47 2012 -0700

    sched: Unthrottle rt runqueues in __disable_runtime()
    
    migrate_tasks() uses _pick_next_task_rt() to get tasks from the
    real-time runqueues to be migrated. When rt_rq is throttled
    _pick_next_task_rt() won't return anything, in which case
    migrate_tasks() can't move all threads over and gets stuck in an
    infinite loop.
    
    Instead unthrottle rt runqueues before migrating tasks.
    
    Additionally: move unthrottle_offline_cfs_rqs() to rq_offline_fair()
    
    Change-Id: If8a4a399f1a14b7f4789c1b205dcfadbde555214
    Signed-off-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/5FBF8E85CA34454794F0F7ECBA79798F379D3648B7@HQMAIL04.nvidia.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Git-commit: a4c96ae319b8047f62dedbe1eac79e321c185749
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit 72cdea620c07efd841e3355cebdcadfec460beb1
Author: Steve Kondik <shade@chemlab.org>
Date:   Wed Aug 28 00:14:55 2013 -0700

    perf: Remove deprecated code
    
    Change-Id: I30b874b35d6e75be3b05245f689f456fdb3cd70c

commit da3de620a349bc2f41e488bfe9245c6cbe6e957a
Author: Neil Leeder <nleeder@codeaurora.org>
Date:   Fri Jan 25 09:06:17 2013 -0500

    arm: common: move cpaccess sysfs node
    
    cpaccess uses sysdev which is being deprecated.
    Move the cpaccess node out of the /sys/devices/system
    directory to /sys/devices. Temporarily leave the original
    sysdev node for migration purposes, until userspace
    applications can be moved to the new node.
    
    Change-Id: Iacc776968f892fc6c6463764e576d987e4371716
    Signed-off-by: Neil Leeder <nleeder@codeaurora.org>

commit 4acacb6d852c3659dd12f329aaf2b588744f9bc4
Author: Rohit Vaswani <rvaswani@codeaurora.org>
Date:   Fri Jan 18 19:11:43 2013 -0800

    ARM: Drop VCM framework
    
    This framework wasn't accepted upstream and is not used. Drop it.
    
    Change-Id: Ieb381a679873cbfb4baf245a5bcb8df1c730d964
    Signed-off-by: Rohit Vaswani <rvaswani@codeaurora.org>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 0407f8f36d33cc38e6f9a87d4deaef6ddd953921
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jul 5 13:11:31 2012 +0100

    ARM: fix warning caused by wrongly typed arm_dma_limit
    
    arch/arm/mm/init.c: In function 'arm_memblock_init':
    arch/arm/mm/init.c:380: warning: comparison of distinct pointer types lacks a cast
    
    by fixing the typecast in its definition when DMA_ZONE is disabled.
    This was missed in 4986e5c7c (ARM: mm: fix type of the arm_dma_limit
    global variable).
    
    Change-Id: Id076f2bebe307609265afdd4229181d2004c5f9c
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 1e125c109e9a4c77784e4bb715e7dc1e3655a255
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Apr 19 17:35:51 2013 -0700

    Revert "Revert "ARM: 7169/1: topdown mmap support""
    
    This reverts commit f27e4f0e730b99ca4dabed0b408d96dbf73a8fac.
    
    An alternate fix has been proposed where userspace programs
    set ADDR_COMPAT_LAYOUT. Bring back the topdown mmap support.
    
    Change-Id: Ibd6e74d406db3a5ddb609e2d2a7a6e9dc2080eca
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 27f71f02a7c5c1f9f4374c612f7c5ee17e722c6b
Author: Neil Leeder <nleeder@codeaurora.org>
Date:   Tue Feb 19 16:10:10 2013 -0500

    Perf: Correct irq for CPU hotplug detection
    
    The platform data was requesting the wrong number,
    it should request the first perf irq item.
    
    Change-Id: I4a25b4704ed9e76172c6b0d4ca4b28a3286ab2ad
    Signed-off-by: Neil Leeder <nleeder@codeaurora.org>

commit 1f87950c91722cf29906ca232219da773a4a415f
Author: Ashwin Chaugule <ashwinc@codeaurora.org>
Date:   Wed Jan 16 11:22:08 2013 -0500

    Perf: Toggle PMU IRQ when CPU's are hotplugged
    
    When a CPU is hotplugged out while a perf session
    is active, disarm the IRQ when the CPU is preparing
    to die. This ensures that perf doesn't lock up when
    it tries to free the irq of a hotplugged CPU.
    Similarly, when a CPU comes online during a perf session
    enable the IRQ so that perf doesn't try to disable
    an unarmed IRQ when it finishes.
    
    Change-Id: Ic4e412e5f1effae0db34a3e4b5e7e5c65faed2a0
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit 0e53dc32b02f516072b5b7fa511cdccf64b8c6c5
Author: Ashwin Chaugule <ashwinc@codeaurora.org>
Date:   Mon Oct 29 16:30:05 2012 -0400

    Perf: Let platforms decide IRQ request methods.
    
    This is in preparation for adding support for the unicore A5
    and dualcore A5, both of which have the same MIDR value.
    
    Instead of adding extra parsing to the ARM generic perf_event file,
    this patch moves it to the 'mach' directory where targets types
    can be detected in an implementation specific manner.
    
    The default behavior is maintained for all other ARM targets.
    
    Change-Id: I041937273dbbd0fa4c602cf89a2e0fee7f73342b
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit 10e31c79fca4ea36499f56317958d9574591f874
Author: Ashwin Chaugule <ashwinc@codeaurora.org>
Date:   Tue Oct 9 13:35:16 2012 -0400

    Perf: Make L1 PMU IRQ name target independent
    
    Remove target name from IRQ string, since the percpu IRQ API
    is shared with all Qcomm targets.
    
    Change-Id: Id0e7d9267654b373e9360806900259e00bf5a0ab
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit 2db44627fd4e2bdffa8ea19180af0a7e1b783ec0
Author: Ashwin Chaugule <ashwinc@codeaurora.org>
Date:   Tue Jan 8 15:55:06 2013 -0500

    Perf: Restore counter after powercollapse for generic ARM PMU's
    
    The MSM SoC's which have ARM's CPU's can power collapse. Ensure
    the CPU side PMU's correctly restore the counters after coming
    out of power collapse.
    
    Change-Id: I544a1dd8ced26f726ba115d14867d9e34c2a7944
    Signed-off-by: Ashwin Chaugule <ashwinc@codeaurora.org>

commit 1dca76ffe776142de552c7b0322a6e65bfe3e34d
Author: Neil Leeder <nleeder@codeaurora.org>
Date:   Mon Jan 7 15:12:45 2013 -0500

    msm: perf: add debug patch logging framework
    
    Provide a mechanism to track which msm perf patches
    are present in a kernel. Some kernel branches do
    not include all patches which causes problems trying
    to debug perf issues. This framework provides a way
    to keep track of which patches have been included
    in a build.
    
    Change-Id: Ib8ef311454564c4609d94decd93e039c80104275
    Signed-off-by: Neil Leeder <nleeder@codeaurora.org>

commit 57c6d7943e810c26313b25a27093069305b0c30d
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Fri Mar 8 15:11:03 2013 -0800

    ARM: mm: Remove SW emulation for ARM domain manager
    
    8x50 is no longer supported in the msm-3.4 kernel, so remove this
    feature.
    
    Change-Id: I2156ef22cca82d3cce6a7d39e366b93cab32f811
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 899314c0a45c54808b26b379fe333dd409d26fd1
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Mon Mar 25 23:37:45 2013 +0530

    msm: pm: avoid array overrun for msm_pm_sleep_modes
    
    The array is not defined for the maximum possible size.
    So it is possible that the array may overrun when it is
    dereferenced using MSM_PM_MODE macro.
    
    Add a dummy entry to make sure array is of max size.
    
    Change-Id: I5783b0aa6c8295c5c5aabcb498700c6af1a3eba5
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit b881e37a47fd938296338718e22600484f18f0ae
Author: Patrick Daly <pdaly@codeaurora.org>
Date:   Thu Aug 8 11:59:23 2013 -0700

    msm: cpufreq: Update frequency index
    
    A deprecated api can be used to limit the minimum and maximum cpu speeds.
    Another driver temporarily needs to switch back to this api, instead of
    using the new version. Make this api functional again by ensuring a stale
    value is updated properly.
    
    Change-Id: Ia63772e43ec534cf2bb58e7627a87315dae5d891
    Signed-off-by: Patrick Daly <pdaly@codeaurora.org>

commit 6b455ff398281e4db2f597eaf59c4d566c34d072
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Sat Jun 8 04:03:32 2013 -0700

    msm: cpufreq: Add support for CPU clocks and msm-cpufreq device
    
    Add support for using clock APIs to do CPU/L2 frequency scaling. When clock
    APIs are used to control CPU/L2 frequencies, the cpufreq driver must handle
    the CPU -> L2 -> DDR bandwidth voting. Use a msm-cpufreq device to provide
    the per SoC list of CPU frequencies and their L2/DDR bandwidth mapping
    information.
    
    If CPU/L2 clocks are not available, fallback to using acpuclock APIs. We
    eventually want to remove the use of non-standard acpuclock APIs.
    
    Change-Id: I2c4e2c3967d73e8cdbd9833f3cb36f3d75e27b4a
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 1f7a8c6b7de83fbb7b4ecf49013b8fcc2969fe7f
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 27 23:29:43 2013 -0700

    jf: Cleanup
    
    Change-Id: I0580ee57d9b9a3392388513f09f085dfbdb2d1f5

commit 853b8e47ed971529581f64ddb37d795cd47077d7
Author: Pratik Patel <pratikp@codeaurora.org>
Date:   Thu Jun 13 23:21:40 2013 -0700

    coresight: add jtag fuse driver
    
    Add support for JTag Fuse driver which can be used by other
    JTag save-restore driver(s) to query the state of the jtag fuses to
    determine if the Hardware they manage is functionally disabled or
    not.
    
    Drivers can then take necessary actions like failing the probe if
    the Hardware they manage is functionally disabled.
    
    Change-Id: Ie8c0dc159e52cf869d3ed63b45e5332d3e380e6d
    Signed-off-by: Pratik Patel <pratikp@codeaurora.org>

commit 0617310b47355fbe0e31e2403953eaaab50ab006
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 27 23:25:08 2013 -0700

    underp
    
    Change-Id: I1345b31ae45e22a9df3bb3be2b02d4f259a16b28

commit dbb8932f450b32afe69442a53358041023f71b31
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Mon Jun 17 13:53:57 2013 -0400

    msm: mdp: Disable early suspend locally
    
    undefing at the header level could mess with other components.
    
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit 07f284de46bdbd26a56a35a82f1f6110096b6238
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Tue Jun 4 12:18:35 2013 -0400

    msm: mdp: Disable early suspend only for the MDP driver
    
    Early suspend is deprecated in the framework but disabling it
    completely can affect other subsystems. Disabling only for the
    MDP driver.
    
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit e50bec95e659445e6abf3087a238e83623603254
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 27 23:17:20 2013 -0700

    Fix compilation issues
    
    Change-Id: I0f1bef2ff96a7989a3524af57f3f3064e4cd3c25

commit a55bcff668888b21daf5f5a08094a9cef47ad675
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Apr 13 12:32:09 2012 +0200

    mm: vmalloc: use const void * for caller argument
    
    'const void *' is a safer type for caller function type. This patch
    updates all references to caller function type.
    
    Change-Id: If950cfcfc63911756ac3709c8bf6da10c8b98f1b
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Git-commit: 5e6cafc83e30f0f70c79a2b7aef237dc57e29f02
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 4ac5500969a4fbfaa91607500357f079038852fe
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Jul 30 09:11:33 2012 +0200

    ARM: dma-mapping: remove custom consistent dma region
    
    This patch changes dma-mapping subsystem to use generic vmalloc areas
    for all consistent dma allocations. This increases the total size limit
    of the consistent allocations and removes platform hacks and a lot of
    duplicated code.
    
    Atomic allocations are served from special pool preallocated on boot,
    because vmalloc areas cannot be reliably created in atomic context.
    
    Change-Id: Ibb2230e80249598a81122083bf3fa2f050a0a71e
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Git-commit: e9da6e9905e639b0f842a244bc770b48ad0523e9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixups and tweaking of some prototypes]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit bee6fe7ad8ceb649c562da52a21d1fbe24183da9
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Mon Mar 18 21:50:47 2013 -0700

    msm: acpuclock-krait: Enable HFPLL for init only if switching to it
    
    The existing code left the HFPLL enabled in hfpll_init(), even if
    the CPU was detected to be at a non-HFPLL rate. Correct this for
    power savings between when hfpll_init() and the first runtime CPU
    frequency switch happen. This also ensure votes for HFPLL
    regulators are not left unnecessarily asserted.
    
    Change-Id: Iaca5dc7e4769bdbd494d669726ba9b500256f793
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 8f2840f69b9305781d10efb8b4cd735a0316b0ae
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Tue Dec 11 09:22:53 2012 -0800

    msm: Kconfig: Disable SPARSEMEM for A-Family targets
    
    SPARSEMEM is no longer needed since the features that required it
    (memory hotplug, for example) have been superseded by other
    solutions. Remove it since it introduces overhead.
    
    CRs-Fixed: 430996
    Change-Id: I25ff8591dae1e48b5b0bf8a0669196a6d7d0cd85
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit 81b63be38ca784943dd840dd61aa31f4c848ba44
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Wed Jun 19 11:17:41 2013 -0700

    iommu: msm: Handle unmapping of PTE properly
    
    The unmap api is currently not handling unmapping of page table
    entries (PTE) properly. The generic function that calls the msm
    unmap API expects the unmap call to unmap as much as possible
    and then return the amount that was unmapped.
    In addition the unmap function does not support an arbitrary input
    length. However, the function that calls the msm unmap function
    assumes that this is supported.
    
    Both these issues can cause mappings to not be unmapped which will
    cause subsequent mappings to fail because the mapping already exists.
    
    Change-Id: I638d5c38673abe297a701de9b7209c962564e1f1
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit eabbed240e09209847bb9a4019c9be31bf5f1963
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Fri Apr 19 13:45:03 2013 -0600

    iommu: msm: prevent partial mappings on error
    
    If msm_iommu_map_range() fails mid way through the va
    range with an error, clean up the PTEs that have already
    been created so they are not leaked.
    
    Change-Id: Ie929343cd6e36cade7b2cc9b4b4408c3453e6b5f
    CRs-Fixed: 478304
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 4c09bd119c99bb61f5618af992188f8822c2d7aa
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Apr 22 10:01:55 2013 -0700

    iommu: msm: Don't treat address 0 as an error case
    
    Currently, the iommu page table code treats a scattergather
    list with physical address 0 as an error. This may not be
    correct in all cases. Physical address 0 is a valid part
    of the system and may be used for valid page allocations.
    Nothing else in the system checks for physical address 0
    for error so don't treat it as an error.
    
    Change-Id: Ie9f0dae9dace4fff3b1c3449bc89c3afdd2e63a0
    CRs-Fixed: 478304
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 685123c5235c6a80d6c83797bba50e8df187ffcc
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Mon Jan 21 14:09:15 2013 -0700

    iommu: msm: check range before mapping for IOMMU-v1
    
    Make sure iommu_map_range() does not leave a partial
    mapping on error if part of the range is already mapped.
    
    Change-Id: I108b45ce8935b73ecb65f375930fe5e00b8d91eb
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit dfae18804660cbc8a4306a8c6c135509146ac718
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri Mar 8 10:50:48 2013 -0800

    iommu: msm: Use phys_addr_t for physical addresses
    
    IOMMU map and unmap function should be using phys_addr_t
    instead of unsigned int which will not work properly with
    LPAE.
    
    Change-Id: I22b31b4f13a27c0280b0d88643a8a30d019e6e90
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 6257b9363252ca6852b945a4f72a9914cd33695b
Author: Kevin Matlage <kmatlage@codeaurora.org>
Date:   Fri Feb 1 12:41:04 2013 -0700

    iommu: msm: Let IOMMUv1 use all possible page sizes
    
    Allow the IOMMUv1 to use 16M, 1M, 64K or 4K iommu
    pages when physical and virtual addresses are
    appropriately aligned. This can reduce TLB misses
    when large buffers are mapped.
    
    Change-Id: Iffcaa04097fc3877962f3954d73a6ba448dca20b
    Signed-off-by: Kevin Matlage <kmatlage@codeaurora.org>

commit 74a511352ed4d19045ec255f4e915aacd37298bc
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Tue Jan 15 15:38:52 2013 -0800

    msm: iommu: re-use existing buffers for `extra' mappings
    
    There is a bug on some hardware that requires us to overmap Iommu
    mappings by 2x. Currently, we set aside a dummy buffer onto which we
    map all of these dummy mappings. In general, for large mappings it's
    nice to use the more efficient iommu_map_range instead of calling
    iommu_map repeatedly. However, with our current approach in
    msm_iommu_map_extra we can't use iommu_map_range for page_sizes larger
    than the dummy buffer. To avoid wasting memory by increasing the size
    of the dummy buffer, we can simply remap on top of the the buffer
    being mapped in the first place. Since the second mapping should never
    be used (besides by the buggy hardware) this should not be a problem.
    
    Re-use existing buffers for all `extra' mappings. Essentially, map the
    same physical address range twice. To be extra safe, make the second
    mapping read-only.
    
    Change-Id: I35462ad50de8da1f2befa3e3f0895925535cdc98
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit c28b4144a282fe8d2b5b581448eefec0f1fedd34
Author: Chintan Pandya <cpandya@codeaurora.org>
Date:   Tue Jan 29 19:40:01 2013 +0530

    ion: cma: Add debug heap ops for CMA heap
    
    For tracking CMA allocation by address and fragmentation
    (if any), add debug heap ops which gives buffer allocation
    info.
    
    Change-Id: Ia8bed38034b85b2d4dcf84811a348bbbe50dc16b
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit c19da4b2bdc111dd1fdf4aa2fb042b1b91f6aad6
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Apr 9 10:48:22 2013 -0700

    lib: memory_alloc: Support 64-bit physical addresses
    
    The current memory allocation code does not support physical
    addresses above 32-bits, meaning that LPAE is not supported.
    Change the types of the code internally to support 64-bit
    physical addresses. Full 64-bit virtual addresses may not be
    fully supported so this code will still need an update when
    that happens.
    
    Change-Id: I0c4a7b76784981ffcd7f4776d51e05f3592bb7b8
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 48c8a4ee50de019a5a0d66206fd41fbc1829ece6
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Apr 8 14:43:14 2013 -0700

    lib: genalloc: Use 64 bit types for holding allocations
    
    genalloc may be used to allocate physical addresses in addition
    to virtual addresses. On LPAE based systems, physical addresses
    are 64-bit which does not fit in an unsigned long. Change the
    type used for allocation internally to be 64 bit to ensure that
    genalloc can properly allocate physical addresses > 4GB.
    
    Ideally this will just be a temporary workaround until either
    a proper fix comes elsewhere or an alternative API is used to
    manage physical memory.
    
    Change-Id: Ib10b887730e0c6916de5d1e6f77e771c6cde14bb
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2372f6395682892c7cff3accacf6959d438e8826
Author: Stepan Moskovchenko <stepanm@codeaurora.org>
Date:   Mon Jan 21 13:27:25 2013 -0800

    lib: vsprintf: Add %pa format specifier for phys_addr_t types
    
    Add the %pa format specifier for printing a phys_addr_t
    type and its derivative types (such as resource_size_t),
    since the physical address size on some platforms can vary
    based on build options, regardless of the native integer
    type.
    
    Change-Id: I2ba0003d689a9a2bd13f1a1e2d897b6eacc5d224
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 01aaa5d8e305d2e3a0dacaa9920f41caa138e334
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Mon Jul 15 11:52:09 2013 -0700

    kernel/lib: add additional debug capabilites for data corruption
    
    Data corruptions in the kernel often end up in system crashes that
    are easier to debug closer to the time of detection. Specifically,
    if we do not panic immediately after lock or list corruptions have been
    detected, the problem context is lost in the ensuing system mayhem.
    Add support for allowing system crash immediately after such corruptions
    are detected. The CONFIG option controls the enabling/disabling of the
    feature.
    
    Change-Id: I9b2eb62da506a13007acff63e85e9515145909ff
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit e026fb984f49fd63fb536d43221dc8f9d365cfb1
Author: Abhijeet Dharmapurikar <adharmap@codeaurora.org>
Date:   Wed Aug 7 20:17:07 2013 -0700

    irq: display the wakeup depth and disable depth of an irq
    
    The disable depth helps to track whether the interrupt is disabled
    of not. This is immensely useful in debugging interrupt related issues
    esp in cases when the driver enables/disables a device's
    interrupt at runtime.
    
    Also, the wakeup depth helps to track which interrupts are configured
    as wakeup. This helps to debug issues related to an interrupt not waking
    up the system if it were configured a wakeup interrupt.
    
    Add code to show the wakeup depth and disable depth.
    
    Change-Id: I780a8f7dd24d4acfb2da98761c873dd09a154a2e
    Signed-off-by: Abhijeet Dharmapurikar <adharmap@codeaurora.org>

commit 56a23f9b00683ff450cb7729e87c61d76e9252a2
Author: Jin Hong <jinh@codeaurora.org>
Date:   Mon Sep 10 09:37:25 2012 -0700

    ARM: hw_breakpoint: enable HAVE_HW_BREAKPOINT feature flag
    
    HAVE_HW_BREAKPOINT was disabled temporarily from commit
    480a82f59712136b51ee5ca05c4f22c1738010c5. Issue not seen with the
    latest kernel hence re-enable the feature and fix merge issue from
    previous kernel upgrade.
    
    Change-Id: Ia6e7e8282c848dd73108e4d951b3375bb7caa63c
    Signed-off-by: Jin Hong <jinh@codeaurora.org>
    Signed-off-by: Taniya Das <tdas@codeaurora.org>

commit 807ef2cabcb494bfb457af1463a7f00783221b46
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Sep 18 20:23:13 2012 -0700

    ARM: hw_breakpoint: Clear breakpoints before enabling monitor mode
    
    The reset value of the BCR, BVR, WCR, and WVR registers are all
    UNKNOWN on ARMv7. Unfortunately, reset_ctrl_regs() clears these
    registers *after* enabling monitor mode, not before, and so some
    implementations may experience UNPREDICTABLE behavior if the
    reset values of these registers are non-zero. Clear the
    breakpoints before enabling monitor mode so that we don't
    experience boot hangs/loops due to breakpoints being enabled
    out of reset.
    
    Change-Id: I029fbe40725e803183544ddd6fe40285de4137e8
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit aa967f07b37fc360325c48a9e3b5b9c954bea850
Author: Taniya Das <tdas@codeaurora.org>
Date:   Fri Aug 24 20:15:24 2012 +0530

    ARM: fiq: Add fiq_set_type to configure lines for FIQ
    
    The lines to be used as FIQ needs to be configured to be
    set for the type as EGDE/LEVEL.
    
    Change-Id: Iee746662ecb3e36141aabfa5c5e9813144a836ad
    Signed-off-by: Taniya Das <tdas@codeaurora.org>

commit 3cb307c54d282e73b68f23949c06bd8651f073c0
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Apr 10 13:23:27 2013 -0700

    arm: arch_timer: Allow mmio and cp15 timers to coexist
    
    We need to support both instances of the architected timers at
    runtime so that we can register an mmio timer as the broadcast
    device. We also need to mark the cp15 timers as FEAT_C3_STOP
    because they lost their state in certain low power modes.
    
    Get rid of the pointer indirection and just have multiple
    functions for the mmio and cp15 cases. We also assume that the
    first frame is always available to us to simplify the logic. This
    should be good enough for us for now.
    
    Change-Id: I96f529744f9fa47c04efaf716e475267bae7043c
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit e081177579e397e1b4c0717686842502afba0c3e
Author: Abhimanyu Kapur <abhimany@codeaurora.org>
Date:   Tue Mar 26 16:26:49 2013 -0700

    ARM: architected timers: remove percpu_timer_setup
    
    A call to percpu_timer_setup is not needed as its done
    by the smp routines. This also leads to a compilation
    failure on non-SMP targets.
    
    Change-Id: I261f9c3df659f469de883263bf5f8e2a768a6b1d
    Signed-off-by: Abhimanyu Kapur <abhimany@codeaurora.org>

commit 9e6f225e5c6031ffa27432ff8b56aedd269dcb74
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jan 20 10:47:00 2012 +0000

    ARM: architected timers: add support for UP timer
    
    If CONFIG_LOCAL_TIMERS is not defined, let the architected timer
    driver register a single clock_event_device that is used as a
    global timer.
    
    Change-Id: I28f8380ef88f16739c9f75ca21ecccb8e6241711
    Git-commit: 273d16adbccdfe3e4a9d02d286b8f1d76dc9e63f
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Abhimanyu Kapur <abhimany@codeaurora.org>

commit 89f9cb041dbc32fbf0cd015daf7beeb826ff5694
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Thu Jul 26 12:00:03 2012 -0700

    arm: Move to upstream udelay via timer implementation
    
    This is a squash of a handful of changes and reverts of the
    Qualcomm specific implementation:
    
      Revert "arm: Implement a timer based __delay() loop"
    
      This reverts commit 976eafa8b18252876e15f861944acf693b07ce7e.
    
      Revert "arm: Allow machines to override __delay()"
    
      This reverts commit bc0ef8ab167272890f1aab62928b04a9aeb87ce9.
    
      Revert "arm: Translate delay.S into (mostly) C"
    
      This reverts commit 8d5868d8205d10a0a8e423f53e9cc9bb3e9d1a34.
    
      ARM: 7451/1: arch timer: implement read_current_timer and get_cycles
    
      This patch implements read_current_timer using the architected timers
      when they are selected via CONFIG_ARM_ARCH_TIMER. If they are detected
      not to be usable at runtime, we return -ENXIO to the caller.
    
      Furthermore, if read_current_timer is exported then we can implement
      get_cycles in terms of it for use as both an entropy source and for
      implementing __udelay and friends.
    
      Tested-by: Shinya Kuribayashi <shinya.kuribayashi.px@renesas.com>
      Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
      Signed-off-by: Will Deacon <will.deacon@arm.com>
      Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
      ARM: 7452/1: delay: allow timer-based delay implementation to be
      selected
    
      This patch allows a timer-based delay implementation to be selected by
      switching the delay routines over to use get_cycles, which is
      implemented in terms of read_current_timer. This further allows us to
      skip the loop calibration and have a consistent delay function in the
      face of core frequency scaling.
    
      To avoid the pain of dealing with memory-mapped counters, this
      implementation uses the co-processor interface to the architected timers
      when they are available. The previous loop-based implementation is
      kept around for CPUs without the architected timers and we retain both
      the maximum delay (2ms) and the corresponding conversion factors for
      determining the number of loops required for a given interval. Since the
      indirection of the timer routines will only work when called from C,
      the sa1100 sleep routines are modified to branch to the loop-based delay
      functions directly.
    
      Tested-by: Shinya Kuribayashi <shinya.kuribayashi.px@renesas.com>
      Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
      Signed-off-by: Will Deacon <will.deacon@arm.com>
      Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
      ARM: delay: set loops_per_jiffy when moving to timer-based loop
    
      The delay functions may be called by some platforms between switching to
      the timer-based delay loop but before calibration. In this case, the
      initial loops_per_jiffy may not be suitable for the timer (although a
      compromise may be achievable) and delay times may be considered too
      inaccurate.
    
      This patch updates loops_per_jiffy when switching to the timer-based
      delay loop so that delays are consistent prior to calibration.
    
      Signed-off-by: Will Deacon <will.deacon@arm.com>
    
      ARM: delay: add registration mechanism for delay timer sources
    
      The current timer-based delay loop relies on the architected timer to
      initiate the switch away from the polling-based implementation. This is
      unfortunate for platforms without the architected timers but with a
      suitable delay source (that is, constant frequency, always powered-up
      and ticking as long as the CPUs are online).
    
      This patch introduces a registration mechanism for the delay timer
      (which provides an unconditional read_current_timer implementation) and
      updates the architected timer code to use the new interface.
    
      Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
      Signed-off-by: Will Deacon <will.deacon@arm.com>
    
      ARM: export default read_current_timer
    
      read_current_timer is used by get_cycles since "ARM: 7538/1: delay:
      add registration mechanism for delay timer sources", and get_cycles
      can be used by device drivers in loadable modules, so it has to
      be exported.
    
      Without this patch, building imote2_defconfig fails with
    
      ERROR: "read_current_timer" [crypto/tcrypt.ko] undefined!
    
      Signed-off-by: Arnd Bergmann <arnd@arndb.de>
      Cc: Stephen Boyd <sboyd@codeaurora.org>
      Cc: Jonathan Austin <jonathan.austin@arm.com>
      Cc: Will Deacon <will.deacon@arm.com>
      Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Change-Id: If1ad095d6852f5966ea995856103e06de6ab2f59
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    
    Conflicts:
    	arch/arm/lib/delay.c

commit fca5df5d88e972265138b46f7502b87788102282
Author: Neil Leeder <nleeder@codeaurora.org>
Date:   Thu May 23 16:05:41 2013 -0400

    msm: arm: make nohlt readable
    
    Make the debugfs node 'nohlt' readable.
    
    It is a common use case to want to turn nohlt
    on or off, but there is no way to see the current state.
    Writing to nohlt increments or decrements its reference
    counter, and does not absolutely set the state. By making
    it readable, its current state can be checked so that
    it can be determined if a write is needed to change the
    state.
    
    Change-Id: I08e327808299aef3e125f04bff8b6aad28d020cc
    Signed-off-by: Neil Leeder <nleeder@codeaurora.org>

commit 81a4df8f508b2b73da23871430d137be473fd274
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Mon Jun 17 15:40:58 2013 -0700

    ARM: sched_clock: Load cycle count after epoch stabilizes
    
    There is a small race between when the cycle count is read from
    the hardware and when the epoch stabilizes. Consider this
    scenario:
    
     CPU0                           CPU1
     ----                           ----
     cyc = read_sched_clock()
     cyc_to_sched_clock()
                                     update_sched_clock()
                                      ...
                                      cd.epoch_cyc = cyc;
      epoch_cyc = cd.epoch_cyc;
      ...
      epoch_ns + cyc_to_ns((cyc - epoch_cyc)
    
    The cyc on cpu0 was read before the epoch changed. But we
    calculate the nanoseconds based on the new epoch by subtracting
    the new epoch from the old cycle count. Since epoch is most likely
    larger than the old cycle count we calculate a large number that
    will be converted to nanoseconds and added to epoch_ns, causing
    time to jump forward too much.
    
    Fix this problem by reading the hardware after the epoch has
    stabilized.
    
    Change-Id: I995133b229b2c2fedd5091406d1dc366d8bfff7b
    Cc: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Git-commit: 336ae1180df5f69b9e0fb6561bec01c5f64361cf
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [sboyd: reworked for file movement kernel/time -> arm/kernel]
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 8e140f38740983c1f401c9c6f57339adf30b312c
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Feb 9 05:52:45 2013 +0100

    ARM: 7643/1: sched: correct update_sched_clock()
    
    If we want load epoch_cyc and epoch_ns atomically,
    we should update epoch_cyc_copy first of all.
    This notify reader that updating is in progress.
    
    If we update epoch_cyc first like as current implementation,
    there is subtle error case.
    Look at the below example.
    
    <Initial Condition>
    cyc = 9
    ns = 900
    cyc_copy = 9
    
    == CASE 1 ==
    <CPU A = reader>           <CPU B = updater>
                               write cyc = 10
    read cyc = 10
    read ns = 900
                               write ns = 1000
                               write cyc_copy = 10
    read cyc_copy = 10
    
    output = (10, 900)
    
    == CASE 2 ==
    <CPU A = reader>           <CPU B = updater>
    read cyc = 9
                               write cyc = 10
                               write ns = 1000
    read ns = 1000
    read cyc_copy = 9
                               write cyc_copy = 10
    output = (9, 1000)
    
    If atomic read is ensured, output should be (9, 900) or (10, 1000).
    But, output in example case are not.
    
    So, change updating sequence in order to correct this problem.
    
    Change-Id: Ia9196dd50a519f516f70c3138233624b669ef96a
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 7c4e9ced424be4d36df6a3e3825763e97ee97607
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit c162c1b8f38210d03a8e843fb765cf8d33da34ee
Author: Felipe Balbi 2 <balbi@ti.com>
Date:   Tue Oct 23 19:00:03 2012 +0100

    ARM: 7565/1: sched: stop sched_clock() during suspend
    
    The scheduler imposes a requirement to sched_clock()
    which is to stop the clock during suspend, if we don't
    do that any RT thread will be rescheduled in the future
    which might cause any sort of problems.
    
    This became an issue on OMAP when we converted omap-i2c.c
    to use threaded IRQs, it turned out that depending on how
    much time we spent on suspend, the I2C IRQ thread would
    end up being rescheduled so far in the future that I2C
    transfers would timeout and, because omap_hsmmc depends
    on an I2C-connected device to detect if an MMC card is
    inserted in the slot, our rootfs would just vanish.
    
    arch/arm/kernel/sched_clock.c already had an optional
    implementation (sched_clock_needs_suspend()) which would
    handle scheduler's requirement properly, what this patch
    does is simply to make that implementation non-optional.
    
    Note that this has the side-effect that printk timings
    won't reflect the actual time spent on suspend so other
    methods to measure that will have to be used.
    
    This has been tested with beagleboard XM (OMAP3630) and
    pandaboard rev A3 (OMAP4430). Suspend to RAM is now working
    after this patch.
    
    Thanks to Kevin Hilman for helping out with debugging.
    
    Change-Id: Ie2f9e3b22eb3d1f3806cf8c598f22e2fa1b8651f
    Acked-by: Kevin Hilman <khilman@ti.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 6a4dae5e138a32b45ca5218cc2b81802f9d378c3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit da63529ce1cac9f43f9abbb8e88f97b79b2bbc75
Author: Colin Cross <ccross@android.com>
Date:   Tue Aug 7 19:05:10 2012 +0100

    ARM: 7486/1: sched_clock: update epoch_cyc on resume
    
    Many clocks that are used to provide sched_clock will reset during
    suspend.  If read_sched_clock returns 0 after suspend, sched_clock will
    appear to jump forward.  This patch resets cd.epoch_cyc to the current
    value of read_sched_clock during resume, which causes sched_clock() just
    after suspend to return the same value as sched_clock() just before
    suspend.
    
    In addition, during the window where epoch_ns has been updated before
    suspend, but epoch_cyc has not been updated after suspend, it is unknown
    whether the clock has reset or not, and sched_clock() could return a
    bogus value.  Add a suspended flag, and return the pre-suspend epoch_ns
    value during this period.
    
    The new behavior is triggered by calling setup_sched_clock_needs_suspend
    instead of setup_sched_clock.
    
    Change-Id: I7441ef74dc6802c00eea61f3b8c0a25ac00a724d
    Signed-off-by: Colin Cross <ccross@android.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CRs-Fixed: 497236
    Git-commit: 237ec6f2e51d2fc2ff37c7c5f1ccc9264d09c85b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 829ae4ce7bbe1139ef878942a3cff4fa1c30334b
Author: Tingting Yang <tingting@codeaurora.org>
Date:   Thu Aug 8 09:57:42 2013 +0800

    ARM: smp: Save CPU registers before IPI_CPU_STOP processing
    
    When a kernel panic occurs on one CPU, other CPUs are instructed to stop
    execution via the IPI_CPU_STOP message. These other CPUs dump their stack,
    which may not be good enough to reconstruct their context to perform
    post-mortem analysis. Dump each CPU's context
    (before it started procesing the IPI) into a globally accessible structure
    to allow for easier post-mortem debugging.
    
    Change-Id: I68ac75f73d7ddaebaff9122b23e341bcb00e8fb9
    Signed-off-by: Tingting Yang <tingting@codeaurora.org>

commit ec03f56c457ee9f930e5dd5bcbd0db784ab17e42
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Tue Jun 11 22:36:08 2013 -0700

    ARM: Flush the caches for non panicking CPUs in case of a kernel panic
    
    In case of a kernel panic, only the panicking CPU does an entire
    cache flush. This means that certain dirty cache lines in the L1
    caches of the other CPUs may never get flushed. This gives us
    improper RAM dumps. Add cache flushing for all the online CPUs.
    The outer domain is not flushed since it is already being done by
    the panicking CPU.
    
    Change-Id: Ibf844ecf6b4dbc3c623789f72a26936aeb4a7306
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit 82a94d3679bb26a10d9f652b3369d90b7a2e8e2d
Author: Peter Maydell <peter.maydell@linaro.org>
Date:   Thu Jul 12 23:57:35 2012 +0100

    ARM: 7465/1: Handle >4GB memory sizes in device tree and mem=size@start option
    
    The memory regions which are passed to arm_add_memory() from
    device tree blobs via early_init_dt_add_memory_arch() can
    have sizes which are larger than will fit in a 32 bit integer,
    so switch to using a phys_addr_t to hold them, to avoid
    silently dropping the top 32 bits of the size. Similarly, use
    phys_addr_t in early_mem() so that mem=size@start command line
    options specifying more than 4GB behave sensibly.
    
    Change-Id: I8ea0fa31904903aa78750dadb2e830fa5c6deb1b
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Maydell <peter.maydell@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Git-commit: a5d5f7daa744b34477c4a12728bde0a1694a1707
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit c6b5e8839067b7b491bb995e21156573b7f06e67
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Nov 8 11:10:01 2012 -0800

    ARM: SoC: add per-platform SMP operations
    
    This adds a 'struct smp_operations' to abstract the CPU initialization
    and hot plugging functions on SMP systems, which otherwise conflict
    in a multiplatform kernel. This also helps shmobile and potentially
    others that have more than one method to do these.
    
    To allow the kernel to continue building, the platform hooks are
    defined as weak symbols which are overrided by the platform code.
    Once all platforms are converted, the "weak" attribute will be
    removed and the function made static.
    
    Unlike the original version from Marc, this new version from Arnd
    does not use a generalized abstraction for per-soc data structures
    but only tries to solve the problem for the SMP operations. This
    way, we can collapse the previous four data structures into a
    single struct, which is less systematic but also easier to follow
    as a causal reader.
    
    Change-Id: I9d022d4e789f8f87f26a237fa2e9e410be070976
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    [rameezmustafa@codeaurora.org: backport to kernel 3.4]
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit a989e8ec53286b884dfe1947060693576e0f4a95
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Aug 25 13:23:48 2013 -0700

    dma-contiguous: Fix warning when device tree is disabled
    
    Change-Id: I4ed7b4a92e257919d5edae16e22e92cff4317241

commit 9296ec19dae791d4d056b2fba2b7521e49570ddd
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jun 12 09:38:43 2013 -0700

    arm: Add definitions for pte_mkexec/pte_mknexec
    
    Other architectures define pte_mkexec to mark a pte as executable.
    Add pte_mkexec for ARM to get the same functionality. Although no
    other architectures currently define it, also add pte_mknexec to
    explicitly allow a pte to be marked as non executable.
    
    Change-Id: I673b48a1d93bf00682edbb3609a478eb28eb67a1
    CRs-Fixed: 498398
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2f6fafe3dd0473a0b3e22ca4b394184d1ceb77ee
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Aug 25 13:19:23 2013 -0700

    arm: mm: Fix a merge issue
    
    Change-Id: Ia8eb75606ed6659b4800290372cba0709d7915cf

commit ef5531768452b14a5fec4abc77e0466b49a08e47
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Jun 6 12:05:01 2012 +0200

    ARM: mm: fix type of the arm_dma_limit global variable
    
    arm_dma_limit stores physical address of maximal address accessible by DMA,
    so the phys_addr_t type makes much more sense for it instead of u32. This
    patch fixes the following build warning:
    
    arch/arm/mm/init.c:380: warning: comparison of distinct pointer types lacks a cast
    
    Change-Id: Iaba492cb9d35692b0830239b044f9c4e32d7ac16
    Reported-by: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit aac87fc5c3874fddb26593b9fded6fd35556955f
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Thu May 9 16:11:45 2013 -0700

    msm: Update meminfo to reflect consecutive memory holes
    
    The device tree now has a node for each of the memory regions
    belonging to the subsystems. The meminfo blocks are updated to
    reflect these memory holes in such a way that every meminfo block
    is of a non-zero size. The start and end of the largest memory
    hole is calculated using the meminfo.
    
    Change-Id: I9eb9984f9d74cf2a493846cb2ae7f83db9f92719
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit fc085d59083a1fef1f52a6205a2cf09bb0a32004
Author: Larry Bassel <lbassel@codeaurora.org>
Date:   Mon Feb 25 10:54:16 2013 -0800

    msm: correctly handle multiple memory holes
    
    The function adjust_meminfo() did not correctly set the
    size of any meminfo which was beyond the second memory hole.
    
    Change-Id: I5bf6c5b30f7657ce9b975c9b4afb8fb3b2ce343f
    CRs-Fixed: 455187
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit 72e670fe3de5409d65320819a8d8eded4cf6da1a
Author: Greg Reid <greid@codeaurora.org>
Date:   Fri Oct 12 12:20:31 2012 -0400

    kernel: gtod: Add MSM-specific user-accessible timers
    
    Enable MSM-specific timers to be readable from user-space.
    This allows implementation of a higher-performance
    gettimeofday in user-space.
    
    Change-Id: I1f322b5396ee335b10aeb81c681593621d151176
    Signed-off-by: Brent DeGraaf <bdegraaf@codeaurora.org>

commit 077414976975ddd0c2d791bad8473600bda6d13e
Author: Greg Reid <greid@codeaurora.org>
Date:   Fri Oct 12 12:14:12 2012 -0400

    kernel: ARM-specific implementation of user-accessible timers
    
    Expose a read-only timer page to user-space to allow for a more
    efficient implementation of gettimeofday in user-space.
    
    Change-Id: Ibd6ce41f5f4ff9435e4bf4582ef5de721283d56b
    Signed-off-by: Brent DeGraaf <bdegraaf@codeaurora.org>

commit 951dd8fb4c83ddcc1a96b36f52e016065dda5e09
Author: Greg Reid <greid@codeaurora.org>
Date:   Fri Oct 12 12:07:37 2012 -0400

    kernel: Add hooks for user-accessible timers in the kernel.
    
    Hooks for user-accessible timers allow implementation of a
    more efficient gettimeofday in user-space.
    
    Change-Id: If2f63d010c1cf142eb84f3745617e756913e46f7
    Signed-off-by: Brent DeGraaf <bdegraaf@codeaurora.org>

commit 834241b975e3bcb142b90ac05f93871e774bc052
Author: Greg Reid <greid@codeaurora.org>
Date:   Fri Oct 12 11:58:45 2012 -0400

    kernel: gtod: vsyscall
    
    Introduce kernel user helper vsyscall to allow a higher performance
    gettimeofday implementation in user-space.
    
    Change-Id: I9b9971d217382129170ecd6a35461539dac9eebb
    Signed-off-by: Brent DeGraaf <bdegraaf@codeaurora.org>

commit fda05f1baa37bb063e3078f8b9436289b2e5ccce
Author: Stepan Moskovchenko <stepanm@codeaurora.org>
Date:   Thu Sep 27 13:25:23 2012 -0700

    ARM: proc: Add Krait proc info
    
    Add processor info for the Qualcomm, Inc. Krait family of
    processors, to use the generic ARMv7 initialisation
    procedure but explicitly enable the IDIV hardware
    capability flag.
    
    Change-Id: I080f60189fada2e4f87c79c6b8ef522cdc070759
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit 2847593367ca303eee7a2478ef80abf02cd2f06f
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jan 16 18:23:19 2013 -0800

    ARM: dma-mapping: Allow highmem pages to not have a mapping
    
    The DMA_ATTR_NO_KERNEL_MAPPING is used to make sure that CMA
    pages have no kernel mapping. Add support to make sure that
    highmem pages have no mapping.
    
    Change-Id: Ife76df126ecfedf0dba81a35e0de8a1787355b3d
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 31f6ee53e49591aa13e9e4b3d99c57f60a156893
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Jun 17 10:53:05 2013 -0700

    drivers: Add option to reserve default CMA region
    
    CMA provides good utilization of memory but for some use cases, the
    allocation time is too costly. Add a Kconfig option to allow the
    default region to be permanently set aside for contiguous use cases.
    
    Change-Id: I1eef508d37cf6ae3b7b7a652fc59391b186fc122
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 8d80eed6778adc681a9e8952200ff210a43c96ab
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Apr 26 15:51:06 2013 -0700

    cma: Allow option to use strict memblock_reserved memory
    
    Despite all the performance optimizations, some clients are
    still unable to use CMA because of the allocation latency.
    Rather than make those clients use a separate set of APIs,
    extend the CMA code to allow clients to keep the memory out of
    the buddy allocator. Since the pages never actually go to the
    buddy allocator, allocation and freeing is only based on the bitmap
    allocator to find the appropriate region.
    
    Change-Id: Ia31bb1212fd7b19280361128453c8d25369ce592
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a2e3e93a6c230261e82da942a2f7392052d12e05
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Mar 19 12:49:10 2013 -0700

    cma: use MEMBLOCK_ALLOC_ANYWHERE for placing CMA regions
    
    MEMBLOCK_ALLOC_ANYWHERE allocates blocks from anywhere,
    including highmem. Use this flag to allow CMA regions to
    be placed in highmem as opposed to lowmem.
    
    Change-Id: Id5fa36a96e46d60f0e898d764a1f4c8a0a37f5f8
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 23f7a12a5a5ec9c7996748923c6fe10fa55186c7
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu Mar 14 19:13:49 2013 -0700

    cma: Remove restriction on region names
    
    CMA currently restricts region names to 'region@x'. Devicetree
    does not support the same value of x to be used multiple times.
    This means that the devicetree cannot have multiple regions
    be dynamically placed (x = 0). Remove the naming restriction
    for CMA regions.
    
    Change-Id: If647f8d7e6323497952431ae5b8cae05ba17af50
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 9075404ff3f5c599a800c65538093d0ca5b4c8cc
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Feb 26 10:38:34 2013 -0800

    cma: Add support for associating regions by name
    
    Currently, the devicetree lookup code assumes that all
    CMA regions are present at a fixed address and uses the
    fixed address for associating CMA regions to devices.
    This presents a problem for dynamically assigning regions.
    Device names get mangled/changed between flattened and
    populated devicetree so relying on that is unworkable.
    Add a separate name binding to allow lookup later between
    devices.
    
    Change-Id: Iaacd9888ea708d7293f1120e2b8c473c5c601f3d
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 0be43ecc8adda406629f76b3eaf5ab16853dc0b0
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Feb 26 10:33:04 2013 -0800

    cma: Fix up devicetree bindings
    
    The correct binding for regions is linux,contiguous-regions.
    Fix it.
    
    Change-Id: I4bbb4cd3e880c75d917b5a5a081861b3197adfa3
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a370e068b706fc15184b15154a940964073238e2
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Feb 26 10:26:30 2013 -0800

    cma: Remove __init annotations from data structures
    
    Several of the CMA data structures are used after initialization
    remove the __init annotations from them.
    
    Change-Id: Iff48ed88eef7b8fffdfba4b868cc69ded3c6df42
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 0e3072176ccb661edc88cde7444983fab923b8c1
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Feb 14 13:45:28 2013 +0100

    drivers: dma-contiguous: add initialization from device tree
    
    Add device tree support for contiguous memory regions defined in device
    tree. Initialization is done in 2 steps. First, the contiguous memory is
    reserved, what happens very early, when only flattened device tree is
    available. Then on device initialization the corresponding cma regions are
    assigned to device structures.
    
    Change-Id: Ic242499b64875ee57a346d7cbc8a34ebd64e68d2
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 9cb92691b49ee0d06a061cc6ee4e2687cb52f569
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Feb 14 13:45:27 2013 +0100

    drivers: dma-contiguous: clean source code and prepare for device tree
    
    This patch cleans the initialization of dma contiguous framework. The
    all-in-one dma_declare_contiguous() function is now separated into
    dma_contiguous_reserve_area() which only steals the the memory from
    memblock allocator and dma_contiguous_add_device() function, which
    assigns given device to the specified reserved memory area. This improves
    the flexibility in defining contiguous memory areas and assigning device
    to them, because now it is possible to assign more than one device to
    the given contiguous memory area. This split in initialization is also
    required for upcoming device tree support.
    
    Change-Id: Ibddd1c9abc6550ee62b09645e7a3355256838bfe
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 18c406eb5bd0008295b159f364b922741ac24137
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Jul 6 12:02:04 2012 +0200

    mm: cma: fix condition check when setting global cma area
    
    dev_set_cma_area incorrectly assigned cma to global area on first call
    due to incorrect check. This patch fixes this issue.
    
    Change-Id: I24f410bb08c1419e1f8408f30536d82886e104e0
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 7696c14f80890598c465ba535cb997209d55f4af
Author: Michal Nazarewicz <mina86@mina86.com>
Date:   Wed Sep 5 07:50:41 2012 +0200

    drivers: dma-contiguous: refactor dma_alloc_from_contiguous()
    
    The dma_alloc_from_contiguous() function returns either a valid pointer
    to a page structure or NULL, the error code set when pageno >= cma->count
    is not used at all and can be safely removed.
    
    This commit also changes the function to avoid goto and have only one exit
    path and one place where mutex is unlocked.
    
    Change-Id: I6de94bd6fbe2288382ca9b5e5a1f691aabe0bae5
    Signed-off-by: Michal Nazarewicz <mina86@mina86.com>
    [fixed compilation break caused by missing semicolon]
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 26e4dfb654afbdcad3f427b099edffe1329bd4c9
Author: Vitaly Andrianov <vitalya@ti.com>
Date:   Wed Dec 5 09:29:25 2012 -0500

    drivers: cma: represent physical addresses as phys_addr_t
    
    This commit changes the CMA early initialization code to use phys_addr_t
    for representing physical addresses instead of unsigned long.
    
    Without this change, among other things, dma_declare_contiguous() simply
    discards any memory regions whose address is not representable as unsigned
    long.
    
    This is a problem on 32-bit PAE machines where unsigned long is 32-bit
    but physical address space is larger.
    
    Change-Id: I0651ae4bfdbd86139398433f4a3f632efd482b8a
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 6910f39418b831dae61d85fce59d1b2960c26042
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Aug 27 20:27:19 2012 +0200

    mm: cma: fix alignment requirements for contiguous regions
    
    Contiguous Memory Allocator requires each of its regions to be aligned
    in such a way that it is possible to change migration type for all
    pageblocks holding it and then isolate page of largest possible order from
    the buddy allocator (which is MAX_ORDER-1). This patch relaxes alignment
    requirements by one order, because MAX_ORDER alignment is not really
    needed.
    
    Change-Id: I57798e709636873da27520bc42f7bdb03af15beb
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    CC: Michal Nazarewicz <mina86@mina86.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 49767ac619eeedf16e0f6e3fbb3ced2ce31bf9c4
Author: Liam Mark <lmark@codeaurora.org>
Date:   Wed Jan 16 10:14:40 2013 -0800

    ion: tracing: add ftrace events for ion allocations
    
    Add ftrace events for ion allocations to make it easier to profile
    their performance.
    
    Change-Id: I9f32e076cd50d7d3a145353dfcef74f0f6cdf8a0
    Signed-off-by: Liam Mark <lmark@codeaurora.org>

commit 5b0437e7f49b4f9cb2d695776d3c7e1d2c3e7c0a
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Jun 28 12:52:17 2013 -0700

    mm: Remove __init annotations from free_bootmem_late
    
    free_bootmem_late is currently set up to only be used in init
    functions. Some clients need to use this function past initcalls.
    The functions themselves have no restrictions on being used later
    minus the __init annotations so remove the annotation.
    
    Change-Id: I7c7e15cf2780a8843ebb4610da5b633c9abb0b3d
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit bdf79f5609a8a8b290d2444088473db71181399e
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Fri Jun 14 17:39:33 2013 -0700

    msm: Increase the kernel virtual area to include lowmem
    
    Even though lowmem is accounted for in vmalloc space, allocation
    comes only from the region bounded by VMALLOC_START and VMALLOC_END.
    The kernel virtual area can now allocate from any unmapped region
    starting from PAGE_OFFSET.
    
    Change-Id: I291b9eb443d3f7445fd979bd7b09e9241ff22ba3
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit b4c1864213286d8b6aac7b0ca6c943306f3b7974
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Mon Jun 10 17:14:21 2013 -0700

    msm: Allow lowmem to be non contiguous and mixed.
    
    Any image that is expected to have a lifetime of
    the entire system can give the virtual address
    space back for use in vmalloc.
    
    Change-Id: I81ce848cd37e8573d706fa5d1aa52147b3c8da12
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit c24e39dfe0486210c1589034581dec696d0ac712
Author: Larry Bassel <lbassel@codeaurora.org>
Date:   Tue Apr 2 10:55:31 2013 -0700

    msm: simplify placing memory pools
    
    The algorithm for placing memory pools was manually inspecting
    the memblock data structures and then doing a memblock_remove()
    on the region it decided that the memory pool should be located at.
    
    If other code already did a memblock_remove() it was possible
    that the memblock data structures which this algorithm
    was inspecting would not be correct and the memory pool might
    be placed overlapping this previous allocation.
    
    Instead use arm_memblock_steal() which will not have this
    problem (and also lets the memblock subsystem do
    the allocation without any manual inspection of memblocks).
    
    Change-Id: I7db5cba0a8d5f222f9f2f131b4b42eb81171c45b
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit 41c8b247792f0c2242de5a5f57c318eaa71ad9c4
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Mar 19 10:18:16 2013 -0700

    msm: Remove android_pmem.h from board and devices files
    
    CONFIG_ANDROID_PMEM is completely deprecated. Remove all references
    to the android pmem header.
    
    Change-Id: Ic68a3af6b9cd01a439a841af26ae6dba09db2911
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d17d54d801a7e709607202c34f8e3793ce4f062f
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Mar 20 15:04:10 2013 -0700

    msm: Completely remove memory from the memblock allocator
    
    memblock_remove may not be sufficient to prevent placing
    regions on top of each other. Call memblock_reserve and
    memblock_free as well to completely remove the memory
    from the allocator.
    
    Change-Id: Iec481d7ce7a51c243e43a2d9be2fbfbf7b2717ed
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f36b5cc935a21180d638738a747bca902fc4ac52
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Wed Oct 3 17:01:40 2012 -0700

    msm: memory: use memblock instead of meminfo
    
    msm currently uses meminfo (not memblock) in memory.c for several
    purposes (mainly the placement of the memory pool(s)).  When the code
    was written originally, memblock didn't exist yet.
    
    Now that memblock exists, this is not strictly correct, but works as
    long as nobody calls memblock_remove() besides this code (if that
    occurs then the meminfo data no longer reflect reality and will likely
    cause overlap problems).
    
    To avoid problems meminfo usage should be replaced with memblock.
    
    Also, since memblock regions of the same type can't be contiguous, the
    total_size function, which calculated the aggregate size of contiguous
    meminfo banks, can be removed.
    
    Change-Id: I34054c4fe367986f472bdaae65be67cd8fbf273a
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit ea4816fbb798f89e3247bd3bc4ac847016cbf3db
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Wed Oct 3 16:43:28 2012 -0700

    msm: memory: remove obsolete stable_size function
    
    The notion of `stable' memory is going away (i.e. all memory is now
    treated as `stable'). Remove the stable_size function. Remove the
    notion of stability from the total_stable_size function and rename it
    to total_size.
    
    Change-Id: I8fd08a36b4948d57f430aea9113a0c704b8a00d0
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit 20bab06485a3ef31c602209b24356af81a88117f
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Mar 12 14:01:38 2013 -0700

    msm: Remove alloc bootmem aligned
    
    This function has no more callers and should never be used again.
    Remove it.
    
    Change-Id: I3006c03ba2294c7dfb49539dff92b21b6670b4be
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2606149ea4084d5deb89f66c5f2661355f3fbe5f
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 27 22:40:44 2013 -0700

    jf: Remove obsolete DMM
    
    Change-Id: I74e6213ad69e2aad88dd646d0f41d5db5ee9e596

commit 985d895f447b25f10b87fbd22a7d5dcff0e9a4fb
Author: Larry Bassel <lbassel@codeaurora.org>
Date:   Tue Mar 5 15:25:46 2013 -0800

    msm: remove obsolete DMM code and config options
    
    DMM is no longer supported, but there were still fragments
    of the DMM support dependent on the obsolete config options
    CONFIG_ENABLE_DMM and CONFIG_FIX_MOVABLE_ZONE present.
    Remove these config options and the associated code.
    
    Change-Id: I01c715c8d9c65eebc7ff123a8a0878166b606e0c
    CRs-fixed: 460735
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit 5c7ce5169c94323b31bb2e598201a55bf38d09df
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Apr 23 16:32:15 2013 -0700

    arm: mm: Use phys_addr_t properly for ioremap functions
    
    Several of the ioremap functions use unsigned long in places
    resulting in truncation if physical addresses greater than
    4G are passed in. Change the types appropriately.
    
    Change-Id: I1f8fb6999d6dbc4ca84c14c8163a72c1dc341143
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit b12923476490be46d534bb64c67c070334766941
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Mar 18 11:50:22 2013 -0700

    msm: Use phys_addr_t for the memory hole
    
    On systems with 64-bit physical addresses, unsigned long
    is not big enough to store the memory hole information.
    Use phys_addr_t to store this information.
    
    Change-Id: I5d5f2fff9864a0da4f8f14085d9d61a1419fc550
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit f7bd17fc80fed8abc6fde364cd0d0eb5f2becb86
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu Mar 14 20:08:00 2013 -0700

    msm: Account for unaligned memory hole
    
    The memory hole is required to be section size aligned. The
    PMD size internally may be greater than the section size. If the
    end of the memory hole is not PMD aligned, we may end up with the
    case where the physical address is PMD aligned but not the virtual
    address or vice versa. This results in crashes in features such
    as CMA that (reasonablly) assume that the virtual address will be PMD
    aligned if and only if the physical address is aligned. Account
    for this in the memory hole code by aligning the virtual address
    up to PMD_SIZE if there is a mismatch with the physical address.
    This does result in a loss in virtual address space but the
    amount should be small compared to what is gained by by turning
    on CONFIG_DONT_MAP_HOLE_AFTER_MEMBANK0. The virtual address loss
    will go away if the memory hole is properly aligned to PMD_SIZE.
    
    Change-Id: I985568a7cc6bc22bdd65586111ff078ab61b6c34
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 42534ac5ec68b4237e4c731ad770fc57087e3d70
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Apr 5 14:12:53 2013 -0700

    arm: highmem: Add support for flushing kmap_atomic mappings
    
    The highmem code provides kmap_flush_unused to ensure all kmap
    mappings are really removed if they are unused. This code does not
    handle kmap_atomic mappings since they are managed separately.
    This prevents an issue for any code which relies on having absolutely
    no mappings for a particular page. Rather than pay the penalty of
    having CONFIG_DEBUG_HIGHMEM on all the time, add functionality
    to remove the kmap_atomic mappings in a similar way to kmap_flush_unused.
    
    Change-Id: Ieb25da809b377b1fae1629e2cb75f8aebc1c1bca
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2efdf48e047baf535acffdab0e4fd71d9f6bcece
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Aug 13 00:22:28 2012 +0100

    ARM: Allow arm_memblock_steal() to remove memory from any RAM region
    
    Allow arm_memblock_steal() to remove memory from any RAM region,
    including highmem areas.  This allows memory to be stolen from the
    very top of declared memory, including highmem areas, rather than
    our precious lowmem.
    
    Change-Id: Ib4dbc4e7bc34d402134d96db5ed8118eaffa0245
    Acked-by: Sascha Hauer <s.hauer@pengutronix.de>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Git-commit: 7ac68a4c1de6aac5b0bb231fe6d8505ebe5686d9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit bcb33e485b486fe99083715805ade129e772f716
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Fri Apr 5 03:16:14 2013 +0100

    ARM: 7693/1: mm: clean-up in order to reduce to call kmap_high_get()
    
    In kmap_atomic(), kmap_high_get() is invoked for checking already
    mapped area. In __flush_dcache_page() and dma_cache_maint_page(),
    we explicitly call kmap_high_get() before kmap_atomic()
    when cache_is_vipt(), so kmap_high_get() can be invoked twice.
    This is useless operation, so remove one.
    
    v2: change cache_is_vipt() to cache_is_vipt_nonaliasing() in order to
    be self-documented
    
    Change-Id: I36e5a63fdae158d9e7562f564f7e593b351f6960
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Git-commit: dd0f67f4747797f36f0c6bab7fed6a1f2448476d
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit 893d722ad6225560effc6ced773092447adf4a24
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Thu Jul 18 15:29:20 2013 -0700

    msm: Adjust the low memory boundary
    
    The unused virtual address space in low memory is given to
    vmalloc for use. This reduces the low memory space and
    increases vmalloc space. Adjust the vmalloc_min in order
    to increase the low memory.
    
    Change-Id: I0dbef5b6e5ec3d19f5f93f06ed03a4cf1215dc4d
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit 953a88c2b02a81782b906d78784c062b66be0398
Author: Larry Bassel <lbassel@codeaurora.org>
Date:   Mon Jul 29 13:43:17 2013 -0700

    msm: Make CONFIG_STRICT_MEMORY_RWX even stricter
    
    If CONFIG_STRICT_MEMORY_RWX was set, the first section (containing
    the kernel page table and the initial code) and the section
    containing the init code were both given RWX permission, which is
    a potential security hole.
    
    Pad the first section after the initial code (which will never
    be executed when the MMU is on) to make the rest of the kernel
    text start in the second section and make the first section RW.
    
    Move some data which had ended up in the "init text"
    section into the "init data" one, as this is RW, not RX.
    Make the "init text" RX.
    
    We will not free the section containing the "init text",
    because if we do, the kernel will allocate memory for RW data there.
    
    Change-Id: I6ca5f4e07342c374246f04a3fee18042fd47c33b
    CRs-fixed: 513919
    Signed-off-by: Larry Bassel <lbassel@codeaurora.org>

commit 967c4ad54dcf7240bd5168f1947607b7431e91d5
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Jul 9 11:06:35 2013 -0700

    arm: mm: Add export symbol for set_memory_* functions
    
    Modules may need to use the set_memory_* functions if they
    explicitly need to have self modifying code or other features
    that rely on changing memory that normally shouldn't be changed.
    Match other architectures and add the exports.
    
    Change-Id: Ifa473d6d57ce113227fa4cd6bd25f7b18bd1cca0
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d8f641977a92b6f3734c1ac026041521a2178bec
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jun 12 09:44:18 2013 -0700

    arm: mm: Define set_memory_* functions for ARM
    
    Other architectures define various set_memory functions to allow
    attributes to be changed (e.g. set_memory_x, set_memory_rw, etc.)
    Currently, these functions are missing on ARM. Define these in an
    appropriate manner for ARM.
    
    Change-Id: I5083aa985d31e195f138ce7ed26d2a1d4b80faae
    CRs-Fixed: 498398
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ee41b0aea8dbbcd1fd593aa721cad83b975e5e03
Author: R Sricharan <r.sricharan@ti.com>
Date:   Sun Mar 17 10:35:41 2013 +0530

    ARM: LPAE: Fix mapping in alloc_init_section for unaligned addresses
    
    With LPAE enabled, alloc_init_section() does not map the entire
    address space for unaligned addresses.
    
    The issue also reproduced with CMA + LPAE. CMA tries to map 16MB
    with page granularity mappings during boot. alloc_init_pte()
    is called and out of 16MB, only 2MB gets mapped and rest remains
    unaccessible.
    
    Because of this OMAP5 boot is broken with CMA + LPAE enabled.
    Fix the issue by ensuring that the entire addresses are
    mapped.
    
    Change-Id: I035cc491c4bccd810ecd53c60cbff0494899fcfa
    Signed-off-by: R Sricharan <r.sricharan@ti.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <chris@cloudcar.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Patch-mainline: linux-arm-kernel @ 03/18/2013, 22:05
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d70b0f4233ac1a47e4fb176123a63f96448fd743
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Feb 27 15:05:34 2013 -0800

    Revert "ARM: allow the kernel text section to be made read-only"
    
    This reverts commit e5e483d13369ab62d95f1738edce3cd64c7ca2ff.
    
    This change breaks LPAE support. Other patches provide the same
    functionality so just revert the patch.
    
    Change-Id: Iea1e88064a1618ca1fc9d717ee5a4ffe77681745
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Stepan Moskovchenko <stepanm@codeaurora.org>

commit b7a576948a15f73aa1f86150cf21db2351381137
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Thu Nov 1 21:57:36 2012 -0700

    msm: 8974: Enable DONT_MAP_HOLE_AFTER_MEMBANK0
    
    If this config option is enabled, the kernel does not map the
    memory corresponding to the largest hole into the virtual
    memory resulting in more lowmem. If multiple holes exist, the
    largest hole in the first 256MB of the memory is not mapped.
    
    In device tree based targets the kernel is no longer dependant
    on the bootloader for the start and size of the memory holes.
    Thus the meminfo is adjusted to reflect the memory hole, if it
    is present, using the start and end of the memory hole, while
    parsing the device tree.
    
    For non-device tree based targets, the meminfo is used to find
    the start and size of the memory hole. No adjustment is needed
    to be made to the meminfo, as the hole is taken into account,
    based on the memory information passed by the atags.
    
    Change-Id: Ib2619f72ac2b4142330534c8bbe1c7d9f64ea38c
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>

commit f20c2c05c4cf6cf2dff117fbfd928feba16ee198
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Aug 22 12:26:47 2012 +0530

    arm: mm: Add VM_ARM_EMPTY_MAPPING flag to mark the PMD gaps
    
    ioremap optimization to return the addresses from the static mapping
    if already present added the feature to add the pmd empty gaps into
    the VM area but don't map such gaps in this commit "19b52abe3c5d".
    
    This introduces the problem when the pfn is 0x0 and ends up into the
    empty section gap found by the fill_pmd_gaps. If the driver calls
    the ioremap(0, PAGE_SIZE) then it will return the address which is never
    mapped, and access to that address will result in the crash.
    
    To resolve this mark the empty PMD gaps with new flag VM_ARM_EMPTY_MAPPING
    instead of the VM_ARM_STATIC_MAPPING, so that ioremap only checks
    the VM_ARM_STATIC_MAPPING flag and skips over the VM_ARM_EMPTY_MAPPING
    sections.
    
    CRs-Fixed: 385828
    Change-Id: I71e053f05abd2f611a6f17e7c86635404c31751e
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Trilok Soni <tsoni@codeaurora.org>

commit 943aeacd5e6674d0e3c1cf715592a79dc112a4ae
Author: Yan He <yanhe@codeaurora.org>
Date:   Thu Jul 18 10:28:23 2013 -0700

    msm: sps: free the interrupt for satellite mode
    
    When BAM is registered in satellite mode on apps side, we do not
    need to disable BAM HW when apps side does not use that BAM any
    more. But we still should free the interrupt. This is addressed
    in this change.
    
    Change-Id: I955ea1a341036efa82f0ecc0f45cad5ec41651ae
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 8a9d59ce8377eb70886e96ebe0b74e65a5d945c0
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon May 20 13:26:20 2013 -0700

    msm: sps: turn off BAM DMA clock after initial config
    
    For power saving, SPS driver will turn off BAM DMA clock after the
    initial configuration of BAM DMA. The clients of BAM DMA will vote
    for the clock when BAM DMA is used.
    
    Change-Id: I388a21cff3c619ef49705a064d7d5ef760fa9ef8
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 118c9f023f0f17d3bdd347f32123210c3805394d
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon May 20 13:17:29 2013 -0700

    msm: sps: enable BAM DMA client to control BAM DMA clock
    
    For power saving reason, SPS driver will turn off the clock of
    BAM DMA after the initial configuration. A new API is provided
    to BAM DMA clients to vote for or relinquish BAM DMA clock when
    they start/stop using BAM DMA.
    
    Change-Id: I03f8a6cfa19c1722ca4e92e0785ca309fdd10ba2
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit fc00d3b24e308e7a6bff6fcef4e2f8fa9248cb2d
Author: Yan He <yanhe@codeaurora.org>
Date:   Fri May 17 13:14:25 2013 -0700

    msm: sps: remove the address checking for descriptors
    
    0x0 is a valid physical address for drivers to use. Thus, remove
    the checking of physical address for descriptors.
    
    Change-Id: I19b8e215a4538ec0a9b7def505f9577e204c7730
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 1282922bff9a36d5c6e45f503d414500527d1c6b
Author: Yan He <yanhe@codeaurora.org>
Date:   Tue Feb 26 17:42:19 2013 -0800

    msm: sps: improve BAM inactivity timer function
    
    1> Add the support for BAM global inactivity timer which is a new feature
    in B family.
    2> There is an update that changes the time unit of the inactivity timer
    counter from 0.1ms to 0.125ms. Thus, we adjust the parameter in SW driver
    for this update.
    3> Inactivity timer timeout interrupt will become a new type of BAM global
    interrupts, and it is a normal interrupt different from other BAM global
    error interrupts. Thus, change the log level for callback message from INFO
    to DEBUG.
    4> Change timer result argument of inactivity timer API to an optional one
    for client drivers.
    
    Change-Id: If223ef311f0393f2f2243f6b8a862fe760ff5214
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit cb9ffba1819726d5c831f49ca2078bbe8613b32a
Author: Yan He <yanhe@codeaurora.org>
Date:   Mon Apr 8 13:00:56 2013 -0700

    msm: sps: adapt to clock API change
    
    With the change in clock driver, clk_get() can return -EPROBE_DEFER even
    when the clock is actually present in HW. Thus, we need to return
    -EPROBE_DEFER in SPS driver probe function if clk_get() fails.
    
    Change-Id: Ia1b9f18425ca5432f4a56419adf142f2674ce64a
    Signed-off-by: Yan He <yanhe@codeaurora.org>

commit 3e434586384602faa890882a8116ae5a08fced29
Author: Markus F.X.J. Oberhumer <markus@oberhumer.com>
Date:   Mon Feb 4 02:26:14 2013 +0100

    CHROMIUM: BACKPORT: lib/lzo: huge LZO decompression speedup on ARM by using unaligned access
    
    Signed-off-by: Markus F.X.J. Oberhumer <markus@oberhumer.com>
    (cherry picked from commit 42b775abafafdf811ef300e869a4e1480ef1cf95)
    Signed-off-by: Stephen Barber <smbarber@chromium.org>
    
    BUG=chromium:269723
    TEST=Boot and verify that read/write performance has improved by dd'ing
    to /dev/zram0. Verify that checksum of decompressed data matches
    original data.
    
    Backported from linux-next (next-20130806).
    
    Change-Id: I382e112b35fbb62439b53c23534c753b14ceb1ff
    Reviewed-on: https://gerrit.chromium.org/gerrit/64996
    Reviewed-by: Olof Johansson <olofj@chromium.org>
    Commit-Queue: Stephen Barber <smbarber@chromium.org>
    Tested-by: Stephen Barber <smbarber@chromium.org>

commit bc265d8b28e5d50a72dc3209f131315de1fd6047
Author: Markus F.X.J. Oberhumer <markus@oberhumer.com>
Date:   Mon Aug 13 17:25:44 2012 +0200

    CHROMIUM: BACKPORT: lib/lzo: Update LZO compression to current upstream version
    
    This commit updates the kernel LZO code to the current upsteam version
    which features a significant speed improvement - benchmarking the Calgary
    and Silesia test corpora typically shows a doubled performance in
    both compression and decompression on modern i386/x86_64/powerpc machines.
    
    Signed-off-by: Markus F.X.J. Oberhumer <markus@oberhumer.com>
    (cherry picked from commit 8b975bd3f9089f8ee5d7bbfd798537b992bbc7e7)
    Signed-off-by: Stephen Barber <smbarber@chromium.org>
    
    BUG=chromium:269723
    TEST=Boot and verify that read/write performance has improved by dd'ing
    to /dev/zram0. Verify that checksum of decompressed data matches
    original data.
    
    Backported from Linux 3.9.
    
    Change-Id: Iea48a73a45c38d4d78d140ed5efc3fcf60d07ca1
    Reviewed-on: https://gerrit.chromium.org/gerrit/64995
    Reviewed-by: Olof Johansson <olofj@chromium.org>
    Commit-Queue: Stephen Barber <smbarber@chromium.org>
    Tested-by: Stephen Barber <smbarber@chromium.org>

commit 7fea60fda1c22119df5fcc6e226a7bbae1638c44
Author: Luigi Semenzato <semenzato@chromium.org>
Date:   Tue Aug 13 15:29:41 2013 -0700

    CHROMIUM: zram: finish fixing 32-bit overflow
    
    Nasty bug.  PAGE_MASK is unsigned, so it extends to 64 bit with zeros.
    The first read in the zram block device is done by mkswap, which
    tries to read at the end of device.  zram_set_disksize is a side
    effect of the first read, so the disk size is now silently changed
    from 6GB to 2GB.  The read fails, but mkswap does not complain either:
    it finds the size of the device by trial and error and continues.
    Swapon detects the mismatch and fails.
    
    BUG=chromium:245703
    TEST=ran before and after change with various printks
    Signed-off-by: Luigi Semenzato <semenzato@chromium.org>
    
    Change-Id: I01750554964756c2759375df1f14d7bb8b859ccf
    Reviewed-on: https://gerrit.chromium.org/gerrit/65752
    Tested-by: Luigi Semenzato <semenzato@chromium.org>
    Reviewed-by: Mike Frysinger <vapier@chromium.org>
    Commit-Queue: Luigi Semenzato <semenzato@chromium.org>

commit cfebcbfc3d9e057094ea6e3387cb54bc4f0961e1
Author: Luigi Semenzato <semenzato@chromium.org>
Date:   Mon Aug 5 15:46:52 2013 -0700

    CHROMIUM: zram: fix 32-bit overflow
    
    The 64-bit division operator is not available in 32-bit kernels,
    so have to use do_div.
    
    BUG=chromium:245703
    TEST=compiled
    
    Change-Id: I44ab763ea9e115cef6212f6524518cf7d9eac8c7
    Signed-off-by: Luigi Semenzato <semenzato@chromium.org>
    Reviewed-on: https://gerrit.chromium.org/gerrit/64695
    Reviewed-by: Doug Anderson <dianders@chromium.org>

commit 8d0283f78724077c457949aa7b951dd204ba6ebf
Author: Puneet Kumar <puneetster@chromium.org>
Date:   Wed Nov 7 23:47:01 2012 -0800

    CHROMIUM: mm: Fix calculation of dirtyable memory
    
    The system uses global_dirtyable_memory() to calculate
    number of dirtyable pages/pages that can be allocated
    to the page cache.  A bug causes an underflow thus making
    the page count look like a big unsigned number.  This in turn
    confuses the dirty writeback throttling to aggressively write
    back pages as they become dirty (usually 1 page at a time).
    
    Fix is to ensure there is no underflow while doing the math.
    
    Signed-off-by: Sonny Rao <sonnyrao@chromium.org>
    Signed-off-by: Puneet Kumar <puneetster@chromium.org>
    
    BUG=chrome-os-partner:16011
    TEST=Manual; boot kernel, powerwash, login with testaccount and
    make sure no jank occurs on sync of applications
    
    Change-Id: I614e7c3156e014f0f28a4ef9bdd8cb8a2cd07b2a
    Reviewed-on: https://gerrit.chromium.org/gerrit/37612
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Olof Johansson <olofj@chromium.org>
    Commit-Ready: Puneet Kumar <puneetster@chromium.org>
    Reviewed-by: Puneet Kumar <puneetster@chromium.org>
    Tested-by: Puneet Kumar <puneetster@chromium.org>

commit 1f895f16199696194564145c47f96789938375aa
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Mon Jul 29 19:16:43 2013 +0530

    msm_fb: display: Add compulsory wait after DTV's TG is disabled.
    
    When DTV timing generator is disabled during dtv_off, a wait for
    at least a frame period is required. This is done by waiting on
    vsync. In some scenarios, vsync interrupt is received such that, no
    wait really happens which results in MDP hangs.
    Add wait for 20ms after TG is disabled, to ensure wait for at least
    a frame period. Also, print timeout error in case wait for vsync
    times out.
    
    Change-Id: I960326e495bc8dc108bf6c6f1994ea3071f1c06d
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit c4154e95542aeb078becf30ae0a18227f5747656
Author: Mayank Goyal <goyalm@codeaurora.org>
Date:   Wed Jan 16 22:39:54 2013 +0530

    msm_fb: display: Avoid dsi clock warnings during boot
    
    Correctly set up the dsi_clk_rate in
    cont_splash_clk_ctrl before enabling them.
    
    Crs-fixed: 389531
    Change-Id: Iee13e6d871ca1fdb7f3f45a5e3b36784145abd86
    Signed-off-by: Mayank Goyal <goyalm@codeaurora.org>

commit 5422e93ed904a9d3a8a54dc04cf11d78dd3225e0
Author: Sangani Suryanarayana Raju <csuryan@codeaurora.org>
Date:   Mon May 27 20:26:55 2013 +0530

    msm: display: Proper handling of prepare and unprepare clocks
    
    This change handles prepare and unprepare clocks properly to
    fix clock mismatches.
    
    Change-Id: I09986aa1f0e1a5be2e07655a1db0d23760dd0725
    Signed-off-by: Sangani Suryanarayana Raju <csuryan@codeaurora.org>
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit ed050f2b5c5f7b0f1e7b7f267476620984e5ea46
Author: Ravishangar Kalyanam <rkalya@codeaurora.org>
Date:   Mon Jul 23 18:26:09 2012 -0700

    msm: display: Add DSI clock control for MDP GDHS power collapse
    
    Enable DSI clock and regulator before enabling/disabling power collapse
    for MDP GDHS.
    
    (cherry picked from commit ad3200279e62eff01fd1bb9136cc9c0d11e10380)
    
    (cherry picked from commit 58c5992c710df379e26bb210e00c74da501db9d2)
    
    Change-Id: I76b91260c61abf214e688c0dea6989ed2cfa3801
    Signed-off-by: Ravishangar Kalyanam <rkalya@codeaurora.org>
    Signed-off-by: Siddhartha Agrawal <agrawals@codeaurora.org>
    Signed-off-by: Ram Kumar Chakravarthy Chebathini <rcheba@codeaurora.org>
    Signed-off-by: Dhivya Subramanian <dthiru@codeaurora.org>

commit 3868b6708633d8f722a8de06049efbc059e090a0
Author: Abhishek Kharbanda <akharban@codeaurora.org>
Date:   Fri May 17 12:04:30 2013 -0700

    msm_fb: hdmi: change HDMI_PHY_REG1 based on source.
    
    Set value of HDMI_PHY_REG1[2:0] for output voltage swing control.
    
    CRs-Fixed: 488207
    Change-Id: I9aeba47491467a14a3141cf7688a95a2f43e80f5
    Signed-off-by: Abhishek Kharbanda <akharban@codeaurora.org>

commit 2e30b8fb377a479e460dc84f69c48646e08193ce
Author: Mukesh Jha <cmjha@codeaurora.org>
Date:   Mon May 13 07:55:36 2013 -0700

    msm_fb: hdmi: change HDMI_PHY_REG1 based on foundry.
    
    Set value of HDMI_PHY_REG1[2:0] for output voltage swing control,
    to prevent fallout for Vlow for GF foundry MSM's.
    
    Change-Id: Ifb0f694226896fa1ad74dd84fb489f015901fc3e
    Signed-off-by: Abhishek Kharbanda <akharban@codeaurora.org>
    Signed-off-by: Mukesh Jha <cmjha@codeaurora.org>

commit dc13f6267db6db8ca53f856f51ff25f0641eddc1
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Thu May 23 16:19:47 2013 -0700

    msm: cpufreq: Relax constraints on "msm-cpufreq" workqueue
    
    This workqueue is not used in memory reclaim paths, so the
    WQ_MEM_RECLAIM flag is not needed. Additionally, there is no
    need to restrict the queue to having one in-flight work item.
    Remove these constraints.
    
    Change-Id: I9edde40917d3ec885ce061133de20680634321d0
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 7751fa9cea76644c4ec6c60ace5ac78366a0a889
Author: Saravana Kannan <skannan@codeaurora.org>
Date:   Fri Jun 7 17:23:09 2013 -0700

    msm: cpufreq: Ensure cpufreq change happens on corresponding CPU
    
    Checking the cpus_allowed mask of the current thread before changing the
    frequency doesn't guarantee that the rest of the execution will continue on
    the same CPU. The only way to guarantee this is to schedule a work on the
    specific CPU and also prevent hotplug  of that CPU (already done by
    existing code).
    
    Change-Id: I51a02fcc777a47d3c16f2d83c47e96f2c59f7ae6
    Signed-off-by: Saravana Kannan <skannan@codeaurora.org>

commit 4feb2a5bee8d2a1ad73f17ae5e37899e98aec580
Author: Praveen Chidambaram <pchidamb@codeaurora.org>
Date:   Tue Apr 30 15:25:08 2013 -0600

    msm: cpufreq: Initialize cpufreq driver early at boot
    
    MSM cpufreq driver registering with the CPUFreq framework during
    late_initcall() may cause the device to be available a few seconds after
    the clock driver is ready for changing frequency. This could be bad for
    boot and/or thermal performance, since the cpu will continue to boot at
    whatever frequency the bootloaders set it to.
    
    Initialize cpufreq driver as part of device_initcalL() but only after
    all the clock drivers have had a chance to initialize their devices.
    
    Change-Id: Iaa4867108f59a368ee5b511d35ea2e8e219d39d3
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>

commit abcde42e61db7593c06a6bb0d108a4a5532eef09
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Wed Feb 13 23:01:06 2013 +0530

    msm: hotplug: remove code related to cpu_killed
    
    Earlier the completion event cpu_killed was used to synchronize
    notify to hot plug framework that the CPU is dead. But now it is
    replaced with another completion event cpu_died and it is placed
    in the hotplug framework itself. So there is no need for a
    platform to implement a completion event.
    
    But in msm platform driver the completion event related code
    was not removed with the changes in hotplug framework. This
    patch removes the unused code.
    
    Change-Id: I4418e5e397821e39a7b6a6efa997c4a866e9b46b
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit cdf2e8ea9e2c8a32bce3c26eb7021d16de0a9465
Author: Mahesh Sivasubramanian <msivasub@codeaurora.org>
Date:   Mon May 20 13:50:16 2013 -0600

    msm: pm-8x60: Remove timeout waiting for cpu to power collapse
    
    The delays in the CPU_DYING notication path could result in the core
    initiating the hotplug to send out CPU_DEAD notifications after
    msm_pm_wait_cpu_shutdown() times out. This could result in a unknown
    resets when regulator/clocks are disabled in the CPU_DEAD path. To
    prevent unknown resets, wait infinitely to enter power collapse but
    throw a warning after the 1ms mark.
    
    The platform_cpu_kill() path is also executed in the reboot path when a
    core tries to stop secondary cores. In these cases, platform_cpu_kill
    shouldn't wait infinitely for the secondary cores to power collapse. To
    circumvent this scenario, track dying cpus as a part of the hotplug
    notification and skip waiting for the core to power collpase during
    reboot path.
    
    Change-Id: I144e95a6d019e897381d2bca62ab50ecefd56b5b
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>

commit 26e2d29c01a266ec9210c84164746dffe6c0364f
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Aug 14 14:27:41 2013 -0700

    trace: power: Adding trace events for cpufreq
    
    Added two events to help debugging cpufreq:
    1) cpufreq_sampling_event: To capture governor sampling instants
       when cpu load is evaluated and an appropriate frequency is
       selected.
    2) cpufreq_freq_synced: to capture frequency synchronization of
       destination cpu with the source cpu triggered by foreground
       thread migrations.
    
    Change-Id: I75bc7c3104b6e0750d9ede95602098d335f4f533
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit 4b17c4ffb16c9e18a763204c0d22c0d760d58117
Author: Chintan Pandya <cpandya@codeaurora.org>
Date:   Tue Jul 2 16:00:49 2013 -0700

    sysctl: Don't scan for the leaks on headers
    
    These header allocations have life cycle till the device goes
    shutdown. So, considering them as leak is false positive. Remove
    them by marking kmemleak_not_leak
    
    CRs-Fixed: 466552
    Change-Id: Id1571b78365e533ddfe866d45cef8f89b0b62bc7
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit 7a3c7dc295fc968d9d8032f3beb5ad1ab32c03ff
Author: xiaogang <xiaogang@codeaurora.org>
Date:   Tue May 28 15:38:17 2013 +0800

    fs: vfat: reduce the worst case latencies
    
    When a block partition is mounted with FAT file system
    and MS_DIRSYNC option is used, some file system operations
    like create, rename shall sleep in caller's context until
    all the metadata have been committed to the non-volatile memory.
    Since this operation is blocking call for user context,
    the WRITE_SYNC option must be used instead of WRITE
    (async operation) which incur inherent latencies while
    flushing the meta-data corresponding to directory entries
    
    Change-Id: I41c514889873a39d564271db0a421e6c66e5ae33
    Signed-off-by: xiaogang <xiaogang@codeaurora.org>

commit c1828cccb63a49079ff1c2e8de01e8d29f450a91
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 21 00:15:51 2012 -0500

    jbd2: fix assertion failure in jbd2_journal_flush()
    
    The following race is possible between start_this_handle() and someone
    calling jbd2_journal_flush().
    
    Process A                              Process B
    start_this_handle().
      if (journal->j_barrier_count) # false
      if (!journal->j_running_transaction) { #true
        read_unlock(&journal->j_state_lock);
                                           jbd2_journal_lock_updates()
                                           jbd2_journal_flush()
                                             write_lock(&journal->j_state_lock);
                                             if (journal->j_running_transaction) {
                                               # false
                                             ... wait for committing trans ...
                                             write_unlock(&journal->j_state_lock);
        ...
        write_lock(&journal->j_state_lock);
        if (!journal->j_running_transaction) { # true
          jbd2_get_transaction(journal, new_transaction);
        write_unlock(&journal->j_state_lock);
        goto repeat; # eventually blocks on j_barrier_count > 0
                                             ...
                                             J_ASSERT(!journal->j_running_transaction);
                                               # fails
    
    We fix the race by rechecking j_barrier_count after reacquiring j_state_lock
    in exclusive mode.
    
    Reported-by: yjwsignal@empal.com
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: stable@vger.kernel.org
    
    CRs-Fixed: 468318
    Change-Id: Ibe417ee5c50e257b7521b5ce8bbfa4e13547d4c5
    Git-commit: d7961c7fa4d2e3c3f12be67e21ba8799b5a7238a
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Taniya Das <tdas@codeaurora.org>

commit 322c3dd11a781c5e50b28ef51885f546904b350d
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Jul 25 13:45:21 2013 +0300

    block: row: Remove warning massage from add_request
    
    Regular priority queues is marked as "starved" if it skipped a dispatch
    due to being empty. When a new request is added to a "starved" queue
    it will be marked as urgent.
    The removed WARN_ON was warning about an impossible case when a regular
    priority (read) queue was marked as starved but wasn't empty. This is
    a possible case due to the bellow:
    If the device driver fetched a read request that is pending for
    transmission and an URGENT request arrives, the fetched read will be
    reinserted back to the scheduler. Its possible that the queue it will be
    reinserted to was marked as "starved" in the meanwhile due to being empty.
    
    CRs-fixed: 517800
    Change-Id: Iaae642ea0ed9c817c41745b0e8ae2217cc684f0c
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 6b602ad5ef56054c2c48bd70691dab24f428f473
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Sun Jul 7 17:33:05 2013 +0300

    block: Add URGENT request notification support to CFQ scheduler
    
    When the scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. This is done
    in order to reduce the latency of an urgent request. For example:
    long WRITE may be stopped to handle an urgent READ.
    
    Change-Id: I3072b8a1423870fed9c04c28d93caaf9557a7b89
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit cdebe1177e8bea510c7849e799d00d1b2371db50
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Mon Jul 8 20:17:26 2013 +0530

    msm: vidc: Add LTR feature for H264 encoder
    
    This change adds Long Term Reference picture
    selection feature for H264 video encoder.
    
    Change-Id: Ida957df244d8715c955afe8d809708123a94e999
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 90097c298df257dff006a59ae3437884a2c8cb46
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Thu Aug 22 10:40:48 2013 -0700

    cpufreq: ondemand: Fix hotplug deadlock with store_powersave_bias
    
    store_powersave_bias() acquires the hotplug lock and the dbs_mutex
    lock, but does so in the wrong order.  Deadlocks like the following
    can result.
    
    Thread A:
        get_online_cpus+0x3c/0x5c           <- acquires 'cpu_hotplug.lock'
        store_powersave_bias+0x80/0x3f4     <- acquires 'dbs_mutex'
        kobj_attr_store+0x14/0x20
        sysfs_write_file+0x108/0x13c
        vfs_write+0xb0/0x128
        sys_write+0x38/0x64
    
    Thread B:
        cpufreq_governor_dbs+0x7c/0x55c     <- acquires 'dbs_mutex'
        __cpufreq_governor+0x90/0xe0
        __cpufreq_set_policy+0x1b0/0x258
        cpufreq_add_dev_interface+0x2cc/0x334
        cpufreq_add_dev+0x514/0x580
        cpufreq_cpu_callback+0x88/0x9c
        notifier_call_chain+0x38/0x68
        __cpu_notify+0x28/0x40
        _cpu_up+0xe4/0x118                  <- acquires 'cpu_hotplug.lock'
        cpu_up+0x64/0x80
        store_online+0x48/0x78
        dev_attr_store+0x18/0x24
        sysfs_write_file+0x108/0x13c
        vfs_write+0xb0/0x128
        sys_write+0x38/0x64
    
    Fix this by flipping the order in which the locks are acquired and
    released in store_powersave_bias so that it is the same as in the
    hotplug path.
    
    Change-Id: Idc59fb29d60b8f7fceb8ed0f2bb9eff4670abda7
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit a960088541debd9f78d952e6033005a1548f8d69
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Fri May 3 17:57:57 2013 -0700

    cpufreq: Adding traces to the cpufreq
    
    Logging the following events in ondemand for debugging purposes:
    1) When thread migration of a foreground event triggers frequency
       synchronization of destination cpu with the source cpu.
    2) Ondemand sampling instant when cpu load is evaluated and an
       appropriate frequency is selected corresponding to the load.
    
    Change-Id: I44341c660b66135943f235d467090e3cc863a2fb
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit 3da37f455b0a9c3948923f4dd3a115cf30844be9
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Wed Jul 31 15:33:24 2013 -0700

    cpufreq: ondemand: Fix update_sampling_rate race with hotplug
    
    update_sampling_rate has a for loop which goes through each
    online cpu and possibly queue up the ondemand work for them.
    But while doing this it doesnt take any hotplug lock which
    could potentially cause a race condition where ondemand work
    is queued after the hotplug code (which sets the policy to NULL)
    in the governor has cancelled any pending work. This could cause
    a crash while trying to access the NULL policy in dbs_check_cpu.
    
    Protecting the for_each_online_cpu loop with get_online_cpus()
    and put_online_cpus().
    
    Change-Id: Ia3f43ca7e4bed542834ab03ca1191d728f13311c
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit 09730107a2d02d6f3a8697c2968493771b41ae5c
Author: Dilip Gudlur <dgudlur@codeaurora.org>
Date:   Mon Jun 17 13:04:31 2013 -0700

    cpufreq: ondemand: add input_boost interface
    
    Currently Ondemand governor handles any input event
    like touch by scaling the CPU frequency to maximum
    available on the target.
    
    This change adds a new sysfs interface "input_boost"
    whereby the CPU will scale to this frequency on input events.
    The value of this sysfs is user defined so input events
    can now be handled by scaling the CPU to lower frequencies
    than target max.
    
    Change-Id: I5428fd8797c9984b17a66b01a44557f2160e8b68
    Signed-off-by: Dilip Gudlur <dgudlur@codeaurora.org>

commit fb384bfba9329960424aed21be8794225e23e222
Author: Deepak Katragadda <dkatraga@codeaurora.org>
Date:   Tue Jul 9 10:13:03 2013 -0700

    msm: cpufreq: Only apply driver limits for scaling_min/max_freq writes
    
    When new values are written to scaling_min/max_freq sysfs files, the
    current code applies all the limits imposed by various ADJUST and
    INCOMPATIBLE notifiers handlers before storing them.
    
    When the ADJUST and/or INCOMPATIBLE notifiers change their limits
    frequently, this behavior makes it almost impossible for a
    user/userspace process to store the scaling_min/max_freq limits
    that go beyond the instantaneous limits imposed by the notifiers.
    Since these sysfs nodes are typically meant to set limits that need
    to be enforced for the foreseeable future, this is not a very user
    friendly behavior.
    
    So, change the behavior to only apply limits that are enforced by
    the cpufreq driver. Typically, these are just the absolute limits
    of the HW and don't change very often.
    
    Change-Id: I1ccfaa2d1ee4ea595f882485d359dbdb407a0176
    Signed-off-by: Deepak Katragadda <dkatraga@codeaurora.org>

commit 579988d536d86d325a8cfdd6b42d16d41233b2b0
Author: Tingwei Zhang <tingwei@codeaurora.org>
Date:   Wed Jul 3 16:28:24 2013 +0800

    cpufreq: ondemand: Boost CPU frequency only for touch input
    
    Originally CPU frequency were boosted for any input event.
    In some case, sensor sends a lot of input event, which
    keep CPU in high frequency.  CPU frequency only need to
    be boosted when real user interaction happens.
    
    Change-Id: Ia3ad755b98d8363a17729926610b5dd6f0075288
    CR-Fixed: 507519
    Signed-off-by: Tingwei Zhang <tingwei@codeaurora.org>

commit 71de3a5ed731804780ae948a6b82e1cee947892f
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Mon Jun 17 17:56:27 2013 -0700

    cpufreq: ondemand: Disable freq sync feature in store_powersave_bias
    
    Turn off frequency synchronization of CPUs on thread migrations
    when powersave bias is enabled. This is done to prevent re-arming
    of the dbs timer work (which is cancelled by store_powersave_bias)
    by the sync_thread.
    
    Change-Id: I165dd591845c1d66d01a14e8dfc44c767c677b0d
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>

commit bad7c7d9f3308b1fd15ffde1c69295347a069c40
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Mon Aug 5 16:32:13 2013 -0700

    lib: spinlock_debug: increase spin dump timeout
    
    On certain targets the worst case console buffer draining time can
    exceed the aggressive spin dump timeout value. Relax the timeout value
    to cater to these scenarios.
    
    Change-Id: I7312ffe06bafd2dc7adf0f4e8b73a53fc5839235
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit e4e7260a1e3b2edb94e173cbbe4ae240801ccbb0
Author: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>
Date:   Thu Aug 15 11:26:26 2013 -0700

    Revert "msm: move printk out of spin lock low_water_lock"
    
    This reverts commit 749c4210f338496abf2e16ef51308ff756b34f7b.
    
    This patch is a hack intended to solve a particular instance of a more
    generic problem. The cause of the spindump timeout has been fixed with
    the commit f37a3dc536e4ecd72961a2d9c77f5587c5c55d8f. Furthermore this
    change also introduces a race condition since the printk is actually
    intended to be protected with the spinlock.
    
    Change-Id: I8ca057f47760c7bb5b826cceeac92ae5e124e904
    Signed-off-by: Syed Rameez Mustafa <rameezmustafa@codeaurora.org>

commit ff5c6dfd5df0d89a6153759d4224f94c4b93cff6
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Tue Aug 13 09:31:32 2013 +0530

    PM / QoS: Fix deadlock during PM QoS vote to default value
    
    pm_qos_work_fn() calls pm_qos_update_request() which does
    cancel_delayed_work_sync() on the same pm_qos_work_fn()
    causing deadlock. Fix this by updating the target with
    default timeout value directly instead of calling
    pm_qos_update_request().
    
    CRs-Fixed: 526216
    Change-Id: I7de2fb1c89f87b0ebf7427116b8920aec50d5b2b
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 9b609be5ba0a7b1023caac9acf9a0aa292591a18
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Sep 7 18:22:28 2012 +0100

    ARM: 7527/1: uaccess: explicitly check __user pointer when !CPU_USE_DOMAINS
    
    The {get,put}_user macros don't perform range checking on the provided
    __user address when !CPU_HAS_DOMAINS.
    
    This patch reworks the out-of-line assembly accessors to check the user
    address against a specified limit, returning -EFAULT if is is out of
    range.
    
    [will: changed get_user register allocation to match put_user]
    [rmk: fixed building on older ARM architectures]
    
    CRs-Fixed: 504011
    Change-Id: I3818045a136fcdf72deb1371b132e090fd7ed643
    Reported-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Git-commit: 8404663f81d212918ff85f493649a7991209fa04
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 1fbdb42472d0f29a99919d921f0569f818df009f
Author: Asish Bhattacharya <asishb@codeaurora.org>
Date:   Wed Feb 13 22:06:41 2013 +0530

    ASoC: usb: move the switch setting to more appropriate location
    
    There is a delay between creation of sound card and creating of
    corresponding steam. Currently we set the switch at the time of
    creation of card. And later streams get created, so its more
    optimal if we set the switch only after creating the stream.
    
    Change-Id: Ic9f29d7a8dbcd32cde0097289ce6e1404a837372
    Signed-off-by: Asish Bhattacharya <asishb@codeaurora.org>

commit d625c0b078a4539878e086f6d96801624e410134
Author: Alex Fradkin <afradkin@codeaurora.org>
Date:   Thu Jan 3 13:03:24 2013 +0200

    msm: usbaudio: Fix memory allocation for USB audio module init
    
    Fix memory allocation issue in snd_usb_audio_init()
    use sizeof(struct) insead of sizeof(pointer)
    
    Change-Id: I270bbd7111e97dd4e0e92378f61d5025e16f5fca
    Signed-off-by: Alex Fradkin <afradkin@codeaurora.org>

commit c5b43ad289adcf4d63b6fbb636e12a8e151fe05a
Author: Deepa Madiregama <dmadireg@codeaurora.org>
Date:   Thu Jan 17 14:01:25 2013 +0530

    ASoC: msm: qdsp6: Add error check for memory commands
    
    - Write failures observed during audio playback.
    - Error checks are missing for memory map and memory unmap cmds,
      we are proceeding to write data even if memory map is failed. Due
      to this pcm_out_write failures are observed.
    - Add proper error handling for memory map and unmap cmds.
    
    CRs-Fixed: 485051
    Change-Id: Iedd369fcd655b02a213277e973059612c040fe34
    Signed-off-by: Deepa Madiregama <dmadireg@codeaurora.org>

commit 7f316380f7d47213e138ec579eb84e1a592bbc01
Author: Laxminath Kasam <lkasam@codeaurora.org>
Date:   Tue Dec 18 13:57:45 2012 +0530

    ASoC: audio: fix unlock Null pointer issue
    
    -In afe_open, trying to unlock mutex on variable
    that is already freed.
    -correct the sequence by unlock mutex first then
    free the variable.
    
    CRs-Fixed: 423196
    Change-Id: If7df0e0f46c7d16843c6d52ae821974cc74539ff
    Signed-off-by: Laxminath Kasam <lkasam@codeaurora.org>
    (cherry picked from commit 4c120ae85c6c5ac2adfdf61956b45d02461a93e3)

commit 49b6e6ae503e44b84dfa3f8e4a0920bebf39c587
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Aug 13 15:16:21 2013 -0700

    sound: Enable the switch device for USB digital audio
    
     * Samsung has this commented out in favor of their proprietary dock
       solution, but we can support both.
     * Enables full support for USB DACs on Android :)
    
    Change-Id: Id6d576886924e6c57d6e3fe920eee3ee1833ad98

commit 07ed1705bab76931547e2a4c5e81a0a202633481
Author: Vidyakumar Athota <vathota@codeaurora.org>
Date:   Tue Mar 5 10:51:32 2013 -0800

    ASoC: apq8064 : Fix for voice over USB in Fusion target
    
    - currently APQ assumes 8k and mono as USB parameters for
      voice over USB but MDM configures with 48k and stereo
      as USB parameters which results a mismatch between APQ
      and MDM configurations
    - Set the APQ parameters in sync with MDM
    
    Change-Id: Ibbdf6bb620b4fd2c25ba6879477488c85c5defc9
    Signed-off-by: VidyaKumar Athota <vathota@codeaurora.org>
    CRs-Fixed: 415079

commit 60057efaea0b4e54550129827fd7c62d58eca6eb
Author: Deepa Madiregama <dmadireg@codeaurora.org>
Date:   Fri Mar 1 12:04:19 2013 +0530

    USB: gadget: f_audio_source: Switch to DMA memory to support MMAP
    
    - Music takes more time to resume on speaker after USB unplug.
    - USB driver does not support the Non-blocking mode i.e.
      MMAP mode. When the USB is unplugged, write is blocked for
      10 sec, and after a 10 sec timeout, the USB accessory device
      is closed and the device switch to speaker occurs.
    - Changes were made to implement the MMAP interface in the
      driver that is Non-blocking. Hence, the write thread gets
      unblocked immediately and the switch to speaker is
      instantaneous.
    
    CRs-Fixed: 453134
    Change-Id: I3020f56db31e78aed114e6a4021f519b1ed234c3
    Signed-off-by: Deepa Madiregama <dmadireg@codeaurora.org>

commit e1497bb3df405b6d0707efb153c278d5be0a32b9
Author: Mike Galbraith <bitbucket@online.de>
Date:   Mon Jan 28 11:19:25 2013 +0000

    sched: Fix select_idle_sibling() bouncing cow syndrome If the previous CPU is cache affine and idle, select it. The current implementation simply traverses the sd_llc domain, taking the first idle CPU encountered, which walks buddy pairs hand in hand over the package, inflicting excruciating pain.
    
    1 tbench pair (worst case) in a 10 core + SMT package:
    
      pre   15.22 MB/sec 1 procs
      post 252.01 MB/sec 1 procs
    
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1359371965.5783.127.camel@marge.simpson.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Conflicts:
    	kernel/sched/fair.c
    
    Change-Id: I4ca60be0d759d4881fad55be78d9f075aea50121

commit 6b891101f876036ca0ccba754a80de17a637a381
Author: Tingting Yang <tingting@codeaurora.org>
Date:   Tue Aug 6 11:12:52 2013 +0800

    msm: move printk out of spin lock low_water_lock
    
    cpu3 stuck in printk more time in spin lock low_water_lock cause cpu0
    get spin lock fail and system crashed.
    
    CRs-Fixed: 521570
    Change-Id: I75356a4b4171ae2888ce6cce792f569b5ca8cdcf
    Signed-off-by: Tingting Yang <tingting@codeaurora.org>

commit 30c11ddf748b0af1ddfef7d6d6b57a354010ae19
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Tue Jul 30 14:22:33 2013 -0700

    sched: change WARN_ON_ONCE to WARN_ON in try_to_wake_up_local()
    
    The WARN_ON_ONCE() calls at the beginning of try_to_wake_up_local()
    were recently converted from BUG_ON() calls. If these hit it indicates
    something is wrong and that may contribute to other system instability.
    To eliminate the risk of an instance of one of these errors going
    un-noticed because there was an earlier instance that occured long ago,
    change to WARN_ON(). If there ever is a flood of these there are bigger
    problems.
    
    Change-Id: I392832e2b6ec24b3569b001b1af9ecd4ed6828e7
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 67bab34e595a53cdeaae6d93a63c96bef908484a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 18 12:22:34 2013 -0700

    sched: Convert BUG_ON()s in try_to_wake_up_local() to WARN_ON_ONCE()s
    
    try_to_wake_up_local() should only be invoked to wake up another
    task in the same runqueue and BUG_ON()s are used to enforce the
    rule. Missing try_to_wake_up_local() can stall workqueue
    execution but such stalls are likely to be finite either by
    another work item being queued or the one blocked getting
    unblocked.  There's no reason to trigger BUG while holding rq
    lock crashing the whole system.
    
    Convert BUG_ON()s in try_to_wake_up_local() to WARN_ON_ONCE()s.
    
    Change-Id: I75fdaaf4dcaefcf3893e0404d98c8aa91f89934d
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20130318192234.GD3042@htj.dyndns.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Git-commit: 383efcd00053ec40023010ce5034bd702e7ab373
    CRs-Fixed: 519942
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 4fa474d2ffa15f4d53de82bb223fb75fa19be37c
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue Dec 4 11:11:31 2012 -0500

    mm: vmscan: do not keep kswapd looping forever due to individual uncompactable zones
    
    When a zone meets its high watermark and is compactable in case of
    higher order allocations, it contributes to the percentage of the node's
    memory that is considered balanced.
    
    This requirement, that a node be only partially balanced, came about
    when kswapd was desparately trying to balance tiny zones when all bigger
    zones in the node had plenty of free memory.  Arguably, the same should
    apply to compaction: if a significant part of the node is balanced
    enough to run compaction, do not get hung up on that tiny zone that
    might never get in shape.
    
    When the compaction logic in kswapd is reached, we know that at least
    25% of the node's memory is balanced properly for compaction (see
    zone_balanced and pgdat_balanced).  Remove the individual zone checks
    that restart the kswapd cycle.
    
    Otherwise, we may observe more endless looping in kswapd where the
    compaction code loops back to reclaim because of a single zone and
    reclaim does nothing because the node is considered balanced overall.
    
    See for example
    
      https://bugzilla.redhat.com/show_bug.cgi?id=866988
    
    Change-Id: I9c8f180466fea814c3038fa019f317799ddc0bcb
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-and-tested-by: Thorsten Leemhuis <fedora@leemhuis.info>
    Reported-by: Jiri Slaby <jslaby@suse.cz>
    Tested-by: John Ellson <john.ellson@comcast.net>
    Tested-by: Zdenek Kabelac <zkabelac@redhat.com>
    Tested-by: Bruno Wolff III <bruno@wolff.to>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c702418f8a2fa6cc92e84a39880d458faf7af9cc
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit fda5bf4dfc7f9691929836c11daac0e75186dac2
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Nov 29 13:54:23 2012 -0800

    mm: vmscan: fix endless loop in kswapd balancing
    
    Kswapd does not in all places have the same criteria for a balanced
    zone.  Zones are only being reclaimed when their high watermark is
    breached, but compaction checks loop over the zonelist again when the
    zone does not meet the low watermark plus two times the size of the
    allocation.  This gets kswapd stuck in an endless loop over a small
    zone, like the DMA zone, where the high watermark is smaller than the
    compaction requirement.
    
    Add a function, zone_balanced(), that checks the watermark, and, for
    higher order allocations, if compaction has enough free memory.  Then
    use it uniformly to check for balanced zones.
    
    This makes sure that when the compaction watermark is not met, at least
    reclaim happens and progress is made - or the zone is declared
    unreclaimable at some point and skipped entirely.
    
    Change-Id: I17a375f356189bde63406e2127153287208157a8
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: George Spelvin <linux@horizon.com>
    Reported-by: Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
    Reported-by: Tomas Racek <tracek@redhat.com>
    Tested-by: Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 60cefed485a02bd99b6299dad70666fe49245da7
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit 008f02a9a4c0885a8a018149a42c49d0a0a83336
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Dec 20 15:05:06 2012 -0800

    compaction: fix build error in CMA && !COMPACTION
    
    isolate_freepages_block() and isolate_migratepages_range() are used for
    CMA as well as compaction so it breaks build for CONFIG_CMA &&
    !CONFIG_COMPACTION.
    
    This patch fixes it.
    
    Change-Id: I5163a4f20cc2002e19f87d1652ccabd75694b644
    [akpm@linux-foundation.org: add "do { } while (0)", per Mel]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 010fc29a45a2e8dbc08bf45ef80b8622619aaae0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 23c65796ff9442a4b78387afdf6d59486db75585
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Jan 11 14:32:16 2013 -0800

    mm: compaction: partially revert capture of suitable high-order page
    
    Eric Wong reported on 3.7 and 3.8-rc2 that ppoll() got stuck when
    waiting for POLLIN on a local TCP socket.  It was easier to trigger if
    there was disk IO and dirty pages at the same time and he bisected it to
    commit 1fb3f8ca0e92 ("mm: compaction: capture a suitable high-order page
    immediately when it is made available").
    
    The intention of that patch was to improve high-order allocations under
    memory pressure after changes made to reclaim in 3.6 drastically hurt
    THP allocations but the approach was flawed.  For Eric, the problem was
    that page->pfmemalloc was not being cleared for captured pages leading
    to a poor interaction with swap-over-NFS support causing the packets to
    be dropped.  However, I identified a few more problems with the patch
    including the fact that it can increase contention on zone->lock in some
    cases which could result in async direct compaction being aborted early.
    
    In retrospect the capture patch took the wrong approach.  What it should
    have done is mark the pageblock being migrated as MIGRATE_ISOLATE if it
    was allocating for THP and avoided races that way.  While the patch was
    showing to improve allocation success rates at the time, the benefit is
    marginal given the relative complexity and it should be revisited from
    scratch in the context of the other reclaim-related changes that have
    taken place since the patch was first written and tested.  This patch
    partially reverts commit 1fb3f8ca0e92 ("mm: compaction: capture a
    suitable high-order page immediately when it is made available").
    
    Change-Id: I985725a72aac0fdecbf4310c04d176f39e0386dd
    Reported-and-tested-by: Eric Wong <normalperson@yhbt.net>
    Tested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 8fb74b9fb2b182d54beee592350d9ea1f325917a
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 89440e9bc483237e4dd7fb9eade4841d92c22992
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Dec 11 16:02:57 2012 -0800

    mm: cma: skip watermarks check for already isolated blocks in split_free_page()
    
    Since commit 2139cbe627b8 ("cma: fix counting of isolated pages") free
    pages in isolated pageblocks are not accounted to NR_FREE_PAGES counters,
    so watermarks check is not required if one operates on a free page in
    isolated pageblock.
    
    Change-Id: Id9b38d2e4e504336e11274a30044e3aacbc37d03
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 2e30abd1730751d58463d88bc0844ab8fd7112a9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixups due to being merged
    out of order. Bring over an added 'feature' where we don't
    obey watermarks for CMA pages]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit bce78e294dc114b7da2cf1c154f3341c3f258f16
Author: Jason Liu <r64343@freescale.com>
Date:   Fri Jan 11 14:31:47 2013 -0800

    mm: compaction: fix echo 1 > compact_memory return error issue
    
    when run the folloing command under shell, it will return error
    
      sh/$ echo 1 > /proc/sys/vm/compact_memory
      sh/$ sh: write error: Bad address
    
    After strace, I found the following log:
    
      ...
      write(1, "1\n", 2)               = 3
      write(1, "", 4294967295)         = -1 EFAULT (Bad address)
      write(2, "echo: write error: Bad address\n", 31echo: write error: Bad address
      ) = 31
    
    This tells system return 3(COMPACT_COMPLETE) after write data to
    compact_memory.
    
    The fix is to make the system just return 0 instead 3(COMPACT_COMPLETE)
    from sysctl_compaction_handler after compaction_nodes finished.
    
    Change-Id: I6c379ce7c345dc3df326a49e34da60dfafcafa32
    Signed-off-by: Jason Liu <r64343@freescale.com>
    Suggested-by: David Rientjes <rientjes@google.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 7964c06d66c76507d8b6b662bffea770c29ef0ce
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 89e8699703106838047fb97d5fa7eb2af11ae9c0
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 12:00:10 2012 +0100

    mm: compaction: Add scanned and isolated counters for compaction
    
    Compaction already has tracepoints to count scanned and isolated pages
    but it requires that ftrace be enabled and if that information has to be
    written to disk then it can be disruptive. This patch adds vmstat counters
    for compaction called compact_migrate_scanned, compact_free_scanned and
    compact_isolated.
    
    With these counters, it is possible to define a basic cost model for
    compaction. This approximates of how much work compaction is doing and can
    be compared that with an oprofile showing TLB misses and see if the cost of
    compaction is being offset by THP for example. Minimally a compaction patch
    can be evaluated in terms of whether it increases or decreases cost. The
    basic cost model looks like this
    
    Fundamental unit u:	a word	sizeof(void *)
    
    Ca  = cost of struct page access = sizeof(struct page) / u
    
    Cmc = Cost migrate page copy = (Ca + PAGE_SIZE/u) * 2
    Cmf = Cost migrate failure   = Ca * 2
    Ci  = Cost page isolation    = (Ca + Wi)
    	where Wi is a constant that should reflect the approximate
    	cost of the locking operation.
    
    Csm = Cost migrate scanning = Ca
    Csf = Cost free    scanning = Ca
    
    Overall cost =	(Csm * compact_migrate_scanned) +
    	      	(Csf * compact_free_scanned)    +
    	      	(Ci  * compact_isolated)	+
    		(Cmc * pgmigrate_success)	+
    		(Cmf * pgmigrate_failed)
    
    Where the values are read from /proc/vmstat.
    
    This is very basic and ignores certain costs such as the allocation cost
    to do a migrate page copy but any improvement to the model would still
    use the same vmstat counters.
    
    Change-Id: I9db1a609fc86a95e3fd8d3774de994197ecb9adf
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Git-commit: 397487db696cae0b026a474a5cd66f4e372995e6
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit b1878369541725835ce224957aaadcf72cdfaec5
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 10:46:20 2012 +0100

    mm: compaction: Move migration fail/success stats to migrate.c
    
    The compact_pages_moved and compact_pagemigrate_failed events are
    convenient for determining if compaction is active and to what
    degree migration is succeeding but it's at the wrong level. Other
    users of migration may also want to know if migration is working
    properly and this will be particularly true for any automated
    NUMA migration. This patch moves the counters down to migration
    with the new events called pgmigrate_success and pgmigrate_fail.
    The compact_blocks_moved counter is removed because while it was
    useful for debugging initially, it's worthless now as no meaningful
    conclusions can be drawn from its value.
    
    Change-Id: I43d66b61a2a6be571ed025213d7f1b9defb1a18f
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Git-commit: 5647bc293ab15f66a7b1cda850c5e9d162a6c7c2
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 940d60cab0451068a2a31bab0bc2e860573aa815
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Dec 6 19:01:14 2012 +0000

    mm: compaction: validate pfn range passed to isolate_freepages_block
    
    Commit 0bf380bc70ec ("mm: compaction: check pfn_valid when entering a
    new MAX_ORDER_NR_PAGES block during isolation for migration") added a
    check for pfn_valid() when isolating pages for migration as the scanner
    does not necessarily start pageblock-aligned.
    
    Since commit c89511ab2f8f ("mm: compaction: Restart compaction from near
    where it left off"), the free scanner has the same problem.  This patch
    makes sure that the pfn range passed to isolate_freepages_block() is
    within the same block so that pfn_valid() checks are unnecessary.
    
    In answer to Henrik's wondering why others have not reported this:
    reproducing this requires a large enough hole with the right aligment to
    have compaction walk into a PFN range with no memmap.  Size and
    alignment depends in the memory model - 4M for FLATMEM and 128M for
    SPARSEMEM on x86.  It needs a "lucky" machine.
    
    Change-Id: I5de91fc8d9792cb339b87fe9ef058143cde5995a
    Reported-by: Henrik Rydberg <rydberg@euromail.se>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 60177d31d215bc2b4c5a7aa6f742800e04fa0a92
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 8cfd903ad295594ddb8cdab3f1bfaf3c2949bf89
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 13:56:57 2012 -0700

    mm: compaction: correct the nr_strict va isolated check for CMA
    
    Thierry reported that the "iron out" patch for isolate_freepages_block()
    had problems due to the strict check being too strict with "mm:
    compaction: Iron out isolate_freepages_block() and
    isolate_freepages_range() -fix1".  It's possible that more pages than
    necessary are isolated but the check still fails and I missed that this
    fix was not picked up before RC1.  This same problem has been identified
    in 3.7-RC1 by Tony Prisk and should be addressed by the following patch.
    
    Change-Id: I2aefac60b55ee48cf3f9f31dbd7894c024e64f28
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Tony Prisk <linux@prisktech.co.nz>
    Reported-by: Thierry Reding <thierry.reding@avionic-design.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 0db63d7e25f96e2c6da925c002badf6f144ddf30
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 20c57033a6b80729ead7b0a419d3caef8c34b64c
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:48 2012 -0700

    CMA: migrate mlocked pages
    
    Presently CMA cannot migrate mlocked pages so it ends up failing to allocate
    contiguous memory space.
    
    This patch makes mlocked pages be migrated out.  Of course, it can affect
    realtime processes but in CMA usecase, contiguous memory allocation failing
    is far worse than access latency to an mlocked page being variable while
    CMA is running.  If someone wants to make the system realtime, he shouldn't
    enable CMA because stalls can still happen at random times.
    
    Change-Id: I560f43fdeb94f8fd2a4cc9e2ac12a1593ca38ecb
    [akpm@linux-foundation.org: tweak comment text, per Mel]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e46a28790e594c0876d1a84270926abf75460f61
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixups]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 86544c325a3bf92d49a3a4797ca958a93b0140ea
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:47 2012 -0700

    mm: compaction: clear PG_migrate_skip based on compaction and reclaim activity
    
    Compaction caches if a pageblock was scanned and no pages were isolated so
    that the pageblocks can be skipped in the future to reduce scanning.  This
    information is not cleared by the page allocator based on activity due to
    the impact it would have to the page allocator fast paths.  Hence there is
    a requirement that something clear the cache or pageblocks will be skipped
    forever.  Currently the cache is cleared if there were a number of recent
    allocation failures and it has not been cleared within the last 5 seconds.
    Time-based decisions like this are terrible as they have no relationship
    to VM activity and is basically a big hammer.
    
    Unfortunately, accurate heuristics would add cost to some hot paths so
    this patch implements a rough heuristic.  There are two cases where the
    cache is cleared.
    
    1. If a !kswapd process completes a compaction cycle (migrate and free
       scanner meet), the zone is marked compact_blockskip_flush. When kswapd
       goes to sleep, it will clear the cache. This is expected to be the
       common case where the cache is cleared. It does not really matter if
       kswapd happens to be asleep or going to sleep when the flag is set as
       it will be woken on the next allocation request.
    
    2. If there have been multiple failures recently and compaction just
       finished being deferred then a process will clear the cache and start a
       full scan.  This situation happens if there are multiple high-order
       allocation requests under heavy memory pressure.
    
    The clearing of the PG_migrate_skip bits and other scans is inherently
    racy but the race is harmless.  For allocations that can fail such as THP,
    they will simply fail.  For requests that cannot fail, they will retry the
    allocation.  Tests indicated that scanning rates were roughly similar to
    when the time-based heuristic was used and the allocation success rates
    were similar.
    
    Change-Id: If690ae126badb9f9cc5632e9ffb9d376bf210fb0
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 62997027ca5b3d4618198ed8b1aba40b61b1137b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 9df38c53eb6346357dfa6fb8f3336cd08d8be181
Author: Aaditya Kumar <aaditya.kumar.30@gmail.com>
Date:   Tue Jul 17 15:48:07 2012 -0700

    mm: fix lost kswapd wakeup in kswapd_stop()
    
    Offlining memory may block forever, waiting for kswapd() to wake up
    because kswapd() does not check the event kthread->should_stop before
    sleeping.
    
    The proper pattern, from Documentation/memory-barriers.txt, is:
    
       ---  waker  ---
       event_indicated = 1;
       wake_up_process(event_daemon);
    
       ---  sleeper  ---
       for (;;) {
          set_current_state(TASK_UNINTERRUPTIBLE);
          if (event_indicated)
             break;
          schedule();
       }
    
       set_current_state() may be wrapped by:
          prepare_to_wait();
    
    In the kswapd() case, event_indicated is kthread->should_stop.
    
      === offlining memory (waker) ===
       kswapd_stop()
          kthread_stop()
             kthread->should_stop = 1
             wake_up_process()
             wait_for_completion()
    
      ===  kswapd_try_to_sleep (sleeper) ===
       kswapd_try_to_sleep()
          prepare_to_wait()
               .
               .
          schedule()
               .
               .
          finish_wait()
    
    The schedule() needs to be protected by a test of kthread->should_stop,
    which is wrapped by kthread_should_stop().
    
    Reproducer:
       Do heavy file I/O in background.
       Do a memory offline/online in a tight loop
    
    Change-Id: I2e38559304610f11a33ac89653321f8f2684bc49
    Signed-off-by: Aaditya Kumar <aaditya.kumar@ap.sony.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 1c7e7f6c0703d03af6bcd5ccc11fc15d23e5ecbe
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 9712c67b4d1e61b4373c2deec15f5a52dc6af842
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:45 2012 -0700

    mm: compaction: Restart compaction from near where it left off
    
    This is almost entirely based on Rik's previous patches and discussions
    with him about how this might be implemented.
    
    Order > 0 compaction stops when enough free pages of the correct page
    order have been coalesced.  When doing subsequent higher order
    allocations, it is possible for compaction to be invoked many times.
    
    However, the compaction code always starts out looking for things to
    compact at the start of the zone, and for free pages to compact things to
    at the end of the zone.
    
    This can cause quadratic behaviour, with isolate_freepages starting at the
    end of the zone each time, even though previous invocations of the
    compaction code already filled up all free memory on that end of the zone.
     This can cause isolate_freepages to take enormous amounts of CPU with
    certain workloads on larger memory systems.
    
    This patch caches where the migration and free scanner should start from
    on subsequent compaction invocations using the pageblock-skip information.
     When compaction starts it begins from the cached restart points and will
    update the cached restart points until a page is isolated or a pageblock
    is skipped that would have been scanned by synchronous compaction.
    
    Change-Id: Ib788e0bfd803f5abdd8476b9e203a0b6420e194b
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c89511ab2f8fe2b47585e60da8af7fd213ec877e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixup]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 4fe252252cef7f72450f5ac02c5b0ea1cea9a27f
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:43:01 2012 -0700

    mm: clean up __count_immobile_pages()
    
    The __count_immobile_pages() naming is rather awkward.  Choose a more
    clear name and add a comment.
    
    Change-Id: Ic1d8573bbc7eaa82dd5e3f9a1199ee6dd4ac9fc0
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 80934513b230bfcf70265f2ef0fdae89fb391633
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 95abe4cf390494954c9e73d197768f64f188cfa8
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:42:59 2012 -0700

    mm: do not use page_count() without a page pin
    
    d179e84ba ("mm: vmscan: do not use page_count without a page pin") fixed
    this problem in vmscan.c but same problem is in __count_immobile_pages().
    
    I copy and paste d179e84ba's contents for description.
    
    "It is unsafe to run page_count during the physical pfn scan because
    compound_head could trip on a dangling pointer when reading
    page->first_page if the compound page is being freed by another CPU."
    
    Change-Id: I2b620c07adfa2c5a0f56219a11c042dc97428085
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Wanpeng Li <liwp.linux@gmail.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 97d255c816946388bab504122937730d3447c612
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 5163811370259f73d1538357510c3db91fe04516
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Dec 20 15:05:18 2012 -0800

    mm: cma: WARN if freed memory is still in use
    
    Memory returned to free_contig_range() must have no other references.
    Let kernel to complain loudly if page reference count is not equal to 1.
    
    Change-Id: If1b84bb383d97eff441d7a1e18b874c64b7f5f85
    [rientjes@google.com: support sparsemem]
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bcc2b02f4c1b36bc67272df7119b75bac78525ab
    Git-repo: git://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 2c56c00315995a196ead921bcc681fd402878f9c
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Feb 18 07:17:06 2013 -0800

    Revert "mm: cma: on movable allocations try MIGRATE_CMA first"
    
    This reverts commit b5662d64fa5ee483b985b351dec993402422fee3.
    
    Using CMA pages first creates good utilization but has some
    unfortunate side effects. Many movable allocations come from
    the filesystem layer which can hold on to pages for long periods
    of time which causes high allocation times (~200ms) and high
    rates of failure. Revert this patch and use alternate allocation
    strategies to get better utilization.
    
    Change-Id: I917e137d5fb292c9f8282506f71a799a6451ccfa
    CRs-Fixed: 452508
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d4a6f16bb774d1be855f6a9302435979ec8e9252
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:41 2012 -0700

    mm: compaction: cache if a pageblock was scanned and no pages were isolated
    
    When compaction was implemented it was known that scanning could
    potentially be excessive.  The ideal was that a counter be maintained for
    each pageblock but maintaining this information would incur a severe
    penalty due to a shared writable cache line.  It has reached the point
    where the scanning costs are a serious problem, particularly on
    long-lived systems where a large process starts and allocates a large
    number of THPs at the same time.
    
    Instead of using a shared counter, this patch adds another bit to the
    pageblock flags called PG_migrate_skip.  If a pageblock is scanned by
    either migrate or free scanner and 0 pages were isolated, the pageblock is
    marked to be skipped in the future.  When scanning, this bit is checked
    before any scanning takes place and the block skipped if set.
    
    The main difficulty with a patch like this is "when to ignore the cached
    information?" If it's ignored too often, the scanning rates will still be
    excessive.  If the information is too stale then allocations will fail
    that might have otherwise succeeded.  In this patch
    
    o CMA always ignores the information
    o If the migrate and free scanner meet then the cached information will
      be discarded if it's at least 5 seconds since the last time the cache
      was discarded
    o If there are a large number of allocation failures, discard the cache.
    
    The time-based heuristic is very clumsy but there are few choices for a
    better event.  Depending solely on multiple allocation failures still
    allows excessive scanning when THP allocations are failing in quick
    succession due to memory pressure.  Waiting until memory pressure is
    relieved would cause compaction to continually fail instead of using
    reclaim/compaction to try allocate the page.  The time-based mechanism is
    clumsy but a better option is not obvious.
    
    Change-Id: I17a4887aca9bb3d2d9d3756089ad7c9b89922727
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bb13ffeb9f6bfeb301443994dfbf29f91117dfb3
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Context fixup due to merging patches out of order]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e5871a768c46f5318ee0c06d1cb1cd1c5132ceb1
Author: Heesub Shin <heesub.shin@samsung.com>
Date:   Mon Jan 7 11:10:13 2013 +0900

    cma: redirect page allocation to CMA
    
    CMA pages are designed to be used as fallback for movable allocations
    and cannot be used for non-movable allocations. If CMA pages are
    utilized poorly, non-movable allocations may end up getting starved if
    all regular movable pages are allocated and the only pages left are
    CMA. Always using CMA pages first creates unacceptable performance
    problems. As a midway alternative, use CMA pages for certain
    userspace allocations. The userspace pages can be migrated or dropped
    quickly which giving decent utilization.
    
    Change-Id: I6165dda01b705309eebabc6dfa67146b7a95c174
    CRs-Fixed: 452508
    [lauraa@codeaurora.org: Missing CONFIG_CMA guards, add commit text]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ceaa0698b65ff327f5319b630725da3ca3b91692
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:36 2012 -0700

    mm: compaction: acquire the zone->lock as late as possible
    
    Compaction's free scanner acquires the zone->lock when checking for
    PageBuddy pages and isolating them.  It does this even if there are no
    PageBuddy pages in the range.
    
    This patch defers acquiring the zone lock for as long as possible.  In the
    event there are no free pages in the pageblock then the lock will not be
    acquired at all which reduces contention on zone->lock.
    
    Change-Id: Iaf8b30f94e43b03ac93d751b7b97dbe0b76b87af
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Tested-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f40d1e42bb988d2a26e8e111ea4c4c7bac819b7e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit cb90664fe5b10ec7c38b535f6b797b636b2db89e
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:33 2012 -0700

    mm: compaction: acquire the zone->lru_lock as late as possible
    
    Richard Davies and Shaohua Li have both reported lock contention problems
    in compaction on the zone and LRU locks as well as significant amounts of
    time being spent in compaction.  This series aims to reduce lock
    contention and scanning rates to reduce that CPU usage.  Richard reported
    at https://lkml.org/lkml/2012/9/21/91 that this series made a big
    different to a problem he reported in August:
    
       http://marc.info/?l=kvm&m=134511507015614&w=2
    
    Patch 1 defers acquiring the zone->lru_lock as long as possible.
    
    Patch 2 defers acquiring the zone->lock as lock as possible.
    
    Patch 3 reverts Rik's "skip-free" patches as the core concept gets
    	reimplemented later and the remaining patches are easier to
    	understand if this is reverted first.
    
    Patch 4 adds a pageblock-skip bit to the pageblock flags to cache what
    	pageblocks should be skipped by the migrate and free scanners.
    	This drastically reduces the amount of scanning compaction has
    	to do.
    
    Patch 5 reimplements something similar to Rik's idea except it uses the
    	pageblock-skip information to decide where the scanners should
    	restart from and does not need to wrap around.
    
    I tested this on 3.6-rc6 + linux-next/akpm. Kernels tested were
    
    akpm-20120920	3.6-rc6 + linux-next/akpm as of Septeber 20th, 2012
    lesslock	Patches 1-6
    revert		Patches 1-7
    cachefail	Patches 1-8
    skipuseless	Patches 1-9
    
    Stress high-order allocation tests looked ok.  Success rates are more or
    less the same with the full series applied but there is an expectation
    that there is less opportunity to race with other allocation requests if
    there is less scanning.  The time to complete the tests did not vary that
    much and are uninteresting as were the vmstat statistics so I will not
    present them here.
    
    Using ftrace I recorded how much scanning was done by compaction and got this
    
                                3.6.0-rc6     3.6.0-rc6   3.6.0-rc6  3.6.0-rc6 3.6.0-rc6
                                akpm-20120920 lockless  revert-v2r2  cachefail skipuseless
    
    Total   free    scanned         360753976  515414028  565479007   17103281   18916589
    Total   free    isolated          2852429    3597369    4048601     670493     727840
    Total   free    efficiency        0.0079%    0.0070%    0.0072%    0.0392%    0.0385%
    Total   migrate scanned         247728664  822729112 1004645830   17946827   14118903
    Total   migrate isolated          2555324    3245937    3437501     616359     658616
    Total   migrate efficiency        0.0103%    0.0039%    0.0034%    0.0343%    0.0466%
    
    The efficiency is worthless because of the nature of the test and the
    number of failures.  The really interesting point as far as this patch
    series is concerned is the number of pages scanned.  Note that reverting
    Rik's patches massively increases the number of pages scanned indicating
    that those patches really did make a difference to CPU usage.
    
    However, caching what pageblocks should be skipped has a much higher
    impact.  With patches 1-8 applied, free page and migrate page scanning are
    both reduced by 95% in comparison to the akpm kernel.  If the basic
    concept of Rik's patches are implemened on top then scanning then the free
    scanner barely changed but migrate scanning was further reduced.  That
    said, tests on 3.6-rc5 indicated that the last patch had greater impact
    than what was measured here so it is a bit variable.
    
    One way or the other, this series has a large impact on the amount of
    scanning compaction does when there is a storm of THP allocations.
    
    This patch:
    
    Compaction's migrate scanner acquires the zone->lru_lock when scanning a
    range of pages looking for LRU pages to acquire.  It does this even if
    there are no LRU pages in the range.  If multiple processes are compacting
    then this can cause severe locking contention.  To make matters worse
    commit b2eef8c0 ("mm: compaction: minimise the time IRQs are disabled
    while isolating pages for migration") releases the lru_lock every
    SWAP_CLUSTER_MAX pages that are scanned.
    
    This patch makes two changes to how the migrate scanner acquires the LRU
    lock.  First, it only releases the LRU lock every SWAP_CLUSTER_MAX pages
    if the lock is contended.  This reduces the number of times it
    unnecessarily disables and re-enables IRQs.  The second is that it defers
    acquiring the LRU lock for as long as possible.  If there are no LRU pages
    or the only LRU pages are transhuge then the LRU lock will not be acquired
    at all which reduces contention on zone->lru_lock.
    
    Change-Id: If518f3acb4db4ba579b708889de0fa9f42366899
    [minchan@kernel.org: augment comment]
    [akpm@linux-foundation.org: tweak comment text]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 2a1402aa044b55c2d30ab0ed9405693ef06fb07c
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 4668319a1fd8b3d97d79fadd36bd0f4e1e29f412
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:31 2012 -0700

    mm: compaction: Update try_to_compact_pages()kerneldoc comment
    
    Parameters were added without documentation, tut tut.
    
    Change-Id: I1355906b3a3a6e3319a0fedc8ba28c3327a0c8f2
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 661c4cb9b829110cb68c18ea05a56be39f75a4d2
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit baad33815f1b0290b35258fed25585d56f2fba11
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:30 2012 -0700

    mm: compaction: move fatal signal check out of compact_checklock_irqsave
    
    Commit c67fe3752abe ("mm: compaction: Abort async compaction if locks
    are contended or taking too long") addressed a lock contention problem
    in compaction by introducing compact_checklock_irqsave() that effecively
    aborting async compaction in the event of compaction.
    
    To preserve existing behaviour it also moved a fatal_signal_pending()
    check into compact_checklock_irqsave() but that is very misleading.  It
    "hides" the check within a locking function but has nothing to do with
    locking as such.  It just happens to work in a desirable fashion.
    
    This patch moves the fatal_signal_pending() check to
    isolate_migratepages_range() where it belongs.  Arguably the same check
    should also happen when isolating pages for freeing but it's overkill.
    
    Change-Id: I026ab765b4160bdb6bbb8b7359be24b6159e382c
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3cc668f4e30fbd97b3c0574d8cac7a83903c9bc7
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 67ba7ea05e9896c9a6051c0736fa7135237ca4fb
Author: Shaohua Li <shli@kernel.org>
Date:   Mon Oct 8 16:32:27 2012 -0700

    mm: compaction: abort compaction loop if lock is contended or run too long
    
    isolate_migratepages_range() might isolate no pages if for example when
    zone->lru_lock is contended and running asynchronous compaction. In this
    case, we should abort compaction, otherwise, compact_zone will run a
    useless loop and make zone->lru_lock is even contended.
    
    An additional check is added to ensure that cc.migratepages and
    cc.freepages get properly drained whan compaction is aborted.
    
    Change-Id: Ia33b52655f9926d0bb2cde95492066bd8132149d
    [minchan@kernel.org: Putback pages isolated for migration if aborting]
    [akpm@linux-foundation.org: compact_zone_order requires non-NULL arg contended]
    [akpm@linux-foundation.org: make compact_zone_order() require non-NULL arg `contended']
    [minchan@kernel.org: Putback pages isolated for migration if aborting]
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e64c5237cf6ff474cb2f3f832f48f2b441dd9979
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 9d35dff52f0a5d9d4cc53cb768fe5e6817413544
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:29:12 2012 -0700

    mm: compaction: capture a suitable high-order page immediately when it is made available
    
    While compaction is migrating pages to free up large contiguous blocks
    for allocation it races with other allocation requests that may steal
    these blocks or break them up.  This patch alters direct compaction to
    capture a suitable free page as soon as it becomes available to reduce
    this race.  It uses similar logic to split_free_page() to ensure that
    watermarks are still obeyed.
    
    Change-Id: I46fc38ca67bc50aa7a77a59255caf563f50343a9
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 1fb3f8ca0e9222535a39b884cb67a34628411b9f
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit e2cf04b78e4af4f82fe1aff0c71ed21383467790
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:29:09 2012 -0700

    mm: compaction: update comment in try_to_compact_pages
    
    Allocation success rates have been far lower since 3.4 due to commit
    fe2c2a106663 ("vmscan: reclaim at order 0 when compaction is enabled").
    This commit was introduced for good reasons and it was known in advance
    that the success rates would suffer but it was justified on the grounds
    that the high allocation success rates were achieved by aggressive
    reclaim.  Success rates are expected to suffer even more in 3.6 due to
    commit 7db8889ab05b ("mm: have order > 0 compaction start off where it
    left") which testing has shown to severely reduce allocation success
    rates under load - to 0% in one case.
    
    This series aims to improve the allocation success rates without
    regressing the benefits of commit fe2c2a106663.  The series is based on
    latest mmotm and takes into account the __GFP_NO_KSWAPD flag is going
    away.
    
    Patch 1 updates a stale comment seeing as I was in the general area.
    
    Patch 2 updates reclaim/compaction to reclaim pages scaled on the number
    	of recent failures.
    
    Patch 3 captures suitable high-order pages freed by compaction to reduce
    	races with parallel allocation requests.
    
    Patch 4 fixes the upstream commit [7db8889a: mm: have order > 0 compaction
    	start off where it left] to enable compaction again
    
    Patch 5 identifies when compacion is taking too long due to contention
    	and aborts.
    
    STRESS-HIGHALLOC
    		 3.6-rc1-akpm	  full-series
    Pass 1          36.00 ( 0.00%)    51.00 (15.00%)
    Pass 2          42.00 ( 0.00%)    63.00 (21.00%)
    while Rested    86.00 ( 0.00%)    86.00 ( 0.00%)
    
    From
    
      http://www.csn.ul.ie/~mel/postings/mmtests-20120424/global-dhp__stress-highalloc-performance-ext3/hydra/comparison.html
    
    I know that the allocation success rates in 3.3.6 was 78% in comparison
    to 36% in in the current akpm tree.  With the full series applied, the
    success rates are up to around 51% with some variability in the results.
    This is not as high a success rate but it does not reclaim excessively
    which is a key point.
    
    MMTests Statistics: vmstat
    Page Ins                                     3050912     3078892
    Page Outs                                    8033528     8039096
    Swap Ins                                           0           0
    Swap Outs                                          0           0
    
    Note that swap in/out rates remain at 0. In 3.3.6 with 78% success rates
    there were 71881 pages swapped out.
    
    Direct pages scanned                           70942      122976
    Kswapd pages scanned                         1366300     1520122
    Kswapd pages reclaimed                       1366214     1484629
    Direct pages reclaimed                         70936      105716
    Kswapd efficiency                                99%         97%
    Kswapd velocity                             1072.550    1182.615
    Direct efficiency                                99%         85%
    Direct velocity                               55.690      95.672
    
    The kswapd velocity changes very little as expected.  kswapd velocity is
    around the 1000 pages/sec mark where as in kernel 3.3.6 with the high
    allocation success rates it was 8140 pages/second.  Direct velocity is
    higher as a result of patch 2 of the series but this is expected and is
    acceptable.  The direct reclaim and kswapd velocities change very little.
    
    If these get accepted for merging then there is a difficulty in how they
    should be handled.  7db8889a ("mm: have order > 0 compaction start off
    where it left") is broken but it is already in 3.6-rc1 and needs to be
    fixed.  However, if just patch 4 from this series is applied then Jim
    Schutt's workload is known to break again as his workload also requires
    patch 5.  While it would be preferred to have all these patches in 3.6 to
    improve compaction in general, it would at least be acceptable if just
    patches 4 and 5 were merged to 3.6 to fix a known problem without breaking
    compaction completely.  On the face of it, that would force
    __GFP_NO_KSWAPD patches to be merged at the same time but I can do a
    version of this series with __GFP_NO_KSWAPD change reverted and then
    rebase it on top of this series.  That might be best overall because I
    note that the __GFP_NO_KSWAPD patch should have removed
    deferred_compaction from page_alloc.c but it didn't but fixing that causes
    collisions with this series.
    
    This patch:
    
    The comment about order applied when the check was order >
    PAGE_ALLOC_COSTLY_ORDER which has not been the case since c5a73c3d ("thp:
    use compaction for all allocation orders").  Fixing the comment while I'm
    in the general area.
    
    Change-Id: Ida65d938b78618ec098d5e511e88cf39578ba606
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 4ffb6335da87b51c17e7ff6495170785f21558dd
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 6556b6e2b1adf6b6d401e691207085f4b85176dd
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Aug 21 16:16:17 2012 -0700

    mm: compaction: Abort async compaction if locks are contended or taking too long
    
    Jim Schutt reported a problem that pointed at compaction contending
    heavily on locks.  The workload is straight-forward and in his own words;
    
    	The systems in question have 24 SAS drives spread across 3 HBAs,
    	running 24 Ceph OSD instances, one per drive.  FWIW these servers
    	are dual-socket Intel 5675 Xeons w/48 GB memory.  I've got ~160
    	Ceph Linux clients doing dd simultaneously to a Ceph file system
    	backed by 12 of these servers.
    
    Early in the test everything looks fine
    
      procs -------------------memory------------------ ---swap-- -----io---- --system-- -----cpu-------
       r  b       swpd       free       buff      cache   si   so    bi    bo   in   cs  us sy  id wa st
      31 15          0     287216        576   38606628    0    0     2  1158    2   14   1  3  95  0  0
      27 15          0     225288        576   38583384    0    0    18 2222016 203357 134876  11 56  17 15  0
      28 17          0     219256        576   38544736    0    0    11 2305932 203141 146296  11 49  23 17  0
       6 18          0     215596        576   38552872    0    0     7 2363207 215264 166502  12 45  22 20  0
      22 18          0     226984        576   38596404    0    0     3 2445741 223114 179527  12 43  23 22  0
    
    and then it goes to pot
    
      procs -------------------memory------------------ ---swap-- -----io---- --system-- -----cpu-------
       r  b       swpd       free       buff      cache   si   so    bi    bo   in   cs  us sy  id wa st
      163  8          0     464308        576   36791368    0    0    11 22210  866  536   3 13  79  4  0
      207 14          0     917752        576   36181928    0    0   712 1345376 134598 47367   7 90   1  2  0
      123 12          0     685516        576   36296148    0    0   429 1386615 158494 60077   8 84   5  3  0
      123 12          0     598572        576   36333728    0    0  1107 1233281 147542 62351   7 84   5  4  0
      622  7          0     660768        576   36118264    0    0   557 1345548 151394 59353   7 85   4  3  0
      223 11          0     283960        576   36463868    0    0    46 1107160 121846 33006   6 93   1  1  0
    
    Note that system CPU usage is very high blocks being written out has
    dropped by 42%. He analysed this with perf and found
    
      perf record -g -a sleep 10
      perf report --sort symbol --call-graph fractal,5
        34.63%  [k] _raw_spin_lock_irqsave
                |
                |--97.30%-- isolate_freepages
                |          compaction_alloc
                |          unmap_and_move
                |          migrate_pages
                |          compact_zone
                |          compact_zone_order
                |          try_to_compact_pages
                |          __alloc_pages_direct_compact
                |          __alloc_pages_slowpath
                |          __alloc_pages_nodemask
                |          alloc_pages_vma
                |          do_huge_pmd_anonymous_page
                |          handle_mm_fault
                |          do_page_fault
                |          page_fault
                |          |
                |          |--87.39%-- skb_copy_datagram_iovec
                |          |          tcp_recvmsg
                |          |          inet_recvmsg
                |          |          sock_recvmsg
                |          |          sys_recvfrom
                |          |          system_call
                |          |          __recv
                |          |          |
                |          |           --100.00%-- (nil)
                |          |
                |           --12.61%-- memcpy
                 --2.70%-- [...]
    
    There was other data but primarily it is all showing that compaction is
    contended heavily on the zone->lock and zone->lru_lock.
    
    commit [b2eef8c0: mm: compaction: minimise the time IRQs are disabled
    while isolating pages for migration] noted that it was possible for
    migration to hold the lru_lock for an excessive amount of time. Very
    broadly speaking this patch expands the concept.
    
    This patch introduces compact_checklock_irqsave() to check if a lock
    is contended or the process needs to be scheduled. If either condition
    is true then async compaction is aborted and the caller is informed.
    The page allocator will fail a THP allocation if compaction failed due
    to contention. This patch also introduces compact_trylock_irqsave()
    which will acquire the lock only if it is not contended and the process
    does not need to schedule.
    
    Change-Id: Ia5318c923b903948072ff279dc9aed698bb6d02d
    Reported-by: Jim Schutt <jaschut@sandia.gov>
    Tested-by: Jim Schutt <jaschut@sandia.gov>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c67fe3752abe6ab47639e2f9b836900c3dc3da84
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Minor context fixup in isolate_migratepages_range]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ecee5d58558de3b05acc885488491f657790d1b9
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Aug 21 16:16:03 2012 -0700

    mm/compaction.c: fix deferring compaction mistake
    
    Commit aff622495c9a ("vmscan: only defer compaction for failed order and
    higher") fixed bad deferring policy but made mistake about checking
    compact_order_failed in __compact_pgdat().  So it can't update
    compact_order_failed with the new order.  This ends up preventing
    correct operation of policy deferral.  This patch fixes it.
    
    Change-Id: I8d65a5511d90e6c02bb58cd7bc5743e4327271f9
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c81758fbe0fdbbc0c74b37798f55bd9c91d5c068
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit d13461487ae7378771ece3dd5c1a549a4165f566
Author: David Rientjes <rientjes@google.com>
Date:   Wed Jul 11 14:02:13 2012 -0700

    mm, thp: abort compaction if migration page cannot be charged to memcg
    
    If page migration cannot charge the temporary page to the memcg,
    migrate_pages() will return -ENOMEM.  This isn't considered in memory
    compaction however, and the loop continues to iterate over all
    pageblocks trying to isolate and migrate pages.  If a small number of
    very large memcgs happen to be oom, however, these attempts will mostly
    be futile leading to an enormous amout of cpu consumption due to the
    page migration failures.
    
    This patch will short circuit and fail memory compaction if
    migrate_pages() returns -ENOMEM.  COMPACT_PARTIAL is returned in case
    some migrations were successful so that the page allocator will retry.
    
    Change-Id: I339ec3ed10c388d43367485fccc846a3b6bc81fc
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 4bf2bba3750f10aa9e62e6949bc7e8329990f01b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a06a6dc2491238d5a1af35b32d4cca6b244c8342
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:51 2012 -0700

    cma: decrease cc.nr_migratepages after reclaiming pagelist
    
    reclaim_clean_pages_from_list() reclaims clean pages before migration so
    cc.nr_migratepages should be updated.  Currently, there is no problem but
    it can be wrong if we try to use the value in future.
    
    Change-Id: I8b3f1238645ba1b3adcc0fe3c41e10f7074b9a96
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: beb51eaa88238daba698ad837222ad277d440b6d
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Change cc to be structure instead of pointer]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a99a51ad22919c43449d569f40d56849e63b2904
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:31:55 2012 -0700

    mm: cma: discard clean pages during contiguous allocation instead of migration
    
    Drop clean cache pages instead of migration during alloc_contig_range() to
    minimise allocation latency by reducing the amount of migration that is
    necessary.  It's useful for CMA because latency of migration is more
    important than evicting the background process's working set.  In
    addition, as pages are reclaimed then fewer free pages for migration
    targets are required so it avoids memory reclaiming to get free pages,
    which is a contributory factor to increased latency.
    
    I measured elapsed time of __alloc_contig_migrate_range() which migrates
    10M in 40M movable zone in QEMU machine.
    
    Before - 146ms, After - 7ms
    
    Change-Id: Ia527b7253bc5fa63b555ac445b676588b6def119
    [akpm@linux-foundation.org: fix nommu build]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Mel Gorman <mgorman@suse.de>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Rik van Riel <riel@redhat.com>
    Tested-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 02c6de8d757cb32c0829a45d81c3dfcbcafd998b
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    [lauraa@codeaurora.org: Fixups in mm/internal.h due to contexts]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 4b4c8ae9516cd7bce6ecfca7307448b34a983d9a
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:59 2012 -0700

    mm/vmscan: remove update_isolated_counts()
    
    update_isolated_counts() is no longer required, because lumpy-reclaim was
    removed.  Insanity is over, now there is only one kind of inactive page.
    
    Change-Id: Ib2a40af679a00d23b22800d0e513f60838285a15
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 95d918fc009072c2f88ce2e8b5db2e5abfad7c3e
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 549ddc105579f0c1ca984123e054b19f52a2ff84
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:58 2012 -0700

    mm/vmscan: push lruvec pointer into isolate_lru_pages()
    
    Move the mem_cgroup_zone_lruvec() call from isolate_lru_pages() into
    shrink_[in]active_list().  Further patches push it to shrink_zone() step
    by step.
    
    Change-Id: I59593e52a524b1b6713c0421c3ed956f78c1e1a8
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 5dc35979e444b50d5551bdeb7a7abc5c69c875d0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 3b010517e16908dc9ddee24ad8644064f01f8324
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:59 2012 -0700

    mm/vmscan: push zone pointer into shrink_page_list()
    
    It doesn't need a pointer to the cgroup - pointer to the zone is enough.
    This patch also kills the "mz" argument of page_check_references() - it is
    unused after "mm: memcg: count pte references from every member of the
    reclaimed hierarch"
    
    Change-Id: I9b219780be851f696dc5e3b7fc21889035d00313
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 6a18adb35c27848195c938b0779ce882d63d3ed1
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 85c4157201ddd0b28012bd3a0598dada52ba1898
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue May 29 15:06:25 2012 -0700

    mm: memcg: count pte references from every member of the reclaimed hierarchy
    
    The rmap walker checking page table references has historically ignored
    references from VMAs that were not part of the memcg that was being
    reclaimed during memcg hard limit reclaim.
    
    When transitioning global reclaim to memcg hierarchy reclaim, I missed
    that bit and now references from outside a memcg are ignored even during
    global reclaim.
    
    Reverting back to traditional behaviour - count all references during
    global reclaim and only mind references of the memcg being reclaimed
    during limit reclaim would be one option.
    
    However, the more generic idea is to ignore references exactly then when
    they are outside the hierarchy that is currently under reclaim; because
    only then will their reclamation be of any use to help the pressure
    situation.  It makes no sense to ignore references from a sibling memcg
    and then evict a page that will be immediately refaulted by that sibling
    which contributes to the same usage of the common ancestor under
    reclaim.
    
    The solution: make the rmap walker ignore references from VMAs that are
    not part of the hierarchy that is being reclaimed.
    
    Flat limit reclaim will stay the same, hierarchical limit reclaim will
    mind the references only to pages that the hierarchy owns.  Global
    reclaim, since it reclaims from all memcgs, will be fixed to regard all
    references.
    
    Change-Id: I3a3f39693cf5644870213df28238acf00d7417dd
    [akpm@linux-foundation.org: name the args in the declaration]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Konstantin Khlebnikov<khlebnikov@openvz.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c3ac9a8ade65ccbfd145fbff895ae8d8d62d09b0
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 22c16249b6aeb585843f2a8b4a33e623f000fe13
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue May 29 15:06:24 2012 -0700

    kernel: cgroup: push rcu read locking from css_is_ancestor() to callsite
    
    Library functions should not grab locks when the callsites can do it,
    even if the lock nests like the rcu read-side lock does.
    
    Push the rcu_read_lock() from css_is_ancestor() to its single user,
    mem_cgroup_same_or_subtree() in preparation for another user that may
    already hold the rcu read-side lock.
    
    Change-Id: I0ec7ec0059ed588d8f85bc9be8fdc42ce0ca7f5d
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Git-commit: 91c63734f6908425903aed69c04035592f18d398
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ca0b21e48e166e5dc69aef26efac5fe3184da1a8
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:57 2012 -0700

    mm/vmscan: store "priority" in struct scan_control
    
    In memory reclaim some function have too many arguments - "priority" is
    one of them.  It can be stored in struct scan_control - we construct them
    on the same level.  Instead of an open coded loop we set the initial
    sc.priority, and do_try_to_free_pages() decreases it down to zero.
    
    Change-Id: I7c03b4367fe3787dfd82afe1ff21469a99bdb04f
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 9e3b2f8cd340e13353a44c9a34caef2848131ed7
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 88157109a6d4722cc6a5d3225c6cec949575672a
Author: Rik van Riel <riel@redhat.com>
Date:   Tue May 29 15:06:18 2012 -0700

    mm: remove swap token code
    
    The swap token code no longer fits in with the current VM model.  It
    does not play well with cgroups or the better NUMA placement code in
    development, since we have only one swap token globally.
    
    It also has the potential to mess with scalability of the system, by
    increasing the number of non-reclaimable pages on the active and
    inactive anon LRU lists.
    
    Last but not least, the swap token code has been broken for a year
    without complaints, as reported by Konstantin Khlebnikov.  This suggests
    we no longer have much use for it.
    
    The days of sub-1G memory systems with heavy use of swap are over.  If
    we ever need thrashing reducing code in the future, we will have to
    implement something that does scale.
    
    Change-Id: I6d287cfc3c3206ca24da2de0c1392e5fdfcfabe8
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Bob Picco <bpicco@meloft.net>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: e709ffd6169ccd259eb5874e853303e91e94e829
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 307095ac7d8eefcb0abaa3f13da5c6fe83d239b8
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:57 2012 -0700

    mm/memcg: use vm_swappiness from target memory cgroup
    
    Use vm_swappiness from memory cgroup which is triggered this memory
    reclaim.  This is more reasonable and allows to kill one argument.
    
    Change-Id: I6aa49763a5746f021ae084885df6764bb7835a62
    [akpm@linux-foundation.org: fix build (patch skew)]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujtisu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3d58ab5c97fa2d145050242137ac39ca7d3bc2fc
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 9d3ac3ccc0595fa7dca54240a5299187319c40d5
Author: Satoru Moriya <satoru.moriya@hds.com>
Date:   Tue May 29 15:06:47 2012 -0700

    mm: avoid swapping out with swappiness==0
    
    Sometimes we'd like to avoid swapping out anonymous memory.  In
    particular, avoid swapping out pages of important process or process
    groups while there is a reasonable amount of pagecache on RAM so that we
    can satisfy our customers' requirements.
    
    OTOH, we can control how aggressive the kernel will swap memory pages with
    /proc/sys/vm/swappiness for global and
    /sys/fs/cgroup/memory/memory.swappiness for each memcg.
    
    But with current reclaim implementation, the kernel may swap out even if
    we set swappiness=0 and there is pagecache in RAM.
    
    This patch changes the behavior with swappiness==0.  If we set
    swappiness==0, the kernel does not swap out completely (for global reclaim
    until the amount of free pages and filebacked pages in a zone has been
    reduced to something very very small (nr_free + nr_filebacked < high
    watermark)).
    
    Change-Id: Ic9575cf7fc82ed69cde58d77c1e82036afc9dffd
    Signed-off-by: Satoru Moriya <satoru.moriya@hds.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: fe35004fbf9eaf67482b074a2e032abb9c89b1dd
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 73876bd5fa799a84f1e381458bc6ea6a11e79544
Author: Hugh Dickins <hughd@google.com>
Date:   Tue May 29 15:06:52 2012 -0700

    mm/memcg: scanning_global_lru means mem_cgroup_disabled
    
    Although one has to admire the skill with which it has been concealed,
    scanning_global_lru(mz) is actually just an interesting way to test
    mem_cgroup_disabled().  Too many developer hours have been wasted on
    confusing it with global_reclaim(): just use mem_cgroup_disabled().
    
    Change-Id: I9f8f70b41b002cc8c4583a0a9869459da24b4fe5
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c3c787e8c38557ccf44c670d73aebe630a2b1479
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 4d66048a8bf34171aa8389ca33f3987156706ca9
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:54 2012 -0700

    mm/memcg: kill mem_cgroup_lru_del()
    
    This patch kills mem_cgroup_lru_del(), we can use
    mem_cgroup_lru_del_list() instead.  On 0-order isolation we already have
    right lru list id.
    
    Change-Id: I403d40074299fb5f125603435c057071714d5b92
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: bbf808ed7de68fdf626fd4f9718d88cf03ce13a9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 1d296d5acf0c999b672d34dee778cc3ad4b4c8b1
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:54 2012 -0700

    mm: remove lru type checks from __isolate_lru_page()
    
    After patch "mm: forbid lumpy-reclaim in shrink_active_list()" we can
    completely remove anon/file and active/inactive lru type filters from
    __isolate_lru_page(), because isolation for 0-order reclaim always
    isolates pages from right lru list.  And pages-isolation for lumpy
    shrink_inactive_list() or memory-compaction anyway allowed to isolate
    pages from all evictable lru lists.
    
    Change-Id: I2a1a0325b1d193f4ca5e3ea7d5eda9b8bf7c6698
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: f3fd4a61928a5edf5b033a417e761b488b43e203
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 13f747079a820fd08f7b09f733e4805848488295
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:20 2012 -0700

    mm: vmscan: remove reclaim_mode_t
    
    There is little motiviation for reclaim_mode_t once RECLAIM_MODE_[A]SYNC
    and lumpy reclaim have been removed.  This patch gets rid of
    reclaim_mode_t as well and improves the documentation about what
    reclaim/compaction is and when it is triggered.
    
    Change-Id: If95bc163647b1cfb93d7f3b8435060fed1e2aabf
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 23b9da55c5b0feb484bd5e8615f4eb1ce4169453
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit c67ad328fe64c068bf5c34192c525ae90086c3ee
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:19 2012 -0700

    mm: vmscan: do not stall on writeback during memory compaction
    
    This patch stops reclaim/compaction entering sync reclaim as this was
    only intended for lumpy reclaim and an oversight.  Page migration has
    its own logic for stalling on writeback pages if necessary and memory
    compaction is already using it.
    
    Waiting on page writeback is bad for a number of reasons but the primary
    one is that waiting on writeback to a slow device like USB can take a
    considerable length of time.  Page reclaim instead uses
    wait_iff_congested() to throttle if too many dirty pages are being
    scanned.
    
    Change-Id: I14f312b1a51ee093d9d4adda5c87e57f1b83e03d
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 41ac1999c3e3563e1810b14878a869c79c9368bb
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit ec6c299a552181c8554d824004cade2dca1cefce
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue May 29 15:06:19 2012 -0700

    mm: vmscan: remove lumpy reclaim
    
    This series removes lumpy reclaim and some stalling logic that was
    unintentionally being used by memory compaction.  The end result is that
    stalling on dirty pages during page reclaim now depends on
    wait_iff_congested().
    
    Four kernels were compared
    
      3.3.0     vanilla
      3.4.0-rc2 vanilla
      3.4.0-rc2 lumpyremove-v2 is patch one from this series
      3.4.0-rc2 nosync-v2r3 is the full series
    
    Removing lumpy reclaim saves almost 900 bytes of text whereas the full
    series removes 1200 bytes.
    
         text     data      bss       dec     hex  filename
      6740375  1927944  2260992  10929311  a6c49f  vmlinux-3.4.0-rc2-vanilla
      6739479  1927944  2260992  10928415  a6c11f  vmlinux-3.4.0-rc2-lumpyremove-v2
      6739159  1927944  2260992  10928095  a6bfdf  vmlinux-3.4.0-rc2-nosync-v2
    
    There are behaviour changes in the series and so tests were run with
    monitoring of ftrace events.  This disrupts results so the performance
    results are distorted but the new behaviour should be clearer.
    
    fs-mark running in a threaded configuration showed little of interest as
    it did not push reclaim aggressively
    
      FS-Mark Multi Threaded
                              3.3.0-vanilla       rc2-vanilla       lumpyremove-v2r3       nosync-v2r3
      Files/s  min           3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Files/s  mean          3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Files/s  stddev        0.00 ( 0.00%)        0.00 ( 0.00%)        0.00 ( 0.00%)        0.00 ( 0.00%)
      Files/s  max           3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)        3.20 ( 0.00%)
      Overhead min      508667.00 ( 0.00%)   521350.00 (-2.49%)   544292.00 (-7.00%)   547168.00 (-7.57%)
      Overhead mean     551185.00 ( 0.00%)   652690.73 (-18.42%)   991208.40 (-79.83%)   570130.53 (-3.44%)
      Overhead stddev    18200.69 ( 0.00%)   331958.29 (-1723.88%)  1579579.43 (-8578.68%)     9576.81 (47.38%)
      Overhead max      576775.00 ( 0.00%)  1846634.00 (-220.17%)  6901055.00 (-1096.49%)   585675.00 (-1.54%)
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             309.90    300.95    307.33    298.95
      User+Sys Time Running Test (seconds)        319.32    309.67    315.69    307.51
      Total Elapsed Time (seconds)               1187.85   1193.09   1191.98   1193.73
    
      MMTests Statistics: vmstat
      Page Ins                                       80532       82212       81420       79480
      Page Outs                                  111434984   111456240   111437376   111582628
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                           44881       27889       27453       34843
      Kswapd pages scanned                        25841428    25860774    25861233    25843212
      Kswapd pages reclaimed                      25841393    25860741    25861199    25843179
      Direct pages reclaimed                         44881       27889       27453       34843
      Kswapd efficiency                                99%         99%         99%         99%
      Kswapd velocity                            21754.791   21675.460   21696.029   21649.127
      Direct efficiency                               100%        100%        100%        100%
      Direct velocity                               37.783      23.375      23.031      29.188
      Percentage direct scans                           0%          0%          0%          0%
    
    ftrace showed that there was no stalling on writeback or pages submitted
    for IO from reclaim context.
    
    postmark was similar and while it was more interesting, it also did not
    push reclaim heavily.
    
      POSTMARK
                                           3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      Transactions per second:               16.00 ( 0.00%)    20.00 (25.00%)    18.00 (12.50%)    17.00 ( 6.25%)
      Data megabytes read per second:        18.80 ( 0.00%)    24.27 (29.10%)    22.26 (18.40%)    20.54 ( 9.26%)
      Data megabytes written per second:     35.83 ( 0.00%)    46.25 (29.08%)    42.42 (18.39%)    39.14 ( 9.24%)
      Files created alone per second:        28.00 ( 0.00%)    38.00 (35.71%)    34.00 (21.43%)    30.00 ( 7.14%)
      Files create/transact per second:       8.00 ( 0.00%)    10.00 (25.00%)     9.00 (12.50%)     8.00 ( 0.00%)
      Files deleted alone per second:       556.00 ( 0.00%)  1224.00 (120.14%)  3062.00 (450.72%)  6124.00 (1001.44%)
      Files delete/transact per second:       8.00 ( 0.00%)    10.00 (25.00%)     9.00 (12.50%)     8.00 ( 0.00%)
    
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             113.34    107.99    109.73    108.72
      User+Sys Time Running Test (seconds)        145.51    139.81    143.32    143.55
      Total Elapsed Time (seconds)               1159.16    899.23    980.17   1062.27
    
      MMTests Statistics: vmstat
      Page Ins                                    13710192    13729032    13727944    13760136
      Page Outs                                   43071140    42987228    42733684    42931624
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                               0           0           0           0
      Kswapd pages scanned                         9941613     9937443     9939085     9929154
      Kswapd pages reclaimed                       9940926     9936751     9938397     9928465
      Direct pages reclaimed                             0           0           0           0
      Kswapd efficiency                                99%         99%         99%         99%
      Kswapd velocity                             8576.567   11051.058   10140.164    9347.109
      Direct efficiency                               100%        100%        100%        100%
      Direct velocity                                0.000       0.000       0.000       0.000
    
    It looks like here that the full series regresses performance but as
    ftrace showed no usage of wait_iff_congested() or sync reclaim I am
    assuming it's a disruption due to monitoring.  Other data such as memory
    usage, page IO, swap IO all looked similar.
    
    Running a benchmark with a plain DD showed nothing very interesting.
    The full series stalled in wait_iff_congested() slightly less but stall
    times on vanilla kernels were marginal.
    
    Running a benchmark that hammered on file-backed mappings showed stalls
    due to congestion but not in sync writebacks
    
      MICRO
                                           3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             308.13    294.50    298.75    299.53
      User+Sys Time Running Test (seconds)        330.45    316.28    318.93    320.79
      Total Elapsed Time (seconds)               1814.90   1833.88   1821.14   1832.91
    
      MMTests Statistics: vmstat
      Page Ins                                      108712      120708       97224      110344
      Page Outs                                  155514576   156017404   155813676   156193256
      Swap Ins                                           0           0           0           0
      Swap Outs                                          0           0           0           0
      Direct pages scanned                         2599253     1550480     2512822     2414760
      Kswapd pages scanned                        69742364    71150694    68839041    69692533
      Kswapd pages reclaimed                      34824488    34773341    34796602    34799396
      Direct pages reclaimed                         53693       94750       61792       75205
      Kswapd efficiency                                49%         48%         50%         49%
      Kswapd velocity                            38427.662   38797.901   37799.972   38022.889
      Direct efficiency                                 2%          6%          2%          3%
      Direct velocity                             1432.174     845.464    1379.807    1317.446
      Percentage direct scans                           3%          2%          3%          3%
      Page writes by reclaim                             0           0           0           0
      Page writes file                                   0           0           0           0
      Page writes anon                                   0           0           0           0
      Page reclaim immediate                             0           0           0        1218
      Page rescued immediate                             0           0           0           0
      Slabs scanned                                  15360       16384       13312       16384
      Direct inode steals                                0           0           0           0
      Kswapd inode steals                             4340        4327        1630        4323
    
      FTrace Reclaim Statistics: congestion_wait
      Direct number congest     waited                 0          0          0          0
      Direct time   congest     waited               0ms        0ms        0ms        0ms
      Direct full   congest     waited                 0          0          0          0
      Direct number conditional waited               900        870        754        789
      Direct time   conditional waited               0ms        0ms        0ms       20ms
      Direct full   conditional waited                 0          0          0          0
      KSwapd number congest     waited              2106       2308       2116       1915
      KSwapd time   congest     waited          139924ms   157832ms   125652ms   132516ms
      KSwapd full   congest     waited              1346       1530       1202       1278
      KSwapd number conditional waited             12922      16320      10943      14670
      KSwapd time   conditional waited               0ms        0ms        0ms        0ms
      KSwapd full   conditional waited                 0          0          0          0
    
    Reclaim statistics are not radically changed.  The stall times in kswapd
    are massive but it is clear that it is due to calls to congestion_wait()
    and that is almost certainly the call in balance_pgdat().  Otherwise
    stalls due to dirty pages are non-existant.
    
    I ran a benchmark that stressed high-order allocation.  This is very
    artifical load but was used in the past to evaluate lumpy reclaim and
    compaction.  Generally I look at allocation success rates and latency
    figures.
    
      STRESS-HIGHALLOC
                       3.3.0-vanilla       rc2-vanilla  lumpyremove-v2r3       nosync-v2r3
      Pass 1          81.00 ( 0.00%)    28.00 (-53.00%)    24.00 (-57.00%)    28.00 (-53.00%)
      Pass 2          82.00 ( 0.00%)    39.00 (-43.00%)    38.00 (-44.00%)    43.00 (-39.00%)
      while Rested    88.00 ( 0.00%)    87.00 (-1.00%)    88.00 ( 0.00%)    88.00 ( 0.00%)
    
      MMTests Statistics: duration
      Sys Time Running Test (seconds)             740.93    681.42    685.14    684.87
      User+Sys Time Running Test (seconds)       2922.65   3269.52   3281.35   3279.44
      Total Elapsed Time (seconds)               1161.73   1152.49   1159.55   1161.44
    
      MMTests Statistics: vmstat
      Page Ins                                     4486020     2807256     2855944     2876244
      Page Outs                                    7261600     7973688     7975320     7986120
      Swap Ins                                       31694           0           0           0
      Swap Outs                                      98179           0           0           0
      Direct pages scanned                           53494       57731       34406      113015
      Kswapd pages scanned                         6271173     1287481     1278174     1219095
      Kswapd pages reclaimed                       2029240     1281025     1260708     1201583
      Direct pages reclaimed                          1468       14564       16649       92456
      Kswapd efficiency                                32%         99%         98%         98%
      Kswapd velocity                             5398.133    1117.130    1102.302    1049.641
      Direct efficiency                                 2%         25%         48%         81%
      Direct velocity                               46.047      50.092      29.672      97.306
      Percentage direct scans                           0%          4%          2%          8%
      Page writes by reclaim                       1616049           0           0           0
      Page writes file                             1517870           0           0           0
      Page writes anon                               98179           0           0           0
      Page reclaim immediate                        103778       27339        9796       17831
      Page rescued immediate                             0           0           0           0
      Slabs scanned                                1096704      986112      980992      998400
      Direct inode steals                              223      215040      216736      247881
      Kswapd inode steals                           175331       61548       68444       63066
      Kswapd skipped wait                            21991           0           1           0
      THP fault alloc                                    1         135         125         134
      THP collapse alloc                               393         311         228         236
      THP splits                                        25          13           7           8
      THP fault fallback                                 0           0           0           0
      THP collapse fail                                  3           5           7           7
      Compaction stalls                                865        1270        1422        1518
      Compaction success                               370         401         353         383
      Compaction failures                              495         869        1069        1135
      Compaction pages moved                        870155     3828868     4036106     4423626
      Compaction move failure                        26429       23865       29742       27514
    
    Success rates are completely hosed for 3.4-rc2 which is almost certainly
    due to commit fe2c2a106663 ("vmscan: reclaim at order 0 when compaction
    is enabled").  I expected this would happen for kswapd and impair
    allocation success rates (https://lkml.org/lkml/2012/1/25/166) but I did
    not anticipate this much a difference: 80% less scanning, 37% less
    reclaim by kswapd
    
    In comparison, reclaim/compaction is not aggressive and gives up easily
    which is the intended behaviour.  hugetlbfs uses __GFP_REPEAT and would
    be much more aggressive about reclaim/compaction than THP allocations
    are.  The stress test above is allocating like neither THP or hugetlbfs
    but is much closer to THP.
    
    Mainline is now impaired in terms of high order allocation under heavy
    load although I do not know to what degree as I did not test with
    __GFP_REPEAT.  Keep this in mind for bugs related to hugepage pool
    resizing, THP allocation and high order atomic allocation failures from
    network devices.
    
    In terms of congestion throttling, I see the following for this test
    
      FTrace Reclaim Statistics: congestion_wait
      Direct number congest     waited                 3          0          0          0
      Direct time   congest     waited               0ms        0ms        0ms        0ms
      Direct full   congest     waited                 0          0          0          0
      Direct number conditional waited               957        512       1081       1075
      Direct time   conditional waited               0ms        0ms        0ms        0ms
      Direct full   conditional waited                 0          0          0          0
      KSwapd number congest     waited                36          4          3          5
      KSwapd time   congest     waited            3148ms      400ms      300ms      500ms
      KSwapd full   congest     waited                30          4          3          5
      KSwapd number conditional waited             88514        197        332        542
      KSwapd time   conditional waited            4980ms        0ms        0ms        0ms
      KSwapd full   conditional waited                49          0          0          0
    
    The "conditional waited" times are the most interesting as this is
    directly impacted by the number of dirty pages encountered during scan.
    As lumpy reclaim is no longer scanning contiguous ranges, it is finding
    fewer dirty pages.  This brings wait times from about 5 seconds to 0.
    kswapd itself is still calling congestion_wait() so it'll still stall but
    it's a lot less.
    
    In terms of the type of IO we were doing, I see this
    
      FTrace Reclaim Statistics: mm_vmscan_writepage
      Direct writes anon  sync                         0          0          0          0
      Direct writes anon  async                        0          0          0          0
      Direct writes file  sync                         0          0          0          0
      Direct writes file  async                        0          0          0          0
      Direct writes mixed sync                         0          0          0          0
      Direct writes mixed async                        0          0          0          0
      KSwapd writes anon  sync                         0          0          0          0
      KSwapd writes anon  async                    91682          0          0          0
      KSwapd writes file  sync                         0          0          0          0
      KSwapd writes file  async                   822629          0          0          0
      KSwapd writes mixed sync                         0          0          0          0
      KSwapd writes mixed async                        0          0          0          0
    
    In 3.2, kswapd was doing a bunch of async writes of pages but
    reclaim/compaction was never reaching a point where it was doing sync
    IO.  This does not guarantee that reclaim/compaction was not calling
    wait_on_page_writeback() but I would consider it unlikely.  It indicates
    that merging patches 2 and 3 to stop reclaim/compaction calling
    wait_on_page_writeback() should be safe.
    
    This patch:
    
    Lumpy reclaim had a purpose but in the mind of some, it was to kick the
    system so hard it trashed.  For others the purpose was to complicate
    vmscan.c.  Over time it was giving softer shoes and a nicer attitude but
    memory compaction needs to step up and replace it so this patch sends
    lumpy reclaim to the farm.
    
    The tracepoint format changes for isolating LRU pages with this patch
    applied.  Furthermore reclaim/compaction can no longer queue dirty pages
    in pageout() if the underlying BDI is congested.  Lumpy reclaim used
    this logic and reclaim/compaction was using it in error.
    
    Change-Id: Ib2992962c9e99cf250a7f859bb2a67034051e4d4
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: c53919adc045bf803252e912f23028a68525753d
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit a4e8f3073d37cddde83e29730ae67e23cbd47045
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:53 2012 -0700

    mm: push lru index into shrink_[in]active_list()
    
    Let's toss lru index through call stack to isolate_lru_pages(), this is
    better than its reconstructing from individual bits.
    
    Change-Id: Id07444ba97af9699ecfff1750db13cb2fee147fc
    [akpm@linux-foundation.org: fix kerneldoc, per Minchan]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 3cb9945179bd04e9282f31a1173ac11b1300c462
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 731ff4aab70194fd9586c3fc9e70c0521f86d0d3
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Fri Aug 9 15:10:24 2013 -0500

    jf: add support for new sysinterface
    
    kanged from codeworkx
    Change-Id: I837593c59c5439ab4e18d176713614292bceba81

commit 5caa736a4ddfb47f2c46fb9bab16fd78516905c1
Author: Rajesh Kemisetti <rajeshk@codeaurora.org>
Date:   Wed Jul 3 20:45:37 2013 +0530

    msm: kgsl: Map sync lock variables to every pagetable
    
    Map sync lock variables to every pagetable since we don't
    use TTBR1 anymore.
    
    Change-Id: If2d43c4c57d0cdcf6076229dfad32c0375e74df4
    Signed-off-by: Rajesh Kemisetti <rajeshk@codeaurora.org>

commit cb3618f9451564cb5611758ba1d5acf56f57aadc
Author: Tarun Karra <tkarra@codeaurora.org>
Date:   Mon Jul 22 18:55:16 2013 -0700

    msm: kgsl: Use no GPU fault tolerance flag passed by UMD driver
    
    Kernel context creation is ignoring "No GPU fault
    tolerance" flag passed in by UMD driver. This change forces
    kernel to honor "No GPU fault tolerance" flag passed in by
    UMD driver and not run GFT on contexts that have this flag set.
    Ignoring this flag causes OpenCL to run GFT which is not correct.
    
    CRs-Fixed: 510754
    Change-Id: I52b7236d259ec930d07ddae1605e1e683a54d2ba
    Signed-off-by: Tarun Karra <tkarra@codeaurora.org>

commit a0a829605ecd9cd1bceb90bf21b73542484f1e99
Author: Vladimir Razgulin <vrazguli@codeaurora.org>
Date:   Tue Jul 23 17:09:21 2013 -0600

    msm: kgsl: Wake up gpu on kgsl_ioctl_timestamp_event only if necessary
    
    Any call of kgsl_ioctl_timestamp_event wakes up gpu. In some cases
    (like retired timestamp) it's not necessary. Wake up gpu only if it's
    necessary.
    
    Signed-off-by: Vladimir Razgulin <vrazguli@codeaurora.org>
    Change-Id: I845faf6e9c6e4c9882a68489b64223919be3d21c

commit 638e2b32637695fca7823d5de42404a8c3d909c7
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Fri Jul 12 12:51:39 2013 +0530

    msm: rotator: Wait for the pending commits in finish IOCTL
    
    Due to asynchronuous rotator mechanism, sometimes the
    MSM_ROTATOR_IOCTL_FINISH arrives before the previously queued
    do_rotate work is completed. This causes fence to be signalled
    before the buffer is used by rotator. In case of fast YUV 2 pass
    scenario, this causes IOMMU page fault on 2 pass buffers, since
    the buffer is unmapped when the rotator is still using it. Hence,
    wait for the pending commit works to be finished before releasing
    the fence and freeing the 2 pass buffers.
    
    Change-Id: Iec9edd11406d102c7dd102c2ad7935184bbbba93
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit 9d76c84cfe7ef07c9e588d72940127100e31365b
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Tue Jun 25 16:54:01 2013 -0400

    msm: mdp: Update sync pt. behaviour
    
    Add a retire fence and clean up the existing release fence
    implementation
    
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>
    Change-Id: Iecf7e95d0786ac43aa6a12d70442936600c0249a

commit f9581b93f0e5c6c38e578ab4bc2f7448071cdbe1
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Mon Jun 24 19:14:21 2013 -0400

    msm: mdp: Change msmfb kworker to kthread
    
    This worker can be a longer running task depending on the number
    of frames queued, hence change it to a thread.
    
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>
    Change-Id: Iebf7e95d0486ac43aa6a12d70442936600c0249a

commit b69f6d7be9436509ef468851e4debd6334bd6e2b
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Tue Jun 25 17:58:16 2013 -0400

    msm: display: command mode panel release fence
    
    For command mode panel, release fence should be signaled after the buffer is
    sent to the panel.  For performance consideration, allow one more commit to be
    queued.  Keep the second kick off as short as enough.
    
    Change-Id: I19af075ebf480d3e81b46b1cd2ef21a2893f143e
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 2d895591d9bdb97d52830216612179ed160f8e3a
Author: Shuzhen Wang <shuzhenw@codeaurora.org>
Date:   Thu Jun 6 21:06:06 2013 -0700

    video: msm: Increase video encoding output buffer size
    
    When bistream buffer size is half of YUV buffer size, video core
    may be put into a bad state where output buffers are not returned
    back to user space. Increasing the output buffer size solves the issue.
    
    Signed-off-by: Shuzhen Wang <shuzhenw@codeaurora.org>

commit 4d5471a2ef18f34d2d8e6d59907ae9a2d3fd2662
Author: Sidipotu Ashok <sashok@codeaurora.org>
Date:   Wed Mar 13 13:32:33 2013 +0530

    ASoC: msm: FENS Support for VOIP calls
    
    Add support FENS for Voip calls. Support is added by adding new
    Mixer control
    
    CRs-Fixed: 460878
    Change-Id: I4c05155ac1de7b8da5b5e1ae24776c3e98b353f2
    Signed-off-by: Sidipotu Ashok <sashok@codeaurora.org>

commit 7b52ab135cdd844cbe05e64b07bf92dcb71c4c81
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Sat Aug 3 02:20:22 2013 -0500

    jf: MH1 (4.3) kernel merge
    
    Conflicts:
    	drivers/cpufreq/cpufreq_ondemand.c
    	drivers/sensorhub/ssp_firmware.c
    	drivers/video/msm/mdp4_util.c
    	drivers/video/msm/mipi_samsung_octa_video_full_hd_pt.c
    	firmware/ssp_rev03.fw.ihex
    
    Conflicts:
    	drivers/char/msm_rotator.c
    	drivers/gpu/msm/adreno.c
    	drivers/gpu/msm/adreno.h
    	drivers/gpu/msm/adreno_a3xx.c
    	drivers/gpu/msm/kgsl.c
    	drivers/gpu/msm/kgsl_iommu.c
    	drivers/gpu/msm/kgsl_pwrctrl.c
    	drivers/gpu/msm/kgsl_pwrctrl.h
    	drivers/gpu/msm/kgsl_sharedmem.c
    	drivers/hwmon/epm_adc.c
    	drivers/video/msm/mdp.h
    	drivers/video/msm/mdp4.h
    	drivers/video/msm/mdp4_overlay.c
    	drivers/video/msm/mdp4_util.c
    	drivers/video/msm/msm_fb.c
    	drivers/video/msm/msm_fb.h
    	drivers/video/msm/vidc/1080p/ddl/vcd_ddl_helper.c
    	drivers/video/msm/vidc/1080p/ddl/vcd_ddl_properties.c
    	drivers/video/msm/vidc/1080p/ddl/vcd_ddl_shared_mem.h
    	drivers/video/msm/vidc/common/dec/vdec.c
    	drivers/video/msm/vidc/common/enc/venc.c
    	drivers/video/msm/vidc/common/vcd/vcd_sub.c
    	include/linux/msm_mdp.h
    	include/linux/msm_rotator.h
    	include/linux/msm_vidc_enc.h
    	include/media/msm/vcd_property.h
    	include/video/msm_hdmi_modes.h
    
    Change-Id: I9558558777a11cc8f6b633e397c809aa6993ceaf

commit 49e88cd8b1ed0f7ea6b62556ba88502cc727cce8
Author: Archana Sathyakumar <asathyak@codeaurora.org>
Date:   Thu Apr 18 15:34:31 2013 -0600

    msm:rq_stats: Register for Cpufreq policy notification
    
    Currently the load of the system is calculated using the max
    frequency that is read during startup(It does not get updated
    when the policy changes). As a result, the average load of the
    system is incorrectly calculated and the system might not bring
    the cpus online to manage the additional load. This
    reduces the performance of the system.
    
    To fix this issue, register rq_stats to receive cpufreq policy
    notifications. Update the policy's max frequency when a
    notification is received and use that value to calculate the load.
    
    Change-Id: Icc78e28c736c170e198f723fd96c13dfb2dafe8a
    Signed-off-by: Archana Sathyakumar <asathyak@codeaurora.org>
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit 1ad823952b20594ab3b964e4e61747f5d5098ba3
Author: Archana Sathyakumar <asathyak@codeaurora.org>
Date:   Fri Mar 22 17:05:32 2013 -0600

    msm:rq_stats: Fix hotplug attribute during the suspend failure
    
    If the system suspend fails, the hotplug_disable attribute remains
    set. This prevents any change to core status from mpdecision. Reset
    the value of this attribute if the module receives PM_POST_RESTORE
    notification.
    
    Change-Id: I22fd639134adbeb8f8f96603e285151480f537ec
    Signed-off-by: Archana Sathyakumar <asathyak@codeaurora.org>

commit 5c2f7b182b88a2edb71df20dca68838b37695953
Author: Archana Sathyakumar <asathyak@codeaurora.org>
Date:   Wed Feb 20 10:35:35 2013 -0700

    msm:rq_stats: Add hotplug disable attribute
    
    Register for suspend and resume notifications from kernel. This should
    be done as part of core_initcall so as to receive notification as close
    as possible to the actual event. Add hotplug_disable attribute to
    rq-stats node. Set/clear this attribute based on the suspend/resume events.
    
    CRS-fixed: 452456
    Change-Id: I76fee349b57f47df40785e9c2503e2e1cc6e064e
    Signed-off-by: Archana Sathyakumar <asathyak@codeaurora.org>

commit 4b62b7ce0734e92d9f216bad3297b80e64e1a5a0
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Thu Feb 14 21:15:03 2013 +0530

    msm: rq_stats: Fix missing initalization of cur_freq
    
    Initialize cur_freq variable of each online cpu to the current
    running cpu clock frequency. This avoids zeroing out the load
    calculations until the first notification of the change in CPU
    frequency comes in after CPUFREQ governors are intialized.
    
    Also when a non boot cpu is up, update its cur_freq variable.
    
    (cherry picked from commit 1633a48b16614e6b85c047a19ec93867c2ced3b3)
    
    Change-Id: I950686deff02e2339eefae7d09392dda2174a545
    CRs-fixed: 446017
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit d61be0ecb2b17489c8f0460be09bd148046098bf
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Sun May 19 16:36:48 2013 +0530

    msm: pm: add failed stats
    
    for idle and suspend failed cases the stats
    are not updated correctly, they are just added to
    success bucket.
    
    This patch is to log the failed stats separately.
    
    Change-Id: I4b290944e3f4b74ada13ef0ba51b812c83b5ec5d
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit 43a2e4819d381cbb367fbb1e81794895f427ace5
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Mon Jun 3 16:15:48 2013 +0530

    msm: pm: send notification only for SPC and PC
    
    Today the pm notification is sent for all the power
    modes of each CPU. This is not necessary for shallow
    power modes.
    
    This change is to send the notification for only
    standalone power collapse for each core and idle
    power collapse of core0.
    
    Change-Id: I9b3df3d72e43d645387c94b1b9c5da3bcf2f2e5f
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit d9a29b2d83b97401342f8c70fc8dc7d90a9734b8
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Wed May 15 15:36:58 2013 +0530

    msm: pm-8x60: Use relative time for events
    
    There is a bug when adjusting for event wakeups. The adjusted time
    is being set in absolute mode incorrectly. Instead just use relative
    times.
    
    Without this core wakeups may not happen on time.
    
    CRs-fixed: 486466
    Change-Id: If625f25bd573b05befb6e0f922c80db646428b86
    Signed-off-by: Girish Mahadevan <girishm@codeaurora.org>
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit e611a9cefabd0bc8ade0f4001f85a32fedc1146a
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Fri Apr 5 03:16:14 2013 +0100

    ARM: 7693/1: mm: clean-up in order to reduce to call kmap_high_get()
    
    In kmap_atomic(), kmap_high_get() is invoked for checking already
    mapped area. In __flush_dcache_page() and dma_cache_maint_page(),
    we explicitly call kmap_high_get() before kmap_atomic()
    when cache_is_vipt(), so kmap_high_get() can be invoked twice.
    This is useless operation, so remove one.
    
    v2: change cache_is_vipt() to cache_is_vipt_nonaliasing() in order to
    be self-documented
    
    Change-Id: I36e5a63fdae158d9e7562f564f7e593b351f6960
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Git-commit: dd0f67f4747797f36f0c6bab7fed6a1f2448476d
    Git-repo: https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/
    [Few merge conflict resolved]
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>
    
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit 8269eeececed761c6b49238d7d0c17769c286c59
Author: Rajesh Kemisetti <rajeshk@codeaurora.org>
Date:   Fri Jun 21 13:40:24 2013 +0530

    msm: kgsl: Add a check to free the kgsl_timeline object.
    
    Add a check to free the kgsl_timeline object only after
    destroy flag is set. This is to avoid further references to timeline
    from kgsl if it is already freed and also to catch any unbalanced
    kref count.
    
    Change-Id: I8017b9a928a26b703eb92b17ab6a361e4b479e5a
    Signed-off-by: Rajesh Kemisetti <rajeshk@codeaurora.org>

commit 4cdda06c8d5b12a4a3edd550de6949019493e8ce
Author: Zhong Liu <zhongl@codeaurora.org>
Date:   Mon Jul 8 14:43:50 2013 -0700

    msm: kgsl: send ringbuffer NOP padding with the next command
    
    If there isn't enough room at the bottom of the ringbuffer for a
    whole command, the remaining space is filled with NOPs and
    the command starts again at the top of the ringbuffer, the write
    pointer of the ringbuffer shall update accordingly; the existing
    implementation sends out in-complete NOP command which may potentially
    cause GPU hang. This fix submits the NOP command along with the next
    command instead of submitting them separately to have GPU read both
    commands in the same fetch.
    
    Change-Id: Ia3c9933c11d986c6743d8026b809bbcb1eaf54bf
    Signed-off-by: Zhong Liu <zhongl@codeaurora.org>

commit 23fb76999d251674ec0095e421b60a999144365f
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Thu Jul 18 16:20:08 2013 -0600

    msm: kgsl: Use the kmalloc/vmalloc trick for the sharedmem page array
    
    It was previously assumed that most GPU memory allocations would be
    small enough to allow us to fit the array of page pointers into one
    or two pages allocated via kmalloc.  Recent reports have proven
    those assumptions to be wrong - allocations on the order of 32MB will
    end up trying to get 8 pages from kmalloc and 8 contiguous pages
    on a busy system are a rare beast indeed.
    
    So use the usual kmalloc/vmalloc trick instead - use kmalloc for the
    page array when we can and vmalloc if we can't.
    
    CRs-fixed: 513469
    Change-Id: Ic0dedbad0a5b14abe6a8bd73342b3e68faa8c8b7
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit edc3c99d48625efa654012560200811a24f299c0
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Jul 30 03:45:22 2013 -0700

    video: msm: Resync
    
    Change-Id: I6c7f8e8698ec1807a2abe0729f90ecf53df4b9cd

commit 1e271b78defce5a088bec505696bfd6e16c9d1b1
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Jul 30 03:23:36 2013 -0700

    video: msm: Fix underrun while scaling
    
     * Fix the clock calculation during scaling to avoid underruns.
     * Reproduce by launching and exiting Quadrant
    
    Change-Id: I6f3405e726e20d42140c868bcc07a118f041ef9a

commit 678d57044ebc9f3cb33d9718170f73309ac1afa8
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Jul 30 03:04:24 2013 -0700

    video: msm: Enable register dump on underrun
    
    Change-Id: Ia4150c80c78e45b35ac7ab1fc1364d1f48a6dde5

commit f4addcfaed543bb5cdd4012334aabb365f52c578
Author: Mekala Natarajan <mekalan@codeaurora.org>
Date:   Tue Mar 19 17:47:50 2013 -0700

    ASoC: msm: Flexible period size for pcm playback
    
    PCM driver was configured for fixed buffer
    size on the playback path. With this, varying
    the buffer sizes on the playback path was not
    possible. To fix this, support for flexible
    period sizes is added by setting different
    values for min and max buffer sizes
    
    Change-Id: I7f69db4940f67e5e3a795101af1b1682afbdb530
    Signed-off-by: Alexy Joseph <alexyj@codeaurora.org>
    Signed-off-by: Mekala Natarajan <mekalan@codeaurora.org>

commit 60de5f42977aed02c73829d44384efe78bc758ab
Author: Ajay Dudani <adudani@codeaurora.org>
Date:   Wed Feb 13 21:26:16 2013 -0800

    Revert "ASoC: msm: Decrease the playback period size of PCM driver"
    
    This reverts commit b0580099ac7d5e735fb42946612a499bf196cda1.
    
    Change-Id: I487797334cf75978daa3c536ff92de8a54b1f7fb
    Signed-off-by: Ajay Dudani <adudani@codeaurora.org>

commit ca0b3c5de489442bf4f71c323bb60a91343a6208
Author: Mekala Natarajan <mekalan@codeaurora.org>
Date:   Wed Apr 24 15:36:52 2013 -0700

    ASoC: msm: Change QOS value for low latency path
    
    There were variations in the time taken to
    return from each write. This was due
    to the delay in switching in and out of
    power collapse. To manage this set QOS
    value to 1ms so that the core has enough
    time to wake up from power collapse
    
    Signed-off-by: Alexy Joseph <alexyj@codeaurora.org>
    Signed-off-by: Mekala Natarajan <mekalan@codeaurora.org>

commit ff7d62bfb3d16fb44eaa63f45afc98c4a5d1cb29
Author: Mekala Natarajan <mekalan@codeaurora.org>
Date:   Tue Mar 19 17:54:45 2013 -0700

    ASoC: msm: Reduce min buffer size for low latency
    
    The current low latency driver has 512 bytes as
    the min buffer size threshold. With this reducing
    the playback time to lower values is not possible.
    Setting it to 128 bytes gives us more room
    to try out lower buffer sizes from the user space
    
    Signed-off-by: Alexy Joseph <alexyj@codeaurora.org>
    Signed-off-by: Mekala Natarajan <mekalan@codeaurora.org>

commit 7622a5b2ef747fd65650554dc0465caa701c8e73
Author: Asish Bhattacharya <asishb@codeaurora.org>
Date:   Wed May 15 08:41:08 2013 +0530

    ASoC: msm: Fix wrong wait_event_timeout timeout checks
    
    wait_event_timeout returns zero when there is a timeout.
    Change fixes the condition check to handle timeouts.
    
    Change-Id: I01184cce0c98a82bb205023b07dd4dd2d91b42ad
    CRs-Fixed: 487821
    Signed-off-by: Asish Bhattacharya <asishb@codeaurora.org>
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit 5b9aa455c4a05cfc987934c1e2085eef69f3b63d
Author: Helen Zeng <xiaoyunz@codeaurora.org>
Date:   Tue Sep 11 18:32:24 2012 -0700

    Asoc: msm: 8064: Set channel as Mono for Incall Music Delivery
    
    For incall music delivery, there is only one share channel is
    supported between MDM and APQ. If the media is stereo, afe tries to
    open two share channels and causes music playback failure.
    Add fixup fucntion to always set the channel as 1 whenever the media
    is stereo or mono. And make sure only one share channel is opened.
    
    (cherry picked from commit 548741eb8164c54197d2625120481dd1b51242d1)
    
    Change-Id: I3b2b9a4d3be69797fa2e6f3a507cf360dcf1f22c
    CRs-Fixed: 398100
    Signed-off-by: Helen Zeng <xiaoyunz@codeaurora.org>
    Signed-off-by: Jessica Gonzalez <jgaona@codeaurora.org>

commit ed466d7923b51d0d7ad87a777b856409a26984b3
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Jul 28 17:50:47 2013 -0700

    Revert "Revert "Revert "ARM: 7169/1: topdown mmap support"""
    
    This reverts commit 44a1c3ee83110175604e7076bc6fdacd75689343.

commit 495bb874ddd929d36e0a0157d92ea435c2fd5b4f
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Jul 28 13:39:41 2013 -0700

    video: msm: Fix tearing
    
     * Undo Samsung's damage in video mode driver. We need to wait for
       DMA_P in the latest driver code or tearing will result.
    
    Change-Id: Id30ccdf27dd76631e494bbef9814be36a0af05dc

commit 6ab09a7ba64019d539aeb13b3493520c5ef8bd36
Author: Prakash Kamliya <pkamliya@codeaurora.org>
Date:   Mon Jul 22 17:48:16 2013 +0530

    sync: Limit logging to particular fence on timeout and error
    
    Change-Id: I5bd7d62260666cdd2a32212f0e828d2d5d7d7bba
    Signed-off-by: Prakash Kamliya <pkamliya@codeaurora.org>

commit 31477956cbeaeabe77750f10e3a29c788cb7ca00
Author: Erik Gilling <konkers@android.com>
Date:   Mon Feb 4 12:37:16 2013 -0800

    sync: don't log wait timeouts when timeout = 0
    
    Change-Id: If7542002e10da886af7e7cbc06a0abf023c15893
    Signed-off-by: Erik Gilling <konkers@android.com>
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit 1bfa2e11eec6920fc1aee8956a1e1643528b289b
Author: Mayank Goyal <goyalm@codeaurora.org>
Date:   Wed Jan 16 13:59:16 2013 +0530

    msm_fb: display: release mdp clk in mdp4_overlay_writeback_on()
    
    mdp clk is not released in mdp4_overlay_writeback_on()
    
    CRs-fixed: 433986
    
    Change-Id: Ida121e03b0f08ca312c60fb8d945fe2011df853f
    Signed-off-by: Mayank Goyal <goyalm@codeaurora.org>

commit a5e03f782a419a0be6eb9a107c8ca2de8a73afef
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Fri Feb 1 12:19:47 2013 -0800

    msm_fb: display: initialize dmap_comp before wait4dmap
    
    blt_wait flag will be set when both two writeback buffers
    are used. In this case, kickoff can not be proceeded until
    next dmap_done to release writeback buffer. This patch
    initialize dmap_comp before wait4dmap to make sure
    wait_for_completion is enforced.
    
    CRs-fixed: 448168
    Change-Id: I64f2ad599b18f9d8b346c41a55829c564fcced0b
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 4c343ce994620327ea0abe71fbe395d27cf804cf
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Wed Feb 27 15:41:49 2013 -0800

    msm_fb: display: fix fence fd list be put twice
    
    msm_fb_wait_for_fence() will put fence fd list if sync_fence_wait()
    failed. After that same fd list wil be put one more time. This
    cause a fd put twice and cause system crash at the time fd
    was closed.
    
    Change-Id: If6a358e051c98841e34c23d81098243003ef0836
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 9927d77d03ebfa2a10d23311b3356b71e4c7f6cd
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Sun Feb 3 14:17:06 2013 -0500

    msm: display: reduce timeline to 1 for smart panel
    
    For smart display, dsi command and wfd, since it has its
    own buffer, the input buffer can be released once the
    display commit finishes, hence the fence threshold for
    creation can be smaller. This patch reduce the timeline
    from 2 to 1 for samrt panels. Also, it needs to
    wait4dmap/wait4ov after kicked off. Otherwise, tearing may
    happen.
    
    Change-Id: I9eca871d8d863ef23b1d67b6e1e9cffea40ae4a6
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit bbb10cd4b96a05385b72f396a0f349f18b8cf040
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Mon Dec 10 14:04:00 2012 -0800

    msm_fb: display: add wfd clk ctrl in pipe_commit
    
    mdp_clk_ctrl on is not called when commit is from overlay commit path
    which can cause mdp hang. mdp_clk_ctrl off should be after mdp
    overlay2 is done, so schedule a work in overlay2 done isr.
    
    Change-Id: I94fd08da27cfda114f5cc0056a5103f0a9b772f6
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 97100bc26c45c7d0e6a085dfaf4565ea6367f455
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Jul 28 10:33:56 2013 -0700

    video: msm: Fix a mismerge
    
    Change-Id: Ie60bcc3ee9562f54942a34450b3431ee12c79776

commit 4b76f9640eb88a8c1ad44b39141f94d2e9f4f104
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Mon Mar 18 11:48:29 2013 -0700

    msm_fb: display: change TE simulation and read pointer interrupt timing
    
    The current parameters for command mode TE simulation on mdp side are
    not straightforward and cause double read pointer interrupts. Now make
    read pointer interrupt and line counter reset happen right at h/w
    vsync, and kick_off start position and height are set to back porch
    and max respectively, so timing is much simplified and the chance of
    tearing is reduced.
    
    CRs-fixed: 463273
    Change-Id: Ia25710556bc2b031cdfaaf08fee034baab33d124
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit 3465db50bcde87c3b0eafdf630084a1dde249171
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Wed Feb 6 09:46:19 2013 -0800

    msm_fb: display: remove extra mdp4_dsi_cmd_clk_check()
    
    Since mdp4_dsi_cmd_clk_check() is called inside of
    mdp4_dsi_cmd_pipe_commit() also, mdp4_dsi_cmd_clk_check()
    is called two times from pan display route. This patch
    remove mdp4_dsi_cmd_clk_check() from mdp4_dsi_cmd_overlay()
    to avoid mdp4_dsi_cmd_clk_check() called twice from
    pan display route.
    
    CRs-fixed: 449809
    Change-Id: Idbeb968e6af73abac2bb8c800a8c55015ea6a0bf
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 1ab2f53f98948d2edca40f0261d56624e6ac32bd
Author: Ping Li <quicpingli@codeaurora.org>
Date:   Fri Mar 15 00:30:03 2013 -0400

    msm:video: Enable LUT update on DSI command mode panel
    
    CRs-Fixed: 462625
    
    Change-Id: Ieb670abbed0ad06a9c758fb044bf71b6fc2c71b0
    Signed-off-by: Ping Li <quicpingli@codeaurora.org>

commit 418f46dfbdb7014d8bb91feeaa70d3f0fe56ec9f
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun Jul 28 10:26:04 2013 -0700

    Revert "Revert "msm: display: Do not commit in unset for external.""
    
    This reverts commit 39bc80e73d0a1b238a8584ae414e3fac152196cb.

commit 44a1c3ee83110175604e7076bc6fdacd75689343
Author: Ajay Dudani <adudani@codeaurora.org>
Date:   Fri Apr 19 13:18:13 2013 -0700

    Revert "Revert "ARM: 7169/1: topdown mmap support""
    
    This reverts commit f27e4f0e730b99ca4dabed0b408d96dbf73a8fac.
    
    With 01b1dee in system/core to set ADDR_COMPAT_LAYOUT, this is
    not needed any longer.
    
    Bug: 8470684
    Signed-off-by: Ajay Dudani <adudani@codeaurora.org>
    Acked-by: Laura Abbot <lauraa@codeaurora.org>

commit f19efe042fa834cf0c346f7525dc62742f6cc6f0
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 18 09:14:12 2012 +0000

    tcp: fix FIONREAD/SIOCINQ
    
    tcp_ioctl() tries to take into account if tcp socket received a FIN
    to report correct number bytes in receive queue.
    
    But its flaky because if the application ate the last skb,
    we return 1 instead of 0.
    
    Correct way to detect that FIN was received is to test SOCK_DONE.
    
    Reported-by: Elliot Hughes <enh@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 97bc6bb096614ae8aa6dc02a95cf8c5b945058bf
Author: Sameer Thalappil <sameert@codeaurora.org>
Date:   Tue Mar 19 14:28:37 2013 -0700

    nl80211/cfg80211: add VHT MCS support
    
    Add support for reporting and calculating VHT MCSes.
    
    Note that I'm not completely sure that the bitrate
    calculations are correct, nor that they can't be
    simplified.
    
    Change-Id: Id4c132850a85ff59f0fc16396763ed717689bec0
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Git-commit: db9c64cf8d9d3fcbc34b09d037f266d1fc9f928c
    Git-repo:
    git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Sameer Thalappil <sameert@codeaurora.org>

commit acf590c987bf0dffaa9987d90d45a2eae9f03145
Author: Lorenzo Colitti <lorenzo@google.com>
Date:   Wed Mar 6 13:14:38 2013 -0800

    net: ipv6: Don't purge default router if accept_ra=2
    
    Setting net.ipv6.conf.<interface>.accept_ra=2 causes the kernel
    to accept RAs even when forwarding is enabled. However, enabling
    forwarding purges all default routes on the system, breaking
    connectivity until the next RA is received. Fix this by not
    purging default routes on interfaces that have accept_ra=2.
    
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>
    Acked-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 1756297f3c463e8e0f21a9dfab393e30ec89c065
Author: Iliyan Malchev <malchev@google.com>
Date:   Sat May 18 15:43:50 2013 -0700

    timed_gpio: fix order of destruction
    
    Signed-off-by: Iliyan Malchev <malchev@google.com>

commit 8395f2c28cf9a03ac3443d07862e315a45d8424b
Author: JP Abgrall <jpa@google.com>
Date:   Mon Apr 8 15:09:26 2013 -0700

    netfilter: qtaguid: rate limit some of the printks
    
    Some of the printks are in the packet handling path.
    We now ratelimit the very unlikely errors to avoid
    kmsg spamming.
    
    Signed-off-by: JP Abgrall <jpa@google.com>

commit e9c5f3d5aa9a62d98b1522131e322d2c797f8865
Author: JP Abgrall <jpa@google.com>
Date:   Wed Feb 6 17:40:07 2013 -0800

    netfilter: xt_qtaguid: Allow tracking loopback
    
    In the past it would always ignore interfaces with loopback addresses.
    Now we just treat them like any other.
    This also helps with writing tests that check for the presence
    of the qtaguid module.
    
    Signed-off-by: JP Abgrall <jpa@google.com>

commit 2f323d43defe45edc405efd34fe00e2eafff123d
Author: JP Abgrall <jpa@google.com>
Date:   Fri Jan 4 18:18:36 2013 -0800

    netfilter: xt_qtaguid: remove AID_* dependency for access control
    
    qtaguid limits what can be done with /ctrl and /stats based on group
    membership.
    This changes removes AID_NET_BW_STATS and AID_NET_BW_ACCT, and picks
    up the groups from the gid of the matching proc entry files.
    
    Signed-off-by: JP Abgrall <jpa@google.com>
    Change-Id: I42e477adde78a12ed5eb58fbc0b277cdaadb6f94

commit 2069e1f4f1125463cd400e83d0aad5c486e798cd
Author: JP Abgrall <jpa@google.com>
Date:   Mon Jan 28 16:50:44 2013 -0800

    netfilter: xt_qtaguid: extend iface stat to report protocols
    
    In the past the iface_stat_fmt would only show global bytes/packets
    for the skb-based numbers.
    For stall detection in userspace, distinguishing tcp vs other protocols
    makes it easier.
    Now we report
      ifname total_skb_rx_bytes total_skb_rx_packets total_skb_tx_bytes
      total_skb_tx_packets {rx,tx}_{tcp,udp,ohter}_{bytes,packets}
    
    Bug: 6818637
    
    CRs-Fixed: 509089 511633
    Change-Id: I06b91826d142fda8444aa6ebe3d0efd6a0a1ce8b
    Signed-off-by: JP Abgrall <jpa@google.com>
    Git-commit: 01d1733e27c7c03dc700cd8cd7658b326bc0b1b2
    Git-repo: https://android.googlesource.com/kernel/common/
    Signed-off-by: Sridhar Gujje <sgujje@codeaurora.org>

commit 39bc80e73d0a1b238a8584ae414e3fac152196cb
Author: Steve Kondik <shade@chemlab.org>
Date:   Fri Jul 26 17:34:41 2013 -0700

    Revert "msm: display: Do not commit in unset for external."
    
    This reverts commit e23729eaae21cf37781142611a1edec86bee7c5a.
    
    Revert "msm: mdp: Wait unconditionally in internal vsync waits"
    
    This reverts commit 8f4ed8e2f33c662acf71f246a9dab689c44251e3.
    
    Revert "msm: mdp: Use a waitqueue for vsync notifications"
    
    This reverts commit 1b835d2d5d1e612774c613150fd2cb75b10a626d.

commit 0643c4303b919180852300202364b0ca069fb5c3
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Tue Jul 9 18:45:26 2013 +0530

    msm: vidc: Amend error check conditions on ION APIs
    
    Amend error check conditions appropriately to return
    properly from video applications in case of ION API
    failures.
    
    Change-Id: Ibe95a8438a66e88b35dbff0af8842ff4b038c5e1
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 7a0aecb4b990c073ef3ea5772da11ae1405e37e3
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Jun 7 19:44:14 2013 +0530

    msm: vidc: set EOS on output buffer pending transaction
    
    If EOS is signalled while an output buffer is in transaction,
    mark EOS when that buffer is returned.
    This is observed in case client sends an empty buffer with EOS
    
    Change-Id: I9b4ce6331eb7a70886c1802d1f99e1d77d65fa64
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 2e5c4bc1cfb400bdb568edf30686a890681e16cc
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Jun 7 19:32:53 2013 +0530

    vidc: Fix EOS handling if video h/w has a frame in transaction
    
    In case of video hardware is encoding a frame and EOS buffer with
    with zero data length arrived, set the EOS flag to the transaction
    available in the transaction table which will be passed to the
    client in frame done callback from the video hardware.
    
    Change-Id: I03862906be603ff769fb6aa0a2191a9ea87529ca
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 84624c509c59a6f12de4aa06ec26233773b27722
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Jun 7 19:23:32 2013 +0530

    msm: vidc: Reset stop_called state when START is called
    
    The driver needs to reset the stop_called field once START called.
    Otherwise in START, STOP, START, STOP sequences the secondary STOP will
    be treated as a duplicate STOP and ignored.
    
    Change-Id: I8b7d069236027a01103c80b7deeb19c331ede05a
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit d8e318a21360190c435ad9b04ad1058a47f957fd
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Jun 7 19:08:30 2013 +0530

    msm: vidc: Don't free shared memory on channel close
    
    The driver already frees the memory in vcd_ddl.c at ddl_close().
    No need to prematurely free the memory here.
    
    Change-Id: If1ddacf68770113423d4aeb2b72e79caf5ca07ce
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit e23729eaae21cf37781142611a1edec86bee7c5a
Author: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>
Date:   Tue Jul 16 10:39:50 2013 +0530

    msm: display: Do not commit in unset for external.
    
    In case of external, commit is happening in unset itself which
    is causing flickers. This patch fixes the issue.
    
    Change-Id: Id55346be7a155108954e23bef236db80e7b65152
    Signed-off-by: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>

commit 8f4ed8e2f33c662acf71f246a9dab689c44251e3
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Thu May 16 12:20:55 2013 -0400

    msm: mdp: Wait unconditionally in internal vsync waits
    
    Do not wait for timestamp to change for internal waits as the
    waitqueue can be woken up from places where the vsync timestamp
    doesn't change.
    
    Change-Id: Ibd922d09975fad79065792f5da44816f3fea3044
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit 1b835d2d5d1e612774c613150fd2cb75b10a626d
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Sat May 11 08:32:55 2013 -0400

    msm: mdp: Use a waitqueue for vsync notifications
    
    Use a waitqueue instead of completions and make sure the vsync
    show event always returns the timestamp except when it is
    interrupted. This simplifies the vsync wait mechanism both here
    and in userspace and avoids any unnecessary locking.
    
    Change-Id: Ia3c2fce24e50d6b1e7fa1b36c668d5cc8678a1c6
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit 775dea4a4f14b222f817d822c5c58f1578a7b970
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Tue Feb 19 10:32:31 2013 -0500

    msm: display: dsi command mode vsync time generation
    
    In each vsync cycle there will be two vsync interrupts
    generated, need ignore the wrong one to provide the correct vsync
    timestamp.
    
    CRs-fixed: 452269
    Change-Id: Ic13eea77d15eb46f4879f818d346425c63efe41a
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit d116618dc25eb5288f9d996403282d5cbf0a5bed
Author: Steve Kondik <shade@chemlab.org>
Date:   Fri Jul 26 15:28:46 2013 -0700

    gpu: msm: Cleanup after merge
    
    Change-Id: I099b713103f62d3787ad8f63940f3a3a13a5f1c1

commit 03c57d400249284273d87fdcf3bb834552363414
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Mon Mar 25 14:30:55 2013 -0600

    msm: kgsl: a2xx: Check the right interrupt status bits
    
    Check the actual interrupt status bits in a2xx_irq_pending instead
    of reading the mask registers which were always set and returning
    true. Troutslap for the original author (me).
    
    CRs-fixed: 467381
    Change-Id: Ic0dedbada1c5fa64548221718c1eaf9b6eaaa1e8
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 42fba7cc898de0de409ab8a749c9e5565ec4acf2
Author: Rajesh Kemisetti <rajeshk@codeaurora.org>
Date:   Fri Jul 19 21:25:49 2013 +0530

    msm: kgsl: Make use of performance counter for A2xx.
    
    Add new performance counter for a2xx to track number
    of ALU instructions executed. This will help fault tolerance
    detection logic to get more accurate data about the GPU.
    
    Change-Id: I4a1afcfa12fd55a8ca1c4827f1fa7ee8e322e9c5
    Signed-off-by: Rajesh Kemisetti <rajeshk@codeaurora.org>

commit 642b093e3a5a846f36a01e279418caca01fc84c2
Author: Lynus Vaz <lvaz@codeaurora.org>
Date:   Tue Jun 18 21:49:23 2013 +0530

    msm: kgsl: Fix the extra draw packet on resume for A305
    
    The extra draw command on resume was missing some state. Add the missing
    state registers, to avoid stability issues.
    
    Change-Id: Iad51cbe3be7b2951563c989625a690178491e0c8
    Signed-off-by: Lynus Vaz <lvaz@codeaurora.org>

commit db054013ea800ed2747c9ff9db08d143b792c533
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Thu Jan 17 00:53:26 2013 -0800

    msm: kgsl: Add a section to list memory entries in snapshot
    
    Add a section that lists all memory entries of a process
    in which the hang has supposedly occurred. The entries are
    of (gpuaddr, size, memtype) tuples.
    
    Change-Id: I981d2325346455a6775af1691474c02422581b4f
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>

commit 515be8c597771048671ff52f5fd7f7a0dfcbe0cf
Author: Lynus Vaz <lvaz@codeaurora.org>
Date:   Tue Nov 20 18:26:56 2012 +0530

    msm: kgsl: Submit a draw command on resume
    
    For A305, submit an extra draw command while resuming the GPU from
    SLUMBER state. This fixes corruption seen when the screen is
    turned on.
    
    Change-Id: Iafed90b617b0a700ac3f2afcb6d9ecadca590914
    Signed-off-by: Lynus Vaz <lvaz@codeaurora.org>

commit b51638fa3e2755073c068141473b6aab70804e40
Author: Rajesh Kemisetti <rajeshk@codeaurora.org>
Date:   Wed Jul 3 00:14:09 2013 +0530

    msm: kgsl: Skip perf counters and Sync lock for A2xx.
    
    Fix includes two changes:
    
    - Skip Perf counters for A2xx since these are intended only for A3xx.
    - Don't use GPU-CPU sync lock for A2xx to avoid bootup failure.
    
    Change-Id: I9e1d349b95dd0f6470ab507167b53540f1f1acf7
    Signed-off-by: Rajesh Kemisetti <rajeshk@codeaurora.org>

commit d922364327158267a4eccac0a6e1e0bad7e2743f
Author: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
Date:   Mon Jul 1 11:58:21 2013 -0600

    msm: kgsl: Add a missing mutex unlock
    
    Adding a missing mutex unlock where we return from
    kgsl_late_resume_driver if we fail to wake up the device.
    
    CRs-fixed: 500422
    Change-Id: I26b56d95a9427798719fb162d1b1edaf641ae57b
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>

commit 1afc167864bb2a2ee520895e6c393473129d4a28
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Mon Jun 10 17:59:38 2013 -0700

    msm: kgsl: Do not hold memory spinlock when calling find region function
    
    The function kgsl_sharedmem_find_region holds the memory spinlock
    at the beginning of the function so we do not need to hold the lock
    before calling the function
    
    Change-Id: I20ee32e0ed6aee6ed61cdd4fb7a9cc08a876fc84
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 20c58dc662381f0af3eb2044ccefdaf3bffdb24e
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Fri Jun 7 21:19:43 2013 -0700

    msm: kgsl: Only initialize process structure once
    
    If the debug root of the process structure is initialized
    then it means that the remaining fields will also be
    initialized. Check this field at the beginning of the
    initialization routine and skip to end of the function. This
    also by-passes the reinitialization of some fields because
    of errorneous NULL pointer check of rb node. The rb node is
    always initialized to NULL so checking if it is NULL to
    determine whether the pointer is initialized or not is wrong.
    
    CRs-Fixed: 498014
    Change-Id: I73b2124c037187bc96942714dac62c0a72c26372
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 9d51ea67303211ffb6bfeff5e7eade64f13c3c17
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Wed Jun 5 21:58:44 2013 -0700

    msm: kgsl: Only reference the rb_node after taking spinlock
    
    Derefernce the rb node after taking spinlock since the root
    node can change and is supposed to be guarded by the spinlock
    
    CRs-Fixed: 495129
    Change-Id: I63de7755cf9d5fb548c082f43403000bcef5721b
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit fe44acf0c9d1848d87fd12942702d0f331339231
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Thu Jun 20 13:53:24 2013 -0600

    msm: kgsl: Prevent race conditions when freeing memory
    
    Multiple threads can call an ioctl to free a memory region.
    Only one of these threads must be allowed to free memory and
    the rest can return without freeing. Add a new pending flag
    which is tested and set in a critical section guarded by
    spinlock to prevent multiple ioctl threads from freeing the
    same memory.
    Also, a thread could be freeing a memory region that is
    in use by another thread. Ensure that the detachment of the
    memory from the process list and freeing of the memory always
    happens in the same thread. This will prevent a situation where
    the memory is being used by a thread which is detached from the
    process list and is yet to be freed.
    Seperate the selection of gpuaddr and mapping into pagetable. The
    gpuaddr assignment needs to be done with process memory lock held
    in a critical section but the mapping into pagetable can be done
    separately without holding memory lock.
    
    CRs-Fixed: 495144
    Change-Id: Idf85fbd4bca29c18597f4b0e737c207f002ab266
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 24c2f46fc47db23fb74de1ca5bc6cd65658b6daf
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:27:01 2013 -0700

    msm: kgsl: Use the correct length when looking for address collision
    
    Use the right length of allocation from the aligned base address of
    CPU virtual map to look for collision in the GPU vitual map. This
    prevents discarding of usable address ranges due to false collision
    hits.
    
    CRs-Fixed: 492041
    Change-Id: I370e6a31f98803e8ca6858a5562f47afeeaa157e
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit d47b730af956487bf78fcca3d5af22998da1d661
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:12:26 2013 -0600

    msm: kgsl: fix kgsl_mem_entry refcounting
    
    Make kgsl_sharedmem_find* return a reference to the
    entry that was found. This makes using an entry
    without the mem_lock held less race prone.
    
    Change-Id: If6eb6470ecfea1332d3130d877922c70ca037467
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 4a98705ac9f88e5faa1f4db68993969a2be19f22
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:12:18 2013 -0600

    msm: kgsl: add guard page support for imported memory
    
    Imported memory buffers sometimes do not have enough
    padding to prevent page faults due to overzealous
    GPU prefetch. Attach guard pages to their mappings
    to prevent these faults.
    
    Because we don't create the scatterlist for some
    types of imported memory, such as ion, the guard
    page is no longer included as the last entry in
    the scatterlist. Instead, it is handled by
    size ajustments and a separate iommu_map() call
    in the kgsl_mmu_map() and kgsl_mmu_unmap() paths.
    
    Change-Id: I3af3c29c3983f8cacdc366a2423f90c8ecdc3059
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 17b4c5b28cc4f941a5263203a5332f16fcbac295
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:12:11 2013 -0600

    msm: kgsl: prevent race between mmap() and free on timestamp
    
    When KGSL_MEMFLAGS_USE_CPU_MAP is set, we must check that the
    address from get_unmapped_area() is not used as part of a
    mapping that is present only in the GPU pagetable and not the
    CPU pagetable. These mappings can occur because when a buffer
    is freed on timestamp, the CPU mapping is destroyed immediately
    but the GPU mapping is not destroyed until the GPU timestamp
    has passed.
    
    Because kgsl_mem_entry_detach_process() removed the rbtree
    entry before removing the iommu mapping, there was a window
    of time where kgsl thought the address was available even
    though it was still present in the iommu pagetable. This
    could cause the address to get assigned to a new buffer,
    which would cause iommu_map_range() to fail since the old
    mapping was still in the pagetable. Prevent this race by
    removing the iommu mapping before removing the rbtree entry
    tracking the address.
    
    Change-Id: I8f42d6d97833293b55fcbc272d180564862cef8a
    CRs-Fixed: 480222
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit e0e5a0d7b7136cee068f63386467a4d6789eb853
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:12:04 2013 -0600

    msm: kgsl: better handling of virtual address fragmentation
    
    When KGSL_MEMFLAGS_USE_CPU_MAP is enabled, the mmap address
    must try to match the GPU alignment requirements of the buffer,
    as well as include space in the mapping for the guard page.
    This can cause -ENOMEM to be returned from get_unmapped_area()
    when there are a large number of mappings. When this happens,
    fall back to page alignment and retry to avoid failure.
    
    Change-Id: I2176fe57afc96d8cf1fe1c694836305ddc3c3420
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 3f37c88186c8b5544daf899776f11579d8d2b6a9
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:57 2013 -0600

    msm: kgsl: Use CPU path to program pagetable when active count is 0
    
    When active count is 0 then we should use the CPU path to program
    pagetables because the GPU path requires event registration. Events
    can only be queued when active count is valid. Hence, if the active
    count is NULL then use the CPU path.
    
    Change-Id: I70f5894d20796bdc0f592db7dc2731195c0f7a82
    CRs-fixed: 481887
    Signed-off-by: Shubhrapralash Das <sadas@codeaurora.org>

commit 9a65ad36a02ece76f4a2edd70f6a5a1a77646de5
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:48 2013 -0600

    msm: kgsl: Don't hold process list global mutex in process private create
    
    Don't hold process list global mutex for long. Instead make
    use of process specific spin_lock() to serialize access
    to process private structure while creating it. Holding
    process list global mutex could lead to deadlocks as other
    functions depend on it.
    
    CRs-fixed: 480732
    Change-Id: Id54316770f911d0e23384f54ba5c14a1c9113680
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 279c586a3da11e4a2c87f97a6815ff3e2f77c00c
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:41 2013 -0600

    msm: kgsl: Do not return an error on NULL gpu address
    
    If a NULL gpu address is passed to snapshot object tracking
    function then do not treat this as an error and return 0. NULL
    objects may be present in an IB so just skip over these objects
    instead of exiting due to an error.
    
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>
    Change-Id: Ic253722c58b41f41d03f83c77017e58365da01a7

commit 73a8ed2851bd14757d2b9dc97654ab91cba901e4
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:35 2013 -0600

    msm: kgsl: Fix early exit condition in ringbuffer drain
    
    The ringbuffer drain function can be called when the ringbuffer
    start flag is not set. This happens on startup. Hence,
    exiting the function early based on start flag is incorrect.
    Simply execute this function regardless of the start flag.
    
    Change-Id: Ibf2075847f8bb1a760bc1550309efb3c7aa1ca49
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit fc22486c1b0d4ad5ca7f95c19d28dc77a1dcc4b0
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:28 2013 -0600

    msm: kgsl: If adreno start fails then restore state of device
    
    Restore the state of the device back to what it was at the
    start of the adreno_start function if this function fails to
    execute successfully.
    
    Change-Id: I5b279e5186b164d3361fba7c8f8d864395b794c8
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 1f032855a570c4d933a919080d84897d5e420250
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:22 2013 -0600

    msm: kgsl: Fix searching of memory object
    
    Make sure that at least a size of 1 byte is searched when locating
    the memory entry of a region. If size is 0 then a memory region
    whose last address is equal to the start address of the memory being
    searched will be returned which is wrong.
    
    Change-Id: I643185d1fdd17296bd70fea483aa3c365e691bc5
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit aed25344f849e8b7225d5e429063f1ec7b1bfa31
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:16 2013 -0600

    msm: kgsl: Skip cff dump for certain functions when its disabled
    
    Certain functions were generating CFF when CFF was disabled. Make
    sure these functions do not dump CFF when it is disabled.
    
    Change-Id: Ib5485b03b8a4d12f190f188b80c11ec6f552731d
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 74f21622d2426e95b3199358e3d57e39f9c56a53
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:09 2013 -0600

    msm: kgsl: Add global timestamp information to snapshot
    
    Make sure that we always add global timestamp information to
    snapshot. This is needed in playbacks for searching whereabouts
    of last executed IB.
    
    Change-Id: Ica5b3b2ddff6fd45dbc5a911f42271ad5855a86a
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 2b266f9b29bc9ece211d1a1fee404205b53902bb
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:11:02 2013 -0600

    msm: kgsl: Loop till correct index on type0 packets
    
    When searching for memory addresses in type0 packet we were looping
    from start of the type0 packet till it's end, but the first DWORD
    is a header so we only need to loop till packet_size - 1. Fix this.
    
    Change-Id: I278446c6ab380cf8ebb18d5f3ae192d3d7e7db62
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 520b60833bd0c14b774c08686c8441ad8e44be95
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:10:56 2013 -0600

    msm: kgsl: Track memory address from 2 additional registers
    
    Add tracking of memory referenced by VS_OBJ_START_REG and FS_OBJ_START_REG
    registers in snapshot. This makes snapshot more complete in terms of
    tracking data that is used by the GPU at the time of hang.
    
    Change-Id: I7e5f3c94f0d6744cd6f2c6413bf7b7fac4a5a069
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit f368065d6d57daa14ae579ee1c4347208c8e5e90
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:10:49 2013 -0600

    msm: kgsl: In snapshot track a larger object size if address is same
    
    If the object being tracked has the same address as a previously
    tracked object then only track a single object with larger size
    as the smaller object will be a part of the larger one anyway.
    
    Change-Id: I0e33bbaf267bc0ec580865b133917b3253f9e504
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit df2ddc2db94477fc9f3fa4595740a5885affdb8e
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:10:42 2013 -0600

    msm: kgsl: Save the last active context in snapshot
    
    Save the last active context that was executing when the hang happened
    in snapshot.
    
    Change-Id: I2d32de6873154ec6c200268844fee7f3947b7395
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 91d75095a402abb4368ba6a06238067873ab2e5b
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:10:34 2013 -0600

    msm: kgsl: snapshot: Only push the last IB1 and IB2 in the static space
    
    Some IB1 buffers have hundreds of little IB2 buffers and only one of them
    will actually be interesting enough to push into the static space.  Only
    push the last executed IB1 and IB2 into the static space.
    
    Change-Id: Ic0dedbad26fb30fb5bf90c37c29061fd962dd746
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit b482fa8e6f2a2fa5e7f4f9b8db5e0a3396654bd4
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:10:27 2013 -0600

    msm: kgsl: snapshot: Don't keep parsing indirect buffers on failure
    
    Stop parsing an indirect buffer if an error is encountered (such as
    a missing buffer). This is a pretty good indication that the buffers
    are not reliable and the further the parser goes with a unreliable
    buffer the more likely it is to get confused.
    
    Change-Id: Ic0dedbadf28ef374c9afe70613048d3c31078ec6
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 20cbb33091d991aba7a6402ffca854fae908340c
Author: Carter Cooper <ccooper@codeaurora.org>
Date:   Tue May 28 17:10:21 2013 -0600

    msm: kgsl: Enable HLSQ registers in snapshot when available
    
    Reading the HLSQ registers during a GPU hang recovery might cause
    the device to hang depending on the state of the HLSQ block.
    Enable the HLSQ register reads when we know that they will
    succeed.
    
    Change-Id: I69f498e6f67a15328d1d41cc64c43d6c44c54bad
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>

commit 66559845ee576ded0e9a0aa089caf9188279db05
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:10:15 2013 -0600

    msm: kgsl: Don't go to slumber if active_count is non zero
    
    If active_cnt happens to be set when we go into
    kgsl_early_suspend_driver() then don't go to SLUMBER.  This
    avoids trouble if we come back and and try to access the
    hardware while it is off.
    
    Change-Id: Ic0dedbadb13514a052af6199c8ad1982d7483b3f
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit a692c2eae433880200fa49440faf3a999fe4a2ee
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:10:09 2013 -0600

    msm: kgsl: Avoid an array overrun in the perfcounter API
    
    Make sure the passed group is less than the size of the list of
    performance counters.
    
    Change-Id: Ic0dedbadf77edf35db78939d1b55a05830979f85
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 782a5643db99158315911338872308742833e22b
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:10:02 2013 -0600

    msm: kgsl: Only allow two counters for VBIF performance counters
    
    There are only two VBIF counter groups so validate that the user
    doesn't pass in > 1 and clean up the if/else clause.
    
    Change-Id: Ic0dedbad3d5a54e4ceb1a7302762d6bf13b25da1
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit d5bf2695d0693c20f937f0d3a519bcd3099d4ab9
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:09:56 2013 -0600

    msm: kgsl: Add support for VBIF and VBIF_PWR performance counters
    
    These 2 counter groups are also "special cases" that require
    different programming sequences.
    
    Change-Id: I73e3e76b340e6c5867c0909b3e0edc78aa62b9ee
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit e0485467cbef46424645867f31d23e186bbda13d
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:09:49 2013 -0600

    msm: kgsl: map the guard page readonly on the iommu
    
    The guard page needs to be readable by the GPU, due to
    a prefetch range issue, but it should never be writable.
    Change the page fault message to indicate if nearby
    buffers have a guard page.
    
    Change-Id: I3955de1409cbf4ccdde92def894945267efa044d
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit cbdd557a6603500252e124a3bc969211e6b2ba8a
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:09:43 2013 -0600

    msm: kgsl: remove kgsl_mem_entry.flags
    
    The two flags fields in kgsl_memdesc should be enough for
    anyone.  Move the only flag using kgsl_mem_entry, the
    FROZEN flag for snapshot procesing, to use kgsl_memdesc.priv.
    
    Change-Id: Ia12b9a6e6c1f5b5e57fa461b04ecc3d1705f2eaf
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 11681853fb12b78223a8f257a6034422a8634c82
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:09:35 2013 -0600

    msm: kgsl: clean up iommu/gpummu protflag handling
    
    Make kgsl_memdesc_protflags() return the correct type of flags
    for the type of mmu being used. Query the memdesc with this
    function in kgsl_mmu_map(), rather than passing in the
    protflags. This prevents translation at multiple layers of
    the code and makes it easier to enforce that the mapping matches
    the allocation flags.
    
    Change-Id: I2a2f4a43026ae903dd134be00e646d258a83f79f
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit fafc502f43cb6b98fb4b2b4c2cfc23c2e44f210b
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:09:27 2013 -0600

    msm: kgsl: Remove an uneeded register write for A3XX GPUs
    
    A3XX doesn't have the MH block and so the register at 0x40 points
    somewhere else. Luckily the write was harmless but remove it anyway.
    
    Change-Id: Ic0dedbadd1e043cd38bbaec8fcf0c490dcdedc8c
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit a6b322eb2302218e2fb434f5267b93bd6c57b2af
Author: Vladimir Razgulin <vrazguli@codeaurora.org>
Date:   Tue May 28 17:09:21 2013 -0600

    msm: kgsl: Print the nearest active GPU buffers to a faulting address
    
    Print the two active GPU memory entries that bracket a faulting GPU
    address. This will help diagnose premature frees and buffer ovverruns.
    
    Check if the faulting GPU address was freed by the same process.
    
    Change-Id: Ic0dedbadebf57be9abe925a45611de8e597447ea
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    Signed-off-by: Vladimir Razgulin <vrazguli@codeaurora.org>

commit 14499abd9f292ffc1829a419b79145b861b7ad43
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:09:15 2013 -0600

    msm: kgsl: Add a new API to allow sharing of GPU performance counters
    
    Adreno uses programmable performance counters, meaning that while there
    are a limited number of physical counters each counter can be programmed
    to count a vast number of different measurements (we refer to these as
    countables).  This could cause problems if multiple apps want to use
    the performance counters, so this API and infrastructure allows the
    counters to be safely shared.
    
    The kernel tracks which countable is selected for each of the physical
    counters for each counter group (where groups closely match hardware
    blocks). If the desired countable is already in use, or there is an
    open physical counter, then the process is allowed to use the counter.
    
    The get ioctl reserves the counter and returns the dword offset of the
    register associated with that physical counter.  The put ioctl
    releases the physical counter.  The query ioctl gets the countables
    used for all of the counters in the block - up to 8 values can be
    returned.  The read ioctl gets the current hardware value in the counter
    
    Change-Id: Ic0dedbadae1dedadba60f8a3e685e2ce7d84fb33
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>
    
    Conflicts:
    	drivers/gpu/msm/adreno_a3xx.c

commit bed83dd1f5eefe40784416e885a8110501db09c2
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:09:08 2013 -0600

    msm: kgsl: Handle a possible ringbuffer allocspace error
    
    In the GPU specific start functions, account for the possibility
    that ringbuffer allocation routine might return NULL.
    
    Change-Id: Ic0dedbadf6199fee78b6a8c8210a1e76961873a0
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    
    Conflicts:
    	drivers/gpu/msm/adreno_a3xx.c

commit 6cf3828c29c987bf074364b7d2a53bdbfc8c7655
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:09:01 2013 -0600

    msm: kgsl: Convert the Adreno GPU cycle counters to run free
    
    In anticipation of allowing multiple entities to share access to the
    performance counters; make the few performance counters that KGSL
    uses run free.
    
    Change-Id: Ic0dedbadbefb400b04e4f3552eed395770ddbb7b
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    
    Conflicts:
    	drivers/gpu/msm/adreno.h
    	drivers/gpu/msm/adreno_a3xx.c

commit 3a3389af376584970df25ce08ac0b48cb5e06cc7
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:08:54 2013 -0600

    msm: kgsl: Fix compilation errors when CFF is turned on
    
    Fix the compilation errors when option MSM_KGSL_CFF_DUMP option
    is turned on.
    
    Change-Id: I59b0a7314ba77e2c2fef03338e061cd503e88714
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 4eab932e8e95a84f42fe3ee80f5772d7e13f1f09
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:08:48 2013 -0600

    msm: kgsl: Update settings for the A330v2 GPU in 8972v2
    
    The new GPU spin in 8974v2 has some slightly different settings
    then the 8974v1: add support for identifying a v2 spin, add a new
    table of VBIF register settings and update the clock gating
    registers.
    
    Change-Id: Ic0dedbad22bd3ed391b02f6327267cf32f17af3d
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit ba4ce0a0d5d4ebc2e0b048894153e680b35474a8
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:08:42 2013 -0600

    msm: kgsl: Move A3XX VBIF settings decision to a table
    
    The vbif selection code is turning into a long series of if/else
    clauses. Move the decision to a look up table that will be easier
    to update and maintain when when we have eleventy A3XX GPUs.
    
    Change-Id: Ic0dedbadd6b16734c91060d7e5fa50dcc9b8774d
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit f4f5b02966b68d6d1fbb78d08d850b17105d6915
Author: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
Date:   Tue May 28 17:08:36 2013 -0600

    msm: kgsl: Add 8974 default GPR0 & clk gating values
    
    Add correct clock gating values for A330, A305 and A320.
    Add generic function to return the correct default clock
    gating values for the respective gpu. Add default GPR0
    value for A330.
    
    Change-Id: I039e8e3622cbda04924b0510e410a9dc95bec598
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>

commit 0046dcc0af768bed5ea9015cc156bef0be876bf7
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:08:29 2013 -0600

    msm: kgsl: Send the right IB size to adreno_find_ctxtmem
    
    adreno_find_ctxtmem expects byte lengths and we were sending it
    dword lengths which was about as effective as you would expect.
    
    Change-Id: Ic0dedbad536ed377f6253c3a5e75e5d6cb838acf
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 9e71d2aee1d668b8532fade825057ec0ab9a11f9
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:08:17 2013 -0600

    msm: kgsl: Export some kgsl-core functions to EXPORT_SYMBOLS
    
    Export some functions in the KGSL core driver so they can
    be seen by the leaf drivers.
    
    Change-Id: Ic0dedbad5dbe562c2e674f8e885a3525b6feac7b
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit a15f1902aa6bc7f360c9cc62f42fb446021b525f
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:08:10 2013 -0600

    msm: kgsl: Sync memory with CFF from places where it was missing
    
    Before submitting any indirect buffer to GPU via the ringbuffer,
    the indirect buffer memory should be synced with CFF so that the
    CFF capture will be complete. Add the syncing of memory with CFF
    in places where this was missing
    
    Change-Id: I18f506dd1ab7bdfb1a68181016e6f661a36ed5a2
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 7500051c8531715add2d11bf9330abf9ff8840de
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:07:32 2013 -0600

    msm: kgsl: improve active_cnt and ACTIVE state management
    
    Require any code path which intends to touch the hardware
    to take a reference on active_cnt with kgsl_active_count_get()
    and release it with kgsl_active_count_put() when finished.
    These functions now do the wake / sleep steps that were
    previously handled by kgsl_check_suspended() and
    kgsl_check_idle().
    
    Additionally, kgsl_pre_hwaccess() will no longer turn on
    the clocks, it just enforces via BUG_ON that the clocks
    are enabled before a register is touched.
    
    Change-Id: I31b0d067e6d600f0228450dbd73f69caa919ce13
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>
    
    Conflicts:
    	drivers/gpu/msm/kgsl.c

commit 873f3d0b49acf3e75918df183b28bf5542ec97d8
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 17:07:20 2013 -0600

    msm: kgsl: use kgsl_mmu_device_setstate() if the GPU is already idle
    
    Use the default setstate function, which directly reprograms the IOMMU,
    to change IOMMU pagetables or flush the TLB if the GPU is already idle.
    This condition often occurs when the GPU is being powered down. In this
    case it is desirable to avoid the overhead of issuing commands, waiting
    for idle and firing events that results from using the GPU
    command stream to reprogram the IOMMU.
    
    Change-Id: I633002ac49c8fe58df3f1f6a1fd1ddf705fc1733
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit cf3ee973ca23a9a694793a92b823a0ba518244ad
Author: Carter Cooper <ccooper@codeaurora.org>
Date:   Tue May 28 17:07:13 2013 -0600

    msm: kgsl: Add device init function
    
    Some device specific parameters need to be setup only once during
    device initialization. Create an init function for this purpose
    rather than re-doing this init everytime the device is started.
    
    Change-Id: I45c7fcda8d61fd2b212044c9167b64f793eedcda
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>

commit 2bf5e5d501c4c1e5482a652bcb6b288f47dceb50
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:07:01 2013 -0600

    msm: kgsl: Always fire an interupt if requested
    
    Even if the event code specifies a dummy interupt it might still
    get wrapped with the conditional check.  Make sure that all forced
    interrupts from the event code get fired.
    
    Change-Id: Ic0dedbadc4599efcbca65ad93cc27943bad2ca79
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>
    Signed-off-by: Tarun Karra <tkarra@codeaurora.org>
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 9d270d8f51eeeaa0810aa54b05ca21abc317d862
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:05:51 2013 -0600

    msm: kgsl: Print additional registers on IOMMU pagefault
    
    Print more IOMMU registers when a IOMMU pagefault happens which
    report whether the pagefault is a read or write fault.
    
    Change-Id: I27acafa9dcfd0c7de9056151ed1baef7dd2346df
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 41816c4ce6b9b7018a2b87b22f7cfbc5612df911
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Thu Jun 20 10:55:22 2013 -0700

    msm: kgsl: Add a barrier after writing to V2PUR register
    
    Add a memory barrier after writing to V2PUR register of IOMMU. Without
    this barrier the CPU core can stall because of multiple outstanding IOMMU
    register transactions happening in parallel. This barrier synchronizes
    the multiple IOMMU register transactions and prevents the CPU from
    stalling.
    
    Change-Id: I1d715e9fb8287b1f0b009b39b11b5469ab8861c8
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit d94d1db8be019a707706816cc8ae48cddc9fe0df
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:05:45 2013 -0600

    msm: kgsl: Lock ringbuffer translation in TLB
    
    Lock ringbuffer virtual address translation in TLB of IOMMU. This
    is required to prevent a hardware table walk from happening when
    GPU executes commands that program the IOMMU registers. If a hardware
    table walk happens when GPU is programming IOMMU register then it
    can cause a deadlock on the bus.
    
    Change-Id: I99428be7879a5210ba816f5f96864f94714649d7
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit d9a670c53696b55c645d614b68afd5aedc67ab65
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 17:05:38 2013 -0600

    msm: kgsl: Update required timestamps during command submission
    
    Update all the required timestamps when submitting commands to the
    GPU ringbuffer for execution. The global timestamp was not being
    updated in all the required cases. Now the global timestamp and
    per context timestamp are updated if following conditions are met:
    	1. A valid non global context is passed
    	2. The context passed supports per context timestamps
    	3. The command submission is not from kgsl
    If 3 conditions above are not met then only the global timestamp is
    updated.
    
    Change-Id: Ib4fe6fbd2ac57fbc5306377f09072f9e4c09d442
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit 4a8ac34bb8c89d7303809efec0df3a69caf0e6d0
Author: Vladimir Razgulin <vrazguli@codeaurora.org>
Date:   Tue May 28 17:05:31 2013 -0600

    msm: kgsl: Add memfree_history debug feature
    
    A kgsl user can free up an allocation of GPU memory with
    kgsl_ioctl_sharedmem_free() function, but the GPU address
    might still be in use resulting in a GPU page fault.
    A bug of this kind is hard to find because the driver keeps
    no trace of kgsl_ioctl_sharedmem_free calls.
    This change implements a "memfree history" debug feture that
    allows to see an information about a last couple of hundred
    memory free calls in kernel/debug/kgsl/kgsl-3d0/memfree_history
    file.
    
    Change-Id: I12e0e3702db56d99d5de644739b364dea4cc37b1
    Signed-off-by: Vladimir Razgulin <vrazguli@codeaurora.org>

commit 62871996f122e4243ebb3e001849ab64d406a7a9
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:04:33 2013 -0600

    msm: kgsl: Allow tasks to signal pending events
    
    Add a type parameter to event callbacks to give tasks a way to
    communicate error conditions to the callback. Current type values
    are KGSL_EVENT_TIMESTAMP_RETIRED for normal timestamp expiry and
    KGSL_EVENT_CANCELLED if the event is cancelled prematurely.
    
    Change-Id: Ic0dedbad9907cd50642a604a3af562e01a4b4a3b
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit ab010d1e46a9b3152a574747b508c5fbb5b059fd
Author: Carter Cooper <ccooper@codeaurora.org>
Date:   Tue May 28 17:04:26 2013 -0600

    msm: kgsl: Move timestamps inside the context structure
    
    Store the timestamps inside the context rather than in a
    list stored in the ringbuffer.  This allows for easier
    maintanability as well as keeping all context data
    centralized.
    
    Change-Id: I0467d07be6c8bb9f062a81a40629c0288be7e868
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>

commit b7ad3d7b0796f63af3688e7ac7f6e1d0ec2da76a
Author: Anshuman Dani <adani@codeaurora.org>
Date:   Tue Nov 6 22:19:50 2012 +0530

    msm: kgsl: Add CP_WAIT_FOR_IDLE packet before updating timestamp
    
    HW workaround to resolve MMU pagefault caused by memory
    freeing early before GPU completes it.
    
    This is race condition where timestamp is getting updated
    and the event handler runs before the GPU actually got done
    with the timestamp, hence freeing the buffer early while GPU
    still not completed with it.
    
    CRs-Fixed: 469194
    Signed-off-by: Anshuman Dani <adani@codeaurora.org>
    
    Conflicts:
    
    	drivers/gpu/msm/adreno_ringbuffer.c
    
    Change-Id: I6d338d7dca680f57f2c39ed5a14381b261bf8bce
    Signed-off-by: Anshuman Dani <adani@codeaurora.org>

commit 6856c25774fdaad7fde2f23964265b80cb834746
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:04:17 2013 -0600

    msm: kgsl: Fix context reference counting
    
    Get rid of kgsl_find_context. Use instead kgsl_context_get that does
    correct RCU read locking around the itr_find and increases the
    reference count on the context before returning it.  This eliminates
    the chance that a context will be destroyed while somebody is still
    using it.  Of course increased use of kgsl_context_get is accompanied
    by kgsl_context_put in all the right places.
    
    Change-Id: Ic0dedbad73d497fd9b451aefad8e5b28d33b829d
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    
    Conflicts:
    	drivers/gpu/msm/kgsl.c

commit 4970b81161b28deaef0ca27666d02e55b78162a2
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:03:52 2013 -0600

    msm: kgsl: Add a type field to the adreno draw context flags
    
    Allow the user space to pass in a type field to indicate the
    type of upper level library that owns the context. The type
    field is added to all the appropriate ftrace output for easier
    debugging.
    
    Change-Id: Ic0dedbadd42fc5ccfffd89738affd4794a6ab85e
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 3931abc3a2d133ad38ab694dc3d10ebeea9c21b2
Author: Vladimir Razgulin <vrazguli@codeaurora.org>
Date:   Tue May 28 17:03:45 2013 -0600

    msm: kgsl: Use ERR_PTR to return errors from kgsl_create_context()
    
    Normally kgsl_create_context() returns a pointer to a new context.
    In case of errors it just returns NULL pointer, and that doesn't allow
    to propagate the right error code back to the user via
    kgsl_ioctl_drawctxt_create()
    This fix modifies kgsl_create_context to use ERR_PTR macro for passing
    error codes to its caller.
    
    Change-Id: I447c1765828912b0994bcae67a73864e62eef9b6
    Signed-off-by: Vladimir Razgulin <vrazguli@codeaurora.org>

commit dec5c3b97179ef2694e37e401060a65f56c62174
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:03:38 2013 -0600

    msm: kgsl: Remove an uneeded log message that pre-dates tracing
    
    Before we started using tracepoints this log message was used for
    debugging the critical loop. Now that we are smarter remove the
    unneeded log message.
    
    Change-Id: Ic0dedbad57d97294cf2cb3865cf25f698aaea778
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 6a87dae50b384b353fa454bb776cc0c66d5fa901
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:03:32 2013 -0600

    msm: kgsl: Track GPU device resets
    
    Keep track of the number of times the GPU power rails have been
    cycled and provide said number to the user in the device sysfs
    directory.
    
    Change-Id: Ic0dedbad4489524a98e52b9993131257068ae2ef
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 6bc23e86cf49621f1336d5f74f5e6572a5b1735a
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:03:00 2013 -0600

    msm: kgsl: Add a ftrace event for register writes
    
    It is interesting to know when and how we program the hardware so
    add a tracepoint to monitor register writes.  Register reads are
    not traced since they are done much more frequently and are somewhat
    less interesting from a debug perspective.
    
    Change-Id: Ic0dedbad8ee85f154b9ba8731fe2c14603351243
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 03ed53c1e8db2607d082b895c895f198fa8883bd
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 17:02:44 2013 -0600

    msm: kgsl: Verify the user doesn't accidentally submit a zero length IB
    
    Indirect buffers to be executed will not be (and cannot be) zero length.
    Check and reject.
    
    Change-Id: Ic0dedbadea429c4f7bd386c5e64603b51ea8af61
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit f3fcef3c633a629e2f0ecaced14f31ff680b5828
Author: Carter Cooper <ccooper@codeaurora.org>
Date:   Tue May 28 17:01:59 2013 -0600

    msm: kgsl: Return correct timestamp for consumed timestamp reads
    
    adreno_readtimestamp() would return a register value when asked to
    get the KGSL_TIMESTAMP_CONSUMED value.  With per-context timestamps
    this no longer holds valid since the value in the register could
    correspond to a different contexts timestamp.  Return the start of
    packet timestamp for the specified context now that the start of
    timestamp values are correctly submitted.
    
    CRs-fixed: 382888
    Change-Id: Iad56d07eb4a2973a5906d989c96dd117ff902cea
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>

commit fd561cd26bcac95d775e5c80cbf6fa88ada7f600
Author: Anshuman Dani <adani@codeaurora.org>
Date:   Tue May 28 17:01:10 2013 -0600

    msm: kgsl: Add CP_WAIT_FOR_IDLE packet before updating timestamp
    
    HW workaround to resolve MMU pagefault caused by memory
    freeing early before GPU completes it.
    
    This is race condition where timestamp is getting updated
    and the event handler runs before the GPU actually got done
    with the timestamp, hence freeing the buffer early while GPU
    still not completed with it.
    
    Change-Id: I6d338d7dca680f57f2c39ed5a14381b261bf8bce
    Signed-off-by: Anshuman Dani <adani@codeaurora.org>

commit d7791c41aeed5c2d8f6cf5958cd1611bea95cdb9
Author: Carter Cooper <ccooper@codeaurora.org>
Date:   Tue May 28 17:00:06 2013 -0600

    msm: kgsl: Add identifier for internal command submissions
    
    Add an identifier to label internal commands being sent to the
    ringbuffer.  This will be used to further enable debugging
    and hang recovery.
    
    Change-Id: I4b3c10f0c3d290ae092182cbb450abe65e9dda80
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>

commit 236dccdb55e2e556c5f4f89e027a4ef8ac949769
Author: Ranjhith Kalisamy <ranjhith@codeaurora.org>
Date:   Tue May 28 16:59:59 2013 -0600

    msm: kgsl: Log retired timestamps on device wake
    
    Knowing the retired timestamp on wake-up is useful to debug hangs that
    are caused due to clock instability. When we request for clock ON, the
    clock may/may-not be turned ON, but we go ahead assuming it is ON. This
    can be caught by printing the timestamp on wake. The timestamp we print,
    can be compared against the timestamp in postmortem dump (assuming a
    hang happens).
    
    If there is a difference, we can say that GPU has run for sometime after
    wake. So, we needn't suspect clocks.
    
    If there is no difference, we can say that GPU has hung immediately after
    wake. So, we can positively suspect the clocks.
    
    Change-Id: Iedf49977adbb0e61a9b71bbe23acc81f2a75162d
    Signed-off-by: Ranjhith Kalisamy <ranjhith@codeaurora.org>

commit 3b91d26223cb2f445a50858e8be6a53613b9ba63
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 16:59:52 2013 -0600

    msm: kgsl: Do not dump the istore on A3XX cores
    
    The instruction store on A3XX GPU cores isn't really an instruction store
    as on A2XX and it doesn't give us any debugging value. On top of all
    that it can be unstable to read in some situations. Since it is of
    questionable value and questionable stability pull it from the snapshot
    for all A3XX cores and move the functionality to the A2XX specific code.
    
    Change-Id: Ic0dedbada32d8b6b9b93768d148c2624faf3ef30
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 30706be3c0a7fd489bba59956add4755064a132a
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 16:59:35 2013 -0600

    msm: kgsl: don't fault in cached allocations
    
    Make sure cache operations don't hit pagefaults by
    backing the entire vma in mmap() instead of faulting
    in pages as they're touched. Otherwise, there's a
    chance that a later cache operation on the mapping
    could trigger an unhandled page fault leading to
    a kernel panic.
    
    Change-Id: Ia73c8aaed2708c5b9ef46ed50fb0f5cf1ad2450c
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit cd5b8be047416f1809fbfc5101edf53eddc6b8f1
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 16:59:29 2013 -0600

    msm: kgsl: allow consistent CPU and GPU mappings
    
    KGSL_GPUMEM_ALLOC_ID now takes a flag,
    KGSL_MEMFLAGS_USE_CPU_MAP. When set, the GPU
    mapping will be set up to match the CPU mapping
    during mmap().  This feature is only supported when
    using per process pagetables with the IOMMU. The
    flags field of KGSL_GPUMEM_ALLOC_ID is copied back
    to userspace and KGSL_MEMFLAGS_USE_CPU_MAP will
    be cleared when this feature is not supported.
    
    The IOMMU virtual address space has been adjusted
    when perprocess pagetables is enabled so that the
    entire userpace address range (0 to TASK_SIZE) can
    have equivalent mappings on the IOMMU. For buffers
    that do not have equivalent mappings, the address
    range from PAGE_OFFSET to KGSL_IOMMU_GLOBAL_MEM_BASE
    is used.
    
    Change-Id: Ib61c03aa7453c3dd901c41e8fd297f66d402ae1a
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 9ee08adbf87acd9ffd879c955a3cb5c67568daee
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Tue May 28 16:54:19 2013 -0600

    msm: kgsl: Support user-specified caching hints
    
    The user allocating memory can provide hints for the caching
    settings when said buffer is mapped on the user side. Try
    to obey the cache settings when applicable.  The user will
    be responsible for handling cache management both to and
    from the GPU so add a new ioctl IOCTL_KGSL_GPUMEM_CACHE_SYNC to
    support both directions (the old IOCTL_KGSL_SHAREDMEM_FLUSH_CACHE)
    only handled flushing, not invalidating. The legacy ioctl still
    works it just does what it says it does - a cache flush.
    
    Change-Id: Ic0dedbad55ce82f2b01ebc56de30d4649e2e7311
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>

commit 1a585bc835f81050265c58b0af99020d9ab6956b
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 16:54:09 2013 -0600

    msm: kgsl: add IOCTL_KGSL_GPUMEM_ALLOC_ID
    
    Previously, the gpu address has been used to uniquely
    identify each memory allocation. Upcoming patches will
    introduce cases where an allocation does not always
    have a gpu address, so an additional id is needed.
    
    IOCTL_KGSL_GPUMEM_ALLOC_ID allocates pages and returns
    an id.
    
    IOCTL_KGSL_GPUMEM_FREE_ID frees an id. KGSL_SHAREDMEM_FREE
    can still be used to free by GPU address, if it exists.
    
    The id can also be passed to mmap(), shifted left by
    PAGE_SIZE to get a CPU mapping for the buffer.
    
    IOCTL_KGSL_GPUMEM_ALLOC_GET_INFO can be called to retrieve
    the id and other information about the buffer.
    
    Change-Id: I4b45f0660cb9d4a5fb1323ccc6c4aa360791c1ec
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 68730e9bce7544aeaa44ffbc687756a31cd6a82b
Author: Steve Kondik <shade@chemlab.org>
Date:   Fri Jul 26 15:01:48 2013 -0700

    Revert "msm: kgsl: Protect the mem_entry list during deletion"
    
    This reverts commit dacf91dff781e9d416a0e812d80c62681ea8894a.

commit 72cbcc32b6d6eb0b4eb5616fb8a4993c5fad7962
Author: Steve Kondik <shade@chemlab.org>
Date:   Fri Jul 26 14:57:53 2013 -0700

    Revert "msm: kgsl: Don't hold process list global mutex in process private create"
    
    This reverts commit 44486c64a4b60d214a43610fe341fc60d5049bc6.
    
    Change-Id: Ic78f42c48a3259b295038e1bca05274dcd7337be

commit 9d7f71af4f58b3585619d1207a2e891b8e090a5d
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 16:54:00 2013 -0600

    msm: kgsl: disable use of iommu TTBR1
    
    The v1 iommu only supports splitting between TTBR0 and TTBR1
    on a power of two boundary. Cutting off the userspace address
    at 2G (0x80000000) is inconvienient, as the GPU userspace
    address space should align with the CPU address space.
    
    This requires changing how global allocations are managed,
    since there is no longer a separate pagetable for TTBR1.
    The default pagetable is still the master of these allocations
    and maintains the gen_pool for allocating global addresses.
    But now, these regions are mapped into each process pagetable
    by calling kgsl_setup_pt(). This requires kgsl_mmu_map
    and kgsl_mmu_unmap to be able to handle mapping without
    virtual address allocation.
    
    Change-Id: I94e2d63dc7e6a7ef576f993770725b6b7ba14228
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit c9c0778cf6e61bed2aa6d0267cec4c25e3c2cf6f
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 16:53:53 2013 -0600

    msm: kgsl: prevent multiple or partial mmaps of a buffer
    
    These cases have never been a normal operation pattern of the
    userspace driver. Sharing buffers through multiple mappings
    is better left to dma-buf or ion.
    
    Change-Id: I7e7658137937c96b9505d0f912dcb262d652e0c3
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 5729e1d8c7b74a442099ceed8d3c3c0ee3e6bf81
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 16:53:45 2013 -0600

    msm: kgsl: clean up use of memdesc->hostptr
    
    Some external memory types were using hostptr to store
    a userspace virtual address, but other code assumes it is a
    kernel virtual address. Make memdesc->hostpr always
    be the kernel virtual address and add a useraddr
    field for the userspace virtual address.
    
    Change-Id: Id4580a2ff34aeb15f2c1b26a7134f0fd4ec52a6e
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 156269bebc265b89bfa4531cc0d4fe3093b4ed60
Author: Jeremy Gebben <jgebben@codeaurora.org>
Date:   Tue May 28 16:53:38 2013 -0600

    msm: kgsl: use readonly mappings on the iommu
    
    Userspace passes a flag KGSL_MEMFLAGS_GPUREADONLY when
    the gpu should not write to a memory region. Use this
    flag to control IOMMU_WRITE permissions on the buffer.
    
    Change-Id: I5d3fc615dc36687252e2242f63fe74d6ce1c4fbc
    Signed-off-by: Jeremy Gebben <jgebben@codeaurora.org>

commit 7e0a634b13185aeaefb0a3bcf8f3578057c4ddf5
Author: Shubhraprakash Das <sadas@codeaurora.org>
Date:   Tue May 28 16:53:24 2013 -0600

    msm: kgsl: Remove extra interrupts when setting MMU state
    
    The interrupts added to the ringbuffer on PTFLUSH and TLBUPDATE
    were causing a major increase in the number of interrupts from the GPU.
    This was leading to increase in power and loss of performance. Add a check
    to turn off IOMMU clocks when going to SLEEP.
    
    Change-Id: I41617dd3b7b3f7d9622523f2a1407b912dbd989e
    Signed-off-by: Shubhraprakash Das <sadas@codeaurora.org>

commit dbdce0f2a6306029d9909d8ac0f0231c9d6153bb
Author: Carter Cooper <ccooper@codeaurora.org>
Date:   Tue May 28 16:53:17 2013 -0600

    msm: kgsl: Remove ts_notifier_list
    
    Remove ts_notifier_list from the driver since it is not being
    used and is causing extra work to be done in the interrupt
    handlers for A2XX, A3XX and Z180.
    
    Change-Id: I5512e36f1e807f3a3e62aeac54cfd3075d4cf7a4
    Signed-off-by: Carter Cooper <ccooper@codeaurora.org>

commit f56a0262dff7cdea6d184ff52cac95afca46cdd2
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Mon Jun 10 18:15:20 2013 +0530

    msm: vidc: add support for the bitstream_restrict flag
    
    Adds support for the MFC core to add the bitstream_restrict flag in the
    sps/pps for H264 codecs.  This allows for the decoder to determine how
    many frames to buffer when decoding.
    
    Change-Id: Ic0de960d92a771d74c303dac7100734d6411fc45
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 1287846db600a54306262eea6fc2c36e7f240562
Author: Steve Kondik <shade@chemlab.org>
Date:   Fri Jul 26 01:10:39 2013 -0700

    Fix uninitialized variable warnings with gcc-4.7
    
    Change-Id: Ia29ef35dc710721ec15d5e2cf849e027ae0e7724

commit 13a08c437182fa3e4775562ceac4b62082c2bc29
Author: Steve Kondik <shade@chemlab.org>
Date:   Thu Jul 25 23:53:50 2013 -0700

    video: msm: Fix a gcc-4.7 issue
    
    Change-Id: I524ebb010e9af71d93dcf5a4076b90c0047dce6c

commit fbaa5c70a5852f76994d474ed1145175b654f7a6
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Thu Jun 27 14:41:52 2013 +0530

    msm_fb: Make sure MDP clock is ON during register access
    
    Change to make sure that MDP clock is ON while accessing
    MDP fixed arbitration read and write selection
    registers. This avoids target freeze due to unclocked
    register access.
    
    CRs-fixed: 505154
    Change-Id: I1d2e654c7b97ca51e42611a5bd3aefd65421bcf4
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit 44486c64a4b60d214a43610fe341fc60d5049bc6
Author: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
Date:   Wed Apr 3 17:06:48 2013 -0600

    msm: kgsl: Don't hold process list global mutex in process private create
    
    Don't hold process list global mutex for long. Instead make
    use of process specific spin_lock() to serialize access
    to process private structure while creating it. Holding
    process list global mutex could lead to deadlocks as other
    functions depend on it.
    
    Change-Id: I4f60737393efa2001597f0222aa6fe19fb6a07c5
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
    Signed-off-by: Ananta Kishore K <akollipa@codeaurora.org>

commit f221a6660215db142ad1694d7eb76660a524d13b
Author: Stephen Smalley <sds@tycho.nsa.gov>
Date:   Fri May 10 10:16:19 2013 -0400

    Enable setting security contexts on rootfs inodes.
    
    rootfs (ramfs) can support setting of security contexts
    by userspace due to the vfs fallback behavior of calling
    the security module to set the in-core inode state
    for security.* attributes when the filesystem does not
    provide an xattr handler.  No xattr handler required
    as the inodes are pinned in memory and have no backing
    store.
    
    This is useful in allowing early userspace to label individual
    files within a rootfs while still providing a policy-defined
    default via genfs.
    
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Change-Id: I3436cf9ae27ade445e37376d7b9125746b1e506f

commit 564a67bb683f18551a037c33e26c1f6da30f2b45
Merge: bc0acbc d560229
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Tue Jul 16 18:10:48 2013 -0700

    Merge "usb: gadget: mass_storage: added sysfs entry for cdrom to LUNs" into cm-10.1

commit d560229c92089c0687749eac0ac81c866a46d736
Author: FrozenCow <frozencow@gmail.com>
Date:   Tue Jul 16 20:08:28 2013 -0500

    usb: gadget: mass_storage: added sysfs entry for cdrom to LUNs
    
    This patch adds a "cdrom" sysfs entry for each mass_storage LUN, just
    like "ro" sysfs entry. This allows switching between USB and CD-ROM
    emulation without reinserting the module or recompiling the kernel.
    
    Change-Id: I115e3c8b5cf6e473cb081ef84f7be451ecdc1f0f

commit bc0acbc092648ed848b30478878df1e74d5f3a2f
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Tue Jul 2 14:43:13 2013 +0300

    block: row: change hrtimer_cancel to hrtimer_try_to_cancel
    
    Calling hrtimer_cancel with interrupts disabled can result in a livelock.
    When flushing plug list in the block layer interrupts are disabled and an
    hrtimer is used when adding requests from that plug list to the scheduler.
    In this code flow, if the hrtimer (which is used for idling) is set, it's
    being canceled by calling hrtimer_cancel. hrtimer_cancel will perform
    the following in an endless loop:
    1. try cancel the timer
    2. if fails - rest_cpu
    the cancellation can fail if the timer function already started. Since
    interrupts are disabled it can never complete.
    This patch reduced the number of times the hrtimer lock is taken while
    interrupts are disabled by calling hrtimer_try_co_cancel. the later will
    try to cancel the timer just once and return with an error code if fails.
    
    CRs-fixed: 499887
    Change-Id: I25f79c357426d72ad67c261ce7cb503ae97dc7b9
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit effc9d8dd58fc4ed159827f8c8382bc29c4ad963
Author: Lianwei Wang <a22439@motorola.com>
Date:   Fri Apr 26 13:30:51 2013 +0800

    cpufreq: interactive: resched timer if max freq raised
    
    When the policy max freq is raised, and before the timer is
    rescheduled in idle callback, the cpu freq may stuck at a
    lower freq.
    
    The target_freq shall be updated too, else on a high load
    situation, the new_freq is always equal to target_freq and
    which will cause freq stuck at a lower freq too.
    
    Reschedule the timer on gov limits callback.
    
    Change-Id: I6c187001ab43e859731429b64f75a74eebc37a24
    Signed-off-by: Lianwei Wang <a22439@motorola.com>

commit 45336f5f41763dc1ac5a68a7a797255f3437170a
Author: Lianwei Wang <a22439@motorola.com>
Date:   Thu May 16 12:07:23 2013 +0800

    cpufreq: interactive: fix race on cpufreq TRANSITION notifier
    
    The cpufreq TRANSTION notifier callback does not check the
    governor_enabled state on affected CPUS, which will case
    kernel panic in update_load because the policy object maybe
    NULL or invalid when governor_enabled is false.
    
    Change-Id: Ie0f1718124f61e2f9b5da57abc6981ada5b83908
    Signed-off-by: Lianwei Wang <a22439@motorola.com>

commit 1c99080b7d160fcfd1db65f813fd2b5decfd3df8
Author: Minsung Kim <ms925.kim@samsung.com>
Date:   Tue Apr 23 22:32:01 2013 +0900

    cpufreq: interactive: avoid underflow on active time calculation
    
    Check for idle time delta less than elapsed time delta, avoid
    underflow computing active time.
    
    Change-Id: I3e4c6ef1ad794eec49ed379c0c50fa727fd6ad28
    Signed-off-by: Minsung Kim <ms925.kim@samsung.com>

commit 0bce8f17b016a4af534a9a3bebba04c0a1cb6890
Author: Todd Poynor <toddpoynor@google.com>
Date:   Fri Apr 5 13:25:21 2013 -0700

    cpufreq: interactive: reduce chance of zero time delta on load eval
    
    Reschedule load sampling timer after timestamp of sample start taken,
    hold spinlock across entire sequence to avoid preemption.  Avoid the
    WARN for zero time delta in the load sampling timer function.
    
    Change-Id: Idc10a756f09141decb6df92669521a1ebf0dbc10
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit 30d15a18789482e6842a10149f76aecc396b53de
Author: Todd Poynor <toddpoynor@google.com>
Date:   Mon Apr 22 16:44:58 2013 -0700

    cpufreq: interactive: handle errors from cpufreq_frequency_table_target
    
    Add checks for error return from cpufreq_frequency_table_target, and be
    less noisy on the existing call with an error check.  CPU hotplug and
    system shutdown may cause this call to return -EINVAL.
    
    Bug: 8613560
    Change-Id: Id78d8829920462c0db1c7e14e717d91740d6cb44
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit 7ab61ad66b7a2f8ecad5cbd941278b76c2733e57
Author: Todd Poynor <toddpoynor@google.com>
Date:   Tue Jun 4 17:29:38 2013 -0700

    ashmem: avoid deadlock between read and mmap calls
    
    Avoid holding ashmem_mutex across code that can page fault.  Page faults
    grab the mmap_sem for the process, which are also held by mmap calls
    prior to calling ashmem_mmap, which locks ashmem_mutex.  The reversed
    order of locking between the two can deadlock.
    
    The calls that can page fault are read() and the ASHMEM_SET_NAME and
    ASHMEM_GET_NAME ioctls.  Move the code that accesses userspace pages
    outside the ashmem_mutex.
    
    Bug: 9261835
    Change-Id: If1322e981d29c889a56cdc9dfcbc6df2729a45e9
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit fc4d0f3876068825864f3720519ebd8f79432e28
Author: JP Abgrall <jpa@google.com>
Date:   Thu May 30 15:31:17 2013 -0700

    misc: uidstat: avoid create_stat() race and blockage.
    
    * create_stat() race would lead to:
      [   58.132324] proc_dir_entry 'uid_stat/10061' already registered
    
    * blocking kmalloc reported by sbranden
     tcp_read_sock()
      uid_stat_tcp_rcv()
        create_stat()
          kmalloc(GFP_KERNEL)
    
    Signed-off-by: JP Abgrall <jpa@google.com>

commit 5c009cf43d173d1f5848900dd596ef51ac16e22b
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Mon Jul 8 18:00:41 2013 -0500

    MFA Merge:
    
    Change-Id: I9d7521d6af2df93ac48e1b035b4c1eb316cfe240

commit 22f2b44d6357b44cc1a429aea77eb18d9a91e067
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Tue Mar 12 17:03:47 2013 +0530

    mmc: msm_sdcc: check for pending_resume in SDCC system suspend
    
    If system comes out of suspend, msmsdcc_pm_resume() sets the
    host->pending_resume flag if the SDCC wasn't runtime suspended.
    Now if the system again goes to suspend without any SDCC activity
    then host->pending_resume flag will remain set which may cause the SDCC
    resume to happen first and then suspend.
    
    To avoid this unnecessary resume/suspend, make sure that pending_resume
    flag is cleared before calling the msmsdcc_runtime_suspend().
    
    CRs-Fixed: 438081
    Change-Id: I2b667bec704f938a532d4780dd9b7937e2eb12c7
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>

commit 4ca22bab90c3c96a94917b0314bed06227761f62
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Tue Oct 9 20:01:56 2012 +0530

    mmc: msm_sdcc: set DDR timing mode before setting the clock rate
    
    Setting DDR timing mode in controller before setting the clock
    rate will make sure that card don't see the double clock rate
    even for very small amount duration. Some eMMC cards seems to lock
    up if they see clock frequency > 52MHz.
    (cherry picked from commit 2877d919135791d5223a9ba94b2cfc9ba50bc3df)
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>
    
    Change-Id: I7a4ace461e2def6d53863db4b768ec7e497b3095
    Signed-off-by: Neha Pandey <nehap@codeaurora.org>

commit 8007c1bc17853e650a7444aee7573b219cd1fedc
Author: Pratibhasagar V <pratibha@codeaurora.org>
Date:   Thu Sep 20 19:46:11 2012 +0530

    mmc: msm_sdcc: Reset interrupt MASK register (MCI_MASK0) on powering off
    
    In one corner case, the SD card is stuck in a bad state with its DAT0
    line pulled low, and SDCC is waiting on the interrupt when the line
    goes back high. But due its bad state eventually the SD card is removed
    from the system.
    
    Later during re-scanning of the devices, the SD card is power-cycled and
    added to the system. But now the pending interrupts for SD card is
    received as the interrupt MASK register (MCI_MASK0) was not cleared.
    
    To prevent such cases reset the interrupt MASK register (MCI_MASK0) while
    powering off to prevent any pending interrupts after power-cycle.
    (cherry picked from commit a3f8f793b21782c79a9fcc5f7aa1cc27fcbf246e)
    CRs-Fixed: 396706
    Signed-off-by: Pratibhasagar V <pratibha@codeaurora.org>
    
    Change-Id: Idfae18895abf47769328b0a768f83eba2ef573f7
    Signed-off-by: Neha Pandey <nehap@codeaurora.org>

commit 5091b08261c091741a2b2fb63c805fb3d1692800
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Wed Dec 19 19:06:38 2012 +0530

    mmc: core: interrupt Background Operations if it takes too long
    
    Currently we have 4 mins timeout for the blocking BKOPs to complete
    but we have seen instances where card is surprisingly taking even
    longer time to complete background operations.
    
    If card doesn't complete the BKOPs within specified timeout, we
    will send the HPI command to interrupt the ongoing BKOPs.
    
    Change-Id: I5df81bdfd9b19bee30a394ee0ff4390b292691d0
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>
    (cherry picked from commit 1b22222ec61924de9b8ac51f08de113548946eb5)

commit 089e4019df6c3451b9731619a8a237f09e152a0a
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Sat Dec 15 22:05:54 2012 +0530

    mmc: msm_sdcc: don't print the timeout errors for BKOPs
    
    We have seen instances where card is surprisingly taking lot of time
    to complete their bkops operations. And if we let the card take long
    time to complete the BKOPs then it would affect the user experience
    if user tries to launch some application (or any other user operation
    resulting in read from the card) while card is busy doing BKOPs.
    
    User experience is most important aspect so we are reducing the maximum
    BKOPs timeout to max 30 secs and if card doesn't complete the BKOPs in
    the timeout specified above, we will send the HPI command to interrupt
    the on going BKOPs.
    
    If reduce the timeout to 30 secs then we may see the request timeout
    errors during BKOPs so let's silence them out.
    
    Change-Id: I31b11f60d1fae098770aa5797742eaf623b09ac9
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>
    (cherry picked from commit 926a9cb47ebfa40428b49f110135fde9e82f9cbe)

commit 8c58c96bd347c8d881021941e07aeb2a42195437
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Wed Jan 30 16:47:07 2013 +0530

    mmc: msm_sdcc: fix data timeout calculation for DDR bus speed mode
    
    When card operates in DDR (Double Data Rate) bus speed mode,
    most of the HW logic of SDCC controller needs to have clock
    frequency that is twice the clock frequency going out to card.
    For example, if a card operates at 50MHz clock in DDR mode then
    internally SDCC controller logic will operate at a clock of 100MHz.
    
    But SDCC controller's data timeout logic works on the clock rate
    that actually goes to card. So this change calculates the data
    timeout value based on the clock frequency that goes to card rather
    than SDCC controller's clock frequency.
    
    CRs-Fixed: 450675
    Change-Id: Icd8b648eae0ad4efad2598749bed7b0bd65c65c9
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>
    Signed-off-by: Krishna Konda <kkonda@codeaurora.org>

commit 8999c33893b0ff656dab834862a696f8d0abea24
Author: Maya Erez <merez@codeaurora.org>
Date:   Tue Feb 5 13:19:52 2013 +0200

    mmc: msm_sdcc: Reset SDCC controller if check for Tx/Rx active fails
    
    Reset SDCC controller if the check for TX/RX ACTIVE bit fails while
    carrying out DPSM reset after the end of current transaction
    instead of taking the entire system down by doing BUG().
    
    CRs-Fixed: 423356
    Change-Id: I9e2e945eb9a31d12e0a435df29a8215ae634724b
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>
    (cherry picked from commit 0a29588be6d3d6bd9122191e7e850c687bb1b166)
    
    Signed-off-by: Maya Erez <merez@codeaurora.org>

commit 21a2de485acbe01d684f674479218b85dbbae4f3
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Wed Oct 31 22:45:45 2012 +0530

    mmc: msm_sdcc: Enable clock scaling capability
    
    Enable capability, MMC_CAP2_CLK_SCALE to allow mmc core
    to scale the clocks at runtime based on load on card.
    
    Clock scaling in MSM platforms also allow voltage scaling between
    NOMINAL and SVS levels, thus allowing additional power savings.
    
    Change-Id: Id1995ffea7a6c06a2e8b29b43da70894f35f6c0b
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 941d6d5a2813e08cc410d7ae11c1bc53bc12bb54
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Thu May 2 15:36:48 2013 +0530

    mmc: msm_sdcc: fix a type mismatch in msmsdccc_config_dma()
    
    msmsdcc_prep_xfer() function returns a signed integer value but
    caller function msmsdcc_config_dma() is storing this value in an unsigned
    integer variable which is obviously not the correct thing to do.
    
    CRs-Fixed: 476578
    Change-Id: I39159e2f4f73c7f54de359cbe4faba5e902915ff
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>

commit 998676414c63f1e6cb1584df9ca61d1984c60612
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Thu Apr 25 14:50:32 2013 +0530

    mmc: msm_sdcc: Add support for dynamic bus clock vote
    
    SDCC controller on some targets like 8660, 8960, 8064 and their
    variants connects with Daytona Fabric (DFAB). To achieve max.
    possible performance the DFAB clock is voted to 64MHz at the
    cost of some power consumption. Since we now have dynamic clock
    scaling for SD/eMMC clock we can also scale the DFAB clock
    accordingly without impact on performance but have some power
    savings in low throughput cases.
    
    Change-Id: Ic26541d6845b65803aa813d672ccabdfc4658d9c
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 1f622b92c7e751c83e8de035aa6d02a97170742b
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Thu Apr 25 12:39:40 2013 +0530

    mmc: core: Add support for notifying host driver while scaling clocks
    
    Host drivers can participate in clock scaling by registering
    ->notify_load host operation, which allows host driver to carry out
    platform specific operations for further power savings or increasing
    throughput based on whether load is LOW or HIGH respectively. This
    can be applicable to non-ultra high speed cards as well so remove
    the check for ultra high speed cards to initialize clock scaling.
    
    Change-Id: Icaab9520135e384f5470db68b2f25c5cdce5663a
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit dce05698a57622a53faaa8d5331b925d780d9624
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Thu Feb 14 08:13:52 2013 +0530

    mmc: core: Log MMC clock frequency transitions
    
    Use kernel's ftrace support to capture MMC clock frequency
    transitions which can be useful for debugging issues related
    to power consumption.
    
    Usage:
    mount -t debugfs none /sys/kernel/debug
    echo 1 > /sys/kernel/debug/tracing/events/mmc/mmc_clk/enable
    cat /sys/kernel/debug/tracing/trace_pipe
    
    Change-Id: I25c4ee39dcbe30e7665902a9f723a5a421b55ca3
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 736ec768c703ba16f10a095add0af1a4c5e90175
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Fri Feb 1 17:14:33 2013 +0530

    mmc: core: run clock scaling only in valid card state
    
    Clock scaling requires the tuning and if the tuning command
    is sent to the card in invalid state, tuning procedure will
    fail.
    
    With urgent request mechanism implemented, there is a chance
    that clock scaling invoked when card is not in a proper state
    to receive the tuning command.
    
    This change checks the card state (by sending the status command)
    before invoking the clock scaling request.
    
    Change-Id: Icb71a8e74a9afdc70380a5901cd3d28931959e9c
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>

commit 13949e49475bf62cf1af67c358ea6584b81326dc
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Wed Dec 12 09:16:58 2012 +0530

    mmc: core: claim mmc host while enabling clock scaling from userspace
    
    A race condition can occur while enabling clock scaling and removal
    of the card. If the card is removed just after the host->card check
    is successful then there could be NULL pointer dereference later.
    Claim mmc host before card availability check to avoid this race
    condition.
    
    Change-Id: I6339cad5857732ec6b66280940cd6b8b7ed30614
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 6c67272aec2535d075011a91522617135746cccb
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Wed Nov 7 21:23:14 2012 +0530

    mmc: core: Add sysfs entries for dynamic control of clock scaling
    
    Add sysfs attributes to allow dynamic control of clock scaling
    parameters. These attributes are used to enable/disable clock
    scaling at runtime and change the up_threshold, down_threshold,
    and polling_interval values. Complete documentation for these
    sysfs entries are provided at "Documentation/mmc/mmc-dev-attrs.txt".
    
    Change-Id: I4753c75c3b3eeea91e93bceba7af2535b793612e
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 648423cb9ae13c72c1d2ba5b275f42be1cf4d542
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Thu Oct 11 17:17:03 2012 +0530

    mmc: core: Add load based clock scaling support
    
    The SD3.0/eMMC4.5/SDIO3.0 cards can support clock rates upto
    200MHz (SDR104 or HS200 bus speed modes). For some workloads
    like video playback it isn't necessary for these cards to run
    at such high speed. Running at lower frequency, say 50MHz, in
    such cases can still meet the deadlines for data transfers.
    Scaling down the clock frequency dynamically has huge power
    savings not only because the bus is running at lower frequency
    but also has an advantage of scaling down the system core
    voltage, if supported.
    
    Provide an ondemand clock scaling support similar to cpufreq
    ondemand governor having two thresholds, up_threshold and
    down_threshold to decide whether to increase the frequency or
    scale it down respectively. The sampling interval is in the
    order of milliseconds and should be chosen by host drivers that
    enable MMC_CAP2_CLK_SCALE capability to take advantage of clock
    scaling. The sampling interval mainly depends on the the clock
    switching delays and hence a host driver decision. If sampling
    interval is too low frequent switching of frequencies can lead
    to high power consumption and if sampling interval is too high,
    the clock scaling logic would take long time to realize that the
    underlying hardware (controller and card) is busy and scale up
    the clocks.
    
    Change-Id: I22a5054beec41b0b66b3bf030ddfcf284de448b3
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit 0f3fd1199c287d2dbed84070afaa6bd997467a0f
Author: Pratibhasagar V <pratibha@codeaurora.org>
Date:   Fri Apr 26 18:59:55 2013 +0530

    mmc: block: Update error handling if block data is not available
    
    While handling IOCTL command, if the block is not available then in the
    bail-out path it is trying to free the block data which will be executed
    against NULL pointer.
    
    So, don't attempt to free the block data and only release the IOCTL
    data.
    
    CRs-Fixed: 476581
    Change-Id: Icba46c861258d2d84b1fae5a2f5fca52ffe54255
    Signed-off-by: Pratibhasagar V <pratibha@codeaurora.org>

commit 451c7a5f2f2a029875c5eb805ffdf600f671177a
Author: Subhash Jadavani <subhashj@codeaurora.org>
Date:   Mon Jan 7 17:31:43 2013 +0530

    mmc: block: reduce the block timeout to 30 secs
    
    After completion of block write request, MMC block driver waits for the
    card to come out of the programming state. If card doesn't come out of
    the programming state in 10 mins, it would be considered as error
    condition and card would reinitialized. But this 10 mins is just too huge
    and if card is continuously stuck in programming state for 10 mins,
    mmcqd thread would take increase the CPU load significantly as we are
    continuously polling for the card status (by sending status commands).
    
    This patch reduces this timeout from 10 mins to 30 secs which is quite
    reasonable for all well-behaved cards.
    
    Change-Id: I4e8eaf29c836a81419220f312ee867b0dd5cccc7
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>

commit 3ea11f23dd7adba75573c48ff76f6127b6db1ab6
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Wed Jan 23 10:31:38 2013 +0530

    mmc: msm_sdcc: Fix SPS-BAM flags while in producer mode
    
    For read operation (writing data into system memory), the
    SDCC-BAM producer will update the EOT flags while writing
    data done descriptors into pipe memory. It is not necessary
    that the s/w should set EOT flag in system consumer mode as
    that may confuse hardware.
    
    CRs-Fixed: 468046
    Change-Id: I31988fdcfaf6c9fb9c9bfc540837513ea40e125c
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit c4deb5cabea197ff10603ba3ba0d36dbccfc74ea
Author: Trey Ramsay <tramsay@linux.vnet.ibm.com>
Date:   Mon Jan 7 17:26:37 2013 +0530

    mmc: core: Fix some driver hangs when dealing with broken devices
    
    There are infinite loops in the mmc code that can be caused by bad
    hardware.  The code will loop forever if the device never comes back
    from program mode, R1_STATE_PRG, and it is not ready for data,
    R1_READY_FOR_DATA.
    
    A long timeout is added to prevent the code from looping forever.
    The timeout will occur if the device never comes back from program
    state or the device never becomes ready for data.
    
    It's not clear whether the timeout will do more than log a pr_err()
    and then start a fresh hang all over again.  We may need to extend
    this patch later to perform some kind of reset of the device (is
    that possible?) or rejection of new I/O to the device.
    
    Change-Id: I1f3e5fddffb5134743fbfbcdef5e97803fd25fc6
    Signed-off-by: Trey Ramsay <tramsay@linux.vnet.ibm.com>
    Signed-off-by: Chris Ball <cjb@laptop.org>
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    Signed-off-by: Subhash Jadavani <subhashj@codeaurora.org>
    
    Conflicts:
    	drivers/mmc/core/core.c

commit 47d4ad9c8f16685a417922cfc35e2b8c4dcf35dd
Author: Sujit Reddy Thumma <sthumma@codeaurora.org>
Date:   Mon Aug 6 11:23:21 2012 +0530

    mmc: core: Allow changing bus frequency for SD/eMMC cards in runtime
    
    Currently, bus frequency is set during the card initialization and
    never changed until a new card is inserted. In some low power use
    cases, scaling the clock frequencies while the card is in transfer
    state would allow power savings. This change allows bus frequency
    to be changed after the card initialization.
    
    Change-Id: Iac064221199f69d162d91f5311becd735c15700a
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>

commit c609a6ecfe786fdf8007e76fee16b5c2cc1c06ae
Author: Mayank Rana <mrana@codeaurora.org>
Date:   Fri Mar 8 14:09:51 2013 +0530

    msm_serial_hs: Cleanup code to configure/unconfigure UART GPIOs
    
    With current approach UART GPIOs are configure/unconfigure from
    board files by using GPIOLib APIs. UART driver uses gpio_config
    pdata based function pointer to call per uart device related API
    from board file for the same. Hence correct this approach and
    pass only UART GPIOs numbers to UART driver using pdata and
    configure/unconfigure those GPIOs using GPIOLib APIs once UART
    port is being opened and closed respectively.
    
    CRs-Fixed: 460961
    Change-Id: I4bb007b2b476525cb50dd2eb5788c735aae0e32d
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>

commit cf8e676a937d158dcf8034170bf6d28f37e566c1
Author: Mayank Rana <mrana@codeaurora.org>
Date:   Tue Jun 4 14:13:02 2013 +0530

    msm_serial_hs: Use RFR GPIO to communicate remote uart
    
    CTS and RFR GPIOs are used for hardware flow control between
    two uarts. RFR GPIO provides current status of uart receiver
    to remote uart and based on this remote uart sends data.
    Hence use this functionality to prevent remote uart to send
    data when port is being opened, baud rate change operation
    is in progress and port is being closed.
    
    This change also set UART_DM_MR2 (Bit: 8 and 9) before
    configuring connected UART GPIOs to prevent RX_BREAK character
    and any character having PARTIY or FRAMING error which can be
    generated due to configuration of those GPIOs. Allows those
    character once port is opened successfuly.
    
    This change also zeroed uart rx software buffer which would
    help to see how many bytes copied by ADM from received data
    with UART RX FIFO to this memory. This would be useful if
    there is any Rx Stall issue is seen.
    
    CRs-Fixed: 495564
    Change-Id: I69382994356073c4c86f205015503c935ad09e8d
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>

commit eb2782e666698a7b081dca50941b1be8d34ffc86
Author: Saket Saurabh <ssaurabh@codeaurora.org>
Date:   Tue May 14 15:04:59 2013 +0530

    msm_serial_hs: Fixing issue during UART shutdown
    
    During UART port close in msm_hs_shutdown(), flag is_shutdown is
    set as true and then UART interrupt is disabled and UART irq line
    is freed. This is causing corner case in which even if uart shutdown
    is in progress, UART interrupt can come as the UART interrupt is not
    disabled and UART irq line not freed at this point. As flag
    is_shutdown is set as true in msm_hs_shutdown() and then if UART
    interrupt comes, it is hitting BUG_ON in UART Interrupt handler
    msm_hs_isr(). To fix this issue, disable the UART irq and free
    the UART irq line at the beginning of the msm_hs_shutdown() and
    then set the flag is_shutdown.
    
    In msm_hs_shutdown, disabling UART Interrupt and setting flag
    is_shutdown is done with uart spinlock acquired. There is a
    possibility that on other core, msm_hs_isr() might need to wait
    for the same uart spinlock and later when msm_hs_isr() is executed
    BUG_ON is hit. As by this time all UART Interrupt is disabled,
    hence removing the BUG_ON and instead keep prints to notify the
    same.
    
    CRs-Fixed: 487945
    Change-Id: I0a19b8f3098f1c9215935ca7fa9d9e5895c44a8c
    Signed-off-by: Saket Saurabh <ssaurabh@codeaurora.org>

commit db43104496d22b489e515f74fae9a516bfd8ee18
Author: Mayank Rana <mrana@codeaurora.org>
Date:   Fri May 10 19:14:04 2013 +0530

    msm_serial_hs: Improve UART Shutdown path
    
    For uart port close operation, serial core calls msm_hs_stop_rx_locked()
    which sends discard flush request to ADM driver after changing rx.flush
    state machine from FLUSH_NONE to FLUSH_STOP value. Next serial core
    calls msm_hs_shutdown()and it waits for discard flush completion with
    rx.flush state machine to be set as FLUSH_SHUTDOWN to complete uart
    port close operation. With this approach, transfer termination is not
    known by UART Core which may still allow UART core to make CRCI request
    with ADM Core if new data is being received from remote uart. Hence
    invoke Force RxStale from msm_hs_stop_rx_locked() which informs UART
    core to terminate current Rx transfer and do not make any new CRCI
    request with ADM core for any new incoming data. On receiving RxStale
    interrupt, it sends discard flush request to ADM driver to terminate
    transfer with ADM hardware.
    
    This new programming sequence is required to get both UART and ADM
    hardwares in-sync for termination of transfer.
    
    CRs-Fixed: 475230
    Change-Id: I802a456294d4fd2fa51a5f4e918a4f4f22bc5ee3
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>

commit 3905ee43eeb823203f97f2d5a323e5e4fde16053
Author: Caleb Johnson <spidercaleb@gmail.com>
Date:   Sat Jun 29 12:12:31 2013 -0500

    The names to the functions that acquire and release a console lock
    have changed, but kernel/power/consoleearlysuspend.c still uses the old
    names. See http://goo.gl/pxgOK
    
    Change-Id: Ie972e51e1acfcec3a12c8fdc86b9287104346a99

commit 618840248988fda10aa80f0c8922b1b33622dd96
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Wed Jun 26 15:35:06 2013 +0530

    msm: rotator: Extend fast YUV invalid checker for 2-pass
    
    For 2-pass mode, we need to check for the fast YUV constraints
    separately for both the 1st and 2nd pass. Make change to extend
    the fast YUV invalid checker to 2-pass mode.
    
    CRs-fixed: 504007
    Change-Id: I7ef96ff29423fbd5cc454e8fd125b4bb1a9d59e1
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit c4784a3be1c33aee9b5abc0ea857a4105d7ac48f
Author: Dilip Kota <c_dkota@codeaurora.org>
Date:   Wed Jun 26 16:59:17 2013 +0530

    msm_serial_hs: Rx discard flush timeout
    
    While configuring the uart buadrate using set_termios
    function, timeout is occuring for rx discard flush
    operation. This is due to the driver is waiting for
    less time for the DMA call back to occur after programing
    the DMA register with the discard flush command.
    
    To overcome this issue increased the timeout to 300ms.
    
    Change-Id: I60db1f9eea8bb59fab4024e5f5dfcbcdcde253bb
    Signed-off-by: Dilip Kota <c_dkota@codeaurora.org>

commit 4bbfcb9359eaadf9e7e4eab24b20ee2e032799f6
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Wed Jan 30 14:26:42 2013 -0800

    msm_fb: display: add mdp clock control to histogram enable/disable
    
    Add mdp clock control to histogram enable and disable block to
    prevent system from crashing when accessing register with mdp
    clocks off.
    
    Change-Id: Ide808b332b5456f3fc639fcbb0d03d5cad0e387f
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit d2f28a1008fed9e1293d175c6b00dc8b802bbe66
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Wed Jan 23 12:17:15 2013 -0800

    msm_fb: display: get mipi frame rate from panel file
    
    If frame rate value for mipi video panel is coded in panel file it can
    be directly used to avoid calculation error.
    
    Change-Id: I6ad86226c81da4bcf36196385f9ff4bea0bef0ca
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 5c94f185b2861718d1605d0e7121c89767419069
Author: Praveen AC <praveenac@codeaurora.org>
Date:   Wed Mar 6 12:59:09 2013 -0800

    msm: mdp: Revert MDP Port split changes
    
    Revert MDP changes to avoid bus overflow
    seen in camera usecases.
    
    Change-Id: I1c687750bd22c8f64ab28a500562cbfbcb6a6048
    Signed-off-by: Kiran Kumar H N <hurlisal@codeaurora.org>
    Signed-off-by: Praveen Ac <praveenac@codeaurora.org>

commit a26f9269d073a9c1b685396ceb0f0c45e3005826
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Fri Mar 1 16:16:27 2013 -0800

    cpufreq: Retain only online cpus in managed_policy->cpus
    
    cpufreq's handling of cpu online event can incorrectly initialize
    managed_policy->cpus to include offline cpus. This causes cpufreq
    governors to include offline cpus in their idle time computation, which
    causes frequency to shoot up beyond normal levels (on systems where 2 or
    more cpus need to be synchronized wrt frequency change).
    
    This frequency shootup was not seen prior to commit 21111be8 which
    changed idle time accounting for offline cpus. Prior to commit 21111be8,
    idle time statistics for offline cpus would include their offline time
    as well. Post commit 21111be8, offline time is not included in a cpu's
    idle time statistics.
    
    As a result, ondemand (or other cpufreq) governors would see unchanged
    idle time for offline cpus, causing them to interpret those (offline)
    cpus as 100% busy, which is wrong.
    
    This patch fixes the issue by modifying cpufreq's hotplug handling to
    retain only online cpus in managed_policy->cpus field. cpufreq governors
    would then skip such offline cpus from idle time calculations.
    
    Change-Id: Id2eba8c0ec3367609369b2e42a4e1386ff2e7ab5
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 75d8ea706c5176da40ef7b530fa69193c914b2e8
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Sep 11 20:13:10 2012 -0700

    drivers: cpufreq: Send a uevent when governor changes
    
     * Useful so userspace tools can reconfigure.
    
    Change-Id: Ib423910b8b9ac791ebe81a75bf399f58272f64f2

commit 9509f5f6ead3160cce83cbfa72fc03925a380f42
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat Jun 29 22:21:37 2013 -0400

    cpufreq: ondemand: Use the input handler when SEC_DVFS_BOOSTER isn't enabled
    
    Change-Id: I8515fc6f258c6c2601c189291ee75d10fc623db2

commit bb2927a715c86e2d0a77695642c9bea51577adff
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Mon Jul 1 12:30:36 2013 -0500

    fix possible revert going from mfx to google.
    
    Change-Id: Ica27ee99198f5e021e7e27e19387a67b432247e0

commit a662c7f585054be3527ff76466188c48b776bc2a
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Oct 31 17:43:08 2012 -0700

    cpufreq: Avoid using smp_processor_id() in preemptible context
    
    Even though this work item runs on only one cpu at a time (due to
    queue_work_on()) it is possible for the work item to be preempted
    and so use of smp_processor_id() is illegal.
    
    BUG: using smp_processor_id() in preemptible [00000000]
    code: kworker/3:1/4162 caller is dbs_refresh_callback+0xc/0x188
    [<c00151b0>] (unwind_backtrace+0x0/0x120) from [<c0279058>]
    (debug_smp_processor_id+0xbc/0xf0)
    [<c0279058>] (debug_smp_processor_id+0xbc/0xf0) from [<c0454b54>]
    (dbs_refresh_callback+0xc/0x188)
    [<c0454b54>] (dbs_refresh_callback+0xc/0x188) from [<c0087290>]
    (process_one_work+0x354/0x648)
    [<c0087290>] (process_one_work+0x354/0x648) from [<c0089754>]
    (worker_thread+0x1a8/0x2a8)
    [<c0089754>] (worker_thread+0x1a8/0x2a8) from [<c008e480>]
    (kthread+0x90/0xa0)
    [<c008e480>] (kthread+0x90/0xa0) from [<c000f438>]
    (kernel_thread_exit+0x0/0x8)
    
    The intent of the code is to determine which CPU this work item
    is running on, which we can easily do by passing that information
    in a wrapper struct around the work struct. Do this so we avoid
    this problem.
    
    Change-Id: I05ca0ff2b3cbaa239930463ea0760e3e9d75145f
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit 013a8615867137256b3c79048dcf0b60a0d60a5e
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Fri May 31 10:39:31 2013 -0700

    cpufreq: ondemand: change freq sync code to use per-CPU kthreads
    
    The migration notifier may run in a context where it is not safe to
    enqueue a work item. For example:
    
    (__queue_work+0x1f4/0x40c)
    (queue_work_on+0x34/0x44)
    (dbs_migration_notify+0x2c/0x40)
    (notifier_call_chain+0x38/0x68)
    (__atomic_notifier_call_chain+0x34/0x44)
    (atomic_notifier_call_chain+0x14/0x18)
    (try_to_wake_up+0x350/0x36c)
    (autoremove_wake_function+0xc/0x34)
    (__wake_up_common+0x48/0x7c)
    (__wake_up+0x3c/0x50)
    (trustee_thread+0x1d0/0x528)
    (kthread+0x80/0x8c)
    
    The trustee code already takes a lock internal to the workqueue
    implementation and is not expecting a wake up to come around and
    enter the workqueue code again, where the same lock will be needed.
    
    Instead of relying on a workqueue, use a per-CPU kthread to do
    the frequency syncing.
    
    Change-Id: I7555ee40867792fa8ec4ea8f9a6309323775e797
    CRs-Fixed: 501099
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 1b00f62820c9e422fb514935fbcb035b0bd066a4
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Fri May 31 20:02:01 2013 -0700

    cpufreq: ondemand: Fix locking in store_powersave_bias
    
    store_powersave_bias() assumes that dbs_info->cur_policy will
    not change while it is executing. If it does, memory corruption
    can potentially result. Fix this by acquiring the dbs_lock.
    
    Most noticeably, this resolves timer-related list corruptions
    encountered when queue_delayed_work_on() is called from within
    store_powersave_bias() while a CPU is being hotplugged in and
    out.
    
    CRs-Fixed: 494407
    Change-Id: I0fd5cbef6834545274cad9481e574ec50b37b871
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit c6d902735245066e1aa9a28bfab96f53f3a7e13a
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Fri May 31 13:14:44 2013 -0700

    cpufreq: ondemand: Fix store_powersave_bias() race with hotplug
    
    store_powersave_bias() assumes that CPUs will not be taken offline or
    brought online while executing its for_each_online_cpu() loops. Fix
    this race by blocking hotplug while executing in these loop.
    
    CRs-Fixed: 494407
    Change-Id: I0d5520444ec21a8d9b12fb73c06ac8442449fa85
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit e4081b0736ab5a2e4590da7caa339fedcfb969cc
Author: Matt Wagantall <mattw@codeaurora.org>
Date:   Thu May 23 15:52:49 2013 -0700

    cpufreq: Use dedicated high-priority workqueues
    
    In the process of scaling CPU frequencies, cpufreq and the APIS is
    calls may hold resources that will prevent threads critical for
    system stability from running.
    
    Specifically, thermal-mitigation software may need to respond
    quickly to high-temperature conditions and throttle the CPU speed
    in an effort to cool down. If the system is already in the middle
    of a CPU frequency switch being executed on behalf of a cpufreq
    governor, thermal mitigation will be blocked until the scaling
    action has completed. If the system is under heave load of medium
    or high-priority threads, this may take a long while and the system
    may potentially surpass thermal limits in the meantime.
    
    Resolve this by using high priority workqueues for all ondemand
    and conservative governor scaling operations.  This will allow them
    to complete promptly and release their hold on resources necessary
    for maintaining system stability.
    
    Change-Id: I2f56052c131442838036cf4cdd8059f7c09bb805
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit e9dae23edc1005cafac736299152c17496bc2964
Author: Rohit Gupta <rohgup@codeaurora.org>
Date:   Thu Apr 4 15:45:16 2013 -0700

    msm: Synchronize CPU frequency on thread migration
    
    Sometimes when foreground threads, crucial for UI, migrate
    from a core at higher frequency to the one running at a lower
    frequency, it could badly affect performance for cases like
    scrolling.
    This change tries to raise the frequency of the destination core
    to the same value as the source core, if it is not already running
    at the same or higher frequency.  In order for the ondemand
    governor to have a fair idea of load in the next sample, it also
    reschedules the next sample one sampling interval from the
    frequency transition after which the ondemand governor again
    takes over.
    
    Signed-off-by: Rohit Gupta <rohgup@codeaurora.org>
    Change-Id: I9d2fbb166b0cc042d3fd57cea75a5066df4c794b

commit 32497d353302f7bcce59ccd8ab10f42383ccb506
Author: Veena Sambasivan <veenas@codeaurora.org>
Date:   Tue May 14 12:36:48 2013 -0700

    cpufreq: Fix optimal_freq logic on ramp down
    
    When ondemand governor decides to ramp down the frequency, the missing
    check for frequency being lower than optimal_freq was causing frequency
    to be ramped down to optimal_freq even if the frequency decided by ondemand
    governor was higher.
    
    Change-Id: I00f9ae4c61ccda4039f3338dee2c29b8cb790be4
    Signed-off-by: Veena Sambasivan <veenas@codeaurora.org>

commit 02d07ea46beec650afe4e5582bb7b1e6b9ef8542
Author: Lucille Sylvester <lsylvest@codeaurora.org>
Date:   Wed Jan 9 10:41:28 2013 -0700

    msm: kgsl: Lower bounds check the number of power levels
    
    A misconfigured board file could cause an out-of-bounds
    array access through this value.
    
    Change-Id: I64bbad0c096a2efe65376991537d810141370d72
    CRs-Fixed: 439230
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>

commit 9d282d8a3fc1489d215637ad51a9e145f714955c
Author: Tarun Karra <tkarra@codeaurora.org>
Date:   Wed Mar 27 19:37:55 2013 -0700

    msm: kgsl: Allocate space in ringbuffer for EOF commands
    
    When userspace passes end of frame (EOF) flag, ringbuffer is
    inserted with EOF marker commands. Allocate ringbuffer space
    for EOF marker commands.
    
    CRs-Fixed: 469807
    Change-Id: I7de8d1b81d358a8d8753ac84fbdceddad27bb17d
    Signed-off-by: Tarun Karra <tkarra@codeaurora.org>

commit 373476d0e3a89a16edd2aa8204fef2e6860509b0
Author: Lynus Vaz <lvaz@codeaurora.org>
Date:   Mon Mar 4 18:38:33 2013 +0530

    msm: kgsl: Delete the context event while the context is still valid
    
    Make sure that the context is freed only after the event is deleted
    from the list to avoid accessing invalid list pointers. This makes
    sure that the context memory is not accessed after being freed.
    
    Change-Id: I9196219f1472396c0063dbeaa5351f66e6e5648f
    Signed-off-by: Lynus Vaz <lvaz@codeaurora.org>
    Signed-off-by: Uma Maheshwari Bhiram <ubhira@codeaurora.org>

commit b21d390e77e9777b8ed820fab6fcf553c6d70f21
Author: Tarun Karra <tkarra@codeaurora.org>
Date:   Wed Apr 10 10:25:25 2013 -0700

    msm: kgsl: Prevent fault tolerance memory free twice
    
    There is a possibility of fault tolerance data to be freed twice
    if fault tolerance fails. Do not free the data when the data allocation
    fails, take the decision to free the data only at the end of
    fault tolerance.
    
    CRs-Fixed: 469807
    Change-Id: Ifcc9a75e70440be6471b32d8413f4faa01a9ee47
    Signed-off-by: Tarun Karra <tkarra@codeaurora.org>

commit 032ed237e1c9b03d7aca522138e35d7099303cce
Author: Tarun Karra <tkarra@codeaurora.org>
Date:   Wed Mar 27 20:21:02 2013 -0700

    msm: kgsl: Add barriers to GPU fault tolerance memory reads
    
    In GPU fault tolerance When reading from memstore add
    memory barriers. This ensures memstore reads are
    completed and makes GPU fault tolerance code robust.
    
    CRs-Fixed: 469807
    Change-Id: Iefb950ae2647eb8ae16bfb9a8d7ee9885636d920
    Signed-off-by: Tarun Karra <tkarra@codeaurora.org>

commit a3d46dad874243cd7c02a61113c3f81ce5d8b2b1
Author: Tarun Karra <tkarra@codeaurora.org>
Date:   Mon Feb 25 21:58:05 2013 -0800

    msm: kgsl: Fault tolernace for context with pagefault
    
    If pagefault happened in same global timestamp
    as the hang do not attempt replay for fault tolerance.
    This is an improvement from previous policy of not
    attempting fault tolerance for the context
    with pagefault.
    
    CRs-Fixed: 469807
    Change-Id: Idc9512b9fab3c9a2bf0b33a7e06f3070075427ba
    Signed-off-by: Tarun Karra <tkarra@codeaurora.org>

commit 5ef1d8d9bc4e3406a4182a3b820e2c44276b2273
Author: Lucille Sylvester <lsylvest@codeaurora.org>
Date:   Mon Mar 11 15:46:58 2013 -0600

    msm: kgsl: pwrscale functions should be run while GPU is ACTIVE
    
    Base the decision to call power scaling functions on the current
    state, not the requested state.  The GPU may not transition to the
    requested state; call through based on current GPU state instead.
    
    Change-Id: I1524bc92c908dc50a60b5815f2221ae3feef59be
    CRs-Fixed: 441847
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>

commit 5b95fd8030a51ccc69e354ab597ad592a8ccfad6
Author: Lucille Sylvester <lsylvest@codeaurora.org>
Date:   Wed Feb 13 13:32:01 2013 -0700

    msm: kgsl: Bump up the GPU frequency for long batch processing
    
    Compute workloads run without kernel interaction for long periods
    of time.  If one is identified early, increase the frequency to
    finish it as fast as possible, rather than waiting for the standard
    algorithm to do so.
    
    Change-Id: I213ccabfae5e1000cdc34bc1d92bdc3bad86383d
    CRs-Fixed: 441847
    Signed-off-by: Lucille Sylvester <lsylvest@codeaurora.org>
    (cherry picked from commit 14c9cd845d6a93d7ab91fffa90fa316aebb18942)

commit 1d6031373795845bf74c6e4c917f041546e8d716
Author: Jeff Boody <jboody@codeaurora.org>
Date:   Fri Mar 29 13:23:31 2013 -0600

    msm: kgsl: fix paren placement leading to fence timeout
    
    The incorrect paren placement can lead to a fence timeout
    because the last_timestamp is always written. As a result
    the last_timestamp could be corrupted by writing an old
    value leading to a future timestamp_cmp indicating that a
    signaled sync pt was unsignaled.
    
    Change-Id: I4650a467bfdd98a905f86fbec7d833596c67ba19
    Signed-off-by: Jeff Boody <jboody@codeaurora.org>

commit 4ef5811d67edf0d550e186e2a1bc2b3cb0f61f21
Author: Iliyan Malchev <malchev@google.com>
Date:   Fri Dec 7 14:57:47 2012 -0800

    msm: kgsl: initialize kgsl_sync_timeline_ops properly
    
    CRs-fixed: 471346
    Change-Id: Ib8dccb60ead6fcecc0f9beaf271ba28199d404a2
    Signed-off-by: Iliyan Malchev <malchev@google.com>
    Signed-off-by: Jeff Boody <jboody@quicinc.com>

commit 98de233522fa622b96411a5b6544b6500202fd2f
Author: Siddhartha Agrawal <agrawals@codeaurora.org>
Date:   Tue Feb 19 18:49:10 2013 -0800

    msm_fb: display: Configure solidfill color to black
    
    The solidfill color was getting reset to to default state to white.
    Correctly reseting to black by default.
    
    Crs-Fixed: 453600
    Change-Id: I203ce61867d009509bf6acd01169f1fb0b223652
    Signed-off-by: Siddhartha Agrawal <agrawals@codeaurora.org>

commit a20e01c7c8859873f33fcb87d04770cd82dd1dde
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Tue Jan 8 14:28:20 2013 -0500

    msm: display: panel frame rate report
    
    Add metadata ioctl for retrieving panel frame rate.
    Get ready to remove reserved fields usage.
    
    Change-Id: I51a4aed9d85efd5f83a184100b6a3dc682c7a67c
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit 4ec2e73e6f95be06794ab3501864bcb7f421dc79
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Fri Jan 18 10:55:36 2013 -0800

    msm_fb: display: add mdp new update method
    
    Enable new mdp update method which requires flush bit
    to be set before updating mixer cfg register for both
    smart and dump panel
    
    Change-Id: I97b9d221da574a96707748ec83dd1acff6dcb654
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 139985b0bb27052c9d5aeaaf60206c40327f1522
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Fri May 10 15:45:07 2013 +0530

    msm_fb: display: Add minimum src/dst image size validation for MDP
    
    MDP4 has minimum size restriction for both source and destination
    height and width. Add checks to validate source and destination
    layer size.
    
    CRs-fixed: 484728
    Change-Id: I8cb8d4b969cd6ab2d8e99d7ed2ebb1a4adce1ef3
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit c54db1cb976f0707d338140298e335b9b12efe93
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Thu Mar 28 14:02:10 2013 -0700

    msm_fb: display: silent blending error warning message
    
    If there had no any pipe blending stage changes, then do not
    call belnding set up to avoid unnecessary warning messges.
    
    CRs-fixed: 467556
    Change-Id: I07520b3b0917375c41405e736dcb9eb8a3daddc1
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 93a1e20dbdb16178b7aac503034e2b3d00b6813f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 22 14:04:28 2012 +0200

    sched: Make sure to not re-read variables after validation
    
    We could re-read rq->rt_avg after we validated it was smaller than
    total, invalidating the check and resulting in an unintended negative.
    
    Change-Id: I8543974aad539107768e9e513ca3a8c4cb79b2ff
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: David Rientjes <rientjes@google.com>
    Link: http://lkml.kernel.org/r/1337688268.9698.29.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    CRs-Fixed: 497236
    Git-commit: b654f7de41b0e3903ee2b51d3b8db77fe52ce728
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit a32bdac3bfc62c9f13bc85159f79cdaf4f51d0fc
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Tue Jun 11 17:43:09 2013 -0700

    sched: re-calculate a cpu's next_balance point upon sched domain changes
    
    Commit 55ddeb0f (sched: Reset rq->next_interval before going idle) reset
    a cpu's rq->next_balance when pulled_task = 0, which will be true when
    the cpu failed to pull any task, causing it go idle. However that patch
    relied on next_balance being calculated as a result of traversing cpu's
    sched domain hierarchy.
    
    A cpu that is the only online cpu will however not be attached to any
    sched domain hierarchy. When such a cpu calls into idle_balance(), we
    will end up initializing next_balance to be 1sec away! Such a CPU will
    defer load balance check for another 1sec, even though we may bring up
    more cpus in the meantime requiring it to check for load imbalance more
    frequently. This could then lead to increased scheduling latency for
    some tasks.
    
    This patch results in a cpu's next_balance being re-calculated when its
    attaching to a new sched domain hierarchy.  This should let cpus call
    load balance checks at the right time we expect them to!
    
    Change-Id: I855cff8da5ca28d278596c3bb0163b839d4704bc
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 7044a3425557a9e8ab929d84c5602fd5ab134b7b
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Thu May 23 15:24:57 2013 -0700

    sched: remove migration notification from RT class
    
    Commit 88a7e37d265 (sched: provide per cpu-cgroup option to
    notify on migrations) added a notifier call when a task is moved
    to a different CPU. Unfortunately the two call sites in the RT
    sched class where this occurs happens with a runqueue lock held.
    This can result in a deadlock if the notifier call attempts to do
    something like wake up a task.
    
    Fortunately the benefit of 88a7e37d265 comes mainly from notifying
    on migration of non-RT tasks, so we can simply ignore the movements
    of RT tasks.
    
    CRs-Fixed: 491370
    Change-Id: I8849d826bf1eeaf85a6f6ad872acb475247c5926
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit 73bf740e5685f2be0c904f8ae4e0f024ad8ee730
Author: Steve Muckle <smuckle@codeaurora.org>
Date:   Mon Mar 11 16:33:42 2013 -0700

    sched: provide per cpu-cgroup option to notify on migrations
    
    On systems where CPUs may run asynchronously, task migrations
    between CPUs running at grossly different speeds can cause
    problems.
    
    This change provides a mechanism to notify a subsystem
    in the kernel if a task in a particular cgroup migrates to a
    different CPU. Other subsystems (such as cpufreq) may then
    register for this notifier to take appropriate action when
    such a task is migrated.
    
    The cgroup attribute to set for this behavior is
    "notify_on_migrate" .
    
    Change-Id: Ie1868249e53ef901b89c837fdc33b0ad0c0a4590
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>

commit dc638d4cf82a777b7a7b9a4a289c03492dedd59f
Author: Lee Susman <lsusman@codeaurora.org>
Date:   Sun Jun 23 16:27:40 2013 +0300

    block: row-iosched idling triggered by readahead pages
    
    In the current implementation idling is triggered only by request
    insertion frequency. This heuristic is not very accurate and may hit
    random requests that shouldn't trigger idling. This patch uses the
    PG_readahead flag in struct page's flags, which indicates that the page
    is part of a readahead window, to start idling upon dispatch of a request
    associated with a readahead page.
    
    The above readehead flag is used together with the existing
    insertion-frequency trigger. The frequency timer will catch read requests
    which are not part of a readahead window, but are still part of a
    sequential stream (and therefore dispatched in small time intervals).
    
    Change-Id: Icb7145199c007408de3f267645ccb842e051fd00
    Signed-off-by: Lee Susman <lsusman@codeaurora.org>

commit 5f96efb0d471d11b778981621b2706c543bfe8ad
Author: Lee Susman <lsusman@codeaurora.org>
Date:   Sun May 5 17:31:17 2013 +0300

    mm: pass readahead info down to the i/o scheduler
    
    Some i/o schedulers (i.e. row-iosched, cfq-iosched) deploy an idling
    algorithm in order to be better synced with the readahead algorithm.
    Idling is a prediction algorithm for incoming read requests.
    
    In this patch we mark pages which are part of a readahead window, by
    setting a newly introduced flag. With this flag, the i/o scheduler can
    identify a request which is associated with a readahead page. This
    enables the i/o scheduler's idling mechanism to be en-sync with the
    readahead mechanism and, in turn, can increase read throughput.
    
    Change-Id: I0654f23315b6d19d71bcc9cc029c6b281a44b196
    Signed-off-by: Lee Susman <lsusman@codeaurora.org>

commit 07c457cab5bb88dd7112975babdc209ff106b9cf
Author: Lee Susman <lsusman@codeaurora.org>
Date:   Mon Apr 8 13:09:48 2013 +0300

    mm: change initial readahead window size calculation
    
    Change the logic which determines the initial readahead window size
    such that for small requests (one page) the initial window size
    will be x4 the size of the original request, regardless of the
    VM_MAX_READAHEAD value. This prevents a rapid ramp-up
    that could be caused due to increasing VM_MAX_READAHEAD.
    
    Change-Id: I93d59c515d7e6c6d62348790980ff7bd4f434997
    Signed-off-by: Lee Susman <lsusman@codeaurora.org>

commit 7ea685117a83fc10b8c7d6766fcbb9c7bb90425c
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu May 16 14:36:58 2013 +0300

    block: Remove "requeuing urgent req" error messages
    
    It is possible for URGENT request to be requeued/reinserted if it was
    fetched during the creation of a packed list. This end case is rare and is
    not handled at the moment.
    This patch changes the messages notifying of the above to debug level
    (instead of error) in order to clear the dmesg log.
    
    Change-Id: Ie8bc067e61559a6f702077b95c5dbcc426404232
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 39243de2c86ef1d9276a1e16251bd71f62690966
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Wed May 1 14:35:20 2013 +0300

    block: urgent: Fix dispatching of URGENT mechanism
    
    There are cases when blk_peek_request is called not from blk_fetch_request
    thus the URGENT request may be started but the flag q->dispatched_urgent is
    not updated.
    
    Change-Id: I4fb588823f1b2949160cbd3907f4729767932e12
    CRs-fixed: 471736
    CRs-fixed: 473036
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>

commit 2e3ea9d08de1344d6a34a4874f5325929f5a6fc8
Author: Fred Fettinger <fred.fettinger@motorola.com>
Date:   Fri Jan 11 14:20:40 2013 -0600

    base: sync: increase size of sync_timeline name
    
    This makes it possible for drivers to use a longer, more descriptive
    name for a sync_timeline, which improves the readability of the
    sync dump in debugfs.
    
    Change-Id: Ifb83aebf6fd820ebb26aca2ff230ac1116e65ce9
    Git-commit: cb63e61b80ec905f01cbe1c21081aa0d1fef082d
    Git-repo: https://www.codeaurora.org/gitweb/quic/la/?p=kernel/msm.git
    Signed-off-by: Fred Fettinger <fred.fettinger@motorola.com>
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>

commit a57ec469dddf5c5d67f027b247ddf7892fd0a79b
Author: Gagan Mac <gmac@codeaurora.org>
Date:   Thu Oct 4 22:17:35 2012 -0600

    msm: msm_bus: Explicitly clear client requests before removal
    
    Clients can potentially leave non-zero requests before
    unregistering. Such requests persist and cause instability
    and severe power and performance impact.
    
    Protect the system against such clients by explicitly
    clearing the client requests before unregistration
    
    Change-Id: I6a905b097de41af168b5f5048b682ba995b581c2
    CRs-Fixed: 407057
    Signed-off-by: Gagan Mac <gmac@codeaurora.org>

commit 696ad6e01c2442833b3c23aa75b89a130231ca74
Author: Gagan Mac <gmac@codeaurora.org>
Date:   Thu Feb 28 13:25:33 2013 -0700

    msm: msm_bus: Add NULL pointer check
    
    This patch protects the bus driver against clients who try to
    update requests without registering the client
    
    Change-Id: Ie740844218dbabe5fca098798c461590aba42e97
    CRs-Fixed: 459651
    Signed-off-by: Gagan Mac <gmac@codeaurora.org>

commit 7d7f06dcc5c4e70b82c530d3ca1579f5a24ef220
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Thu Jun 20 15:14:57 2013 +0530

    msm: rotator: Pass ION flags correctly for 2-pass buffer allocation
    
    With updated ION interface, ION_SECURE needs to be passed as a
    separate flag. Also, 2-pass buffer for secure session needs to be
    allocated from CP heap instead of non-secure heap.
    
    CRs-fixed: 502038
    Change-Id: I8c5f945dc4d90ce640872e2cd0505ac32e82011d
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit da864b4fa668ad5b4f0e62d6d2dd8a37e59a1500
Author: Mayank Goyal <goyalm@codeaurora.org>
Date:   Wed Nov 7 16:58:09 2012 +0530

    msm: rotator: Add pseudo-planar 422 H1V2 dst format for MDP4
    
    Rotator outputs pseudo-planar 422 H1V2 for interleaved inputs
    on 90 degree rotation.Add changes to support MDP_Y_CRCB_H1V2,
    MDP_Y_CBCR_H1V2 in MDP4.
    
    CRs-Fixed: 371303
    Change-Id: I42ea86992fc77cd335437a47611e9f640f30ed25
    Signed-off-by: Mayank Goyal <goyalm@codeaurora.org>
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit ab5ca26a7bbfb5637b39336936a10c2284159601
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Mon Mar 5 17:00:10 2012 +0530

    msm_fb: display: Fix interleaved 422 input format register settings
    
    Check to update chroma sample information in source format register is
    based on fetch plane value, which does not take into account for
    interleaved 422 input format. Remove this check and set chroma sample
    values for RGB formats as well.
    
    CRs-Fixed: 501035
    Change-Id: If8fd47b6d47b326efd1cb70ce9701ac1d457dcfd
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit 0d54d4d6f0fc0fe2eb99207dd22e8336c649db99
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Sat Mar 3 06:51:20 2012 +0530

    msm_fb: display: Add pseudo-planar 422 H1V2 support in the MDP4
    
    Rotator outputs pseudo-planar 422 H1V2 for interleaved inputs
    on 90 degree rotation. Add changes to support MDP_Y_CRCB_H1V2
    in MDP4.
    
    CRs-Fixed: 371303
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>
    
    Conflicts:
    
    	drivers/video/msm/mdp4_overlay.c
    
    Change-Id: I730a3196df8d0c204e6334c2c8b9199d15cbb16a
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit 4561e939f9d9eae26e5594f265e696736fbd5d63
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Fri Apr 12 17:22:00 2013 +0530

    msm: rotator: Enable support for 2-pass fast YUV mode
    
    Add support for 2-pass rotator operation for downscaling
    and 90 degree rotation when rotator operates in fast YUV
    mode. 2-pass is supported for 420 H2V2 formats.
    
    Change-Id: Ib8ce802a5a8aabe10c88422e3b8d82708c5b05d4
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit eb8847505b07182cc42a56f88c7a28fd750abf1b
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Fri May 31 14:18:24 2013 +0530

    msm: rotator: Add proper checks for enabling Fast YUV
    
    For rotator version 2.0 and above, there is support for fast
    YUV which gives better rotator performance. However, there are
    few constraints related to height under which fast YUV cannot be
    enabled when 90 degree rotation is enabled. Make sure
    to add proper checks before enabling fast YUV.
    
    Change-Id: Iafd204a6e4ef09e1151d01d8fde35359b7c3b7a5
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit 471e84b5e7b9e86e20daf0ee6bb8036e1c22b695
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Fri May 17 18:38:14 2013 -0400

    msm_fb: Increase base fence timeout
    
    The earlier value of 1 second caused unnecessary warnings on
    heavy workloads by the GPU.
    
    Change-Id: Ib4a5b098267251d5c63c4d8d2a2f51b339acc1d2
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>
    Signed-off-by: Lynus Vaz <lvaz@codeaurora.org>

commit 56a52597de5c65becb84bc0e0e04c35424c5ea10
Author: Jeevan Shriram <jshriram@codeaurora.org>
Date:   Sat Jun 1 13:46:23 2013 +0530

    msm_fb: Correct R and G offsets for correct mapping
    
    R and G colors are swapped when ARGC test cases are executed.
    Correct the color mapping to right register offset to fix
    the color swapping issue.
    
    CRs-Fixed: 491215
    
    Change-Id: I4997acce577bdbbdf39e22502a678dbecf0bfc37
    Signed-off-by: Jeevan Shriram <jshriram@codeaurora.org>

commit 971b66454d7352e1efd140481c16378f2e33b114
Author: Alex Wong <waiw@codeaurora.org>
Date:   Mon Apr 29 10:05:18 2013 -0700

    msm: kgsl: Remove extra call to sync_fence_put()
    
    sync_fence_put is called twice and cause NULL pointer dereference.
    
    Change-Id: Ia29e676af28d9040d1ba1ee8669f1306cc8b4035
    Signed-off-by: Alex Wong <waiw@codeaurora.org>

commit f29e29f23db687f5f273b99d1143e8bb075036f2
Author: Fred Fettinger <fred.fettinger@motorola.com>
Date:   Fri Feb 8 16:19:15 2013 -0600

    msm: kgsl: show timestamp in sync dump
    
    Improve the debug information printed for kgsl points and timelines.
    
    kgsl_sync_pt_value_str:
      - show timestamp passed to kgsl_add_fence_event()
    kgsl_sync_timeline_value_str:
      - show most recent timestamp passed to kgsl_sync_timeline_signal()
      - show last retired timestamp according to kgsl_sync_get_timestamp()
    
    Change-Id: I93fe408b06de054b11e05784cd7175ed3fcb76ad
    Git-commit: 595e213985f63d6aa484c966a4f4a710b31ca209
    Git-repo: https://www.codeaurora.org/gitweb/quic/la/?p=kernel/msm.git
    Signed-off-by: Fred Fettinger <fred.fettinger@motorola.com>
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>

commit 8e321dbafac0bc9a57940521b56d1e738850092a
Author: Fred Fettinger <fred.fettinger@motorola.com>
Date:   Fri Jan 11 14:24:12 2013 -0600

    msm: kgsl: generate descriptive names for kgsl-timeline
    
    Generate names for kgsl-timelines using the following format:
    <device>-<thread name>(<tid>)-<proc name>(<pid>)-<context id>
    
    This makes it possible to identify the context of a timeline in the
    sync dump, which makes it much easier to identify which context has
    a GPU timeout.
    
    Change-Id: I0ce0614a53a93fd81094d92c9bef7053e6d416d2
    Git-commit: e3348f389f715cb2143a708ec6796d3f65e03821
    Git-repo: https://www.codeaurora.org/gitweb/quic/la/?p=kernel/msm.git
    Signed-off-by: Fred Fettinger <fred.fettinger@motorola.com>
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>

commit f0f2a3d7c20c3044fa72bc5acfc81359824501a5
Author: Suman Tatiraju <sumant@codeaurora.org>
Date:   Thu May 30 03:41:21 2013 -0700

    msm: kgsl: Always resume the GPU regardless of its state
    
    Make sure we resume the GPU regardless of its current
    state. If the device is not in suspend state restart the
    device and  print an error message.
    
    Change-Id: I9e71cbe27d360dc06513c54f3a734aaea5b10d2b
    Signed-off-by: Suman Tatiraju <sumant@codeaurora.org>

commit 33dc1780131697dfcfa5f4965e110317c330c389
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri May 17 17:09:49 2013 -0700

    gpu: ion: Do fallback when allocating large sizes
    
    The preferred allocation of memory for kernel data structures in
    ION is to use kmalloc. This is mainly for performance reasons. However,
    we are not guaranteed to be able to allocate large sizes. So to keep
    the code performing well and at the same time allow for larger allocations,
    implement a fallback mechanism for allocation of memory for ION kernel data
    structures.
    
    CRs-Fixed: 488521
    Change-Id: I8c6a13ad74cce69f590920d28c01a58f7a540fd2
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>
    [Resolved merge conflict]
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>

commit 9edb6c2b672b0328e78331c7a84b273559efca48
Author: Lynus Vaz <lvaz@codeaurora.org>
Date:   Tue Nov 20 18:26:56 2012 +0530

    msm: kgsl: Submit a draw command on resume
    
    For A305, submit an extra draw command while resuming the GPU from
    SLUMBER state. This fixes corruption seen when the screen is
    turned on.
    
    Change-Id: Iafed90b617b0a700ac3f2afcb6d9ecadca590914
    Signed-off-by: Lynus Vaz <lvaz@codeaurora.org>

commit 65c536ebca30c997598e54a4334bf3667ccb8ed5
Author: Lynus Vaz <lvaz@codeaurora.org>
Date:   Mon May 20 15:23:20 2013 +0530

    msm: kgsl: Protect the mem_entry list during deletion
    
    Use the spinlock to prevent corruption of the mem_entry list during
    deletion. Otherwise the mem_entry list may be modified by another
    thread at the same time resulting in memory corruption.
    
    Change-Id: I5728a8b925753d5fda82413615e5ebaf0366dfd8
    CRs-fixed: 489224
    Signed-off-by: Lynus Vaz <lvaz@codeaurora.org>

commit 8917f09d8e0ed2d75af0fa3ddfda368313510cff
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Fri May 17 13:35:23 2013 -0700

    gpu: msm: Allow retries for 0 order allocation
    
    When allocating order 0 pages we allow the allocator to retry
    the allocation to ensure we can take advantage of pages that
    are being reclaimed. This will help avoid unnecessary out
    of memory errors from kgsl clients.
    
    Change-Id: Iec2a0a1fe10302c320a58cef90ae1ee7e35d71ee
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 50072070085450c5a25d43d872b81dd2ddfc5f0f
Author: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>
Date:   Tue May 21 15:19:16 2013 -0600

    msm: kgsl: Don't consider active count if not in active state
    
    Don't consider active_cnt variable if not already in ACTIVE state.
    The active count variable does not necessarily exactly indicate the
    number of open handles to GPU for switching power state. Hence,
    only wait for active count to reach zero before switching to SUSPEND
    state if, the current state is ACTIVE.
    
    CRs-fixed: 483154
    Change-Id: Ie025a1873cfb9237bdfba89204f4c2116549eba4
    Signed-off-by: Harsh Vardhan Dwivedi <hdwivedi@codeaurora.org>

commit 1d8f7976cc256d0ad47662364322f4d4744233c8
Author: Hareesh Gundu <hareeshg@codeaurora.org>
Date:   Mon May 13 20:25:53 2013 +0530

    msm: kgsl: Add NULL check for next_event hook
    
    KGSL driver needs to check that the function pointer
    is not NULL before calling the hook. next_event
    is an optional function not implemented by the 2d
    core. This fix ensures that optional and unimplemented
    function will not invoked by the driver.
    
    Change-Id: I687b028260e3ec304d973c6a5bded0a8fdbfb73b
    Signed-off-by: Hareesh Gundu <hareeshg@codeaurora.org>

commit 81ba659ea91742d19f4b27ea7696af15a3d147bc
Author: Hareesh Gundu <hareeshg@codeaurora.org>
Date:   Tue May 7 14:52:17 2013 +0530

    msm: kgsl: Insert kgsl idle for iommu-v1 when pagetables are changed.
    
    An idle is needed for iommu-v1 before programming both tlb registers
    and the pagetable register. Without this idle faults are observed on
    2D core because a process uses another processes pagetable.
    
    Change-Id: Ief14cd79b8e7ffaed413da49dc4b9e8416233186
    Signed-off-by: Hareesh Gundu <hareeshg@codeaurora.org>

commit 4bbeb244fbcb5676689067b67b931696f1dfcb7a
Author: Jordan Crouse <jcrouse@codeaurora.org>
Date:   Fri Apr 5 16:22:59 2013 -0600

    msm: kgsl: Don't do intensive memory recovery when allocating big pages
    
    We don't want to incur too much overhead when allocating big pages
    so don't attempt to retry, perform reclaim, or run memory compaction
    on high-order allocations.
    
    Change-Id: Ic0dedbadb354c6faea34abec36aee268ac0f2c34
    Signed-off-by: Jordan Crouse <jcrouse@codeaurora.org>
    Signed-off-by: Hareesh Gundu <hareeshg@codeaurora.org>

commit 111c82188487f8631dedd117e644d8ab6a76949a
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Tue Jun 25 06:59:50 2013 -0500

    upgrade to google sources
    
    Conflicts:
    	arch/arm/mach-msm/board-jf_cri.c
    	arch/arm/mach-msm/board-jf_dcm.c
    	arch/arm/mach-msm/devices-8064.c
    	arch/arm/mach-msm/hotplug.c
    	arch/arm/mach-msm/pm-8x60.c
    	drivers/gpu/ion/msm/msm_ion.c
    	drivers/input/keyboard/gpio_keys.c
    	drivers/media/video/msm/sensors/jc_v4l2.c
    	drivers/video/msm/mdp.c
    	drivers/video/msm/mdp.h
    	drivers/video/msm/mdp4.h
    	drivers/video/msm/mdp4_overlay.c
    	drivers/video/msm/mdp4_overlay_dsi_cmd.c
    	drivers/video/msm/mdp4_overlay_dsi_video.c
    	drivers/video/msm/mdp4_overlay_dtv.c
    	drivers/video/msm/vidc/common/vcd/vcd_power_sm.c
    Change-Id: Icac51ed5ab5e58e0ec1eeaf6c9d4306db6c613c3

commit 103969438ad13a3f525802886c815da64e57dc66
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Mon Jun 24 12:04:14 2013 -0500

    jf: MFx merge
    
    Conflicts:
    	arch/arm/mach-msm/msm_bus/msm_bus_board_8064.c
    	drivers/video/msm/mdp.c
    	drivers/video/msm/mdp.h
    	drivers/video/msm/mdp4_overlay.c
    	drivers/video/msm/mdp4_overlay_dsi_cmd.c
    	drivers/video/msm/mdp4_overlay_dsi_video.c
    	drivers/video/msm/mdp4_overlay_dtv.c
    	drivers/video/msm/mdp4_overlay_lcdc.c
    
    Change-Id: I81fb72095d66511c0d4221b830f72f8b6b85ea47

commit 6e429128eb3c9bfe1eb2749aa2c97b898996df19
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Mon Jun 24 09:38:51 2013 -0700

    Revert "jfactivexx: MEx merge"
    
    merging forks branches is not a good idea
    
    This reverts commit 49f44213a0fa44c2a57acfecb94193d6ec594ebd
    
    Change-Id: I95e55f43810d5164caf60e2205efe82a453e9da8

commit 9cd5bea93c378b2f17bd4c1fffec15f6ba09f3df
Merge: f73f4fe c3ec5f6
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Mon Jun 24 09:38:18 2013 -0700

    Merge "Revert "merge camera jfactive sources"" into cm-10.1

commit c3ec5f6a147dfd30ef40710a0f856b3ca31dc63b
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Mon Jun 24 09:38:00 2013 -0700

    Revert "merge camera jfactive sources"
    
    merging forks branches is not a good idea
    
    This reverts commit 981d1b772d2309f699921c875d586098ca36cbde
    
    Change-Id: I82331cf1c7fc7979ab587ecdd3de43dda45db9f3

commit f73f4fe80ace40f5b5897f9f6c23ab7f7e62091e
Author: Nathan Grebowiec <njgreb@gmail.com>
Date:   Mon Jun 24 09:05:53 2013 -0500

    msm: audio: qdsp6v2: Add size safety check to ACDB driver
    
    Check that the size sent by userspace is not larger
    then the internal amount allowed. This protects
    against overflowing the stack due to an invalid size.
    Change-Id: I8230fdb00a7b57d398929e8ab0eb6587476f3db1
    CRs-fixed: 470222
    Signed-off-by: Ben Romberger <bromberg@codeaurora.org>

commit 981d1b772d2309f699921c875d586098ca36cbde
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Sun Jun 23 04:22:10 2013 -0500

    merge camera jfactive sources
    
    Change-Id: I36911342be2319340f8fe2cd46466c77aafb9fbf

commit 49f44213a0fa44c2a57acfecb94193d6ec594ebd
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Sat Jun 22 05:09:27 2013 -0500

    jfactivexx: MEx merge
    
    this should be good to go, only thing that need fixing drivers/media/video/msm_jfactive need compile fixes but it is disabled by default.
    
    Conflicts:
    	arch/arm/mach-msm/devices-8064.c
    	arch/arm/mach-msm/hotplug.c
    	arch/arm/mach-msm/pm-8x60.c
    	drivers/input/keyboard/gpio_keys.c
    	drivers/media/video/msm/sensors/jc_v4l2.c
    	drivers/mfd/max77693.c
    	drivers/video/msm/mdp4_overlay.c
    	drivers/video/msm/vidc/common/vcd/vcd_power_sm.c
    Change-Id: Ifc399d56cfa702a3c3ab5d539296d111d0d0dde4

commit 4455c413712e78d4a230eae1586c09d59cdf843e
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Fri Jun 21 04:37:59 2013 -0500

    fix possible mismerge
    
    Change-Id: I03576d58331113a903133efd41e9dccc355adb2e

commit d6be223649ccc876fabfe3e43edd4c986e45f733
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Thu Jun 6 19:23:50 2013 +0530

    msm: vidc: Handle mgen2maxi interrupt in video driver
    
    Handle interrupt generated by mgen2maxi h/w and
    avoid unknown interrupts storm which results in
    target reset issues.
    
    Change-Id: Ida24cb2dde15c9cd4a4c1d9ca336b1a4b662c4d1
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 4e1ab77c25d25e4b155363a3d87d7b58a7cc2676
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Wed May 22 04:25:29 2013 +0530

    msm: vidc: Get the current performance level
    
    This change supports client to get the current
    performance level of the video driver via
    IOCTL_GET_PERF_LEVEL. The current performance
    level indicate the number of MBs per second
    is being processed by video hardware.
    
    Change-Id: Ic6f5b2b14e0d77bf801c4f857f8a0e20339c199f
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 9cb6b395b902e69c46d4226c8f0424b304626af0
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Thu May 23 16:38:29 2013 +0530

    msm: vidc: Allow client to set turbo mode
    
    - Allow client to set TURBO performance level
      to video driver.
    - Update max performance level to TURBO perf level
      on supported targets.
    
    Change-Id: I0b3b38140d777984ba5062ddd614c07a002389ba
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 87a75de4fa580b87c7a23b504cc565db02b35949
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:21 2013 +0000

    af_unix: use freezable blocking calls in read
    
    Avoid waking up every thread sleeping in read call on an AF_UNIX
    socket during suspend and resume by calling a freezable blocking
    call.  Previous patches modified the freezer to avoid sending
    wakeups to threads that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: I788246a76780ea892659526e70be018b18f646c4
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4e71318f190c6d95df841c22bf4271fadc24c981
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:20 2013 +0000

    sigtimedwait: use freezable blocking call
    
    Avoid waking up every thread sleeping in a sigtimedwait call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: Ic27469b60a67d50cdc0d0c78975951a99c25adcd
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 685637561f4fdb33fd71e8587917edadd1d56d13
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:19 2013 +0000

    nanosleep: use freezable blocking call
    
    Avoid waking up every thread sleeping in a nanosleep call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: I93383201d4dd62130cd9a9153842d303fc2e2986
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit c76b93092be0f51377d1fa91ceeaf0224d8af9b1
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:18 2013 +0000

    futex: use freezable blocking call
    
    Avoid waking up every thread sleeping in a futex_wait call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: I9ccab9c2d201adb66c85432801cdcf43fc91e94f
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Darren Hart <dvhart@linux.intel.com>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit df30eda2a6d168b9d9f1dab10a39054914e20458
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:17 2013 +0000

    select: use freezable blocking call
    
    Avoid waking up every thread sleeping in a select call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: I0d7565ec0b6bc5d44cb55f958589c56e6bd16348
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e7491ae99af50d3da1e51baa113fc15d28cc6fce
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:16 2013 +0000

    epoll: use freezable blocking call
    
    Avoid waking up every thread sleeping in an epoll_wait call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: I848d08d28c89302fd42bbbdfa76489a474ab27bf
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9eaa48116d15962016298c0362997ba29dc3ee23
Author: Eric Wong <normalperson@yhbt.net>
Date:   Tue Jan 1 21:20:27 2013 +0000

    epoll: prevent missed events on EPOLL_CTL_MOD
    
    commit 128dd1759d96ad36c379240f8b9463e8acfd37a1 upstream.
    
    EPOLL_CTL_MOD sets the interest mask before calling f_op->poll() to
    ensure events are not missed.  Since the modifications to the interest
    mask are not protected by the same lock as ep_poll_callback, we need to
    ensure the change is visible to other CPUs calling ep_poll_callback.
    
    We also need to ensure f_op->poll() has an up-to-date view of past
    events which occured before we modified the interest mask.  So this
    barrier also pairs with the barrier in wq_has_sleeper().
    
    This should guarantee either ep_poll_callback or f_op->poll() (or both)
    will notice the readiness of a recently-ready/modified item.
    
    This issue was encountered by Andreas Voellmy and Junchang(Jason) Wang in:
    http://thread.gmane.org/gmane.linux.kernel/1408782/
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>
    Cc: Hans Verkuil <hans.verkuil@cisco.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Davide Libenzi <davidel@xmailserver.org>
    Cc: Hans de Goede <hdegoede@redhat.com>
    Cc: Mauro Carvalho Chehab <mchehab@infradead.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andreas Voellmy <andreas.voellmy@yale.edu>
    Tested-by: "Junchang(Jason) Wang" <junchang.wang@yale.edu>
    Cc: netdev@vger.kernel.org
    Cc: linux-fsdevel@vger.kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bcf756aadc3a7b292eb8a29f9bc3b3cad27fce2c
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon May 21 21:20:48 2012 +0200

    epoll: Fix user space breakage related to EPOLLWAKEUP
    
    Commit 4d7e30d (epoll: Add a flag, EPOLLWAKEUP, to prevent
    suspend while epoll events are ready) caused some applications to
    malfunction, because they set the bit corresponding to the new
    EPOLLWAKEUP flag in their eventpoll flags and they don't have the
    new CAP_EPOLLWAKEUP capability.
    
    To prevent that from happening, change epoll_ctl() to clear
    EPOLLWAKEUP in epds.events if the caller doesn't have the
    CAP_EPOLLWAKEUP capability instead of failing and returning an
    error code, which allows the affected applications to function
    normally.
    
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit cf727d288b18b07a5739265fdffcbb3094d9a4f6
Author: Arve Hjønnevåg <arve@android.com>
Date:   Tue May 1 21:33:34 2012 +0200

    epoll: Add a flag, EPOLLWAKEUP, to prevent suspend while epoll events are ready
    
    When an epoll_event, that has the EPOLLWAKEUP flag set, is ready, a
    wakeup_source will be active to prevent suspend. This can be used to
    handle wakeup events from a driver that support poll, e.g. input, if
    that driver wakes up the waitqueue passed to epoll before allowing
    suspend.
    
    Signed-off-by: Arve Hjønnevåg <arve@android.com>
    Reviewed-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit fdd4fb410245be56bb0a272266a74fc54ff63b2e
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:15 2013 +0000

    binder: use freezable blocking calls
    
    Avoid waking up every thread sleeping in a binder call during
    suspend and resume by calling a freezable blocking call.  Previous
    patches modified the freezer to avoid sending wakeups to threads
    that are blocked in freezable blocking calls.
    
    This call was selected to be converted to a freezable call because
    it doesn't hold any locks or release any resources when interrupted
    that might be needed by another freezing task or a kernel driver
    during suspend, and is a common site where idle userspace tasks are
    blocked.
    
    Change-Id: Ic4458ae90447f6caa895cc62f08e515caa7790ba
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f773e55227b246a4948a365dcb6aa3c4d9bfff18
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:14 2013 +0000

    freezer: add new freezable helpers using freezer_do_not_count()
    
    Freezing tasks will wake up almost every userspace task from
    where it is blocking and force it to run until it hits a
    call to try_to_sleep(), generally on the exit path from the syscall
    it is blocking in.  On resume each task will run again, usually
    restarting the syscall and running until it hits the same
    blocking call as it was originally blocked in.
    
    To allow tasks to avoid running on every suspend/resume cycle,
    this patch adds additional freezable wrappers around blocking calls
    that call freezer_do_not_count().  Combined with the previous patch,
    these tasks will not run during suspend or resume unless they wake
    up for another reason, in which case they will run until they hit
    the try_to_freeze() in freezer_count(), and then continue processing
    the wakeup after tasks are thawed.
    
    Additional patches will convert the most common locations that
    userspace blocks in to use freezable helpers.
    
    Change-Id: Id909760ce460f2532801a4b00d344f0816bfefc9
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit cad3f60e9c48fc169271eebd9922e7bf9ab774c2
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:13 2013 +0000

    freezer: convert freezable helpers to static inline where possible
    
    Some of the freezable helpers have to be macros because their
    condition argument needs to get evaluated every time through
    the wait loop.  Convert the others to static inline to make
    future changes easier.
    
    Change-Id: I69d3fc10d26522cb9bf3a616ff4f21245f9c071a
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit ae1d6799850a1519d2a53dec4ce8c0307d5705bb
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:12 2013 +0000

    freezer: convert freezable helpers to freezer_do_not_count()
    
    Freezing tasks will wake up almost every userspace task from
    where it is blocking and force it to run until it hits a
    call to try_to_sleep(), generally on the exit path from the syscall
    it is blocking in.  On resume each task will run again, usually
    restarting the syscall and running until it hits the same
    blocking call as it was originally blocked in.
    
    Convert the existing wait_event_freezable* wrappers to use
    freezer_do_not_count().  Combined with a previous patch,
    these tasks will not run during suspend or resume unless they wake
    up for another reason, in which case they will run until they hit
    the try_to_freeze() in freezer_count(), and then continue processing
    the wakeup after tasks are thawed.
    
    This results in a small change in behavior, previously a race
    between freezing and a normal wakeup would be won by the wakeup,
    now the task will freeze and then handle the wakeup after thawing.
    
    Change-Id: I532e62251f58c1a9ca488b3fb6220c53acf7d33d
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit c202368c0a429dd6eb3d026ce0b047cb0bed4701
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:11 2013 +0000

    freezer: skip waking up tasks with PF_FREEZER_SKIP set
    
    Android goes through suspend/resume very often (every few seconds when
    on a busy wifi network with the screen off), and a significant portion
    of the energy used to go in and out of suspend is spent in the
    freezer.  If a task has called freezer_do_not_count(), don't bother
    waking it up.  If it happens to wake up later it will call
    freezer_count() and immediately enter the refrigerator.
    
    Combined with patches to convert freezable helpers to use
    freezer_do_not_count() and convert common sites where idle userspace
    tasks are blocked to use the freezable helpers, this reduces the
    time and energy required to suspend and resume.
    
    Change-Id: I6ba019d24273619849af757a413271da3261d7db
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f64d8937b339e53e7932070eb47b633ca8bc0e49
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:10 2013 +0000

    freezer: shorten freezer sleep time using exponential backoff
    
    All tasks can easily be frozen in under 10 ms, switch to using
    an initial 1 ms sleep followed by exponential backoff until
    8 ms.  Also convert the printed time to ms instead of centiseconds.
    
    Change-Id: I7b198b16eefb623c2b0fc45dce50d9bca320afdc
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    
    Conflicts:
    	kernel/power/process.c

commit 68d465fcb7cfa3d867c13c2876161d93c6ca711e
Author: Mandeep Singh Baines <msb@chromium.org>
Date:   Mon May 6 23:50:09 2013 +0000

    lockdep: check that no locks held at freeze time
    
    We shouldn't try_to_freeze if locks are held.  Holding a lock can cause a
    deadlock if the lock is later acquired in the suspend or hibernate path
    (e.g.  by dpm).  Holding a lock can also cause a deadlock in the case of
    cgroup_freezer if a lock is held inside a frozen cgroup that is later
    acquired by a process outside that group.
    
    History:
    This patch was originally applied as 6aa9707099c and reverted in
    dbf520a9d7d4 because NFS was freezing with locks held.  It was
    deemed better to keep the bad freeze point in NFS to allow laptops
    to suspend consistently.  The previous patch in this series converts
    NFS to call _unsafe versions of the freezable helpers so that
    lockdep doesn't complain about them until a more correct fix
    can be applied.
    
    Change-Id: Ib9d4299fb75a39e611b868be42e413909a994baa
    [akpm@linux-foundation.org: export debug_check_no_locks_held]
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit eeca822719a05286638d7d9f7cb81a14eb219867
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:08 2013 +0000

    lockdep: remove task argument from debug_check_no_locks_held
    
    The only existing caller to debug_check_no_locks_held calls it
    with 'current' as the task, and the freezer needs to call
    debug_check_no_locks_held but doesn't already have a current
    task pointer, so remove the argument.  It is already assuming
    that the current task is relevant by dumping the current stack
    trace as part of the warning.
    
    This was originally part of 6aa9707099c (lockdep: check that
    no locks held at freeze time) which was reverted in
    dbf520a9d7d4.
    
    Change-Id: Idbaf1332ce6c80dc49c1d31c324c7fbf210657c5
    Original-author: Mandeep Singh Baines <msb@chromium.org>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 875518ebaf4ecf99e818bdfd935c1898b4a2c3ae
Author: Colin Cross <ccross@android.com>
Date:   Tue May 7 17:52:05 2013 +0000

    freezer: add unsafe versions of freezable helpers for CIFS
    
    CIFS calls wait_event_freezekillable_unsafe with a VFS lock held,
    which is unsafe and will cause lockdep warnings when 6aa9707
    "lockdep: check that no locks held at freeze time" is reapplied
    (it was reverted in dbf520a).  CIFS shouldn't be doing this, but
    it has long-running syscalls that must hold a lock but also
    shouldn't block suspend.  Until CIFS freeze handling is rewritten
    to use a signal to exit out of the critical section, add a new
    wait_event_freezekillable_unsafe helper that will not run the
    lockdep test when 6aa9707 is reapplied, and call it from CIFS.
    
    In practice the likley result of holding the lock while freezing
    is that a second task blocked on the lock will never freeze,
    aborting suspend, but it is possible to manufacture a case using
    the cgroup freezer, the lock, and the suspend freezer to create
    a deadlock.  Silencing the lockdep warning here will allow
    problems to be found in other drivers that may have a more
    serious deadlock risk, and prevent new problems from being added.
    
    Change-Id: I420c5392bacf68e58e268293b2b36068ad4df753
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit b96305c7e6ae93f7c44898c5a38cce688420a23a
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:06 2013 +0000

    freezer: add unsafe versions of freezable helpers for NFS
    
    NFS calls the freezable helpers with locks held, which is unsafe
    and will cause lockdep warnings when 6aa9707 "lockdep: check
    that no locks held at freeze time" is reapplied (it was reverted
    in dbf520a).  NFS shouldn't be doing this, but it has
    long-running syscalls that must hold a lock but also shouldn't
    block suspend.  Until NFS freeze handling is rewritten to use a
    signal to exit out of the critical section, add new *_unsafe
    versions of the helpers that will not run the lockdep test when
    6aa9707 is reapplied, and call them from NFS.
    
    In practice the likley result of holding the lock while freezing
    is that a second task blocked on the lock will never freeze,
    aborting suspend, but it is possible to manufacture a case using
    the cgroup freezer, the lock, and the suspend freezer to create
    a deadlock.  Silencing the lockdep warning here will allow
    problems to be found in other drivers that may have a more
    serious deadlock risk, and prevent new problems from being added.
    
    Change-Id: Ia17d32cdd013a6517bdd5759da900970a4427170
    Signed-off-by: Colin Cross <ccross@android.com>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 78846f065fb8a4dde292951cce3a3b22f4686065
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Oct 16 15:03:14 2012 -0700

    freezer: add missing mb's to freezer_count() and freezer_should_skip()
    
    commit dd67d32dbc5de299d70cc9e10c6c1e29ffa56b92 upstream.
    
    A task is considered frozen enough between freezer_do_not_count() and
    freezer_count() and freezers use freezer_should_skip() to test this
    condition.  This supposedly works because freezer_count() always calls
    try_to_freezer() after clearing %PF_FREEZER_SKIP.
    
    However, there currently is nothing which guarantees that
    freezer_count() sees %true freezing() after clearing %PF_FREEZER_SKIP
    when freezing is in progress, and vice-versa.  A task can escape the
    freezing condition in effect by freezer_count() seeing !freezing() and
    freezer_should_skip() seeing %PF_FREEZER_SKIP.
    
    This patch adds smp_mb()'s to freezer_count() and
    freezer_should_skip() such that either %true freezing() is visible to
    freezer_count() or !PF_FREEZER_SKIP is visible to
    freezer_should_skip().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 756f263366fcd85815b6bce05e6604a95a4720ca
Author: Colin Cross <ccross@android.com>
Date:   Wed Aug 15 13:10:04 2012 -0700

    HACK: ARM: disable sleeping while atomic warning in do_signal
    
    ARM disables interrupts in do_signal, which triggers a warning in
    try_to_freeze, see details at https://lkml.org/lkml/2011/8/23/221.
    To prevent the warnings, add try_to_freeze_nowarn and call it from
    do_signal.
    
    Change-Id: If7482de21c386adc705fa1ac4ecb8c7ece5bb356
    Signed-off-by: Colin Cross <ccross@android.com>

commit 51d2eb0a84f418a015b11a41ca3442f73f914fec
Author: Ido Shayevitz <idos@codeaurora.org>
Date:   Fri Jun 7 09:30:40 2013 -0500

    Squash of two EHCI: HSIC commits from caf
    
    EHCI: HSIC: disable pm runtime device before remove hcd
      Merge order of commits 21a480e and e50d914 introduced runtime pm
      disable depth counter imbalance fixed by e50d914 again. Fix the
      bug by disabling runtime pm before removing HCD.
    
    Change-Id: I249a4f63db3a975652e6ce15545584e41bae284b
    Signed-off-by: Ido Shayevitz <idos@codeaurora.org>
    
    EHCI: HSIC: fix driver removal function for msm_hsic_host
      Upon driver removal function we need to disable the device pm runtime
      in order to, first, undo all the logic done in probe and second, to keep
      balanced disable depth counter in case of platform driver unbind.
    
      However the removal function can be called also in consequences of
      platform_device_del(), in this case the platform device was already
      disabled and removed so if we call pm_runtime_disable() again we
      decrease the disable_depth for the second time causing for unbalanced
      counter.
    
      Therefore we use an if statement to check if the device was already
      removed or not.
    
    CRs-Fixed: 445674
    Change-Id: I5ff6b20d0f8e811a5c673903ca340f7bcc2e2d78
    Signed-off-by: Ido Shayevitz <idos@codeaurora.org>

commit 198e1c650b6ae900cb1d31ae6e41904d195edc14
Author: Shobhit Pandey <cshopan@codeaurora.org>
Date:   Fri Apr 12 12:49:28 2013 +0530

    msm:board-8064: Correct ab/ib values of 1080p turbo vectors
    
    This change corrects the ab/ib values of turbo vectors
    intended for 1080p encoder.
    
    CRs-Fixed: 473197
    
    Change-Id: I172b241e7b8b7b2d8600647ac35af78ed928e052
    Signed-off-by: Rajeshwar Kurapaty <rkurapat@codeaurora.org>

commit 3ee1293ebb0c12d438b28d44c2e1905d34ba10e3
Author: Adrian Alexei <aalexei@codeaurora.org>
Date:   Thu Apr 4 16:18:51 2013 -0700

    ion: Update ION_SECURE and ION_FORCE_CONTIGUOUS flags
    
    Rename ION_SECURE and ION_FORCE CONTIGUOUS to ION_FLAG_SECURE
    and ION_FLAG_FORCE_CONTIGUOUS respectively, but leave the old
    ones intact for backwards compatability.
    
    Change-Id: I31ee3088ed202f60e14c10ac66d775c64c01c4e3
    Signed-off-by: Adrian Alexei <aalexei@codeaurora.org>

commit 7ebb68b5b38cb8303393697d37ccde3b6459e28f
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Tue Mar 19 17:16:58 2013 -0700

    gpu: ion: enable the kmalloc heap
    
    There are some Ion clients who would like to use the kmalloc
    heap. Enable it.
    
    Change-Id: Ie818be9e1211d52d489ad2cb63d94e2fddfc0f01
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit a828609dcd0b9b4b706e8adca46eac65c6c6bf63
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Sat Feb 9 09:35:30 2013 -0800

    gpu: ion: Add ADSP heap
    
    The ADSP heap is intended to be used for post processing
    on the dsp. Add the definiton of the heap to allow clients
    to have dedicated access
    
    Change-Id: I80040b0cf3311df90970c156aa3329f07fc9a990
    CRs-Fixed: 438281
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>

commit 68303220cc4dfa449a270506fffd4e73a28eed1a
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Mon May 20 21:33:26 2013 +0530

    msm: vidc: Remove unncessary unmap function calls
    
    Calling unncessary unmap function resulting in
    target reset. This change resolves the issue by
    removing unncessary unmap functions in video driver.
    
    Change-Id: I0b501f0a7115bba085b067a5e78172df210da8f1
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 80768e399deb51f95a8e6553d4206177216bde5a
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Thu May 9 11:24:16 2013 +0530

    msm: vidc: Optimize IOMMU map size for H264 decoder
    
    IOMMU map size is not required to be double the actual
    buffer size for H264 decoder. So reducing IOMMU map
    size for H264 format which will enable support for
    two H264 decoder instances in parallel in smooth streamig
    mode where the number of output buffers equal to 18 with
    each buffer size equal to 1080p buffer size.
    
    CRs-fixed: 491718
    Change-Id: Ie6782b2ff58667000213f25482eea151b849762b
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 5891d17b10a81c9865a2c92aff4b3d2caef23cbc
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Mon May 20 16:23:55 2013 +0530

    msm: vidc: Set extradata flag properly
    
    Extradata flag is not set to output buffer header's flag in
    case when a Mpeg2 clip is played and extenstion and user data
    extradata is not enabled.
    This change fixes the issue and set extradata flag properly
    if extradata is enabled and extradata is available with the
    the buffer.
    
    CRs-fixed: 490067
    Change-Id: I029dadb17d98dd37388eee34f51bd0dfaf56d14a
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 17c2697efd007de31d8cdb37ef25618e11f7304f
Author: Rajeshwar Kurapaty <rkurapat@codeaurora.org>
Date:   Tue Jan 15 17:12:31 2013 +0530

    msm:vidc: Add support for MP2 sequence end code
    
    ISDB-T client requires end of sequence notification
    in the flags of the output buffer when sequence end
    code is queued to the video core. Add support by reading
    from video core shared memory and update the output buffer
    flags for MPEG-2 codec.
    
    Change-Id: Idd88fc6a5cdccb41fa61ba902a0014ec8359739a
    Signed-off-by: Rajeshwar Kurapaty <rkurapat@codeaurora.org>

commit 34d9dc2a13c54a78d17f4253a4ae40ff278de647
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Wed Mar 13 14:34:25 2013 +0530

    msm: vidc: Fix codec config frame with EOS flag issue
    
    This change fixes the issue with improper handling of
    codec config frame with eos flag in smooth streaming
    scenario.
    
    CRs-fixed: 467211
    Change-Id: Iafd0a1a5a9073458fd139ed5845585747e51be8a
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 733a1b58cd9b52f54a7e20126a7b237e2ee80935
Author: Kalyan Thota <kalyant@codeaurora.org>
Date:   Thu Apr 4 16:20:04 2013 +0530

    msm_fb: Set RR sched policy for Glupdator thread
    
    Set Round Robin Scheduling policy for GLUpdator
    thread to improve performance.
    
    Change-Id: I637ca364d325d6c847c73e8eec403e76094d93c5
    Signed-off-by: Kalyan Thota <kalyant@codeaurora.org>
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit 19b893dde96bab4f1c9654a274fd47570ccec881
Author: Krishnankutty Kolathappilly <kkolat@codeaurora.org>
Date:   Thu Mar 28 19:20:47 2013 -0700

    asoc: msm: Add flexible period count to pcm record driver
    
     In pcm recording, if buffers are not requested from dsp at real
     time then dsp drops the buffers and sends partial buffers. This
     is causing record failures. Pcm record platform driver has two
     periods. If system is loaded, read request will not be at real
     time. Add flexible period count to pcm record platform driver
     so client can configure the period count.
    
    CRs-Fixed: 467746
    Change-Id: Iadd47979f7b29a01fb8e6479383094a2ed1d93c5
    Signed-off-by: Krishnankutty Kolathappilly <kkolat@codeaurora.org>

commit 4a745726705272c95f7bda1ac0c96953af206090
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat Jun 1 02:07:12 2013 -0700

    asoc: Add stream id to Samsung specific functions
    
    Change-Id: Ibffc4ba974c8139744d3e1914988ae4ac6368eff

commit 0f233266f64df7cb298b136f36f06962f72325f1
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat Jun 1 01:52:05 2013 -0700

    asoc: Kill logspam in compressed audio driver
    
    Change-Id: I415036d4d8fd6729f1ee14a29dafc5efd6b98a4a

commit c7adb332b29ebd38443939956659ac29a0928cdd
Author: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>
Date:   Fri Mar 15 17:25:20 2013 -0700

    ASoc: msm: update the pending buffer flags in underrun
    
    Update the pending buffer flag on a driver underrun.
    This causes a drain call to be rejected if the underrun
    happens immediately after an EOS buffer has been given to the DSP
    following a flush
    
    CRs-Fixed: 463332
    Change-Id: I2ba224a49e5af14ca909300afc0788cb4b21eb16
    Signed-off-by: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>

commit 34d95022b88a334ae048e5e1d5abfd29e39286f9
Author: Haynes Mathew George <hgeorge@codeaurora.org>
Date:   Sat Jan 26 19:19:27 2013 -0800

    ASoC: msm: Resume writes only for start command
    
    For compressed and LPA drivers, set pending_buffers flag to 1 only if the client
    issues a trigger_start command, not a pause_release command. This flag
    is used to initiate writes to the dsp.
    
    Change-Id: I503156863646210b460613a0b9a26a6bfb8854ed
    CRs-Fixed: 442269, 449126
    Signed-off-by: Haynes Mathew George <hgeorge@codeaurora.org>

commit 15125cb1677537dda0494d63ef77cf67f1e03472
Author: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>
Date:   Mon Dec 10 14:15:16 2012 -0800

    ASoC: msm: Fixes for underrun issue in LPA Playback
    
    Update LPA platform driver to register restart callback.
    The implementation of the callback is to issue a pending buffer
    written by the application to restart the session when the dequeue
    process is stopped due to underrun issue.
    The callback is triggered from ALSA core when the framework
    detects that the renderer stopped pulling data but the application
    is still writing data. It runs in atomic context.
    
    Fixes:
    -A disturbance was heard when switching from one song
    to the other, or when the song is paused for greater
    than 3 seconds and then resumed
    -This was due to the previous value of the track
    volume being used in the lpa driver
    -This was fixed by setting 0 as the initial volume
    
    Change-Id: I035760937d032038fb3f3383dc87ecbdc5f4700d
    Signed-off-by: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>

commit c014754f0e424a03a10b07731168f6783a94e240
Author: Amal Paul <amal@codeaurora.org>
Date:   Tue Mar 5 18:59:18 2013 -0800

    ASoC: msm: Fix for improper volume ramp up in tunnel playback
    
    A disturbance is heard when resuming the playback after
    switching from headset to speaker and back to headset with the
    music paused for more than 3 sec. This is due to, data getting
    rendered with the speaker volume where it had to played with headset
    volume. Fix is to set the '0' as the initial volume.
    
    Change-Id: Ie4726a7898620fb912279270e24fa15a45a437f9
    CRs-fixed: 453704
    Signed-off-by: Amal Paul <amal@codeaurora.org>

commit 839ae62908a8eee6ced511de96645064dddc5b37
Author: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>
Date:   Wed Feb 13 22:20:51 2013 -0800

    ASoC: msm: skip sending volume in compressed pass through playback
    
    For the playback use of compressed pass through on HDMI device,
    post processing features are not supported and should not be sent to
    DSP. Hence the change is now to send the volume only for the playback
    of decode and render in DSP and skip for compressed pass through
    
    CRs-Fixed: 442140
    Change-Id: I2125f08baf1676f6bb1d13463a1f01dfe280963f
    Signed-off-by: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>

commit ea3e3aa871d37ad3db312b932677ddc4691c8f6f
Author: Sidipotu Ashok <sashok@codeaurora.org>
Date:   Fri May 17 15:13:30 2013 +0530

    ASoc: msm: Fix for pcm_read getting struck in alsa core.
    
    Advance hardware pointer by period size even if the packet
    coming from qdsp6 is of less size. QDSP6 sometimes
    (for rapid device switches) gives packets worth less
    than period size and this results in hw_ptr appl_ptr mismatch.
    
    CRs-Fixed: 486276
    CRs-Fixed: 488449
    Change-Id: I893f9a52e4cd507fd72e4a7aba3c5fa76527d1bf
    Signed-off-by: Sidipotu Ashok <sashok@codeaurora.org>

commit 42792e0a3c1ad25106ab55e6dd9de8d662b3588a
Author: Patrick Lai <plai@codeaurora.org>
Date:   Sat Nov 17 00:29:07 2012 -0800

    ASoC: msm: flush if prior and current backends rate not matching
    
    It is found that during device switch from one backend
    with one sample rate to another backend with another sample rate,
    the command to QDSP6 ADM which maps audio stream session to a
    particular backend would not get carried out until pending
    data of audio stream session from previous backend is either
    read out or flushed. This scenario occurs when application
    stops providing more buffers to retrieve captured data.
    Remedy is to flush upon detection of rate mismatching
    
    Change-Id: I2c01c036d9bb71f938a6795337f08948bd986553
    CRs-fixed: 422205
    Signed-off-by: Patrick Lai <plai@codeaurora.org>
    Signed-off-by: Joonwoo Park <joonwoop@codeaurora.org>

commit acdf0b6623c1305cc1b757695ee4edb68340957d
Author: Laxminath Kasam <lkasam@codeaurora.org>
Date:   Wed Apr 3 14:03:17 2013 +0530

    ASoC: audio: Fix BE not disconnected even codec teardown
    
    Scenario:
    When FE1(LowLatency) comes in which opens BE1 and puts it in state
    SND_SOC_DPCM_STATE_START. Similarly FE2(LPA) comes in with
    connecting to same BE. Now FE2 is in SND_SOC_DPCM_STATE_PREPARE
    state and waits for TRIGGER_START. Meanwhile FE1 is closed which
    doesn't affect BE1 and FE2 which are connected.
    Also, for FE2 pcm_close is called before TRIGGER_START itself.
    In such a case, BE1 is not getting disconnected (afe_close is
    not called) while Codec is teared down resulting in Slimbus
    overflow.
    
    Fix:
    Whenever BE1 is found to be in SND_SOC_DPCM_STATE_START state
    and FE1 being connected is not started yet, and close sequence
    happens for this FE, then allow hw_free to happen for BE1 if
    this is the only FE connected to this BE1. This allows BE to be
    disconnected which calls afe_close then followed by Codec teardown
    not resulting in any Slimbus overflows.
    
    CRs-Fixed: 468118
    Change-Id: I03a80a12c1fd0cf09bd2e3dfe1f1eb56589016d3
    Signed-off-by: Laxminath Kasam <lkasam@codeaurora.org>

commit c0b884776889602a69ee311112000cd4ce389c14
Author: Venkat Sudhir <vsudhir@codeaurora.org>
Date:   Mon Mar 4 10:32:59 2013 -0800

    ASoC: msm: Add support for external EC reference point
    
    Add mux to sound card topology with output of mux routed
    to CS voice uplink front-end as way to indicate external
    EC reference point. Currently, MI2S_TX is the only
    AFE input port defined for this MUX.
    
    Change-Id: I0c81c579cc01bf92e743eda0e8dee15be2a8c01f
    Signed-off-by: Venkat Sudhir <vsudhir@codeaurora.org>

commit 49d0eb9ac84cb3392754d581d676357f17fd7664
Author: Banajit Goswami <bgoswa@codeaurora.org>
Date:   Thu Feb 14 18:24:08 2013 -0800

    ASoC: Avoid putting stream state to STOP when stream paused
    
    When multiple Front-end are using the same Back-end, putting
    the stream state for a Front-end to STOP will make the the Backend
    stream to be released, when the front-end is paused. This will avoid
    the Backend to be released (when another active front-end stream is
    present).
    Instead, put the stream state to PAUSED, which will keep the Back-end
    ON.
    
    Change-Id: Ic31bf90a09b605b91f6b9fe3c09361b1145581fb
    CRs-fixed: 428825
    Signed-off-by: Banajit Goswami <bgoswa@codeaurora.org>

commit b1229a99987096de8a67abf638f017b7fe5571f7
Author: Krishnankutty Kolathappilly <kkolat@codeaurora.org>
Date:   Sun Apr 7 18:04:18 2013 -0700

    ALSA: include: Add new ioctl for metadata mode
    
    Add a new ioctl to support metadata mode in lpa driver. This ioctl
    allows lpa driver to work in default alsa compliant mode and also
    metadata mode.
    
    CRs-Fixed: 458904
    Change-Id: I8111e8652bbfb6dd93bdbda69bd16f53b45cb756
    Signed-off-by: Krishnankutty Kolathappilly <kkolat@codeaurora.org>

commit e1700dec2a287752b21992d3a9385665276e8282
Author: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>
Date:   Tue Feb 19 23:26:17 2013 -0800

    ASoC: msm: Support independent left-right channel volume control
    
    The current volume sent to DSP is averaged between left and
    right channels. As a result with volume level as zero on one
    channel and non zero on the other channel results to playback on
    both channels. Add support to send left and right volumes to DSP
    
    Change-Id: Icc5f182d138c841607ea356ec37a7f78b4ba770d
    CRs-Fixed: 449284
    Signed-off-by: Subhash Chandra Bose Naripeddy <snariped@codeaurora.org>
    Signed-off-by: Jay Wang <jaywang@codeaurora.org>

commit 96958b16be1d5847c357a4c43d80f642bb653004
Author: Krishnankutty Kolathappilly <kkolat@codeaurora.org>
Date:   Thu Mar 14 09:59:16 2013 -0700

    ASoC: msm: Add metadata mode changes to LPA driver
    
    Playback hangs for three seconds on seeking to EOS. This is
    because the lpa driver works on period size. Partial buffers
    are not handled. Add metadata mode changes to LPA Driver to
    support partial frames.
    
    CRs-Fixed: 458904
    Change-Id: Ieb89b59dbca4b40ec0915a286d53c4617c23c471
    Signed-off-by: Krishnankutty Kolathappilly <kkolat@codeaurora.org>

commit 1fb95ee367c5f17d00e7849b6fc2cd6408bd11f8
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Wed Apr 17 16:11:31 2013 -0700

    gpu: ion: Use correct type for variables
    
    Use correct type for variables to prevent overflow of signed
    type when assiging from an unsigned type.
    
    Change-Id: I0055d35c7310d111de590db3798950fd98f1b298
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 5be877974769630a8bd195573a2882508cff45a5
Author: Neeti Desai <neetid@codeaurora.org>
Date:   Fri Mar 8 17:29:53 2013 -0800

    msm: Add support for ION Flushing without virtual address
    
    Video has very large buffers to be flushed. Due to lack of
    vmalloc space, ion_map_kernel cannot be called on each of
    the video buffers. With this change the ion handle can be
    flushed without the need of the kernel mapping.
    
    Change-Id: If026f21e44a2cce6c2b8c232fc80a69d0dabcd14
    Signed-off-by: Neeti Desai <neetid@codeaurora.org>
    
    Conflicts:
    
    	drivers/gpu/ion/ion_cp_heap.c
    	drivers/gpu/ion/msm/msm_ion.c
    
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 157e9264c232b476a4ff496a4505717d5ec412b6
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Tue Apr 2 14:03:34 2013 -0700

    gpu: ion: Minimize allocation fallback delay
    
    When we allocate from iommu heap we first try to allocate 1M pages. If
    that fails we try 64K pages, which falls back to 4K pages. However,
    we don't want to incur too much overhead when allocating with fallbacks
    so we don't want the higher order page allocation to retry, perform
    reclaim, or run memory compaction.
    
    Configure the GFP flags to ensure that when we allocate pages greater
    than order 0 we don't try to do any retries, reclaim, access emergency
    pools, or run memory compaction. This will ensure lower memory allocation
    latency for applications.
    
    CRs-Fixed: 470389
    Change-Id: Ibb3483dddbedbc733a1f7968821e7bc47bedffcd
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 76c87c52d2b4289667cb5c8443cc3899e3a4efec
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Wed Feb 20 13:55:46 2013 -0800

    gpu: ion: suppress excessive logging for expected failure cases
    
    We expect alloc_pages to fail for high-order pages under stressed
    memory conditions. It should then fall back to lower-order page
    allocations but due to excessive logging the system crashes. Use
    __GFP_NOWARN for high-order allocations to avoid excessive logging.
    
    (cherry picked from commit ff6ea62fe0687e754ddb64492e77fd75ad85fa02)
    
    CRs-Fixed: 454877
    Change-Id: Ie18c2dddd6810038abad9b06c7838ff4a0844b6d
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit 4f3c8f0474421d251bfb2330ebdaa533b9bdd156
Author: Praveen Chavan <pchava@codeaurora.org>
Date:   Mon Feb 11 18:46:23 2013 -0800

    gpu: ion: handle allocation error during heap protection in CP heap
    
    Return error and do not increment ref count in case of failure to
    protect the CP heap.
    CRs-Fixed: 445400
    
    Change-Id: Ia615773fe2c52386d9522de7a182edd633a66e1c
    Signed-off-by: Praveen Chavan <pchava@codeaurora.org>

commit 27a3f399268f6f4fe6b6a80dc20db9adc64bc8e3
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Fri Feb 1 18:30:14 2013 -0800

    gpu: ion: allocate huge pages in iommu heap
    
    As a performance optimization, use higher-order page allocations if
    possible. This can reduce TLB misses and the total number of page
    structures.
    
    CRs-Fixed: 449035
    Change-Id: I6b76ec69599a100fd7209161c4286284ae347be0
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit d07bd074fe52dd97266cbcb942a54b54e17a3734
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Thu Jan 31 10:30:40 2013 -0800

    gpu: ion: replace __GFP_ZERO with manual zero'ing
    
    As a performance optimization, omit the __GFP_ZERO flag when
    allocating individual pages and, instead, zero out all of the pages in
    one fell swoop.
    
    CRs-Fixed: 449035
    Change-Id: Ieb9a895d8792727a8a40b1e27cb1bbeae098f581
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>

commit 1579eeaae7c878c0499f9ecca8d1834b0e115146
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Tue Mar 19 17:37:50 2013 -0700

    gpu: ion: Align va address to biggest buffer size
    
    Align the virtual address to the max buffer size to
    allow IOMMU mappings at the biggest buffer size.
    
    Change-Id: I74ba665c1782e2f0631274766c5caeeb192224e0
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 0cb00c89a4c71782bab194532a34079dffc2ee20
Author: Olav Haugan <ohaugan@codeaurora.org>
Date:   Tue Mar 26 11:25:41 2013 -0700

    gpu: ion: Add null-pointer check after allocation
    
    Check for failed allocation and return appropritate
    return code if memory allocation fails.
    
    Change-Id: Iefb83b5e30c3cc581ccecaf6e59eae621be98083
    Signed-off-by: Olav Haugan <ohaugan@codeaurora.org>

commit 7e2649057187b96e3638b5629b5d4e6075802fad
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Feb 1 22:47:49 2013 +0530

    msm: Add indicator for separate metadata buffers support
    
    Add variable to check if separate metabuffers can be
    enabled in video driver for specific targets.
    
    Change-Id: I0ab3be7077d7d8286bfaa37be63705992939b72d
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 23ff5332dd234866a8a8b2a1b0854a06de68fdec
Author: Srinu Gorle <sgorle@codeaurora.org>
Date:   Mon May 13 20:05:04 2013 +0530

    msm: vidc: Do not reset VCD_FRAME_FLAG_DATACORRUPT flag for VC1
    
    - To propagate VCD_FRAME_FLAG_DATACORRUPT flag to IL client, dont reset.
    
    Change-Id: I68bf4c953c02a2a4327fe6e875863853cb4e0bf8
    CRs-Fixed: 472798
    Signed-off-by: Srinu Gorle <sgorle@codeaurora.org>

commit 3776697b82f8fcda1e843005a274d58b4bea0bac
Author: Shobhit Pandey <cshopan@codeaurora.org>
Date:   Mon May 6 19:18:42 2013 +0530

    msm: vidc: Fix compilation errors
    
    This change removes the compilation error in kernel
    if ddl logs are enabled.
    
    CRs-Fixed: 483776
    Change-Id: Iffc37fcdd4e5ce7ddcd8560d432af5c3f3e6c433
    Signed-off-by: Shobhit Pandey <cshopan@codeaurora.org>

commit c1a05fbf76af247688e1238090dc76bd2f3f2db7
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Mon May 6 17:01:53 2013 +0530

    msm: vidc: Update sub_anchor_mv buffer size
    
    The sub_anchor_mv size is not sufficient if the height
    is more than 1088 in video clips, this change will amend
    the buffer size to avoid video hardware resulting iommu
    crash failures for such clips.
    
    Change-Id: Ie59f04b9edaa9d2364d4e5014d2eb6f882728c76
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 4204a66f1713fd1fe5638da577057c095a07c6d8
Author: Shobhit Pandey <cshopan@codeaurora.org>
Date:   Thu Apr 25 15:45:19 2013 +0530

    msm: vidc: Correct log tagging for kernel messages
    
    This change amends the log tagging in kernel.
    
    CRs-fixed: 478607
    Change-Id: I2c7056d8fc9ec82ffbc6824790c064033481a9af
    Signed-off-by: Shobhit Pandey <cshopan@codeaurora.org>

commit 857070c77c2dea023f4bedb5988c74333e4662a6
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Thu Apr 25 09:46:53 2013 +0530

    msm: vidc: Free buffer pool entry and then delete address table
    
    Free the buffer pool entry first and then delete the
    address table (which will unmap the buffer with iommu)
    else we might get a corner case where the entry was not
    freed but the buffer is unmapped and the next buffer is
    mapped with the same physical address which is still
    available in the buffer pool entry, which results in
    video recording failure.
    
    Change-Id: I6978d5e5de35db63f43a7f38c58940216217b676
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 89cee5f131f6715ead1d6a8366dda91be156e293
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Tue Sep 11 19:44:53 2012 -0400

    msm: vidc: Memset and flush codec context buffers
    
    This change will memset and flush the codec context
    buffers for non-secure case to resolve video driver
    timer expire issues.
    
    Change-Id: I6669481da3076a392061868475fc9844f2c41064
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 5f7a845bac19a17b25f731471b8eb8fbc7a792e9
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Mon Apr 22 16:28:45 2013 +0530

    msm: vidc: Fix metadata buffer size issue
    
    - Metadata buffer size is being set to zero when
      client calls set buffer requirements to video driver.
      This change fix it by properly assigning the
      metadata buffer size to video driver.
    - When continuous mode enabled, do not get min_dpb
      from resource tracker for non-H264 codecs,
      for H264 codec get min_dpb from resource tracker.
    
    Change-Id: Ie16bc47605edc6f67dbf3ae77290d4d764c5c613
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit f252194475fed6cbb5633688fd99ea5238b3c831
Author: Rajeshwar Kurapaty <rkurapat@codeaurora.org>
Date:   Wed Mar 27 16:30:27 2013 +0530

    msm: vidc: Print clip information in the logs
    
    This change prints the required information of
    the clip being played.
    
    CRs-Fixed: 459477
    Change-Id: I2fd2271cbb540e5d57d82cfcbd83cf91f7c55c93
    Signed-off-by: Rajeshwar Kurapaty <rkurapat@codeaurora.org>

commit f59e5cef715d4ff7dd9e0918568d08efc41f1529
Author: Gopikrishnaiah Anandan <agopik@codeaurora.org>
Date:   Fri Aug 17 17:15:02 2012 -0400

    vidc: Handle descriptor buffer error as warning
    
    Driver will consider descriptor buffer empty error
    as warning and will continue to decode next frame.
    
    Change-Id: Ia89ea520131f9b3e1bbe68727c34fb72685d5af9
    Signed-off-by: Gopikrishnaiah Anandan <agopik@codeaurora.org>

commit 0dad38096a445431b81769490975518d02518638
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Feb 22 01:45:30 2013 +0530

    msm: vidc: Fix klocwork error
    
    This change fixes the klocwork errors
    
    Change-Id: I73a6ed0e568e98a51c8e095b824a783f91ce6657
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit d3950da635ec9bb0a6017e8afaf3a2f490564c3f
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Wed Feb 20 16:03:29 2013 +0530

    msm: vidc: Reduce priority of unwanted logs
    
    Change reduces the priority of unwanted logs
    which are coming in kernel logs by default
    in video driver. Now these logs will come
    only when enabled.
    
    Change-Id: I18a534c7aaf8a2819be1b8812fad9e0710c49bd3
    CRs-fixed: 454273
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 0912d2bdaa5e661d1ce5d483a582f2df797eed34
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Tue Feb 12 16:55:21 2013 +0530

    msm: vidc: Add support for NV21 color format encoding
    
    This change adds NV21 color format encoding support.
    
    CRs-fixed: 448511
    Change-Id: If26f1241a006e2d325668b8e189f6ff76df8e125
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit a9dc799e5fb577319f18d3de0a28d1b23eae3701
Author: Shobhit Pandey <cshopan@codeaurora.org>
Date:   Wed Feb 13 09:23:21 2013 +0530

    msm: vidc: Move metadata shared input mem to firmware heap.
    
    Metadata shared input is used to communicate between the
    video core and video driver. Allocate it from firmware
    heap for specific targets as MFC command heap size is
    limited.
    
    CRs-Fixed: 450118
    Change-Id: I3d04cf6dd498ae67adcec7a8cba513bd7932d158
    Signed-off-by: Shobhit Pandey <cshopan@codeaurora.org>
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit f3487682b2dc35f01d0597712af18581cab61b1f
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Sat Feb 2 00:01:05 2013 +0530

    msm: vidc: Add a check for separate metadata buffers
    
    Added a check for separate metadata buffers based on
    the board specific platform data.
    
    Change-Id: I24d51329d93215da75d115ccfa8fc1f3fe260b7f
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 2a8b0080981c4e81ddf40665a87e7a87dca71974
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Thu Nov 1 06:29:56 2012 +0530

    Vidc: Treat Non-IDR frame type error as bit stream error
    
    Hardware error is being generated from driver when video
    core returns NON IDR frame type error during thumbnail
    decoding. This change handles this error by treating
    it as BIT_STREAM_ERR.
    
    Change-Id: I16a19eea679d42a22b336d6ebbd0bd1f57c0a8a4
    CRs-fixed: 414858
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 8bcbd8c1ed3cd0dad225d6a08599de5d365f5608
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Fri Sep 14 16:44:00 2012 +0530

    msm: vidc: Correct the display size of small resolution clips
    
    We change the frame height in the decoder to MDP_MIN_TILE_HEIGHT
    for the videos with frame height less than 96. Due to which these
    videos were not being displayed properly. This change fixes the
    display change.
    
    Change-Id: I7acc74742a8cfbdfc79b02bd20bb9109f4389fa2
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 4711f5dd7cb7f60805511bc1601947c622f79456
Author: Rajeshwar Kurapaty <rkurapat@codeaurora.org>
Date:   Fri Jan 4 16:41:11 2013 -0800

    msm: vidc: Fix various NULL pointer accesses, memory leaks.
    
    This commit fixes various NULL pointer and memory leak bugs.
    
    Change-Id: I3b3ac40b5e440b9665858913921a06de14741b6f
    Signed-off-by: Rajeshwar Kurapaty <rkurapat@codeaurora.org>

commit 5b16f89599d09629381d7bb2b3be59cb4142656c
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Mon Jan 21 12:52:06 2013 +0530

    msm:vidc: Initialize meta buffer header during secure session
    
    Meta buffer header initialization is required if we process
    extradata in secure playback.
    
    Change-Id: Ic8a068e99351f2c970643200d03b1a580ead7480
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit 727032fadd2a93c3765963e3b6ee9022ec1db932
Author: Praveen Chavan <pchava@codeaurora.org>
Date:   Mon Nov 26 16:53:41 2012 -0800

    msm: vidc: unsecure heaps in the right order during error handling
    
    Unsecure MM and CP heaps in the right order in case of failure
    CRs-Fixed: 445400
    
    Change-Id: I2b5d414c5d9f109740a931b75b0cfbdb196c465f
    Signed-off-by: Praveen Chavan <pchava@codeaurora.org>

commit c6f66146afac271cb9a9fc51eea03aa6945536c9
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Fri Feb 15 19:06:49 2013 +0530

    msm: vidc: Fix iommu pagefault in concurrent scenario
    
    IOMMU pagefault is seen while running decoder and
    encoder concurrently for specific codec combination.
    Map double the buffer size to ion_map_iommu() API for
    encoder reconstruction buffers to avoid the pagefault.
    
    Change-Id: Ib5546a42a75c4c628767310f1a291e7ed443056c
    CRs-fixed: 450621
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit f5e275b0ce0dd4672af166a8bd436840e77fa0a2
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Wed Apr 17 16:47:22 2013 +0530

    msm: vidc: Remove kernel mapping on input/output buffers
    
    This change will remove the kernel mapping of input and
    output buffers for both video encoder and decoder to
    avoid the errors resulting from ion_map_kernel() for
    high resolution video concurrency use cases due to the
    limited vmalloc space. It also removed the metadata
    processing in kerner video driver as kernel virtual
    address is removed. The metadata processing can be
    done in user space video component.
    
    Change-Id: I3f2c9b7c13b3e09097ce07ca7b59154b97401052
    CRs-fixed: 471135
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 6ace24c425e4571c0a2140cba51618053cd11602
Author: Maheshwar Ajja <majja@codeaurora.org>
Date:   Tue Apr 2 19:20:44 2013 +0530

    msm: vidc: Remove delayed unmap flag in ion_map_iommu()
    
    This change fixes the iommu map failure in movie studio
    application by removing the delayed unmap flag in
    ion_map_iommu(). The delayed unmap will cause the
    iommu buffer to be unmapped only when all the clients
    unmap the buffer.
    
    Change-Id: I9e226fd56fecfa292e4d77aa94b2883bcfbf6ec2
    CRs-fixed: 464374
    Signed-off-by: Maheshwar Ajja <majja@codeaurora.org>

commit 18c17cb1ad004cb29b5cee8770d7a8dfd8c232de
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Tue Mar 26 19:38:37 2013 +0530

    msm: vidc: Free meta buffers during address table cleanup
    
    This change frees the allocated meta buffers, if any
    present, during video core address table cleanup.
    
    Change-Id: Id8c42491040bc5be09f3bb59780d376e6921c76e
    CRs-fixed: 459980
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit bd6f96a870e3269f0f48bd1a708c8f434152f9b4
Author: Deepak Verma <dverma@codeaurora.org>
Date:   Fri Jan 25 11:57:52 2013 +0530

    msm: vidc: Separate meta buffers support in secure mode
    
    Extradata is appended at the end of each output buffer
    in non secure video use case but in secure video playback,
    the client/CPU don't have access to the output buffer
    to parse the extradata. This change allows the client/CPU
    to parse the extradata by allocating separate buffers for
    video hardware to store extradata.
    
    Change-Id: I12927ea3d142b9cecd6fb1ae1086c5624d0e08d6
    Signed-off-by: Deepak Verma <dverma@codeaurora.org>

commit c3c659c342443cfbe4548e67dc6b34aa56c2cfde
Merge: 1173eb7 629fb78
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Tue Jun 18 16:47:35 2013 -0700

    Merge "msm_serial_hs: Squashed commit of updates from CAF" into cm-10.1

commit 1173eb767fbbd9a549e462df85229b2bde941483
Author: Mike Kasick <mike@kasick.org>
Date:   Sat Jul 7 19:00:47 2012 -0400

    Add and enable kexec hardboot support.
    
    Consists of squashed commits from:
    https://github.com/mkasick/android_kernel_samsung_jfltespr.git
    
    commit 750bb80f2854d6af5273e55ea179a4c60b2d9efc
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sun Jun 2 23:11:24 2013 -0400
    
        Clear download mode flag on kexec hardboot.
    
    commit 138ba851ee949af291eae914f410aea2d85ed9f5
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sun May 12 22:39:26 2013 -0400
    
        Support hard booting to a kexec kernel.
    
        See KEXEC_HARDBOOT config option help for details.
    
    commit 3ab41019a7af08395c233bebb605c2f1ea49c8e0
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sat Jul 7 23:10:24 2012 -0400
    
        Support copying of kernel tagged list (atags) in the decompressor.
    
        This is needed to hardboot kexec a kernel with a new tags list (including a
        new kernel command line), since the new atags would otherwise be lost with
        the limited kernel memory mapping.  Without this patch, a hardboot-kexec'd
        kernel uses the atags provided by the bootloader.
    
    commit 446c3d7a6631877f7dc9142359cf0eae300e9a08
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sat Jul 7 23:09:39 2012 -0400
    
        Enable caching and buffering for all of physical RAM in the decompressor.
    
        Old method is to enable caching and buffering only for the 256 MB at the
        start of the decompressor image.  This makes a hardboot-kexec'd kernel very
        slow to decompress since the decompressor is located far above the kernel
        destination.  This patch reduces boot time from 35 to 8 seconds on epicmtd.
    
    commit d3646be88f5b1f9a8ae714c922ec50c681b7157f
    Author: Mike Kasick <mike@kasick.org>
    Date:   Sat Jul 7 19:00:47 2012 -0400
    
        Enable and fix kexec syscall support.
    
        Use mem_text_write_kernel_word when assigning reboot_code_buffer parameters
        to avoid protection faults (writes to read-only kernel memory) when
        CONFIG_STRICT_MEMORY_RWX is enabled.
    
    Change-Id: I0400ad00fcf02b8ad017f8aa371724a659b930b4

commit 7134c840d83c9ffdbd2f11eae90f7668ee74ea29
Author: Tommi Rantala <tt.rantala@gmail.com>
Date:   Sat Apr 13 19:49:14 2013 +0000

    perf: Treat attr.config as u64 in perf_swevent_init()
    
    Trinity discovered that we fail to check all 64 bits of
    attr.config passed by user space, resulting to out-of-bounds
    access of the perf_swevent_enabled array in
    sw_perf_event_destroy().
    
    Introduced in commit b0a873ebb ("perf: Register PMU
    implementations").
    
    Signed-off-by: Tommi Rantala <tt.rantala@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: davej@redhat.com
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Link: http://lkml.kernel.org/r/1365882554-30259-1-git-send-email-tt.rantala@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Change-Id: I117496399c6bbd94faaef5912d18bdb7f44bcc67

commit 629fb7851b95da74f7b7c71ab029649336543964
Author: Saket Saurabh <ssaurabh@codeaurora.org>
Date:   Mon Oct 29 19:51:28 2012 +0530

    msm_serial_hs: Squashed commit of updates from CAF
    
    msm_serial_hs : handle uart_flush_buffer
    
    Serial core allocates circular buffer for uart tx transfer.
    Circular buffer head and tail is updated in serial core and
    circular buffer tail is updated in uart driver.While uart
    transfer is happening, uart client bluetooth hci_ldisc,calls
    uart_flush_buffer in serial core and sets circular buffer
    head and tail to zero.As uart driver does not know when
    uart_flush_buffer is called in serial core by uart client,it
    updates circular buffer tail for the previous ADM Tx completion.
    This leads to queueing Tx command to ADM although no data is
    there in circular buffer.Because of this,uart client is not
    functional.
    
    Hence adding msm_hs_flush_buffer api which notifies to uart driver
    that uart_flush_buffer is called in serial core by uart client and
    set tty_flush_receive flag to true.In uart interrupt handler ,do
    not update circular buffer tail if uart_flush_buffer is already
    called in serial core by uart client else update circular buffer
    tail on ADM Tx completion.
    
    (cherry picked from commit ce39410ad3db5b9fb446bf5da126585d58fa872a)
    
    Change-Id: I9bce30da218f42739c175d9b3c283ae39b6b5c89
    CRs-Fixed: 419054
    Signed-off-by: Saket Saurabh <ssaurabh@codeaurora.org>
    
    msm_serial_hs: Add support to handle Rx BREAK condition
    
    Currently Rx Break Error condition is not checked using UART_DM_SR
    register. Hence by default null-character is being inserted on
    receiving Rx Error or Rx Break condition. Application sets termios
    c_iflag if it needs any notification related to any Rx Error or
    Break condition. Hence add support to handle Rx Break condition and
    insert null-character conditionally based on termios.c_iflag.
    
    CRs-Fixed: 412201
    Change-Id: I7f496e42757b949f597d5dbe5c1f1ca88ee53c8c
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>
    
    msm_serial_hs: consider flush buffer request only when DMA is in progress
    
    TTY Core uses flush_buffer to clear out any output data before closing
    port, whereas UART application can always request to do flush output data
    before starting data transfer using tty supported ioctl().
    receive_tty_flush is used to make decision on completion of any DMA Tx
    data transfer. Using same approach when there is no DMA Tx transfer is
    happening, it is breaking next DMA Tx transfer functionality. Hence
    update receive_tty_flush only when there is any DMA TX transfer active.
    
    (cherry picked from commit 53069734628ebce341cf1ef778c91dbcbbcf67db)
    
    Change-Id: I26f4ceece422674e1f7647e5dfadb476cec1ef92
    CRs-Fixed: 438057
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>
    
    msm_serial_hs: Wait for discard flush completion for UART Rx channel
    
    Completion of requested flush command with ADM driver is having more
    latency then previously. Hence now it is required to wait for discard
    flush complete if there is more events expected with UART driver which
    would go out of sync without it. Below are 2 instances where it is must
    to wait for discard flush completion requested on UART Rx channel.
    
    1. Changing Baud Rate of UART
    UART application can send baud rate change request based on its requirement
    of communication with connected device on remote uart. Serial core also
    does set by default baud rate when application is opening the uart port. As
    Rx command is queued always with ADM driver from UART driver, for above
    events it is required to flush the same after setting the baud rate. Not
    waiting for completion of Rx flush would allow application to send command
    on Tx or any other ioctl which would reach to connected device but response
    may not be received as Rx flush completion is not received to queue next Rx
    command to ADM. Hence with this there are chances that received data with
    UART wil be lost when Rx flush completion is not received in-time. Hence
    wait for Rx flush completion from set_termios() with timeout as 300 jiffie.
    Rx flush completion time is non-deterministic as it depends on number of
    commands queued to ADM driver from ADM client drivers.
    
    2. While going for UART clock off
    UART clock off has multiple state machines and before going into last
    state it makes sure that there are no pending data in UART Tx and Rx FIFO.
    Although there would be one Rx command queued which is flushed before
    moving to last state and going ahead with UART clock off. If UART clock is
    turned off before Rx flush request is being executed and ADM tried to flush
    Rx command, ADM encounters data bus error on UART Rx Channel. Hence with
    waiting for Rx flush completion, and then doing UART clock off on receving
    the same would resolve ADM data bus error issue on UART Rx channel.
    
    CRs-Fixed: 457769
    Change-Id: I9344277224a2dc3f28140a1f42aa3feed5411b97
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>
    
    msm_serial_hs: Donot register UART device as PM runtime active
    
    Currently UART device is being registered as PM runtime active
    in probe() and set as suspended in shutdown(). With this UART
    driver PM runtime suspend is being called when system suspend
    is invoked and while resuming from the same which invokes UART
    driver LPM functionality. It causes UART communication to stop
    if both UART Tx and RX FIFO are empty.
    
    This change donot register UART device as PM runtime active
    device as UART driver doesn't use PM runtime APIs for its LPM
    functionality.
    
    Change-Id: I33d0c95e4227fdb1b7d6d514498e161e6e4df465
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>
    
    msm_serial_hs: Fix Tx path issues during uart shutdown
    
    UART queues tx transfer request to ADM and upon getting completion
    callback from ADM, schedules tx tasklet in which it enables the
    Tx Ready Interrupt and waits for it. On Tx ready interrupt tx
    dma_in_flight is set to zero indicating no more tx data is pending.
    
    This would cause corner case where if uart shutdown function checks
    dma_in_flight flag between dmov callback and Tx Ready interrupt,
    then this may cause uart shutdown function queuing dmov flush for
    the completed Tx command and with this dmov flush callback never
    be called. To fix this issue, move the dma_in_flight flag reset from
    Tx Ready interrupt to dmov callback.
    
    To avoid issues if any uart clients submitting any Tx requests after
    uart shutdown, add is_shutdown flag and check before tx command
    submission.
    
    CRs-Fixed: 476745
    Change-Id: I05314cf2f8361109302dbc0ddcfb834e9034eab5
    Signed-off-by: Saket Saurabh <ssaurabh@codeaurora.org>
    Signed-off-by: Prasad Sodagudi <psodagud@codeaurora.org>
    
    Conflicts:
    	drivers/tty/serial/msm_serial_hs.c
    
    msm_serial_hs: Fix UART Rx dmov command stall issues
    
    Resetting Rx and Tx HW state machine while Rx dmov command queued
    with ADM, caused dmov stall issues. Hence moving the Rx and Tx reset
    to after dmov_flush. In order to do this, change the Rx path to not
    enqueue dmov request as a part of dmov flush when flush is
    FLUSH_IGNORE.
    
    Removing the unnecessary UART Rx and Tx HW Reset in
    msm_hs_set_bps_locked().
    
    Require to add few clock cycles after Rx and Tx HW reset, hence
    adding 10us delay as safe side. In addition to this, combining
    both Rx and Tx reset in single register write.
    
    CRs-Fixed: 481244
    Change-Id: Ib33feb55ab42b6d30ce19db945fed7496b78debf
    Signed-off-by: Saket Saurabh <ssaurabh@codeaurora.org>
    
    msm_serial_hs: Fix synchronization between ADM and UART Cores
    
    With current UART Core programming sequence, it is seen that ADM
    and UART Core are going out of sync in few usecases with regression.
    It is fixed by using Force RxStale feature of UART Core from
    set_termios() and sending discard flush to ADM which would terminate
    currently queued UART RX CMD to ADM. Add termios_in_progress check
    to make decision about what kind flush to send on receiving RxStale
    interrupt. It makes set_termios() more reboust and also keeps ADM
    and UART Core in-sync.
    
    Add dma_in_flight to keep check of UART Rx CMD status and BUG_ON if
    UART driver is trying to queue one more UART RX CMD to ADM as UART
    driver re-uses same UART RX CMD.
    
    This change also adds check of currently set uartclk rate and next
    required uartclk rate for new baud rate. If both are same it would
    not call clk_set_rate() to set same uartclk rate again.
    
    UART baud rate change request (i.e. executing set_termios()) can come
    asynchronously and time to program ADM hardware with queued CMD by ADM
    driver is non-deterministic. It is required by UART driver to confirm
    that queued RX CMD is programmed with ADM hardware before going with
    requested baud rate change operation. Hence this change wait in
    set_termios() for ADM driver to program queued RX CMD before going ahead
    with baud rate. Otherwise ADM may encounter data bus error with UART RX
    channel which is unrecoverable.
    
    Change-Id: I678e91420b33a49ce8b6a5a29c6309f9ff6f5c4a
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>
    
    msm_serial_hs: Disable UART RxStale before Force RxStale in set_termios
    
    There is possible race for handling normal RxStale event and Force
    Rxstale event with UART Core, when Force RxStale is issued from
    set_termios(). Hence on baud rate change request disable RxStale event
    mechanism, and invoke Force RxStale. With RxStale event mechanism
    disabled, Force RxStale still generates interrupt.
    
    Fix possible race condition if before completion of previous UART RX CMD
    flush, set_termios() request is received. With this case it is quite
    possible that rx_tlet() queues one UART RX CMD with ADM driver and also
    set_termios() queues one more UART RX CMD which would results with
    queueing same UART RX CMD twice with ADM driver. With this device would
    crash as the ADM driver will not be able to de-reference the second UART
    RX CMD work. Hence add check in set_termios with rx.flush value as
    FLUSH_DATA_READY which confirms that flush is initiated but it is not
    completed and wait for rx.flush to become FLUSH_NONE which would happen
    only once flush is completed and new UART RX CMD is being queued with ADM
    driver.
    
    Reduce waiting time for discard flush completion as it is already waited
    for queued UART Rx CMD to be programmed with ADM hardware. Hence change
    wait time out from 300 jiffies to 50 mseconds.
    
    Add an API to get important UART core registers values which can be called
    from error condition. Use the same when discard flush timeout happens with
    set_termios(), when rx.dma_in_flight is not set with set_termios() and when
    multiple UART RX CMD queueing condition is seen in msm_hs_start_rx_locked()
    API.
    
    CRs-Fixed: 486260
    Change-Id: I9fd17e453fad619609e76e8b3a72a4f47210cff9
    Signed-off-by: Mayank Rana <mrana@codeaurora.org>

commit 6d317efac4f1dcc9db07982039b092a32cd7c3ae
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Sun Jun 9 18:17:49 2013 -0500

    jf: JC: allow non samsung app to update firmware
    
    insure stability
    Change-Id: I5eb80edbcdfe48a86668461d781decd5bedef027

commit ea65ad1445fe806f08233b1ed500df875e181c6b
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Fri Jun 7 16:17:50 2013 -0500

    mdp: update to me4.
    
    Conflicts:
    	arch/arm/kernel/process.c
    	arch/arm/mach-msm/msm_bus/msm_bus_board_8064.c
    	arch/arm/mach-msm/msm_bus/msm_bus_core.h
    	drivers/video/msm/mdp.c
    	drivers/video/msm/mdp.h
    	drivers/video/msm/mdp4_overlay.c
    	drivers/video/msm/mdp4_overlay_dsi_cmd.c
    	drivers/video/msm/mdp4_overlay_dsi_video.c
    	drivers/video/msm/mdp4_overlay_dtv.c
    	drivers/video/msm/msm_fb.c
    	include/linux/msm_ion.h
    
    Change-Id: I53986dd9758c7ac7ea64ab5a336b9e51645dedc0

commit 6ff53660c7c280ca73553b868a178cab144a548c
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Fri Jun 7 16:17:50 2013 -0500

    me4 upddate
    
    * revert video/msm to current state
    
    Conflicts:
    	arch/arm/kernel/process.c
    	arch/arm/mach-msm/msm_bus/msm_bus_board_8064.c
    	arch/arm/mach-msm/msm_bus/msm_bus_core.h
    	drivers/video/msm/mdp.c
    	drivers/video/msm/mdp.h
    	drivers/video/msm/mdp4_overlay.c
    	drivers/video/msm/mdp4_overlay_dsi_cmd.c
    	drivers/video/msm/mdp4_overlay_dsi_video.c
    	drivers/video/msm/mdp4_overlay_dtv.c
    	drivers/video/msm/msm_fb.c
    	include/linux/msm_ion.h
    Change-Id: I0686ef4ee1fe9b34ac9beaaa2bd43555234f5f6d

commit 0b40c76da24ad01e2d70f6d05c9c2cb0fd566f64
Author: Shareef Ali <shareefalis@gmail.com>
Date:   Fri Jun 7 01:29:44 2013 -0500

    jf: update to MDM source drop
    
    Change-Id: I6d6f527c306b0ed4ca996d91d0bcff8ea80314cc

commit e40ee662b0ce81513e62fb48d4609d5e75e84803
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Thu Jun 6 23:44:13 2013 -0500

    jf: update to MDL source drop
    
    Conflicts:
    	drivers/video/msm/msm_fb.c
    
    Change-Id: Iaef87dd70eda0f6d3260331714518f6e6cc0a0d8

commit 45ebc5752d461c3eb90deb6dba9862d91b37b089
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 3 00:38:50 2013 -0500

    ARM: mutex: use generic atomic_dec-based implementation for ARMv6+
    
    Commit a76d7bd96d65 ("ARM: 7467/1: mutex: use generic xchg-based
    implementation for ARMv6+") removed the barrier-less, ARM-specific
    mutex implementation in favour of the generic xchg-based code.
    
    Since then, a bug was uncovered in the xchg code when running on SMP
    platforms, due to interactions between the locking paths and the
    MUTEX_SPIN_ON_OWNER code. This was fixed in 0bce9c46bf3b ("mutex: place
    lock in contended state after fastpath_lock failure"), however, the
    atomic_dec-based mutex algorithm is now marginally more efficient for
    ARM (~0.5% improvement in hackbench scores on dual A15).
    
    This patch moves ARMv6+ platforms to the atomic_dec-based mutex code.
    
    Change-Id: I5cb600b15f891dc692356322b38cae1d692a084c
    Cc: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 1297c7d5bdc1d232784409010b5122d4629909da
Merge: 918eab9 2d91185
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Sun Jun 2 22:31:09 2013 -0700

    Merge "board-8064: pmic: enable RTC write" into cm-10.1

commit 918eab9bfe7321ea6048e1a02404bf6cdc808e90
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Fri May 3 08:38:48 2013 -0500

    jf: add support for panel color shifting based on a sysfs interface
    
    Adapted from work by morfic and ktoonsez
    
    Creates a syfs interface here:
      /sys/devices/virtual/sec/tsp/panel_colors
    
    Will accept 0-4 for parameters:
      0 - Cold (more blues)
      1 - Cool
      2 - Normal (stock colors)
      3 - Warm
      4 - Hot (more reds)
    
    Change-Id: I5cc6853d6a33108ece8d0c22fd1866d9fe104efc

commit 2d91185cf0c381d6ddfad50798efb7f9d3ec5e85
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Thu May 30 18:13:57 2013 -0500

    board-8064: pmic: enable RTC write
    
    Change-Id: I8772a6844d3079cf65c769b5429649bae4720aad

commit a79789bb6bb6c214e668593ce630e62cb656e41d
Author: Shareef Ali <shareefalis@cyanogenmod.org>
Date:   Thu May 30 00:44:01 2013 -0500

    jf: compress kernel with xz compression
    
    Change-Id: I82153602b47f70d5cea4757ab765e7b2b9cdce7e
    1

commit 8918b137699c34938117c2e0743764dce43531ab
Author: David Ferguson <ferguson.david@gmail.com>
Date:   Mon May 27 01:29:43 2013 -0500

    fix compiling on Mac with non-GZIP kernel compressions
    
      * Non-GZIP compressions do not include the size in the file. Makefile.lib
        uses the stat command to determine the size of the file and then append
        the file size to the end of the image.
      * On BSD systems like Mac, stat does not recognize -c "%s" instead it
        needs -f "%z". This resulted in a non-booting kernel since the appended
        file size was either corrupt or 0.
      * Run both forms of stat, using whichever does not error out.
      * Allow the 2nd form to print to stderr if something goes wrong to help
        with debug.
    
    Change-Id: I3295d4f59a3f43af476f891aad47f7962ed34a3e

commit ba15d0bfbcd948391f14380ceea7c1767737e2a4
Author: Lynus Vaz <lvaz@codeaurora.org>
Date:   Tue May 7 14:02:33 2013 +0530

    msm: kgsl: Don't access context memory after it's freed
    
    Context memory was potentially accessed after being freed. Fix this
    by incrementing the context's refcount at strategic places.
    
    Change-Id: I31049bdd3eb9ac52160ca593619fcc8a32d4ddee
    Signed-off-by: Lynus Vaz <lvaz@codeaurora.org>

commit 94af02035e2afa7e28f1f6fcd1ad22a3fa15d7a8
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Mon May 13 15:11:55 2013 -0700

    sched: fix reference to wrong cfs_rq
    
    Commit 7db16c8c (sched: Fix SCHED_HRTICK bug leading to late preemption
    of tasks) introduced a bug in sched_slice() calculation by using wrong
    cfs_rq for tasks. rq->cfs was incorrectly used as task's cfs_rq, rather
    than the correct one to which they belonged.
    
    Fix the bug by using correct cfs_rq for tasks.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit c5a6a1be1f93259a439c0c97b9ab92579895e41f
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 14:42:57 2013 -0700

    drivers: staging: Fix Zcache
    
     * Remove dependency on obsolete qcache
     * Remove X86 dependency as this version of zsmalloc supports ARM
     * Fix args for updated zsmalloc
     * Enable for JF
    
    Change-Id: I7a6daf84246df54bb8030268d07a96256db4fc42

commit 06b5417fb26654869c4feff5b854cd9d37f7e145
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 14:21:08 2013 -0700

    video: msm: Remove dead code
    
    Change-Id: If141888cc45a921c97ffd6647b08a1422424b50e

commit 55960c4c85f270bd3a8379293098c44bd507361c
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Mon Apr 1 14:06:36 2013 -0700

    msm_fb: display: keep a minimum mdp ib bandwidth request
    
    The calculated ab and ib bandwidth requests are very small which cause
    underruns if mdp composites only very small layers. For these use
    cases keeping a minimum ib request based on the baselayer size can
    prevent underruns and no power impact is observed.
    
    CRs-fixed: 472444
    Change-Id: I08e77610aafdfa84c5c2a882a0c95a51950aa069
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 4db5144160ea03f7d64fc4c7222bc3b13c2970c7
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 14:17:45 2013 -0700

    video: msm: Fix MDP bus bandwidth calculation
    
     * Set values for 8064 (JF) in board file
     * Remove redundant calculations
     * Use 64-bit unsigned long values where appropriate
    
    Change-Id: I185f077456f55805adec7d74d9845857f264a528

commit e57efb57ce58c2ffba16bed4eaf4fc0e611c22b8
Author: Praveen Chidambaram <pchidamb@codeaurora.org>
Date:   Wed Feb 20 17:48:49 2013 -0700

    msm: pm-data: Enable powercollapse/suspend_enabled for non-boot cpus
    
    Allow non-boot cpus to be hotplugged and power collapsed by default.
    This facilitates the cores to be offlined by kernel drivers during the
    boot process.
    
    CRs-Fixed: 460083
    Change-Id: If44b6b594504dab61dff662c581b60e6fb664936
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>

commit 728511636da99a6adc834c4b7d93e9eb39baf9f3
Author: Gagan Mac <gmac@codeaurora.org>
Date:   Mon Apr 1 21:10:51 2013 -0600

    msm: msm_bus: Fix the 64-bit division during interleaving
    
    When interleaving flag is enabled, the bandwidth is divided
    between ports. There was an error in the division as the
    macro was not correctly updated with the right order of using
    bandwidth and port number when 64-bit division function
    was used by the bus driver.
    Correcting this for fixing the token allocation issues
    observed.
    
    CRs-Fixed: 472791
    Change-Id: Ie7c7f9557006cbb946eaa9eff4b50771cc60525e
    Signed-off-by: Gagan Mac <gmac@codeaurora.org>

commit cd3408b47fe734efd9372bf48d3fb811adf3dbfc
Author: Gagan Mac <gmac@codeaurora.org>
Date:   Wed Mar 13 15:58:45 2013 -0600

    msm: msm_bus: Change MDP port interleaved setting
    
    When display driver requests bandwidth on MDP port 0, it was
    automatically divided between port 0 and port 1 by the bus driver
    as a part of interleaving. Since MDP port 1 was never used,
    this did not have any impact.
    
    Now, the display driver needs to use MDP port 1 such that MDP
    ports should be able to use the bandwidth non-uniformally.
    To enable these ports to use unequal amount of bandwidth,
    it is required that bus driver does not divide bandwidth
    between port 0 and port 1.
    The display team is now responsible for requesting bandwidth
    explicitly on both ports.
    
    CRs-Fixed: 474499
    Change-Id: I826a4c20a113a32f6d6bcf13f1c6474040f35f46
    Signed-off-by: Gagan Mac <gmac@codeaurora.org>
    (cherry-pick from Change-Id: I826a4c20a113a32f6d6bcf13f1c6474040f35f46)

commit ee19efc5f07ad3538c134f28ece5af1c96113453
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 01:54:04 2013 -0700

    jf: Update spm retention sequence to support per core retention
    
     * Merge SPM retention sequence change to JF boards to enable per-core
       retention mode.
    
    Change-Id: I2a7d4af749201e0b76670f3dde1e4050f75936dc

commit 2322256ba5d5417a496041274d4d8f4d81c18cb2
Author: Anji Jonnala <anjir@codeaurora.org>
Date:   Tue Jan 29 13:34:10 2013 +0530

    msm: pm: Update spm retention sequence to support per core retention
    
    On krait revision < 3, power driver is not using full spm sequence and
    calling into trustzone as a part of executing krait retention.
    Also krait retention is not supported on secondary cores.
    
    krait pass3 is upgraded to use full spm sequence, hence add support
    for per core retention and avoid calling into trustzone
    while executing retention.
    
    CRs-fixed: 437281
    Change-Id: I6a2b3d7c4d0fd5b9a340957b16b940b7a09bbd91
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit 38ff55a51fecba9ced5205e14ab712ad74becf27
Author: Anji Jonnala <anjir@codeaurora.org>
Date:   Sat Jan 19 11:22:25 2013 +0530

    board-8960: Update 8960AB and 8064AB SPM sequences.
    
    8960AB and 8064AB have a new revision of the Krait SPM hardware.
    The SPM v1.15x on these targets uses a different bit to control the
    apc_pdn signal.
    Update the SPM sequences for standalone and power collapse to use the
    new sequences.
    L2 SPM or Krait WFI/Retention sequences are not affected.
    
    CRs-fixed: 419258
    Change-Id: I3bcf335ac50cd1181047fe1ee1d56ecd507ae35c
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit 5758a8b5087e439d65835551d2112b7ca13f13cc
Author: Anji Jonnala <anjir@codeaurora.org>
Date:   Sat Mar 9 09:49:11 2013 +0530

    msm: avs: Restore core voltage when disabling AVS
    
    AVS changes the core voltage to match the current operating conditions
    on the part. Not all frequencies supported by the target have characterized
    AVS delay synthesizer (AVSDSCR) values. For frequencies that do not have
    AVSDSCR set, AVS is not enabled and so need to operate at the
    characterized voltage for that frequency. Disabling AVS does not
    however, restore the operating voltage to the value the rail was
    configured to. This may cause frequencies that do not have AVSDSCR to
    run at lower voltage than prescibed.
    Set up the core voltage back to the last known good value when disabling
    AVS.
    
    Change-Id: I5389996288fa49e45c469d2a8917e00900261de7
    CRs-fixed: 457772
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit daf586ac3439cbb26e17532da2e124e50f1ecf63
Author: Anji Jonnala <anjir@codeaurora.org>
Date:   Mon Apr 22 10:01:59 2013 +0530

    msm: hotplug: wait for hotplug completion event from secondary core
    
    Primary core has to wait for hotplug completion event from secondary
    core to avoid race between ongoing hotplug and already queued online
    event. This also prevent CPU_DEAD notifiers from executing before the
    SPM has completed the collapse of the CPU. CPU DEAD notifiers executing
    early have been attributed to MDD hardware block for the CPU being
    turned off prematurely, resulting in crashes.
    The patch also adds support for probing with device tree for 8974
    targets.
    
    Change-Id: Ibd35a7c56f42f156a643ca6d531509f019b2a3be
    CRs-fixed: 438314
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit 7d958c95c66522cf0b75a8d1586818d159634cb8
Author: Praveen Chidambaram <pchidamb@codeaurora.org>
Date:   Tue Feb 26 21:05:41 2013 -0700

    msm: pm-8x60: Setup memory for save/restore cpu registers early
    
    Cores can be offlined early in the boot cycle by certain kernel
    drivers.
    Ensure that the memory to save and restore the cpu registers is
    ready and available early during the boot procedure.
    
    CRs-Fixed: 460083
    Change-Id: I925372f066386ceb1e85184d4c58863835c3e88e
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>

commit 9acdbb33ab28df6853ee0daa43f9c860bf7c1585
Author: Priyanka Mathur <pmathur@codeaurora.org>
Date:   Wed Oct 3 15:15:01 2012 -0700

    msm: pm: Add support for retention on each core
    
    Allow each core on 8974 to enter retention and retain
    the state of the CPU. On 8960 as hardware restriction does
    not permit all cores to enter retention at the same time,
    allow only core 0 to enter retention when all other cores
    are offline to maintain backward compatibility.
    
    CRs-fixed: 437281
    Change-Id: I6690e901afc9fa3a27010c264fa3acc9a0722c1a
    Signed-off-by: Priyanka Mathur <pmathur@codeaurora.org>
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit 84d9dc1cb7cdd5a6e9f2396cce9470dfd892301c
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 01:20:04 2013 -0700

    jf: Add sleep status device
    
    Change-Id: Ie3a28f36bcdaf0b89d20776e9fba3cbd11813a7d

commit 744664afa73bceb9070318509a5be0520411f9b2
Author: Anji Jonnala <anjir@codeaurora.org>
Date:   Tue Apr 16 17:07:52 2013 +0530

    msm: pm: Add support to query cpu status
    
    During hotplug operation primary core is required to query
    cpu registers to determine whether secondary core has
    successfully shutdown.
    
    Change-Id: I81bd47ba851698e86269652afe381b0571ca912a
    Signed-off-by: Anji Jonnala <anjir@codeaurora.org>

commit a834e2d39baa351622ae1df0d148dd696b8440c1
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 01:18:02 2013 -0700

    cm: Update defconfigs
    
     * Enable recommended power options
    
    Change-Id: I8e3c266e29cb84cf45972d8c8503fe38d2e8e8d7

commit e334d59cb99ecee04c48ad91667f5cc390d33c2e
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Mon Dec 17 18:51:12 2012 -0800

    msm: SSR: Remove useless warning
    
    This is a warning so that developers know to add more restart
    order lists to SSR when a new chip is added. This is mostly
    irrelevant now because we assume either entire SoC restart on SSR
    or independent restart on SSR, not group restarts. Remove this
    warning as it is mostly a reminder that nobody is listening for.
    
    Change-Id: Icbf955cb18395d8d5d086b2167c5c329588b9256
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>

commit f78f8ea0015014d522f872151335e7727dd4c972
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Mon Feb 4 13:35:16 2013 -0800

    arm: Remove no-longer-required RCU_NONIDLE wrapper
    
    Commit 21111be8 "ARM: Fix negative idle stats for offline cpu" moved call
    to cpu_die() to occur outside of rcu_idle_enter()/rcu_idle_exit() section.
    As a result, the RCU_NONIDLE() wrapper to complete() call in
    arch/arm/kernel/process.c:cpu_die() is no longer required (and is
    technically incorrect to have). Removing RCU_NONIDLE() wrapper also removes
    this warning seen during CPU offline:
    
    [Note: Below message has been edited to fit 75-char per line limit.
    Insignificant portions of warning message has been removed in each line.]
    
    ------------[ cut here ]------------
    WARNING: at kernel/rcutree.c:456 rcu_idle_exit_common+0x4c/0xe0()
    Modules linked in:
    (unwind_backtrace+0x0/0x120) from (warn_slowpath_common+0x4c/0x64)
    (warn_slowpath_common+0x4c/0x64) from (warn_slowpath_null+0x18/0x1c)
    (warn_slowpath_null+0x18/0x1c) from (rcu_idle_exit_common+0x4c/0xe0)
    (rcu_idle_exit_common+0x4c/0xe0) from (rcu_idle_exit+0xa8/0xc0)
    (rcu_idle_exit+0xa8/0xc0) from (cpu_die+0x24/0x5c)
    (cpu_die+0x24/0x5c) from (cpu_idle+0xdc/0xf0)
    (cpu_idle+0xdc/0xf0) from (0x8160)
    ---[ end trace 61bf21937a496a37 ]---
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Change-Id: I721e6b20651674e6f6f584cf8d814af00b688c91

commit af2ae0ef2bc250a6a1b29b90882f54971736f1d6
Author: Taniya Das <tdas@codeaurora.org>
Date:   Tue Oct 30 15:28:20 2012 +0530

    ARM: Fix negative idle stats for offline cpu
    
    We see negative idle stats because of cpu dying without
    cleaning up idle entry statistics. When a cpu is offline,
    the most immediate thing you'd want to do is just call
    into cpu_die() and not waste time calling notifier(IDLE_START).
    
    CRs-Fixed: 414554
    Change-Id: Iadc6a3ca39997e0ccf65d2a29b004e24b1b211a1
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Signed-off-by: Taniya Das <tdas@codeaurora.org>

commit 44658fe6fc54fdb5aff6bd651ca09ddf5ff863b2
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Wed Nov 28 15:50:20 2012 -0800

    sched: fix rq->lock recursion
    
    Enabling SCHED_HRTICK currently results in rq->lock recursion and a hard
    hang at bootup.  Essentially try_to_wakeup() grabs rq->lock and tries
    arming a hrtimer via hrtimer_restart(), which deep down tries waking up
    ksoftirqd, which leads to a recursive call to try_to_wakeup() and thus
    attempt to take rq->lock recursively!!
    
    This is fixed by having scheduler queue hrtimer via
    __hrtimer_start_range_ns() which avoids waking up ksoftirqd.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Change-Id: I11a13be1d9db3a749614ccf3d4f5fb7bf6f18fa1

commit e640e5a4ba0e9891cd9f7a7cab3eb9df809b594c
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Sat Sep 22 13:55:30 2012 -0700

    rcu: Fix day-one dyntick-idle stall-warning bug
    
    Each grace period is supposed to have at least one callback waiting
    for that grace period to complete.  However, if CONFIG_NO_HZ=n, an
    extra callback-free grace period is no big problem -- it will chew up
    a tiny bit of CPU time, but it will complete normally.  In contrast,
    CONFIG_NO_HZ=y kernels have the potential for all the CPUs to go to
    sleep indefinitely, in turn indefinitely delaying completion of the
    callback-free grace period.  Given that nothing is waiting on this grace
    period, this is also not a problem.
    
    That is, unless RCU CPU stall warnings are also enabled, as they are
    in recent kernels.  In this case, if a CPU wakes up after at least one
    minute of inactivity, an RCU CPU stall warning will result.  The reason
    that no one noticed until quite recently is that most systems have enough
    OS noise that they will never remain absolutely idle for a full minute.
    But there are some embedded systems with cut-down userspace configurations
    that consistently get into this situation.
    
    All this begs the question of exactly how a callback-free grace period
    gets started in the first place.  This can happen due to the fact that
    CPUs do not necessarily agree on which grace period is in progress.
    If a CPU still believes that the grace period that just completed is
    still ongoing, it will believe that it has callbacks that need to wait for
    another grace period, never mind the fact that the grace period that they
    were waiting for just completed.  This CPU can therefore erroneously
    decide to start a new grace period.  Note that this can happen in
    TREE_RCU and TREE_PREEMPT_RCU even on a single-CPU system:  Deadlock
    considerations mean that the CPU that detected the end of the grace
    period is not necessarily officially informed of this fact for some time.
    
    Once this CPU notices that the earlier grace period completed, it will
    invoke its callbacks.  It then won't have any callbacks left.  If no
    other CPU has any callbacks, we now have a callback-free grace period.
    
    This commit therefore makes CPUs check more carefully before starting a
    new grace period.  This new check relies on an array of tail pointers
    into each CPU's list of callbacks.  If the CPU is up to date on which
    grace periods have completed, it checks to see if any callbacks follow
    the RCU_DONE_TAIL segment, otherwise it checks to see if any callbacks
    follow the RCU_WAIT_TAIL segment.  The reason that this works is that
    the RCU_WAIT_TAIL segment will be promoted to the RCU_DONE_TAIL segment
    as soon as the CPU is officially notified that the old grace period
    has ended.
    
    This change is to cpu_needs_another_gp(), which is called in a number
    of places.  The only one that really matters is in rcu_start_gp(), where
    the root rcu_node structure's ->lock is held, which prevents any
    other CPU from starting or completing a grace period, so that the
    comparison that determines whether the CPU is missing the completion
    of a grace period is stable.
    
    Change-Id: Ic77e2bbe865059aa6f363db284473e9adcf8797e
    Reported-by: Becky Bruce <bgillbruce@gmail.com>
    Reported-by: Subodh Nijsure <snijsure@grid-net.com>
    Reported-by: Paul Walmsley <paul@pwsan.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Paul Walmsley <paul@pwsan.com>  # OMAP3730, OMAP4430
    Cc: stable@vger.kernel.org
    Signed-off-by: Jin Hong <jinh@codeaurora.org>

commit 1288241240a63a5b959a105ccc1c50dcea04233a
Author: Venkat Devarasetty <vdevaras@codeaurora.org>
Date:   Mon Apr 29 21:50:57 2013 +0530

    msm: spm: set vdd directly for current cpu
    
    In cases where a cpu is setting vdd for itself we need not
    call msm_spm_smp_set_vdd through smp_call_function_single.
    
    Without this change when CPU calls for itself when interrupts
    are disabled we get following warning
    
    [ 38.407752] ------------[ cut here ]------------
    [ 38.407752] WARNING: at kernel/kernel/smp.c:320
    smp_call_function_single+0xa8/0x1c8()
    [ 38.407752] Modules linked in: adsprpc
    [ 38.407752] [<c00149b8>] (unwind_backtrace+0x0/0x11c) from [<c0082afc>]
    (warn_slowpath_common+0x4c/0x64)
    [ 38.407752] [<c0082afc>] (warn_slowpath_common+0x4c/0x64) from
    [<c0082b2c>] (warn_slowpath_null+0x18/0x1c)
    [ 38.407752] [<c0082b2c>] (warn_slowpath_null+0x18/0x1c) from
    [<c00c510c>] (smp_call_function_single+0xa8/0x1c8)
    [ 38.407752] [<c00c510c>] (smp_call_function_single+0xa8/0x1c8) from
    [<c0058f6c>] (msm_spm_set_vdd+0x48/0x70)
    [ 38.407752] [<c0058f6c>] (msm_spm_set_vdd+0x48/0x70) from [<c0056a98>]
    (msm_pm_power_collapse+0x13c/0x334)
    [ 38.407752] [<c0056a98>] (msm_pm_power_collapse+0x13c/0x334) from
    [<c0057120>] (msm_pm_enter+0x170/0x2dc)
    [ 38.407752] [<c0057120>] (msm_pm_enter+0x170/0x2dc) from [<c00b7580>]
    (suspend_devices_and_enter+0x1d0/0x334)
    [ 38.407752] [<c00b7580>] (suspend_devices_and_enter+0x1d0/0x334) from
    [<c00b7800>] (pm_suspend+0x11c/0x1f8)
    [ 38.407752] [<c00b7800>] (pm_suspend+0x11c/0x1f8) from [<c00b87a4>]
    (suspend+0x68/0x180)
    [ 38.407752] [<c00b87a4>] (suspend+0x68/0x180) from [<c009b050>]
    (process_one_work+0x280/0x488)
    [ 38.407752] [<c009b050>] (process_one_work+0x280/0x488) from
    [<c009b46c>] (worker_thread+0x214/0x3b4)
    [ 38.407752] [<c009b46c>] (worker_thread+0x214/0x3b4) from [<c009f364>]
    (kthread+0x84/0x90)
    [ 38.407752] [<c009f364>] (kthread+0x84/0x90) from [<c000f108>]
    (kernel_thread_exit+0x0/0x8)
    [ 38.407752] ---[ end trace da227214a82491ba ]---
    
    Change-Id: I1aa121f57d74860b6bc8620b40699567e55a9d02
    CRs-Fixed: 480408
    Signed-off-by: Venkat Devarasetty <vdevaras@codeaurora.org>

commit e0625ec383b0f98a7a1d579f7f2e233e5fa1bfe5
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Thu Apr 18 11:42:22 2013 -0700

    sched: Fix SCHED_HRTICK bug leading to late preemption of tasks
    
    SCHED_HRTICK feature is useful to preempt SCHED_FAIR tasks on-the-dot
    (just when they would have exceeded their ideal_runtime). It makes use
    of a a per-cpu hrtimer resource and hence alarming that hrtimer should
    be based on total SCHED_FAIR tasks a cpu has across its various cfs_rqs,
    rather than being based on number of tasks in a particular cfs_rq (as
    implemented currently). As a result, with current code, its possible for
    a running task (which is the sole task in its cfs_rq) to be preempted
    much after its ideal_runtime has elapsed, resulting in increased latency
    for tasks in other cfs_rq on same cpu.
    
    Fix this by alarming sched hrtimer based on total number of SCHED_FAIR
    tasks a CPU has across its various cfs_rqs.
    
    Change-Id: I1f23680a64872f8ce0f451ac4bcae28e8967918f
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit 4b98f78dfbe5f03e8b724c62fcdf9cef8c5d9ace
Author: Leonid Shatz <leonid.shatz@ravellosystems.com>
Date:   Mon Feb 4 14:33:37 2013 +0200

    hrtimer: Prevent hrtimer_enqueue_reprogram race
    
    hrtimer_enqueue_reprogram contains a race which could result in
    timer.base switch during unlock/lock sequence.
    
    hrtimer_enqueue_reprogram is releasing the lock protecting the timer
    base for calling raise_softirq_irqsoff() due to a lock ordering issue
    versus rq->lock.
    
    If during that time another CPU calls __hrtimer_start_range_ns() on
    the same hrtimer, the timer base might switch, before the current CPU
    can lock base->lock again and therefor the unlock_timer_base() call
    will unlock the wrong lock.
    
    [ tglx: Added comment and massaged changelog ]
    
    Change-Id: I327d862c28f24b77a62861764393c4bf660a7318
    Signed-off-by: Leonid Shatz <leonid.shatz@ravellosystems.com>
    Signed-off-by: Izik Eidus <izik.eidus@ravellosystems.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/1359981217-389-1-git-send-email-izik.eidus@ravellosystems.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Git-commit: b22affe0aef429d657bc6505aacb1c569340ddd2
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Taniya Das <tdas@codeaurora.org>

commit f03f55e44420762eb2b9e030bba336a5d365b0cc
Author: Srivatsa Vaddagiri <vatsa@codeaurora.org>
Date:   Thu Mar 7 12:14:53 2013 -0800

    sched: Reset rq->next_interval before going idle
    
    next_balance, the point in jiffy time scale when a cpu will next load
    balance, could have been calculated when the cpu was busy. A busy cpu
    will apply its sched domain's busy_factor (usually > 1) in computing
    next_balance for that sched domain, which causes the (busy) cpu to load
    balance less frequently in its sched domains. However when the same cpu
    is going idle, its next_balance needs to be reset without consideration
    of busy_factor. Failure to do so would not trigger nohz idle balancer on
    that cpu for unnecessarily long time (introducing additional scheduling
    latencies for tasks). Fix bug in scheduler which aims to reset
    next_balance before a cpu goes idle (as per existing comment) but is
    clearly not doing so.
    
    Change-Id: I7e027a51686528c4092d770c7d33c874d38f5df4
    Signed-off-by: Srivatsa Vaddagiri <vatsa@codeaurora.org>

commit fbe121640a473919b0cd4ba47f7321d155b1bee8
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue May 14 01:03:16 2013 -0700

    video: msm: Cleanup after merge
    
    Change-Id: I218a943b3b71b098f7afbf14fa9aca87058d06d6

commit a1f1729a7d3ec620fabaab054c867bab31ffdbf9
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Thu Mar 21 17:27:45 2013 -0700

    msm: display: provide mdp bandwidth parameters from board file
    
    Underrun issues can be solved by changing mdp bandwidth parameters:
    max_bw, ab_factor and ib_factor. However these parameters are not
    universally same for all targets, and one set of values that works for
    one target could cause underruns or higher power on other
    targets. This is board side change.
    
    CRs-Fixed: 464492
    Change-Id: I2a8fbb9018011ddf42dfa50b371e450e38f4351a
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 2fedbb1a5eb4b69b9658404b85cc38958eb8f0c8
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Wed Mar 20 16:31:27 2013 -0700

    msm_fb: display: fix iommu page fault when iommu buffer freed
    
    There has possibility that pipe commit come very close to next vsync.
    This may cause two consecutive pipe_commits happen within same vsync
    period which cause iommu page fault when previous iommu buffer is
    freed since mdp stil fetching it. Set ION_IOMMU_UNMAP_DELAYED flag
    at ion_map_iommu() to add delay un mapping iommu buffer to fix this problem.
    Otherwise four buffer mechanism is needed to avoid this problem..
    Also ion_unmap_iommu() may take long time to free an ion buffer. Move
    mdp4_overlay_iommu_unmap_freelist(mixer) called after stage_commit()
    to increase chance to have pipe_commit (up to stage_commit) is completed
    within same vsync period
    This patch also fix an potential race condition at wait_for_completion
    between show_event thread and wait4vsync thread.
    
    Change-Id: I673b0e368bbcd3120bc26569fe7a7bb82931daaa
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>
    (cherry picked from commit ada63be0fd32e729e4b91aeeea2a5cfe0830148c)
    (cherry picked from commit 799796f15e0b78ffe47975db1cf7f3791cb98129)

commit 3fb78008f59fcbc09089810c5576ad9fdc8cc2f0
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Thu Apr 25 11:55:44 2013 +0530

    msm_fb: Remove the extra freelist in display_commit
    
    There is an extra freelist call to unmap IOMMU source buffers
    in display_commit for LCDC interface. This is causing IOMMU
    page faults during boot up on targets with LCDC interfaces.
    Remove this extra freelist call.
    
    Change-Id: I12a905efc82c5b9751acbc5941a66d0b58027509
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit 8c7f1c4fb17b616ddf71c67157a215aa4aeaa4a1
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Thu Apr 18 18:17:36 2013 -0700

    msm_fb: display: free previous iommu buffer only at overlay_unset
    
    Since wait4dmap is used at pipe_commit, there is possibility that
    iommu buffer had been freed before vsync. Iommu page fault will
    happen under this situation. Therefore only previous played iommu
    buffer can be freed at overlay_unset. However all iommu buffers
    are freed at suspend.
    
    CRs-fixed: 440254
    Change-Id: I9c3b77de255b74fd608aa28bb0f2277a58e2ba97
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 268d0ec90056ec64f5621fdeb2e9da3ffedc40b1
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Tue Apr 23 12:03:06 2013 +0530

    msm_fb: display: Do a pre-fill to writeback buffers.
    
    When switching mdp operating mode to blt, writeback buffers need
    pre-fill. Move the pre-fill code from isr routine to do_blt, where
    actual switch to blt mode happens.
    
    CRs-fixed: 471098
    Change-Id: I0f845397894c749ac518a1fa605f84cd5fc2ad15
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit e8e90b803fec2f19f435efed7dbd06a965b00025
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Thu Apr 11 10:18:00 2013 -0700

    mms_fb: display: init pipe default configure as solidfill
    
    Initialize mdp pipes default config as solid fill so that
    there will have no memory fetch if pipe are staged into
    mixer while flush bits are left behind and set at next vsync.
    This is will prevent mdp iommu page fault from happening
    at address 0x00000000 during resume.
    
    Change-Id: Ide944ec1d069629f1ae2ecd0daf49c69ce2af2eb
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 511e76b4f83a2f4885ece6fadaecb5e0b85c2e88
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Fri Apr 5 11:28:36 2013 -0700

    msm_fb: display: issue completion when disabling vsync irq
    
    When vsync irq is disabled, vsync_wait_cnt is reset to zero and
    completion is issued out as they are done in isr to make sure all
    wait_for_completion is released.
    
    Change-Id: I689f0c8cde43404d0caf5bb4644efa6385798d0e
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit d9a31aed65a3e0fcd7adbc564704b7c0ef356ef2
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Wed Mar 13 13:37:56 2013 -0700

    msm_fb: display: add scaling factor for mdp ab
    
    system profiling found that mdp downscaling needs larger bandwidth
    that is related to mdp ab request, so adding scaling factor to mdp ab
    calculation reduces the chance of underruns.
    
    ab - scaling factors - port0 and port1 split
    ib - 1.5 x ab - no split and same on both ports
    
    Change-Id: I4fde87fce1cd292e317a8261e4700cf98662d09d
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 78f0b7b4c244a3913a2854614d14086b220e10f4
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Fri Mar 8 12:15:42 2013 -0800

    msm_fb: display: balance bus bandwidth on mdp axi port0 and port1
    
    mdp read and write clients are connected on both mdp port0 and port1,
    however, the bus bandwidth request is made on only port0. This change
    divides mdp bw requests to port0 and port1 based on pipe connections
    to mdp ports.
    ab - no scaling factor - port0 and port1 split
    ib - scaling factor - port 0 and port1 split
    
    Change-Id: I9050d1178f634ca908675fa826d08bee09e2a2f3
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 2d4290e925eb6f4816bcc7ffef15bd17bafe29d3
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Wed Mar 6 15:04:32 2013 -0800

    msm_fb: display: add mdp master port1 bandwidth request
    
    mdp bus traffic go through both mdp port0 and port1, but now mdp
    bandwidth request is only for port0.
    
    Change-Id: Ie902de6362cd9225abf6e9c42f260b5af7c3c6f8
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 411770232a05dfb78498dd6312791bbc173c6a37
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Tue Apr 2 09:11:44 2013 -0700

    msm_fb: display: add one more pipe_commit delay before free ion buffer
    
    It is possible that two consective pipe_commits happen within
    same vsync period.  This will cause iommu page fault due to an ion buffer
    that is being fetched by mdp's pipe is freed. It needs one pipe_commit
    delay to free ion buffer to prevent iommu page from happening. Therefore
    an extra free stage is needed to empty ion buffer free list.
    Also, ION_IOMMU_UNMAP_DELAYED flag is not necessary at mapping of an
    ion buffer since one pipe_commit delay had been added.
    
    Change-Id: I340e002e8dc30b6faa0c095ba042eab50e8b7c88
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 7994d5a33702e2fda676d1d8367ddf5f4bdfac8c
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Thu Mar 28 13:36:59 2013 -0700

    msm_fb: display: prepare clocks before enable AHB clock
    
    Clocks need to be prepared before enabled and unprepared after
    clock disabled.
    
    CRs-fixed: 466843
    Change-Id: I5180c98a2c5fd4bd867272ee1d2bf274ff15d215
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 3598e3e6c187b0a3ecc35a98760533b1c1e77e41
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Mon Mar 25 15:36:36 2013 -0400

    msm: display: vsync usage in turning off dsi video
    
    Add timeout to waiting for vsync so that it can come out
    when vsync interrupt is not working.
    Need not wait for vsync after timing engine is off.
    
    CRs-Fixed: 458419
    
    Change-Id: Ib989e435405613d891024ba15144723730454e34
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit a789f062c3377d893bed2888c2f5164df3ecdac1
Author: Padmanabhan Komanduru <pkomandu@codeaurora.org>
Date:   Mon Feb 25 18:42:02 2013 +0530

    msm_fb: Fix adb shell start/stop issue when bypass enabled
    
    Currently when 2/3/4 layer bypass is enabled, doing a adb shell
    stop/start is failing overlay and composition happens through
    GPU. Fix this issue by proper overlay configuration during this
    scenario.
    
    Change-Id: Ia04f1839bf80ed552671e48445af63a4dcd6bdfd
    Signed-off-by: Padmanabhan Komanduru <pkomandu@codeaurora.org>

commit 40aa574cd35769131c74db0b74e68baf99cd5e27
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Wed Mar 20 15:00:34 2013 -0700

    msm_fb: display: get mdp bandwidth parameters from board file
    
    Underrun issues can be solved by changing mdp bandwidth parameters:
    max_bw, ab_factor and ib_factor. However these parameters are not
    universally same for all targets, and one set of values that works for
    one target could cause underruns or higher power on other
    targets. This change is driver change which makes those parameters
    target dependent.
    
    CRs-Fixed: 464492
    Change-Id: Ib23ebc431caabbe91c7ae3bd96bb38789d7ddc90
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 27301c0c1c7539ef7bb78322c0c896079123b9ce
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Fri Mar 15 09:12:43 2013 -0700

    msm_fb: display: reset base layer database
    
    Reset base layer database at suspend so that new base layer can
    be populated properly at next resume.
    
    CRs-fixed: 462289
    Change-Id: Iddffa15ab8d6ca024b37674fbbb412d23d27b59d
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 99f37b6578760c604aced2999a1d09668bba2f4e
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Wed Mar 13 16:26:33 2013 -0700

    msm_fb: display: add mfd null pointer and mfd panel_power_on checks
    
    These checks are necessary for the situations where display is off but
    vsync ctrl enable or disable is still called.
    
    CRs-Fixed: 462586
    Change-Id: I2ee43ff323da38359f3bff28669df19b87d8343d
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit e2d6911206752b9d26bc568d98ff5985997bffba
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Fri Mar 15 19:49:24 2013 +0530

    msm_fb: display: Wait for DMAE for external interface (DTV)
    
    When calling FBIOPAN_DISPLAY for external, there is no wait
    which results in irregular fps when tested with fbtest.
    Enable DMAE wait to wait until DMAE is over thereby fixing
    irregular fps shown in fbtest results.
    
    Crs-fixed: 438078
    Change-Id: Iff528ccc17a1e4078c8825b35d4778a7d9e40e7f
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit d58a2d635c63357ee0bef0b1354d793c85875a5f
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Mon Mar 4 11:38:22 2013 -0800

    msm_fb: display: no native frame buffer memory operations
    
    Since we don't use pan_display, the native frame buffer memory is not
    necessary any more. This is driver change, mainly disabling memory
    operations that are associated with native frame buffers.
    
    CRs-fixed: 458938
    Change-Id: If33bff32ee1d9228ff8a74ca09961a0f10f1fcdd
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 8f4ef67176c647cc2223f56d734b838a9890f505
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Tue Feb 26 15:27:49 2013 -0800

    msm_fb: display: balance secure map and unmap in each mfd
    
    Make sure secure map and unmap get called in each mfd and balanced.
    Otherwise, secure contents can not be played for both primary and
    hdmi.
    
    CRs-fixed: 450427
    Change-Id: I5399be64fbb104dc3cbe2c5c65b3f4406f47473c
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 34d7e95032353e0902a3cab91e4d670935b53dba
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Mon Feb 4 19:19:01 2013 -0500

    msm_fb: Update MDP fence wait behaviour
    
    The current fence wait times out too early in cases where the GPU
    is under a heavy workload. Change this behaviour to warn and wait
    for a longer time for GPU to complete it's work.
    
    Change-Id: I8b20c22733a77c1aaea4a1fd1e4daaa14b5ec2fd
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit 52f7a4ea07e61c2110fe519a54c2a1241db847e1
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Thu Jan 10 13:28:21 2013 -0800

    msm_fb: display: Do clock control within mdp4_hw_init
    
    Since mdp4_hw_init() access mdp registers, mdp clock control
    within mdp4_hw_init() is necessary.
    
    CRs-fixed: 455506
    Change-Id: I768f468314fc920029b4c88e49ef6a74f6b74f58
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit f523060038fabc4b10595f177c6a5376331ddca2
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Tue Feb 19 09:02:37 2013 -0800

    msm_fb: display: add mutex to video_off
    
    It is possible  that pipe_commit still be issued by
    frame work after suspend had been started. Therefore
    mutex lock within video_off is necessary to prevent
    race condition from happening between these two threads.
    
    Change-Id: I57f51744c1654d6def8664c492035631a5b7081e
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 48777231f796fa8d06a013c1cf1e6dc9b90bcc63
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Fri Feb 22 15:46:31 2013 -0800

    msm_fb: display: calculate mdp clock including borderfill pipe
    
    For hdmi case, mdp clock calculation needs including borderfill
    pipe to prevent dtv underrun while system is in suspend.
    
    Crs-fixed: 449027
    Change-Id: I98789343317d5cfdf1f56e496cecb4dd663c907f
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 6268f100fc31bf1f1c6d9fea3d7eeb77c454cc9c
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Thu Feb 14 20:25:21 2013 -0500

    msm: display: reset vsync state when display is turned on
    
    In the case that vsync ctrl is unbalanced, vsync IRQ is cleared
    in suspend,  vsync cannot be enabled after resume.
    Clear the status when display is turned on
    
    CRs-fixed: 449155
    Change-Id: I5ecf23053b60d1fbd91f042758d67d39c83ab444
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit 09db2268dbfded7348526637432f7f5151d20630
Author: Sangani Suryanarayana Raju <csuryan@codeaurora.org>
Date:   Mon Apr 8 16:58:01 2013 +0530

    msm: display: fix iommu page faults for devices with lcdc panel.
    
    Fetch was happening from non-iommu buffer because of wrong mixer
    configuration for lcdc panels.
    
    Change-Id: Ie313ce71ff269108271437f758c0dc37884a240c
    Signed-off-by: Sangani Suryanarayana Raju <csuryan@codeaurora.org>

commit 2dcca0ed6c64505f6582941f54b1c0905a11658a
Author: Mayank Chopra <makchopra@codeaurora.org>
Date:   Wed Aug 22 15:01:15 2012 +0530

    msm_fb: display: Program source address registers correctly
    
    Scale filter FIR uses the adjacent pixels as part of the
    processing. Source address registers should be program such that
    they point to the first valid pixel so that non-ROI pixels are
    ignored. This is done by making source x,y offsets to zero and
    adding the computed offset to source address.
    
    CRs-Fixed: 380020
    Change-Id: I998fdd46f39dc67f32c422ec209da59d1add2e95
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>
    
    Conflicts:
    
    	drivers/video/msm/mdp4_overlay.c
    
    Signed-off-by: Mayank Chopra <makchopra@codeaurora.org>

commit 32d9dd29c8937882ed7ff56982224b3f80d61a60
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Thu Feb 14 11:19:52 2013 -0800

    msm_fb: display: wait4vsync before set suspend flag
    
    Since wait4vsync checks suspend flag, wait4vsync needs to
    be executed before suspend flag set to enforce wait happen.
    Also, when timing generator is turned off, wait4vsync can
    not be used anymore since the last vsync may have passed
    before wait4vsync called. This patch fixed commit_comp
    timeout cause fence timeout happen on hdmi plug and unplug
    cases.
    
    CRs-fixed: 449027
    Change-Id: I0f5520538bb290643a648c937296b6bc097fe051
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 4be3828d033d24e3f2bca78b8607681772e6a270
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Tue Apr 2 18:41:05 2013 -0700

    msm_fb: display: fix the suspend/resume hang caused by wait4vsync stuck
    
    wait_for_completion in wait4vsync stucks after multiple suspend/resume
    tests. The cause is that when show_event timeout happens, or vsync isr
    is disabled for long time, wait_vsync_cnt resets to zero, and in vsync
    isr complete_all has no chance to be called which caused mdp hang. So
    complete_all needs to be called whenever show_event has timeouts.
    
    Change-Id: I124899ff95fd5550d2759d215f16fa9cb8c0c211
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit ea716a455449d9e65e36ec84c54e0901662dbb57
Author: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>
Date:   Sat Mar 30 19:07:10 2013 +0530

    msm: display: Fix composition issues when framework rebooted in suspend.
    
    Currently, if framework is rebooted while in suspend mode, borderfill
    is not unset due to which MDP composition can not be enabled. This patch
    fixes the issue.
    
    Change-Id: I8b671eccedebf2bb08724cf2d284d4ab27a71360
    Signed-off-by: Vishnuvardhan Prodduturi <vproddut@codeaurora.org>

commit a7bf8c71c79deefbf8bcecc9efd4754d3fef5025
Author: Huaibin Yang <huaibiny@codeaurora.org>
Date:   Tue Apr 2 15:09:16 2013 -0700

    msm_fb: display: fix the hang issue caused by solidfill pipe
    
    When mdp pipe solidfill mode is disabled, the staled pipe info is used
    to update mdp pipe regs. This caused mdp hang. Since
    mdp4_overlay_rgb_setup function already resets solidfill mode, this
    patch removes any pipe register updates that are related to disabling
    solidfill mode.
    
    CRs-fixed: 467555
    Change-Id: I23f4b29731c9097ab0cc38baa00cd35bc536d2ba
    Signed-off-by: Huaibin Yang <huaibiny@codeaurora.org>

commit 0277e2e7a16ee6705b8d5763f08e5bc77f2323d6
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Wed Mar 20 16:31:27 2013 -0700

    msm_fb: display: fix iommu page fault when iommu buffer freed
    
    There has possibility that pipe commit come very close to next vsync.
    This may cause two consecutive pipe_commits happen within same vsync
    period which cause iommu page fault when previous iommu buffer is
    freed since mdp stil fetching it. Set ION_IOMMU_UNMAP_DELAYED flag
    at ion_map_iommu() to add delay un mapping iommu buffer to fix this problem.
    Otherwise four buffer mechanism is needed to avoid this problem..
    Also ion_unmap_iommu() may take long time to free an ion buffer. Move
    mdp4_overlay_iommu_unmap_freelist(mixer) called after stage_commit()
    to increase chance to have pipe_commit (up to stage_commit) is completed
    within same vsync period
    This patch also fix an potential race condition at wait_for_completion
    between show_event thread and wait4vsync thread.
    
    Change-Id: I089950ac61c0bd3c4207b8925817577e42fbb258
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit a4005adf0e1e09100b33abadb60880bf887404c0
Author: Mahesh Sivasubramanian <msivasub@codeaurora.org>
Date:   Fri May 10 09:38:51 2013 -0600

    msm: cpufreq: Configure WQ for higer priority
    
    When the workqueue runs at a lower priority, it gets starved by higher
    priority threads. Bump the WQ priority to high to ensure cpufreq work
    queue gets a fair scheduling chance
    
    Change-Id: I9e994da94a347dceb884e72ec3dd3da468a4471d
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>

commit c5f7e0f020b2603b6b90347e9dbfd14ebb235c49
Author: Praveen Chidambaram <pchidamb@codeaurora.org>
Date:   Mon Mar 11 14:50:06 2013 -0600

    Revert "msm: cpufreq: Remove cross-calling limitation"
    
    This reverts commit be01a17afb6b6e85456217caa1ac11eb0b6674e1.
    
    Re-introduce cross calling in cpufreq to prevent race conditions in the
    hardware when changing PLLs.
    
    A core could be in retention, while another core, could try and set the
    frequency of the core in retention, causing a change in PLL and when the
    core exits retention, the PLL mismatch could hang the SPM state machine.
    
    Change-Id: I99b454a0f146652f2d8a369f79f6d35748160dcb
    Signed-off-by: Praveen Chidambaram <pchidamb@codeaurora.org>

commit 6d10efd299729ea955db0355c527ff715c6e60cc
Author: Jennifer Liu <chihning@codeaurora.org>
Date:   Thu May 2 09:53:30 2013 -0700

    msm: cpufreq: Always update frequency limits when core is online
    
    Remove condition in msm_cpufreq_init() so that when cpu is set to
    online, it will call set_cpu_freq() and update the frequency limits.
    
    CRs-Fixed: 462816
    Change-Id: I40ac33c9cb4b42590a83f9d61b74f23379f5642e
    Signed-off-by: Jennifer Liu <chihning@codeaurora.org>

commit cd74d20cadcbc8889c858305e10a11576c65f340
Author: Narayanan Gopalakrishnan <nargop@codeaurora.org>
Date:   Thu Nov 1 10:08:55 2012 -0700

    msm: cpufreq: increase priority of thread that increases frequencies
    
    When the cpufreq governors request for increase in frequency from kworker
    threads, there is chance that the kworker thread is cpu starved due to
    other high priority threads, leading to undesirable increase in frequency
    ramp-up latency. Add the calling thread to SCHED_FIFO policy when
    requesting for increase in frequency and restore back to original policy
    after ramp-up is completed.
    
    Change-Id: Ie4fa199b7f9087717cb94dfcc2eddea27cc0012b
    Signed-off-by: Narayanan Gopalakrishnan <nargop@codeaurora.org>

commit 2e6c2802a32309314a3a391fc262b9d7bb59c344
Author: Mahesh Sivasubramanian <msivasub@codeaurora.org>
Date:   Fri May 10 09:38:51 2013 -0600

    msm: rpm-smd: Configure WQ for higer priority
    
    When the workqueue runs at a lower priority, it gets starved by higher
    priority threads. Bump the WQ priority to high to ensure rpm smd work
    queue gets a fair chance
    
    Change-Id: I06b864611cf45afe6931d6030327806032894663
    Signed-off-by: Mahesh Sivasubramanian <msivasub@codeaurora.org>

commit 5cb10da8152dce907c0b230f7505c19ad667d374
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 13 22:16:45 2012 -0700

    workqueue: reimplement WQ_HIGHPRI using a separate worker_pool
    
    WQ_HIGHPRI was implemented by queueing highpri work items at the head
    of the global worklist.  Other than queueing at the head, they weren't
    handled differently; unfortunately, this could lead to execution
    latency of a few seconds on heavily loaded systems.
    
    Now that workqueue code has been updated to deal with multiple
    worker_pools per global_cwq, this patch reimplements WQ_HIGHPRI using
    a separate worker_pool.  NR_WORKER_POOLS is bumped to two and
    gcwq->pools[0] is used for normal pri work items and ->pools[1] for
    highpri.  Highpri workers get -20 nice level and has 'H' suffix in
    their names.  Note that this change increases the number of kworkers
    per cpu.
    
    POOL_HIGHPRI_PENDING, pool_determine_ins_pos() and highpri chain
    wakeup code in process_one_work() are no longer used and removed.
    
    This allows proper prioritization of highpri work items and removes
    high execution latency of highpri work items.
    
    v2: nr_running indexing bug in get_pool_nr_running() fixed.
    
    v3: Refreshed for the get_pool_nr_running() update in the previous
        patch.
    
    Change-Id: Id843c0a425f51f84083786fbf413d999d35771b7
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Josh Hunt <joshhunt00@gmail.com>
    LKML-Reference: <CAKA=qzaHqwZ8eqpLNFjxnO2fX-tgAOjmpvxgBFjv6dJeQaOW1w@mail.gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Git-commit: 3270476a6c0ce322354df8679652f060d66526dc
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 5e4298ecd928c020164a64d29e8e1f36d99c729b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 13 22:16:44 2012 -0700

    workqueue: introduce NR_WORKER_POOLS and for_each_worker_pool()
    
    Introduce NR_WORKER_POOLS and for_each_worker_pool() and convert code
    paths which need to manipulate all pools in a gcwq to use them.
    NR_WORKER_POOLS is currently one and for_each_worker_pool() iterates
    over only @gcwq->pool.
    
    Note that nr_running is per-pool property and converted to an array
    with NR_WORKER_POOLS elements and renamed to pool_nr_running.  Note
    that get_pool_nr_running() currently assumes 0 index.  The next patch
    will make use of non-zero index.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    v2: nr_running indexing bug in get_pool_nr_running() fixed.
    
    v3: Pointer to array is stupid.  Don't use it in get_pool_nr_running()
        as suggested by Linus.
    
    Change-Id: I46e9488601d764d25e4a6c707de129ab68f7064c
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Git-commit: 4ce62e9e30cacc26885cab133ad1de358dd79f21
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit e720f0e0c3df39ba11e9eb48b02ee3c4d79ba954
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: separate out worker_pool flags
    
    GCWQ_MANAGE_WORKERS, GCWQ_MANAGING_WORKERS and GCWQ_HIGHPRI_PENDING
    are per-pool properties.  Add worker_pool->flags and make the above
    three flags per-pool flags.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    Change-Id: I1824fd1c509d8ac6b0619536621a22b15b316256
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 11ebea50dbc1ade5994b2c838a096078d4c02399
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit f4d7035c8c850fe727bea2ac68b062531cca7772
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: use @pool instead of @gcwq or @cpu where applicable
    
    Modify all functions which deal with per-pool properties to pass
    around @pool instead of @gcwq or @cpu.
    
    The changes in this patch are mechanical and don't caues any
    functional difference.  This is to prepare for multiple pools per
    gcwq.
    
    Change-Id: I4be6727e1cce6f9aa2a0057b96bdc725c84f1ea8
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 63d95a9150ee3bbd4117fcd609dee40313b454d9
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 90d2d7282d59a974cfb49e6a0b07b82a40eee2d3
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: factor out worker_pool from global_cwq
    
    Move worklist and all worker management fields from global_cwq into
    the new struct worker_pool.  worker_pool points back to the containing
    gcwq.  worker and cpu_workqueue_struct are updated to point to
    worker_pool instead of gcwq too.
    
    This change is mechanical and doesn't introduce any functional
    difference other than rearranging of fields and an added level of
    indirection in some places.  This is to prepare for multiple pools per
    gcwq.
    
    v2: Comment typo fixes as suggested by Namhyung.
    
    Change-Id: Iefae84798c2af580f425b92ed79117935d99f21f
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Git-commit: bd7bdd43dcb81bb08240b9401b36a104f77dc135
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 41a3704a7a70162eedee6c2a25c5c5095a843fb3
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 12 14:46:37 2012 -0700

    workqueue: don't use WQ_HIGHPRI for unbound workqueues
    
    Unbound wqs aren't concurrency-managed and try to execute work items
    as soon as possible.  This is currently achieved by implicitly setting
    %WQ_HIGHPRI on all unbound workqueues; however, WQ_HIGHPRI
    implementation is about to be restructured and this usage won't be
    valid anymore.
    
    Add an explicit chain-wakeup path for unbound workqueues in
    process_one_work() instead of piggy backing on %WQ_HIGHPRI.
    
    Change-Id: Iecd17a9935ee28f856d8b726bb4c296762922bed
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Git-commit: 974271c485a4d8bb801decc616748f90aafb07ec
    Git-repo: git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    Signed-off-by: Matt Wagantall <mattw@codeaurora.org>

commit 3eb3348d52eceae5fd1c038105841087ab328c4a
Author: Steve Kondik <shade@chemlab.org>
Date:   Thu May 9 22:39:47 2013 -0700

    msm: restart: Always use Samsung magic restart reason
    
     * If this isn't passed, the bootloader won't honor the values written
       to the bootmode in the param partition.
    
    Change-Id: I03a8bb764bc3392a3a43083b5c17893d86df7da7

commit 30703436fe55657dee45f65724bd8ce426dee64f
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Wed Mar 20 16:31:27 2013 -0700

    msm_fb: display: fix iommu page fault when iommu buffer freed
    
    There has possibility that pipe commit come very close to next vsync.
    This may cause two consecutive pipe_commits happen within same vsync
    period which cause iommu page fault when previous iommu buffer is
    freed since mdp stil fetching it. Set ION_IOMMU_UNMAP_DELAYED flag
    at ion_map_iommu() to add delay un mapping iommu buffer to fix this problem.
    Otherwise four buffer mechanism is needed to avoid this problem..
    Also ion_unmap_iommu() may take long time to free an ion buffer. Move
    mdp4_overlay_iommu_unmap_freelist(mixer) called after stage_commit()
    to increase chance to have pipe_commit (up to stage_commit) is completed
    within same vsync period
    This patch also fix an potential race condition at wait_for_completion
    between show_event thread and wait4vsync thread.
    
    Change-Id: I673b0e368bbcd3120bc26569fe7a7bb82931daaa
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>
    (cherry picked from commit ada63be0fd32e729e4b91aeeea2a5cfe0830148c)
    (cherry picked from commit 799796f15e0b78ffe47975db1cf7f3791cb98129)

commit 809d2c272a2ccb891a9acfcc8265585e73aefedf
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Thu Apr 4 13:25:53 2013 -0400

    msm: display: plane alpha support
    
    Clean up the blending rule to follow blend_op set from hwc.
    Use modulate alpha for plane alpha when it is not 0xff.
    Keep backward compatibility if blend_op is not set.
    
    Change-Id: I02fd3c3c7b5ace2e6eec22b2db0284161404a0fc
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit 1054984fa0859c31a80edf537fb3ca20ff302090
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun May 5 23:27:44 2013 -0700

    cm: Set ROW as default IO scheduler
    
    Change-Id: I42a0faef64590136204439d06bacfa214cf5c388

commit 33435ae76a5cf984eb6b5d10dc61aeec24c2b8cc
Author: Steve Kondik <shade@chemlab.org>
Date:   Sun May 5 23:26:05 2013 -0700

    mmc: Fix warning during bootup
    
     * Don't call get_task_io_context with irqs off (this happens during the
       probe).
    
    Change-Id: Ib56a38dfee7fed2632476fe729332111a22a7e8a

commit b2a8d89dc19d83a4c6511fc9ce6edefa60ca2605
Author: Maya Erez <merez@codeaurora.org>
Date:   Sun Jan 6 10:19:38 2013 +0200

    mmc: Urgent request support
    
    mmc: do not pack random requests
    
    Packed commands causes higher latency since the completion of each
    request is sent to the upper layer upon completion of the complete
    packed request.
    The benefit from this feature is card dependent. Some of the card
    vendors do not have any benefit from using packed commands for random
    requests. In case there is no benefit in random requests packing,
    it is better to disable the packing to prevent this high latency.
    This patch also add the new stop packing reason to the write packing
    statistics.
    
    Change-Id: I141887dcef2ceee14848634cc27c3c85f8edc7a5
    Signed-off-by: Maya Erez <merez@codeaurora.org>
    
    mmc: fix async request mechanism for sequential read scenarios
    
    When current request is running on the bus and if next request fetched
    by mmcqd is NULL, mmc context (mmcqd thread) gets blocked until the
    current request completes. This means that if new request comes in while
    the mmcqd thread is blocked, this new request can not be prepared in
    parallel to current ongoing request. This may result in delaying the new
    request execution and increase it's latency.
    
    This change allows to wake up the MMC thread on new request arrival.
    Now once the MMC thread is woken up, a new request can be fetched and
    prepared in parallel to the current running request which means this new
    request can be started immediately after the current running request
    completes.
    
    With this change read throughput is improved by 16%.
    
    Change-Id: I8d74fda719c89a710330807c9d8994d40c885aa0
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    
    mmc: block: add mmc_blk_disable_wr_packing() disables packing mode
    
    The function disables packing mode. Right now the only reason is
    change in data direction.
    
    Change-Id: I0f4edba3da93fde28cf47ac95754a95e411fa2af
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    
    mmc: Add support to handle Urgent data transfer request
    
    Urgent request notification stops currently running transaction
    on bus.
    In order to decrease a latency of a prioritized requests (aka Urgent
    request), we might want to stop the transmission of a running "low
    priority" request in order to handle the Urgent request. The urgency of
    the request is decided by the block layer I/O scheduler. When the block
    layer notifies the MMC layer of an urgent request and if the MMC layer is
    blocked on current request to complete, it will be woken up.
    
    The decision on whether to stop an ongoing transfer is taken according to
    several parameters, one of them being the number of bytes already
    transferred for ongoing transfer by host controller so far.
    
    To calculate how many bytes were successfully programmed before stop,
    CORRECTLY_PRG_SECTORS_NUM[245:242] parsed from EXT_CSD register. The
    remainder of stopped request (and next prepared request in case it
    exists) re-inserted back to the I/O scheduler to be handled after the
    completion of the urgent request.
    
    In case it is decided not to stop the ongoing transfer, MMC context will
    wait for normal completion of the ongoing transfer, then already prepared
    next request (if it exists) will be re-inserted back into block layer and
    the urgent request fetched.
    
    URGENT_REQUEST handling has following dependencies:
    1. Host controller driver should support mmc_host op named "stop_request"
    2. Block I/O scheduler should support re-insert API
    3. eMMC card should support HPI (High Priority Interrupt) command
    
    If any of the above dependencies are not met then urgent request mechanism
    will not become operational.
    
    Change-Id: Ic3fa1ca9463cc8991aefee940d8bfddf76c111d3
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    
    mmc: core: ignore new request notification, when 2 requests are in progress
    
    In case both host->areq and areq requests exist and the new request
    notification unblocks mmc context, instead of BUG_ON() in mmc_start_req(),
    mmc_wait_for_data_req_done() continue to wait again for the completion
    of the current request. New request notification is ignored in this case.
    
    Change-Id: I5a0201a9fbe4cbaeb0c0a9b83f56f4af74eff95b
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    
    mmc: fix read latency of urgent request
    
    In order to improve read request latency, urgent notification allowed
    anytime, when current or previous requests are existing in the mmc layer.
    It is not allowed to stop following running requests:
    - urgent request: block layer should serialize urgent request notification.
    - read request: it can't be packed, therefore it is better
    to wait for its completion.
    - REQ_FUA: interruption and delay in such request will impact sync process
    of upper layers.
    
    Change-Id: Id7d1480cce2059c1f23a5f29ad8f74e858be1ee6
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    Signed-off-by: Maya Erez <merez@codeaurora.org>
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    mmc: msm_sdcc: enable support to handle Urgent data transfer request
    
    In order to decrease a latency of a prioritized requests (aka Urgent
    request), we might want to stop the transmission of a running "low
    priority" request in order to handle the Urgent request.
    
    URGENT_REQUEST handling has following dependencies:
    1. Host controller driver should support mmc_host op named "stop_request"
    2. Block I/O scheduler should support re-insert API
    3. eMMC card should support HPI (High Priority Interrupt) command
    If any of the above dependencies are not met then urgent request mechanism
    will not become operational.
    
    Change-Id: I976af4fa794fb0fac701fe12f249c36d78080c6e
    Signed-off-by: Konstantin Dorfman <kdorfman@codeaurora.org>
    
    mmc: Do not perform blocking BKOPS
    
    BKOPS operations can take long time and cause bad user experience and
    failure of time critical operations as EFS sync.
    In order to prevent the above failures all the levels will be handled only
    in idle time BKOPS handling and will allow interrupting the BKOPS when
    needed.
    In case the card raised an exception with a need for BKOPS, a flag will
    be set to indicate MMC to start the BKOPS activity when it becomes idle.
    
    Fixed-CRs: 432027
    Change-Id: I5f7b43569c0242f0fea83355f76f286b1ad037bc
    Signed-off-by: Maya Erez <merez@codeaurora.org>
    (cherry picked from commit 5f8ac3b955e44def61c2238192000ffe5f126714)

commit 4879c1f570d8a821a4aa8f03fd7749107ac07e10
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat May 4 22:48:22 2013 -0700

    cm: Disable DVFS touch booster
    
     * We do this from userspace via boostpulse.
    
    Change-Id: Ic1128c54779bf5c45c722f0f9e5610e8387aaa9f

commit 46ba2346b68a0392310930bcdc5bed2557cb4106
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat May 4 05:25:44 2013 -0700

    input: Add option to disable touch booster
    
     * This can also be done from userspace, so allow disabling it.
    
    Change-Id: I17f46abe89ed842d434307e90b7d2f8d4b6e1147

commit a3de8e974d9148ae442f371d2c33a2d9ddee5896
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat May 4 04:49:19 2013 -0700

    jf: Add CM defconfig
    
    Change-Id: I796ad60185d15eed5881c122064096cbf2b5a5a2

commit 6c4df4668ea5e12d5095a3b4aa7958a4d174d623
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat May 4 04:45:34 2013 -0700

    rmnet_usb: Kill some logspam
    
    Change-Id: Ic8fa0f1472c82db9b2168483834e6960920d45a4

commit bbb7c130f0bd3f508b26c78b3d60ec93534834d3
Author: Steve Kondik <shade@chemlab.org>
Date:   Sat May 4 04:41:48 2013 -0700

    Fix compilation issues with CONFIG_SEC_DEBUG disabled.
    
    Change-Id: I597ea4d521cc593e8ae1107710b2b412b456d9d1

commit 2515381417287a7031ed3ac4f1778bd5d5486571
Author: Dan Pasanen <dan.pasanen@gmail.com>
Date:   Sat Apr 20 20:20:27 2013 -0500

    immvibespi: add sysfs interface for controlling vibe intensity
    
    This is based of codeworkx's work on smdk4412
    
    Useful with something like this:
    http://review.cyanogenmod.org/#/c/35981/
    
    Change-Id: I1c1c16b4cefd600ebc6caa603d92044c466cf792

commit 4e818216303455b8f73312555408ccf02a8a3268
Author: Tatyana Brokhman <tlinder@codeaurora.org>
Date:   Thu Sep 20 10:46:10 2012 +0300

    block: Adding ROW scheduling algorithm
    
    This patch adds the implementation of a new scheduling algorithm - ROW.
    The policy of this algorithm is to prioritize READ requests over WRITE
    as much as possible without starving the WRITE requests.
    
    Change-Id: I4ed52ea21d43b0e7c0769b2599779a3d3869c519
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: ROW: Correct minimum values of ROW tunable parameters
    
    The ROW scheduling algorithm exposes several tunable parameters.
    This patch updates the minimum allowed values for those parameters.
    
    Change-Id: I5ec19d54b694e2e83ad5376bd99cc91f084967f5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: ROW: Fix forced dispatch
    
    This patch fixes forced dispatch in the ROW scheduling algorithm.
    When the dispatch function is called with the forced flag on, we
    can't delay the dispatch of the requests that are in scheduler queues.
    Thus, when dispatch is called with forced turned on, we need to cancel
    idling, or not to idle at all.
    
    Change-Id: I3aa0da33ad7b59c0731c696f1392b48525b52ddc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: Add support for reinsert a dispatched req
    
    Add support for reinserting a dispatched request back to the
    scheduler's internal data structures.
    This capability is used by the device driver when it chooses to
    interrupt the current request transmission and execute another (more
    urgent) pending request. For example: interrupting long write in order
    to handle pending read. The device driver re-inserts the
    remaining write request back to the scheduler, to be rescheduled
    for transmission later on.
    
    Add API for verifying whether the current scheduler
    supports reinserting requests mechanism. If reinsert mechanism isn't
    supported by the scheduler, this code path will never be activated.
    
    Change-Id: I5c982a66b651ebf544aae60063ac8a340d79e67f
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: Add API for urgent request handling
    
    This patch add support in block & elevator layers for handling
    urgent requests. The decision if a request is urgent or not is taken
    by the scheduler. Urgent request notification is passed to the underlying
    block device driver (eMMC for example). Block device driver may decide to
    interrupt the currently running low priority request to serve the new
    urgent request. By doing so READ latency is greatly reduced in read&write
    collision scenarios.
    
    Note that if the current scheduler doesn't implement the urgent request
    mechanism, this code path is never activated.
    
    Change-Id: I8aa74b9b45c0d3a2221bd4e82ea76eb4103e7cfa
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    row: Adding support for reinsert already dispatched req
    
    Add support for reinserting already dispatched request back to the
    schedulers internal data structures.
    The request will be reinserted back to the queue (head) it was
    dispatched from as if it was never dispatched.
    
    Change-Id: I70954df300774409c25b5821465fb3aa33d8feb5
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block:row: fix idling mechanism in ROW
    
    This patch addresses the following issues found in the ROW idling
    mechanism:
    1. Fix the delay passed to queue_delayed_work (pass actual delay
       and not the time when to start the work)
    2. Change the idle time and the idling-trigger frequency to be
       HZ dependent (instead of using msec_to_jiffies())
    3. Destroy idle_workqueue() in queue_exit
    
    Change-Id: If86513ad6b4be44fb7a860f29bd2127197d8d5bf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    cfq-iosched: Fix null pointer dereference
    
    NULL pointer dereference can happen in cfq_choose_cfqg()
    when there are no cfq groups to select other than the
    current serving group. Prevent this by adding a NULL
    check before dereferencing.
    
    Unable to handle kernel NULL pointer dereference at virtual address
    [<c02502cc>] (cfq_dispatch_requests+0x368/0x8c0) from
    [<c0243f30>] (blk_peek_request+0x220/0x25c)
    [<c0243f30>] (blk_peek_request+0x220/0x25c) from
    [<c0243f74>] (blk_fetch_request+0x8/0x1c)
    [<c0243f74>] (blk_fetch_request+0x8/0x1c) from
    [<c041cedc>] (mmc_queue_thread+0x58/0x120)
    [<c041cedc>] (mmc_queue_thread+0x58/0x120) from
    [<c00ad310>] (kthread+0x84/0x90)
    [<c00ad310>] (kthread+0x84/0x90) from
    [<c000eeac>] (kernel_thread_exit+0x0/0x8)
    
    CRs-Fixed: 416466
    Change-Id: I1fab93a4334b53e1d7c5dcc8f93cff174bae0d5e
    Signed-off-by: Sujit Reddy Thumma <sthumma@codeaurora.org>
    
    row: Add support for urgent request handling
    
    This patch adds support for handling urgent requests.
    ROW queue can be marked as "urgent" so if it was un-served in last
    dispatch cycle and a request was added to it - it will trigger
    issuing an urgent-request-notification to the block device driver.
    The block device driver may choose at stop the transmission of current
    ongoing request to handle the urgent one. Foe example: long WRITE may
    be stopped to handle an urgent READ. This decreases READ latency.
    
    Change-Id: I84954c13f5e3b1b5caeadc9fe1f9aa21208cb35e
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Add some debug information on ROW queues
    
    1. Add a counter for number of requests on queue.
    2. Add function to print queues status (number requests
       currently on queue and number of already dispatched requests
       in current dispatch cycle).
    
    Change-Id: I1e98b9ca33853e6e6a8ddc53240f6cd6981e6024
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Insert dispatch_quantum into struct row_queue
    
    There is really no point in keeping the dispatch quantum
    of a queue outside of it. By inserting it to the row_queue
    structure we spare extra level in accessing it.
    
    Change-Id: Ic77571818b643e71f9aafbb2ca93d0a92158b199
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: fix sysfs functions - idle_time conversion
    
    idle_time was updated to be stored in msec instead of jiffies.
    So there is no need to convert the value when reading from user or
    displaying the value to him.
    
    Change-Id: I58e074b204e90a90536d32199ac668112966e9cf
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Aggregate row_queue parameters to one structure
    
    Each ROW queues has several parameters which default values are defined
    in separate arrays. This patch aggregates all default values into one
    array.
    The values in question are:
     - is idling enabled for the queue
     - queue quantum
     - can the queue notify on urgent request
    
    Change-Id: I3821b0a042542295069b340406a16b1000873ec6
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Dispatch requests according to their io-priority
    
    This patch implements "application-hints" which is a way the issuing
    application can notify the scheduler on the priority of its request.
    This is done by setting the io-priority of the request.
    This patch reuses an already existing mechanism of io-priorities developed
    for CFQ. Please refer to kernel/Documentation/block/ioprio.txt for
    usage example and explanations.
    
    Change-Id: I228ec8e52161b424242bb7bb133418dc8b73925a
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Idling mechanism re-factoring
    
    At the moment idling in ROW is implemented by delayed work that uses
    jiffies granularity which is not very accurate. This patch replaces
    current idling mechanism implementation with hrtime API, which gives
    nanosecond resolution (instead of jiffies).
    
    Change-Id: I86c7b1776d035e1d81571894b300228c8b8f2d92
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Don't notify URGENT if there are un-completed urgent req
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. If the current
    transmitted request is an urgent request - we don't want it to be
    stopped.
    Due to the above ROW scheduler won't notify of an urgent request if
    there are urgent requests in flight.
    
    Change-Id: I2fa186d911b908ec7611682b378b9cdc48637ac7
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Update initial values of ROW data structures
    
    This patch sets the initial values of internal ROW
    parameters.
    
    Change-Id: I38132062a7fcbe2e58b9cc757e55caac64d013dc
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    [smuckle@codeaurora.org: ported from msm-3.7]
    Signed-off-by: Steve Muckle <smuckle@codeaurora.org>
    
    block: add REQ_URGENT to request flags
    
    This patch adds a new flag to be used in cmd_flags field of struct request
    for marking request as urgent.
    Urgent request is the one that should be given priority currently handled
    (regular) request by the device driver. The decision of a request urgency
    is taken by the scheduler.
    
    Change-Id: Ic20470987ef23410f1d0324f96f00578f7df8717
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Re-design urgent request notification mechanism
    
    When ROW scheduler reports to the block layer that there is an urgent
    request pending, the device driver may decide to stop the transmission
    of the current request in order to handle the urgent one. This is done
    in order to reduce the latency of an urgent request. For example:
    long WRITE may be stopped to handle an urgent READ.
    
    This patch updates the ROW URGENT notification policy to apply with the
    below:
    
    - Don't notify URGENT if there is an un-completed URGENT request in driver
    - After notifying that URGENT request is present, the next request
      dispatched is the URGENT one.
    - At every given moment only 1 request can be marked as URGENT.
      Independent of it's location (driver or scheduler)
    
    Other changes to URGENT policy:
    - Only READ queues are allowed to notify of an URGENT request pending.
    
    CR fix:
    If a pending urgent request (A) gets merged with another request (B)
    A is removed from scheduler queue but is not removed from
    rd->pending_urgent_rq.
    
    CRs-Fixed: 453712
    Change-Id: I321e8cf58e12a05b82edd2a03f52fcce7bc9a900
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: urgent request: remove unnecessary urgent marking
    
    An urgent request is marked by the scheduler in rq->cmd_flags with the
    REQ_URGENT flag. There is no need to add an additional marking by
    the block layer.
    
    Change-Id: I05d5e9539d2f6c1bfa80240b0671db197a5d3b3f
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Prevent starvation of regular priority by high priority
    
    At the moment all REGULAR and LOW priority requests are starved as long as
    there are HIGH priority requests to dispatch.
    This patch prevents the above starvation by setting a starvation limit the
    REGULAR\LOW priority requests can tolerate.
    
    Change-Id: Ibe24207982c2c55d75c0b0230f67e013d1106017
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Update sysfs functions
    
    All ROW (time related) configurable parameters are stored in ms so there
    is no need to convert from/to ms when reading/updating them via sysfs.
    
    Change-Id: Ib6a1de54140b5d25696743da944c076dd6fc02ae
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: urgent request: Update dispatch_urgent in case of requeue/reinsert
    
    The block layer implements a mechanism for verifying that the device
    driver won't be notified of an URGENT request if there is already an
    URGENT request in flight. This is due to the fact that interrupting an
    URGENT request isn't efficient.
    This patch fixes the above described mechanism in case the URGENT request
    was returned back to the block layer from some reason: by requeue or
    reinsert.
    
    CRs-fixed: 473376, 473036, 471736
    Change-Id: Ie8b8208230a302d4526068531616984825f1050d
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    
    block: row: Fix starvation tolerance values
    
    The current starvation tolerance values increase the boot time
    since high priority SW requests are delayed by regular priority requests.
    In order to overcome this, increase the starvation tolerance values.
    
    Change-Id: I9947fca9927cbd39a1d41d4bd87069df679d3103
    Signed-off-by: Tatyana Brokhman <tlinder@codeaurora.org>
    Signed-off-by: Maya Erez <merez@codeaurora.org>

commit 942db076387499b218aa9466f2297b6938bdc731
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Apr 9 02:30:00 2013 -0700

    cpufreq: interactive: Fix uninitialized spinlock
    
    Change-Id: I9959669e0f643ccd2bd1072fae440d8c20f1c9cc

commit fc80e8efd67deba00477eb488cc5aa2e21c3ce44
Author: Sam Leffler <sleffler@chromium.org>
Date:   Wed Jun 27 10:12:04 2012 -0700

    cpufreq: interactive: Bring in line with Google's 3.4
    
    cpufreq: interactive: take idle notifications only when active
    
    Register an idle notifier only when the governor is active.  Also
    short-circuit work of idle end if the governor is not enabled.
    
    Signed-off-by: Sam Leffler <sleffler@chromium.org>
    Change-Id: I4cae36dd2e7389540d337d74745ffbaa0131870f
    
    cpufreq: interactive: keep freezer happy when not current governor
    
    Fix a problem where the hung task mechanism was deeming the interactive
    clock boost thread as hung.  This was because the thread is created at
    module init but never run/woken up until needed.  If the governor is not
    being used this can be forever.  To workaround this explicitly wake up
    the thread once all the necessary data structures are initialized.  The
    latter required some minor code shuffle.
    
    Signed-off-by: Sam Leffler <sleffler@chromium.org>
    Change-Id: Ie2c058dd75dcb6460ea10e7ac997e46baf66b1fe
    
    cpufreq: interactive: handle speed up and down in the realtime task
    
    Not useful to have a separate, non-realtime workqueue for speed down
    events, avoid priority inversion for speed up events.
    
    Change-Id: Iddcd05545245c847aa1bbe0b8790092914c813d2
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: remove input_boost handling
    
    Now handled in userspace Power HAL instead.
    
    Change-Id: I78a4a2fd471308bfcd785bbefcc65fede27314cf
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: always limit initial speed bump to hispeed
    
    First bump speed up to hispeed_freq whenever the current speed is below
    hispeed_freq, instead of only when the current speed is the minimum speed.
    The previous code made it too difficult to use hispeed_freq as a common
    intermediate speed on systems that frequently run at speeds between
    minimum and hispeed_freq.
    
    Change-Id: I04ec30bafabf5741e267ff289209b8c2d846824b
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: run at fraction of hispeed_freq when load is low
    
    When load is below go_hispeed_load, apply the percentage of CPU load to
    a max frequency of hispeed_freq instead of the max speed.  This avoids
    jumping too quickly to hispeed_freq when it is a relatively low
    percentage of max speed.  This also allows go_hispeed_load to be set to
    a high percentage relative to hispeed_freq (as a percentage of max speed,
    again useful when hispeed_freq is a low fraction of max speed), to cap
    larger loads at hispeed_freq.  For example, a load of 60% will typically
    move to 60% of hispeed_freq, not 60% of max speed.  This causes the
    governor to apply two different speed caps, depending on whether load is
    below or above go_hispeed_load.
    
    Also fix the type of hispeed_freq, which was u64, to match other
    speed data types (and avoid overhead and allow division).
    
    Change-Id: Ie2d0668be161c074aaad77db2037505431457b3a
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: pin timers to associated CPU
    
    Helps avoid waking up other CPUs to react to activity on the local CPU.
    
    Change-Id: Ife272aaa7916894a437705d44521b1a1693fbe8e
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: use deferrable timer by default
    
    Avoid wakeups only to handle the governor timer when the system is otherwise
    idle.
    
    For platforms where the power cost of remaining in idle at higher CPU
    speed may outweigh the cost of a governor wakeup from idle to lower the speed,
    set parameter cpufreq_interactive.governidle=1.
    
    Change-Id: Id6c43eb35caecf9b0574fcdd5b769711bc7e6de6
    Signed-off-by: LianWei WANG <a22439@motorola.com>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: kick timer on idle exit past expiry
    
    The deferrable timer list isn't checked on all idle exits, such as when
    hi-res timers expire or ISRs schedule workers.  If the idle loop is
    exited and it's past time to run the governor load polling timer,
    run it immediately.  This ensures we handle load spikes caused by actvity
    that does not run the normal timer list.
    
    Rename the field that timestamps the "time_in_idle" value to be more
    accurate.
    
    Change-Id: Ied590ecbefc83c9a9ec5eb9e31903557f6fa1614
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: trace actual speed in target speed decisions
    
    Tracing adds actual speed since this is expected to be key to the
    choice of target speed.
    
    Change-Id: Iec936102d0010c4e9dfa143c38a9fd0d551189c3
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: change speed according to current speed and target load
    
    Add a target_load attribute that specifies how aggressively the governor is
    to adjust speed to meet the observed load.  New target speed is calculated
    as the current actual speed (may be higher than target speed on SMP) times
    the CPU load (as a fraction) divided by target load (fraction).
    
    cpufreq_frequency_table_target() call use CPUFREQ_RELATION_L to set
    the next higher speed rather than next lower speed.
    
    Change-Id: If432451da82f5fed12e15c9421d7d27792376150
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: apply above_hispeed_delay to each step above hispeed
    
    Apply above_hispeed_delay whenever increasing speed to a new speed above
    hispeed (not just the first step above hispeed).
    
    Change-Id: Ibb7add7db47f2a4306a9458c4e1ebabb60698636
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: allow arbitrary speed / target load mappings
    
    Accept a string of target loads and speeds at which to apply the
    target loads, per the documentation update in this patch.  For example,
    "85 1000000:90 1700000:99" targets CPU load 85% below speed 1GHz,  90%
    at or above 1GHz, until 1.7GHz and above, at which load 99% is targeted.
    
    Attempt to avoid oscillations by evaluating the current speed
    weighted by current load against each new choice of speed, choosing a
    higher speed if the current load requires a higher speed.
    
    Change-Id: Ie3300206047c84eca5a26b0b63ea512e5207550e
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: remove load since last speed change
    
    The longer-term load since last speed change isn't terribly useful,
    may delay recognition of dropping load, and would need forthcoming
    changes to adjust load for changing CPU speeds.  Drop it.
    
    Change-Id: Ic3cbb0542cc3484617031787e03ed9bdd632dec1
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: adjust load for changes in speed
    
    Add notifier for speed transitions.  Keep a count of CPU active
    microseconds times current frequency, converted to a percentage relative
    to the current frequency when load is evaluated.
    
    Change-Id: I5c27adb11081c50490219784ca57cc46e97fc28c
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: specify duration of CPU speed boost pulse
    
    Sysfs attribute boostpulse_duration specifies the duration of boosting CPU
    speed in response to bootpulse events.  Duration is specified in usecs,
    default 80ms.
    
    Change-Id: Ifd41625574891a44f1787a4e85d1e7b4f2afb52b
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: add timer slack to limit idle at speed > min
    
    Always use deferrable timer for load sampling.
    
    Set a non-deferrable timer to an additional slack time to allow prior to
    waking up from idle to drop speed when not at minimum speed.  Slack value
    -1 avoids wakeups to drop speed.  Default is 80ms.
    
    Remove the governidle module param and its timer management in idle.  For
    platforms on which holding speed above mimum in idle costs power, use the
    new timer slack to select how long to wait before waking up to drop speed.
    
    Change-Id: I270b3980667e2c70a68e5bff534124b4411dbad5
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: fix boosting logic
    
    35a84de cpufreq: interactive: apply above_hispeed_delay to each step above hispeed
    
    caused the speed choice logic to osciallate between boosting and not boosting.
    Add back code to ensure speed does not drop below boost frequency while
    boosting.
    
    Change-Id: Id420068480fcc7f5c4989ff523e2a8d22e2f4db2
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: fix racy timer stopping
    
    When stopping the governor, del_timer_sync() can race against an
    invocation of the idle notifier callback, which has the potential
    to reactivate the timer.
    
    To fix this issue, a read-write semaphore is used. Multiple readers are
    allowed as long as pcpu->governor_enabled is true.  However it can be
    moved to false only after taking a write semaphore which would wait for
    any on-going asynchronous activities to complete and prevent any more of
    those activities to be initiated.
    
    [toddpoynor@google.com: cosmetic and commit text changes]
    Change-Id: Ib51165a735d73dcf964a06754c48bdc1913e13d0
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    
    cpufreq: interactive: fix race on timer restart on governor start
    
    Starting the governor, or restarting on a hotplugged-in CPU, can race
    with the timer start in idle, triggering a BUG on timer already pending.
    Start the timer before setting the enable flag, and use enable_sem to
    protect the sequence (and ensure correct order of the update to the
    enable flag).  Delete any existing timer for safety.
    
    Change-Id: Ife77cf9fe099e8fd8543224cbf148c6722c2ffb0
    Reported-by: Francisco Franco <francisco.franco@cloudcar.com>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: default go_hispeed_load 99%, doc updates
    
    Update default go_hispeed_load from 85% to 99%.  Recent changes to the
    governor now use a default target_load of 90%.  go_hispeed_load should
    not be lower than the target load for hispeed_freq, which could lead
    to oscillating speed decisions.  Other recent changes reduce the need
    to dampen speed jumps on load spikes, while input event boosts from
    userspace are the preferred method for anticipating load spikes with
    UI impacts.
    
    General update to the documentation to reflect recent changes.
    
    Change-Id: I1b92f3091f42c04b10503cd1169a943b5dfd6faf
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: init default values at compile time
    
    Change-Id: Ia4966e949a6c24c34fdbd4a6e522cd7c37e4108e
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: don't handle transition notification if not enabled
    
    If multiple governors are in use then avoid processing frequency transition
    notifications for CPUs on which the interactive governor is not enabled.
    
    Change-Id: Ibd75255b921d887501a64774a8c4f62302f2d4e4
    Reported-by: Francisco Franco <francisco.franco@cloudcar.com>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: fix deadlock on spinlock in timer
    
    Need to use irqsave/restore spinlock calls to avoid a deadlock in calls
    from the timer.
    
    Change-Id: I15b6b590045ba1447e34ca7b5ff342723e53a605
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: fix race on governor start/stop
    
    There is race condition when both two cpu do CPUFREQ_GOV_STOP and one cpu
    do CPUFREQ_GOV_START soon. The sysfs_remove_group is not done yet on one
    cpu, but sysfs_create_group is called on another cpu, which cause governor
    start failed and then kernel panic in timer callback because the policy and
    cpu mask are all kfree in cpufreq driver.
    
    Replace atomic with mutex to lock the whole START/STOP sequence.
    
    Change-Id: I3762b3d44315ae021b8275aca84f5ea9147cc540
    Signed-off-by: Lianwei Wang <a22439@motorola.com>
    
    cpufreq: interactive: Enable CPU utilization statistics
    
     * Notify of CPU utilization on the speedchange task. This is required
       for MPDecision to properly consider hotplug scenarios.
    
    Change-Id: If01a2efb7a7630544b16c782989406ac44865f61
    
    cpufreq: interactive: allow arbitrary speed / delay mappings
    
    Accept a string of delays and speeds at which to apply the delay before
    raising each step above hispeed. For example, "80000 1300000:200000
    1500000:40000" means that the delay at or above 1GHz, until 1.3GHz is 80 msecs,
    the delay until 1.5GHz is 200 msecs and the delay at or above 1.5GHz is 40
    msecs when hispeed_freq is 1GHz.
    
    [toddpoynor@google.com: add documentation]
    Change-Id: Ifeebede8b1acbdd0a53e5c6916bccbf764dc854f
    Signed-off-by: Minsung Kim <ms925.kim@samsung.com>
    
    cpufreq: interactive: add io_is_busy interface
    
    Previously the idle time returned from get_cpu_idle_time_us included the
    iowait time. So the iowait time was always calculated as idle time.
    
    But now the idle time returned from get_cpu_idle_time_us does not include
    the iowait time anymore because of below commit which cause the iowait time
    always calculated as busy time:
        6beea0c nohz: Fix update_ts_time_stat idle accounting
    
    Add the io_is_busy interface, as does the ondemand governor, and let the user
    configure the iowait time as busy or idle through the io_is_busy sysfs
    interface.
    
    By default, io_is_busy is disabled.
    
    [toddpoynor@google.com: minor updates]
    Change-Id: If7d70ff864c43bc9c8d7fd7cfc66f930d339f9b4
    Signed-off-by: Lianwei Wang <lian-wei.wang@motorola.com>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: fix crash on error paths in get_tokenized_data
    
    Use separate variable for error code, free proper pointer.
    
    Change-Id: Ia83cccb195997789ac6afbf5b8761f7b278196d6
    Reported-by: Arve Hjønnevåg <arve@android.com>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    
    cpufreq: interactive: base above_hispeed_delay on target freq, not current
    
    Time to wait should be based on the intended target speed, not the
    actual speed (which may be held high by another CPU).
    
    Change-Id: Ifc5bb55d06adddb9a02af90af05398a78f282272
    Reported-by: Arve Hjønnevåg <arve@android.com>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

commit cfbfd5b6930a9d8eb1cacb4b5d6bc66ab82c2101
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Mar 12 00:44:19 2013 -0700

    Revert "cpufreq: fix interactive bug during hotplug."
    
    This reverts commit c5d82e843dc210898d67edc3362cfece50016c77.

commit 2a3c34adc9e73b5f682578f4135da96088f0ff65
Author: Aravind Venkateswaran <aravindh@codeaurora.org>
Date:   Tue Feb 26 17:57:01 2013 -0800

    msm_fb: hdmi: Resolution modes on HDMI
    
    Add a new header file that lists all supported
    HDMI resolution modes and the associated timing
    information.
    
    CRs-Fixed: 470335
    Change-Id: I971422ddb97e3b219cc682032eb67212b434daab
    Signed-off-by: Aravind Venkateswaran <aravindh@codeaurora.org>
    Signed-off-by: Manoj Rao <manojraj@codeaurora.org>

commit 86f659a0cfaae092ffecc0e08192e18111a4e26b
Author: Naseer Ahmed <naseer@codeaurora.org>
Date:   Thu Jan 17 19:08:46 2013 -0500

    msm_fb: display: update var in display commit
    
    Need the value of var in display commit to update the double buffers.
    If not, double buffering doesn't work in minui (eg. recovery and off-charger
    mode)
    
    Change-Id: I1fba52243cc781f18742131759245911d32c9b18
    Signed-off-by: Devin Kim <dojip.kim@lge.com>
    Signed-off-by: Naseer Ahmed <naseer@codeaurora.org>

commit 3f33da1d4db7c5bd735aaccea67cfab91d521cb8
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Mon Apr 8 18:56:58 2013 -0400

    msm: rotator: non-blocking rotate
    
    Send request to a work queue and return right away.
    Work queue will go through the commit queue to do the
    rotation. wait_for_finish can be set to true to make
    rotation go back to blocking.
    
    Change-Id: Ifc2e36bd24d9681d42105f4ccbb62a8777af2a6c
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit a4fcc918f3f5c690b4318f150549c3d5a0940b08
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Wed Feb 20 14:48:06 2013 -0500

    msm: rotator: sync point support
    
    Add MSM_ROTATOR_IOCTL_BUFFER_SYNC ioctl interface.
    Rotator will create a timeline for each session at START, wait for
    input fence and create released fence in this ioctl call,
    signal the timeline after rotation is done.
    
    Change-Id: I3738f8287d804ccd94e0a16ac0afb8b41b299c75
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit 8f93e780b795153809a7ad766c997f0c67e18995
Author: Ken Zhang <kenz@codeaurora.org>
Date:   Mon Dec 17 10:35:15 2012 -0500

    msm: display: maintain smmu for secure video
    
    Currently video maps and unmaps the secure memory.
    It could be too early to unmap it.
    Rotator and MDP will maintain this, only unmap it when
    no more needed. Memory managr has a refence count for it.
    
    Change-Id: I5ca7cb09c0ad4b9e39808e937c2f50264727730c
    Signed-off-by: Ken Zhang <kenz@codeaurora.org>

commit 7df3082815f60560078cc429dc1c8eb772a08664
Author: Kuogee Hsieh <khsieh@codeaurora.org>
Date:   Wed Mar 27 17:41:49 2013 -0700

    msm_fb: display: exit properly when rotator max session reached
    
    When rotator session reach maximum, it should exit without calling
    put_img since no get_img had been called.
    
    CRs-fixed: 466462
    Change-Id: I951570227ebd81da6dad19c1b897cde84f8d0451
    Signed-off-by: Kuogee Hsieh <khsieh@codeaurora.org>

commit 834092a904df841e97a48d6245bc1a0fee38854f
Author: Steve Kondik <shade@chemlab.org>
Date:   Tue Apr 30 20:07:23 2013 -0700

    Initial import of Samsung OSRC code drop for JF
